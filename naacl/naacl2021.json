[
    {
        "id": "2021.naacl-main.62",
        "title": "A Comparative Study on Schema-Guided Dialogue State Tracking",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Frame-based state representation is widely used in modern task-oriented dialog systems to model user intentions and slot values. However, a fixed design of domain ontology makes it difficult to extend to new services and APIs. Recent work proposed to use natural language descriptions to define the domain ontology instead of tag names for each intent or slot, thus offering a dynamic set of schema. In this paper, we conduct in-depth comparative studies to understand the use of natural language description for schema in dialog state tracking. Our discussion mainly covers three aspects: encoder architectures, impact of supplementary training, and effective schema description styles. We introduce a set of newly designed bench-marking descriptions and reveal the model robustness on both homogeneous and heterogeneous description styles in training and evaluation.",
        "author": "Jie Cao; Yi Zhang",
        "authorids": "/j/jie-cao/; /y/yi-zhang/",
        "bibtex": "@inproceedings{cao-zhang-2021-comparative,\n    title = \"A Comparative Study on Schema-Guided Dialogue State Tracking\",\n    author = \"Cao, Jie  and\n      Zhang, Yi\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.62/\",\n    doi = \"10.18653/v1/2021.naacl-main.62\",\n    pages = \"782--796\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.62.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.62/",
        "pdf_size": 1122713,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15796217763121055444&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "School of Computing, University of Utah; AWS AI, Amazon",
        "aff_domain": "cs.utah.edu;amazon.com",
        "email": "cs.utah.edu;amazon.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Utah;Amazon",
        "aff_unique_dep": "School of Computing;AWS AI",
        "aff_unique_url": "https://www.utah.edu;https://aws.amazon.com",
        "aff_unique_abbr": "U of U;AWS",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Utah;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.274",
        "title": "A Context-Dependent Gated Module for Incorporating Symbolic Semantics into Event Coreference Resolution",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Event coreference resolution is an important research problem with many applications. Despite the recent remarkable success of pre-trained language models, we argue that it is still highly beneficial to utilize symbolic features for the task. However, as the input for coreference resolution typically comes from upstream components in the information extraction pipeline, the automatically extracted symbolic features can be noisy and contain errors. Also, depending on the specific context, some features can be more informative than others. Motivated by these observations, we propose a novel context-dependent gated module to adaptively control the information flows from the input symbolic features. Combined with a simple noisy training method, our best models achieve state-of-the-art results on two datasets: ACE 2005 and KBP 2016.",
        "author": "Tuan Lai; Heng Ji; Trung Bui; Quan Hung Tran; Franck Dernoncourt; Walter Chang",
        "authorids": "/t/tuan-lai/; /h/heng-ji/; /t/trung-bui/; /q/quan-hung-tran/; /f/franck-dernoncourt/; /w/walter-chang/",
        "bibtex": "@inproceedings{lai-etal-2021-context,\n    title = \"A Context-Dependent Gated Module for Incorporating Symbolic Semantics into Event Coreference Resolution\",\n    author = \"Lai, Tuan  and\n      Ji, Heng  and\n      Bui, Trung  and\n      Tran, Quan Hung  and\n      Dernoncourt, Franck  and\n      Chang, Walter\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.274/\",\n    doi = \"10.18653/v1/2021.naacl-main.274\",\n    pages = \"3491--3499\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.274.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.274/",
        "pdf_size": 412203,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4598900267009071454&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; Adobe Research; Adobe Research; Adobe Research; Adobe Research",
        "aff_domain": "illinois.edu;illinois.edu;adobe.com;adobe.com;adobe.com;adobe.com",
        "email": "illinois.edu;illinois.edu;adobe.com;adobe.com;adobe.com;adobe.com",
        "github": "https://github.com/laituan245/eventcoref",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;1;1;1;1",
        "aff_unique_norm": "University of Illinois at Urbana-Champaign;Adobe",
        "aff_unique_dep": ";Adobe Research",
        "aff_unique_url": "https://illinois.edu;https://research.adobe.com",
        "aff_unique_abbr": "UIUC;Adobe",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Urbana-Champaign;",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.365",
        "title": "A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Readers of academic research papers often read with the goal of answering specific questions. Question Answering systems that can answer those questions can make consumption of the content much more efficient. However, building such tools requires data that reflect the difficulty of the task arising from complex reasoning about claims made in multiple parts of a paper. In contrast, existing information-seeking question answering datasets usually contain questions about generic factoid-type information. We therefore present Qasper, a dataset of 5049 questions over 1585 Natural Language Processing papers. Each question is written by an NLP practitioner who read only the title and abstract of the corresponding paper, and the question seeks information present in the full text. The questions are then answered by a separate set of NLP practitioners who also provide supporting evidence to answers. We find that existing models that do well on other QA tasks do not perform well on answering these questions, underperforming humans by at least 27 F1 points when answering them from entire papers, motivating further research in document-grounded, information-seeking QA, which our dataset is designed to facilitate.",
        "author": "Pradeep Dasigi; Kyle Lo; Iz Beltagy; Arman Cohan; Noah A. Smith; Matt Gardner",
        "authorids": "/p/pradeep-dasigi/; /k/kyle-lo/; /i/iz-beltagy/; /a/arman-cohan/; /n/noah-a-smith/; /m/matt-gardner/",
        "bibtex": "@inproceedings{dasigi-etal-2021-dataset,\n    title = \"A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers\",\n    author = \"Dasigi, Pradeep  and\n      Lo, Kyle  and\n      Beltagy, Iz  and\n      Cohan, Arman  and\n      Smith, Noah A.  and\n      Gardner, Matt\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.365/\",\n    doi = \"10.18653/v1/2021.naacl-main.365\",\n    pages = \"4599--4610\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.365.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.365/",
        "pdf_size": 524772,
        "gs_citation": 296,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6307051234121361325&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Allen Institute for AI; Allen Institute for AI + Paul G. Allen School of CSE, University of Washington; Allen Institute for AI; Allen Institute for AI; Allen Institute for AI + Paul G. Allen School of CSE, University of Washington; Allen Institute for AI",
        "aff_domain": "allenai.org;allenai.org;allenai.org;allenai.org;allenai.org;allenai.org",
        "email": "allenai.org;allenai.org;allenai.org;allenai.org;allenai.org;allenai.org",
        "github": "",
        "project": "https://allenai.org/project/qasper",
        "author_num": 6,
        "aff_unique_index": "0;0+1;0;0;0+1;0",
        "aff_unique_norm": "Allen Institute for AI;University of Washington",
        "aff_unique_dep": ";Paul G. Allen School of Computer Science & Engineering",
        "aff_unique_url": "https://allenai.org;https://www.cs.washington.edu",
        "aff_unique_abbr": "AI2;UW CSE",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Seattle",
        "aff_country_unique_index": "0;0+0;0;0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.415",
        "title": "A Deep Metric Learning Approach to Account Linking",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We consider the task of linking social media accounts that belong to the same author in an automated fashion on the basis of the content and meta-data of the corresponding document streams. We focus on learning an embedding that maps variable-sized samples of user activity\u2013ranging from single posts to entire months of activity\u2013to a vector space, where samples by the same author map to nearby points. Our approach does not require human-annotated data for training purposes, which allows us to leverage large amounts of social media content. The proposed model outperforms several competitive baselines under a novel evaluation framework modeled after established recognition benchmarks in other domains. Our method achieves high linking accuracy, even with small samples from accounts not seen at training time, a prerequisite for practical applications of the proposed linking framework.",
        "author": "Aleem Khan; Elizabeth Fleming; Noah Schofield; Marcus Bishop; Nicholas Andrews",
        "authorids": "/a/aleem-khan/; /e/elizabeth-fleming/; /n/noah-schofield/; /m/marcus-bishop/; /n/nicholas-andrews/",
        "bibtex": "@inproceedings{khan-etal-2021-deep,\n    title = \"A Deep Metric Learning Approach to Account Linking\",\n    author = \"Khan, Aleem  and\n      Fleming, Elizabeth  and\n      Schofield, Noah  and\n      Bishop, Marcus  and\n      Andrews, Nicholas\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.415/\",\n    doi = \"10.18653/v1/2021.naacl-main.415\",\n    pages = \"5275--5287\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.415.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.415/",
        "pdf_size": 423615,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4451176864751033806&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Human Language Technology Center of Excellence; Human Language Technology Center of Excellence; Human Language Technology Center of Excellence; Human Language Technology Center of Excellence; Human Language Technology Center of Excellence",
        "aff_domain": "jhu.edu;jhu.edu;jhu.edu;jhu.edu;jhu.edu",
        "email": "jhu.edu;jhu.edu;jhu.edu;jhu.edu;jhu.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Human Language Technology Center of Excellence",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.228",
        "title": "A Disentangled Adversarial Neural Topic Model for Separating Opinions from Plots in User Reviews",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The flexibility of the inference process in Variational Autoencoders (VAEs) has recently led to revising traditional probabilistic topic models giving rise to Neural Topic Models (NTM). Although these approaches have achieved significant results, surprisingly very little work has been done on how to disentangle the latent topics. Existing topic models when applied to reviews may extract topics associated with writers\u2019 subjective opinions mixed with those related to factual descriptions such as plot summaries in movie and book reviews. It is thus desirable to automatically separate opinion topics from plot/neutral ones enabling a better interpretability. In this paper, we propose a neural topic model combined with adversarial training to disentangle opinion topics from plot and neutral ones. We conduct an extensive experimental assessment introducing a new collection of movie and book reviews paired with their plots, namely MOBO dataset, showing an improved coherence and variety of topics, a consistent disentanglement rate, and sentiment classification performance superior to other supervised topic models.",
        "author": "Gabriele Pergola; Lin Gui; Yulan He",
        "authorids": "/g/gabriele-pergola/; /l/lin-gui/; /y/yulan-he/",
        "bibtex": "@inproceedings{pergola-etal-2021-disentangled,\n    title = \"A Disentangled Adversarial Neural Topic Model for Separating Opinions from Plots in User Reviews\",\n    author = \"Pergola, Gabriele  and\n      Gui, Lin  and\n      He, Yulan\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.228/\",\n    doi = \"10.18653/v1/2021.naacl-main.228\",\n    pages = \"2870--2883\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.228.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.228/",
        "pdf_size": 612628,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16571294069665476171&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science, University of Warwick, UK; Department of Computer Science, University of Warwick, UK; Department of Computer Science, University of Warwick, UK",
        "aff_domain": "warwick.ac.uk;warwick.ac.uk;warwick.ac.uk",
        "email": "warwick.ac.uk;warwick.ac.uk;warwick.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Warwick",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://warwick.ac.uk",
        "aff_unique_abbr": "Warwick",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2021.naacl-main.5",
        "title": "A Frustratingly Easy Approach for Entity and Relation Extraction",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "End-to-end relation extraction aims to identify named entities and extract relations between them. Most recent work models these two subtasks jointly, either by casting them in one structured prediction framework, or performing multi-task learning through shared representations. In this work, we present a simple pipelined approach for entity and relation extraction, and establish the new state-of-the-art on standard benchmarks (ACE04, ACE05 and SciERC), obtaining a 1.7%-2.8% absolute improvement in relation F1 over previous joint models with the same pre-trained encoders. Our approach essentially builds on two independent encoders and merely uses the entity model to construct the input for the relation model. Through a series of careful examinations, we validate the importance of learning distinct contextual representations for entities and relations, fusing entity information early in the relation model, and incorporating global context. Finally, we also present an efficient approximation to our approach which requires only one pass of both entity and relation encoders at inference time, achieving an 8-16\u00d7 speedup with a slight reduction in accuracy.",
        "author": "Zexuan Zhong; Danqi Chen",
        "authorids": "/z/zexuan-zhong/; /d/danqi-chen/",
        "bibtex": "@inproceedings{zhong-chen-2021-frustratingly,\n    title = \"A Frustratingly Easy Approach for Entity and Relation Extraction\",\n    author = \"Zhong, Zexuan  and\n      Chen, Danqi\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.5/\",\n    doi = \"10.18653/v1/2021.naacl-main.5\",\n    pages = \"50--61\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.5.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.5/",
        "pdf_size": 396740,
        "gs_citation": 603,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=892574543640232856&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science, Princeton University; Department of Computer Science, Princeton University",
        "aff_domain": "cs.princeton.edu;cs.princeton.edu",
        "email": "cs.princeton.edu;cs.princeton.edu",
        "github": "https://github.com/princeton-nlp/PURE",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Princeton University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.princeton.edu",
        "aff_unique_abbr": "Princeton",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.162",
        "title": "A Global Past-Future Early Exit Method for Accelerating Inference of Pre-trained Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Early exit mechanism aims to accelerate the inference speed of large-scale pre-trained language models. The essential idea is to exit early without passing through all the inference layers at the inference stage. To make accurate predictions for downstream tasks, the hierarchical linguistic information embedded in all layers should be jointly considered. However, much of the research up to now has been limited to use local representations of the exit layer. Such treatment inevitably loses information of the unused past layers as well as the high-level features embedded in future layers, leading to sub-optimal performance. To address this issue, we propose a novel Past-Future method to make comprehensive predictions from a global perspective. We first take into consideration all the linguistic information embedded in the past layers and then take a further step to engage the future information which is originally inaccessible for predictions. Extensive experiments demonstrate that our method outperforms previous early exit methods by a large margin, yielding better and robust performance.",
        "author": "Kaiyuan Liao; Yi Zhang; Xuancheng Ren; Qi Su; Xu Sun; Bin He",
        "authorids": "/k/kaiyuan-liao/; /y/yi-zhang/; /x/xuancheng-ren/; /q/qi-su/; /x/xu-sun/; /b/bin-he/",
        "bibtex": "@inproceedings{liao-etal-2021-global,\n    title = \"A Global Past-Future Early Exit Method for Accelerating Inference of Pre-trained Language Models\",\n    author = \"Liao, Kaiyuan  and\n      Zhang, Yi  and\n      Ren, Xuancheng  and\n      Su, Qi  and\n      Sun, Xu  and\n      He, Bin\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.162/\",\n    doi = \"10.18653/v1/2021.naacl-main.162\",\n    pages = \"2013--2023\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.162.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.162/",
        "pdf_size": 2140775,
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1710637264345776965&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Center for Data Science, Peking University+MOE Key Laboratory of Computational Linguistics, School of EECS, Peking University+School of Foreign Languages, Peking University; MOE Key Laboratory of Computational Linguistics, School of EECS, Peking University; MOE Key Laboratory of Computational Linguistics, School of EECS, Peking University; MOE Key Laboratory of Computational Linguistics, School of EECS, Peking University+School of Foreign Languages, Peking University; Center for Data Science, Peking University+MOE Key Laboratory of Computational Linguistics, School of EECS, Peking University; Huawei Noah\u2019s Ark Lab",
        "aff_domain": "pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn;huawei.com",
        "email": "pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn;huawei.com",
        "github": "https://github.com/lancopku/Early-Exit",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+0+0;0;0;0+0;0+0;1",
        "aff_unique_norm": "Peking University;Huawei",
        "aff_unique_dep": "Center for Data Science;Noah\u2019s Ark Lab",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.huawei.com",
        "aff_unique_abbr": "PKU;Huawei",
        "aff_campus_unique_index": "0;;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0+0+0;0;0;0+0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2021.naacl-industry.9",
        "title": "A Hybrid Approach to Scalable and Robust Spoken Language Understanding in Enterprise Virtual Agents",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Spoken language understanding (SLU) extracts the intended mean- ing from a user utterance and is a critical component of conversational virtual agents. In enterprise virtual agents (EVAs), language understanding is substantially challenging. First, the users are infrequent callers who are unfamiliar with the expectations of a pre-designed conversation flow. Second, the users are paying customers of an enterprise who demand a reliable, consistent and efficient user experience when resolving their issues. In this work, we describe a general and robust framework for intent and entity extraction utilizing a hybrid of statistical and rule-based approaches. Our framework includes confidence modeling that incorporates information from all components in the SLU pipeline, a critical addition for EVAs to en- sure accuracy. Our focus is on creating accurate and scalable SLU that can be deployed rapidly for a large class of EVA applications with little need for human intervention.",
        "author": "Ryan Price; Mahnoosh Mehrabani; Narendra Gupta; Yeon-Jun Kim; Shahab Jalalvand; Minhua Chen; Yanjie Zhao; Srinivas Bangalore",
        "authorids": "/r/ryan-price/; /m/mahnoosh-mehrabani/; /n/narendra-gupta/; /y/yeon-jun-kim/; /s/shahab-jalalvand/; /m/minhua-chen/; /y/yanjie-zhao/; /s/srinivas-bangalore/",
        "bibtex": "@inproceedings{price-etal-2021-hybrid,\n    title = \"A Hybrid Approach to Scalable and Robust Spoken Language Understanding in Enterprise Virtual Agents\",\n    author = \"Price, Ryan  and\n      Mehrabani, Mahnoosh  and\n      Gupta, Narendra  and\n      Kim, Yeon-Jun  and\n      Jalalvand, Shahab  and\n      Chen, Minhua  and\n      Zhao, Yanjie  and\n      Bangalore, Srinivas\",\n    editor = \"Kim, Young-bum  and\n      Li, Yunyao  and\n      Rambow, Owen\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-industry.9/\",\n    doi = \"10.18653/v1/2021.naacl-industry.9\",\n    pages = \"63--71\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-industry.9.pdf",
        "site": "https://aclanthology.org/2021.naacl-industry.9/",
        "pdf_size": 491306,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2461777882367387955&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": ";;;;;;;",
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "github": "",
        "project": "",
        "author_num": 8
    },
    {
        "id": "2021.naacl-main.21",
        "title": "A Million Tweets Are Worth a Few Points: Tuning Transformers for Customer Service Tasks",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In online domain-specific customer service applications, many companies struggle to deploy advanced NLP models successfully, due to the limited availability of and noise in their datasets. While prior research demonstrated the potential of migrating large open-domain pretrained models for domain-specific tasks, the appropriate (pre)training strategies have not yet been rigorously evaluated in such social media customer service settings, especially under multilingual conditions. We address this gap by collecting a multilingual social media corpus containing customer service conversations (865k tweets), comparing various pipelines of pretraining and finetuning approaches, applying them on 5 different end tasks. We show that pretraining a generic multilingual transformer model on our in-domain dataset, before finetuning on specific end tasks, consistently boosts performance, especially in non-English settings.",
        "author": "Amir Hadifar; Sofie Labat; Veronique Hoste; Chris Develder; Thomas Demeester",
        "authorids": "/a/amir-hadifar/; /s/sofie-labat/; /v/veronique-hoste/; /c/chris-develder/; /t/thomas-demeester/",
        "bibtex": "@inproceedings{hadifar-etal-2021-million,\n    title = \"A Million Tweets Are Worth a Few Points: Tuning Transformers for Customer Service Tasks\",\n    author = \"Hadifar, Amir  and\n      Labat, Sofie  and\n      Hoste, Veronique  and\n      Develder, Chris  and\n      Demeester, Thomas\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.21/\",\n    doi = \"10.18653/v1/2021.naacl-main.21\",\n    pages = \"220--225\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.21.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.21/",
        "pdf_size": 268447,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11819097566161789086&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "IDLab, Dept. of Information Technology, Ghent University - imec, Belgium; LT3, Dept. of Translation, Interpreting and Communication, Ghent University, Belgium; LT3, Dept. of Translation, Interpreting and Communication, Ghent University, Belgium; IDLab, Dept. of Information Technology, Ghent University - imec, Belgium; IDLab, Dept. of Information Technology, Ghent University - imec, Belgium",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "https://github.com/hadifar/customerservicetasks",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Ghent University",
        "aff_unique_dep": "Dept. of Information Technology",
        "aff_unique_url": "https://www.ugent.be",
        "aff_unique_abbr": "UGent",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Belgium"
    },
    {
        "id": "2021.naacl-main.110",
        "title": "A New Approach to Overgenerating and Scoring Abstractive Summaries",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We propose a new approach to generate multiple variants of the target summary with diverse content and varying lengths, then score and select admissible ones according to users\u2019 needs. Abstractive summarizers trained on single reference summaries may struggle to produce outputs that achieve multiple desirable properties, i.e., capturing the most important information, being faithful to the original, grammatical and fluent. In this paper, we propose a two-staged strategy to generate a diverse set of candidate summaries from the source text in stage one, then score and select admissible ones in stage two. Importantly, our generator gives a precise control over the length of the summary, which is especially well-suited when space is limited. Our selectors are designed to predict the optimal summary length and put special emphasis on faithfulness to the original text. Both stages can be effectively trained, optimized and evaluated. Our experiments on benchmark summarization datasets suggest that this paradigm can achieve state-of-the-art performance.",
        "author": "Kaiqiang Song; Bingqing Wang; Zhe Feng; Fei Liu",
        "authorids": "/k/kaiqiang-song/; /b/bingqing-wang/; /z/zhe-feng/; /f/fei-liu-utdallas/",
        "bibtex": "@inproceedings{song-etal-2021-new,\n    title = \"A New Approach to Overgenerating and Scoring Abstractive Summaries\",\n    author = \"Song, Kaiqiang  and\n      Wang, Bingqing  and\n      Feng, Zhe  and\n      Liu, Fei\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.110/\",\n    doi = \"10.18653/v1/2021.naacl-main.110\",\n    pages = \"1392--1404\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.110.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.110/",
        "pdf_size": 594638,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3454209821196072584&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "University of Central Florida, Orlando, FL; Robert Bosch LLC, Sunnyvale, CA; Robert Bosch LLC, Sunnyvale, CA; University of Central Florida, Orlando, FL",
        "aff_domain": "knights.ucf.edu;us.bosch.com;us.bosch.com;cs.ucf.edu",
        "email": "knights.ucf.edu;us.bosch.com;us.bosch.com;cs.ucf.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "University of Central Florida;Robert Bosch LLC",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ucf.edu;https://www.bosch.com",
        "aff_unique_abbr": "UCF;Bosch",
        "aff_campus_unique_index": "0;1;1;0",
        "aff_campus_unique": "Orlando;Sunnyvale",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.12",
        "title": "A Non-Linear Structural Probe",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Probes are models devised to investigate the encoding of knowledge\u2014e.g. syntactic structure\u2014in contextual representations. Probes are often designed for simplicity, which has led to restrictions on probe design that may not allow for the full exploitation of the structure of encoded information; one such restriction is linearity. We examine the case of a structural probe (Hewitt and Manning, 2019), which aims to investigate the encoding of syntactic structure in contextual representations through learning only linear transformations. By observing that the structural probe learns a metric, we are able to kernelize it and develop a novel non-linear variant with an identical number of parameters. We test on 6 languages and find that the radial-basis function (RBF) kernel, in conjunction with regularization, achieves a statistically significant improvement over the baseline in all languages\u2014implying that at least part of the syntactic knowledge is encoded non-linearly. We conclude by discussing how the RBF kernel resembles BERT\u2019s self-attention layers and speculate that this resemblance leads to the RBF-based probe\u2019s stronger performance.",
        "author": "Jennifer C. White; Tiago Pimentel; Naomi Saphra; Ryan Cotterell",
        "authorids": "/j/jennifer-c-white/; /t/tiago-pimentel/; /n/naomi-saphra/; /r/ryan-cotterell/",
        "bibtex": "@inproceedings{white-etal-2021-non,\n    title = \"A Non-Linear Structural Probe\",\n    author = \"White, Jennifer C.  and\n      Pimentel, Tiago  and\n      Saphra, Naomi  and\n      Cotterell, Ryan\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.12/\",\n    doi = \"10.18653/v1/2021.naacl-main.12\",\n    pages = \"132--138\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.12.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.12/",
        "pdf_size": 315271,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6426959827322964647&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "University of Cambridge; University of Cambridge; University of Edinburgh; ETH Z\u00fcrich",
        "aff_domain": "cam.ac.uk;cam.ac.uk;ed.ac.uk;inf.ethz.ch",
        "email": "cam.ac.uk;cam.ac.uk;ed.ac.uk;inf.ethz.ch",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;2",
        "aff_unique_norm": "University of Cambridge;University of Edinburgh;ETH Z\u00fcrich",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.cam.ac.uk;https://www.ed.ac.uk;https://www.ethz.ch",
        "aff_unique_abbr": "Cambridge;Edinburgh;ETHZ",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "United Kingdom;Switzerland"
    },
    {
        "id": "2021.naacl-main.26",
        "title": "A Simple Approach for Handling Out-of-Vocabulary Identifiers in Deep Learning for Source Code",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "There is an emerging interest in the application of natural language processing models to source code processing tasks. One of the major problems in applying deep learning to software engineering is that source code often contains a lot of rare identifiers, resulting in huge vocabularies. We propose a simple, yet effective method, based on identifier anonymization, to handle out-of-vocabulary (OOV) identifiers. Our method can be treated as a preprocessing step and, therefore, allows for easy implementation. We show that the proposed OOV anonymization method significantly improves the performance of the Transformer in two code processing tasks: code completion and bug fixing.",
        "author": "Nadezhda Chirkova; Sergey Troshin",
        "authorids": "/n/nadezhda-chirkova/; /s/sergey-troshin/",
        "bibtex": "@inproceedings{chirkova-troshin-2021-simple,\n    title = \"A Simple Approach for Handling Out-of-Vocabulary Identifiers in Deep Learning for Source Code\",\n    author = \"Chirkova, Nadezhda  and\n      Troshin, Sergey\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.26/\",\n    doi = \"10.18653/v1/2021.naacl-main.26\",\n    pages = \"278--288\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.26.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.26/",
        "pdf_size": 5360556,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8000723550275527455&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "HSE University; HSE University",
        "aff_domain": "hse.ru;hse.ru",
        "email": "hse.ru;hse.ru",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Higher School of Economics",
        "aff_unique_dep": "",
        "aff_unique_url": "https://hse.ru",
        "aff_unique_abbr": "HSE",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Russia"
    },
    {
        "id": "2021.naacl-main.392",
        "title": "A Simple and Efficient Multi-Task Learning Approach for Conditioned Dialogue Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Conditioned dialogue generation suffers from the scarcity of labeled responses. In this work, we exploit labeled non-dialogue text data related to the condition, which are much easier to collect. We propose a multi-task learning approach to leverage both labeled dialogue and text data. The 3 tasks jointly optimize the same pre-trained Transformer \u2013 conditioned dialogue generation task on the labeled dialogue data, conditioned language encoding task and conditioned language generation task on the labeled text data. Experimental results show that our approach outperforms the state-of-the-art models by leveraging the labeled texts, and it also obtains larger improvement in performance comparing to the previous methods to leverage text data.",
        "author": "Yan Zeng; Jian-Yun Nie",
        "authorids": "/y/yan-zeng/; /j/jian-yun-nie/",
        "bibtex": "@inproceedings{zeng-nie-2021-simple,\n    title = \"A Simple and Efficient Multi-Task Learning Approach for Conditioned Dialogue Generation\",\n    author = \"Zeng, Yan  and\n      Nie, Jian-Yun\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.392/\",\n    doi = \"10.18653/v1/2021.naacl-main.392\",\n    pages = \"4927--4939\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.392.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.392/",
        "pdf_size": 808697,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2284720651420955309&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "DIRO, Universit\u00e9 de Montr\u00e9al; DIRO, Universit\u00e9 de Montr\u00e9al",
        "aff_domain": "umontreal.ca;iro.umontreal.ca",
        "email": "umontreal.ca;iro.umontreal.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Universit\u00e9 de Montr\u00e9al",
        "aff_unique_dep": "DIRO",
        "aff_unique_url": "https://www.umontreal.ca",
        "aff_unique_abbr": "UdeM",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Montr\u00e9al",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2021.naacl-main.201",
        "title": "A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Deep neural networks and huge language models are becoming omnipresent in natural language applications. As they are known for requiring large amounts of training data, there is a growing body of work to improve the performance in low-resource settings. Motivated by the recent fundamental changes towards neural models and the popular pre-train and fine-tune paradigm, we survey promising approaches for low-resource natural language processing. After a discussion about the different dimensions of data availability, we give a structured overview of methods that enable learning when training data is sparse. This includes mechanisms to create additional labeled data like data augmentation and distant supervision as well as transfer learning settings that reduce the need for target supervision. A goal of our survey is to explain how these methods differ in their requirements as understanding them is essential for choosing a technique suited for a specific low-resource setting. Further key aspects of this work are to highlight open issues and to outline promising directions for future research.",
        "author": "Michael A. Hedderich; Lukas Lange; Heike Adel; Jannik Str\u00f6tgen; Dietrich Klakow",
        "authorids": "/m/michael-a-hedderich/; /l/lukas-lange/; /h/heike-adel/; /j/jannik-strotgen/; /d/dietrich-klakow/",
        "bibtex": "@inproceedings{hedderich-etal-2021-survey,\n    title = \"A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios\",\n    author = {Hedderich, Michael A.  and\n      Lange, Lukas  and\n      Adel, Heike  and\n      Str{\\\"o}tgen, Jannik  and\n      Klakow, Dietrich},\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.201/\",\n    doi = \"10.18653/v1/2021.naacl-main.201\",\n    pages = \"2545--2568\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.201.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.201/",
        "pdf_size": 1144550,
        "gs_citation": 393,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6254397823848803832&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Saarland University, Saarland Informatics Campus, Germany; Saarland University, Saarland Informatics Campus, Germany + Bosch Center for Arti\ufb01cial Intelligence, Germany; Bosch Center for Arti\ufb01cial Intelligence, Germany; Bosch Center for Arti\ufb01cial Intelligence, Germany; Saarland University, Saarland Informatics Campus, Germany",
        "aff_domain": "lsv.uni-saarland.de;de.bosch.com;de.bosch.com;de.bosch.com;lsv.uni-saarland.de",
        "email": "lsv.uni-saarland.de;de.bosch.com;de.bosch.com;de.bosch.com;lsv.uni-saarland.de",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0+1;1;1;0",
        "aff_unique_norm": "Saarland University;Bosch Center for Arti\ufb01cial Intelligence",
        "aff_unique_dep": ";Artificial Intelligence",
        "aff_unique_url": "https://www.uni-saarland.de;https://www.bosch-ai.com",
        "aff_unique_abbr": "UdS;BCAI",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Saarland Informatics Campus;",
        "aff_country_unique_index": "0;0+0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2021.naacl-main.144",
        "title": "A Unified Span-Based Approach for Opinion Mining with Syntactic Constituents",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Fine-grained opinion mining (OM) has achieved increasing attraction in the natural language processing (NLP) community, which aims to find the opinion structures of \u201cWho expressed what opinions towards what\u201d in one sentence. In this work, motivated by its span-based representations of opinion expressions and roles, we propose a unified span-based approach for the end-to-end OM setting. Furthermore, inspired by the unified span-based formalism of OM and constituent parsing, we explore two different methods (multi-task learning and graph convolutional neural network) to integrate syntactic constituents into the proposed model to help OM. We conduct experiments on the commonly used MPQA 2.0 dataset. The experimental results show that our proposed unified span-based approach achieves significant improvements over previous works in the exact F1 score and reduces the number of wrongly-predicted opinion expressions and roles, showing the effectiveness of our method. In addition, incorporating the syntactic constituents achieves promising improvements over the strong baseline enhanced by contextualized word representations.",
        "author": "Qingrong Xia; Bo Zhang; Rui Wang; Zhenghua Li; Yue Zhang; Fei Huang; Luo Si; Min Zhang",
        "authorids": "/q/qingrong-xia/; /b/bo-zhang/; /r/rui-wang/; /z/zhenghua-li/; /y/yue-zhang/; /f/fei-huang/; /l/luo-si/; /m/min-zhang/",
        "bibtex": "@inproceedings{xia-etal-2021-unified,\n    title = \"A Unified Span-Based Approach for Opinion Mining with Syntactic Constituents\",\n    author = \"Xia, Qingrong  and\n      Zhang, Bo  and\n      Wang, Rui  and\n      Li, Zhenghua  and\n      Zhang, Yue  and\n      Huang, Fei  and\n      Si, Luo  and\n      Zhang, Min\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.144/\",\n    doi = \"10.18653/v1/2021.naacl-main.144\",\n    pages = \"1795--1804\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.144.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.144/",
        "pdf_size": 533223,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5093016585498428842&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": ";;;;;;;",
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "github": "",
        "project": "",
        "author_num": 8
    },
    {
        "id": "2021.naacl-main.320",
        "title": "A recipe for annotating grounded clarifications",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In order to interpret the communicative intents of an utterance, it needs to be grounded in something that is outside of language; that is, grounded in world modalities. In this paper, we argue that dialogue clarification mechanisms make explicit the process of interpreting the communicative intents of the speaker\u2019s utterances by grounding them in the various modalities in which the dialogue is situated. This paper frames dialogue clarification mechanisms as an understudied research problem and a key missing piece in the giant jigsaw puzzle of natural language understanding. We discuss both the theoretical background and practical challenges posed by this problem and propose a recipe for obtaining grounding annotations. We conclude by highlighting ethical issues that need to be addressed in future work.",
        "author": "Luciana Benotti; Patrick Blackburn",
        "authorids": "/l/luciana-benotti/; /p/patrick-blackburn/",
        "bibtex": "@inproceedings{benotti-blackburn-2021-recipe,\n    title = \"A recipe for annotating grounded clarifications\",\n    author = \"Benotti, Luciana  and\n      Blackburn, Patrick\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.320/\",\n    doi = \"10.18653/v1/2021.naacl-main.320\",\n    pages = \"4065--4077\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.320.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.320/",
        "pdf_size": 454904,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9421059801837085724&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Universidad Nacional de C\u00f3rdoba+CONICET, Argentina; Philosophy and Science Studies+IKH, Roskilde University, Denmark",
        "aff_domain": "unc.edu.ar;ruc.dk",
        "email": "unc.edu.ar;ruc.dk",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;2+3",
        "aff_unique_norm": "Universidad Nacional de C\u00f3rdoba;CONICET;Philosophy and Science Studies;Roskilde University",
        "aff_unique_dep": ";;;IKH",
        "aff_unique_url": "https://www.unc.edu.ar;https://www.conicet.gov.ar;;https://www.ruc.dk",
        "aff_unique_abbr": "UNC;CONICET;;",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;2",
        "aff_country_unique": "Argentina;;Denmark"
    },
    {
        "id": "2021.naacl-main.443",
        "title": "AMR Parsing with Action-Pointer Transformer",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Abstract Meaning Representation parsing is a sentence-to-graph prediction task where target nodes are not explicitly aligned to sentence tokens. However, since graph nodes are semantically based on one or more sentence tokens, implicit alignments can be derived. Transition-based parsers operate over the sentence from left to right, capturing this inductive bias via alignments at the cost of limited expressiveness. In this work, we propose a transition-based system that combines hard-attention over sentences with a target-side action pointer mechanism to decouple source tokens from node representations and address alignments. We model the transitions as well as the pointer mechanism through straightforward modifications within a single Transformer architecture. Parser state and graph structure information are efficiently encoded using attention heads. We show that our action-pointer approach leads to increased expressiveness and attains large gains (+1.6 points) against the best transition-based AMR parser in very similar conditions. While using no graph re-categorization, our single model yields the second best Smatch score on AMR 2.0 (81.8), which is further improved to 83.4 with silver data and ensemble decoding.",
        "author": "Jiawei Zhou; Tahira Naseem; Ram\u00f3n Fernandez Astudillo; Radu Florian",
        "authorids": "/j/jiawei-zhou/; /t/tahira-naseem/; /r/ramon-fernandez-astudillo/; /r/radu-florian/",
        "bibtex": "@inproceedings{zhou-etal-2021-amr,\n    title = \"{AMR} Parsing with Action-Pointer Transformer\",\n    author = \"Zhou, Jiawei  and\n      Naseem, Tahira  and\n      Fernandez Astudillo, Ram{\\'o}n  and\n      Florian, Radu\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.443/\",\n    doi = \"10.18653/v1/2021.naacl-main.443\",\n    pages = \"5585--5598\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.443.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.443/",
        "pdf_size": 1390153,
        "gs_citation": 57,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5015802250472677095&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Harvard University; IBM Research; IBM Research; IBM Research",
        "aff_domain": "g.harvard.edu;us.ibm.com;ibm.com;us.ibm.com",
        "email": "g.harvard.edu;us.ibm.com;ibm.com;us.ibm.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "Harvard University;IBM",
        "aff_unique_dep": ";IBM Research",
        "aff_unique_url": "https://www.harvard.edu;https://www.ibm.com/research",
        "aff_unique_abbr": "Harvard;IBM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.36",
        "title": "APo-VAE: Text Generation in Hyperbolic Space",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Natural language often exhibits inherent hierarchical structure ingrained with complex syntax and semantics. However, most state-of-the-art deep generative models learn embeddings only in Euclidean vector space, without accounting for this structural property of language. In this paper, we investigate text generation in a hyperbolic latent space to learn continuous hierarchical representations. An Adversarial Poincare Variational Autoencoder (APo-VAE) is presented, where both the prior and variational posterior of latent variables are defined over a Poincare ball via wrapped normal distributions. By adopting the primal-dual formulation of Kullback-Leibler divergence, an adversarial learning procedure is introduced to empower robust model training. Extensive experiments in language modeling, unaligned style transfer, and dialog-response generation demonstrate the effectiveness of the proposed APo-VAE model over VAEs in Euclidean latent space, thanks to its superb capabilities in capturing latent language hierarchies in hyperbolic space.",
        "author": "Shuyang Dai; Zhe Gan; Yu Cheng; Chenyang Tao; Lawrence Carin; Jingjing Liu",
        "authorids": "/s/shuyang-dai/; /z/zhe-gan/; /y/yu-cheng/; /c/chenyang-tao/; /l/lawrence-carin/; /j/jingjing-liu/",
        "bibtex": "@inproceedings{dai-etal-2021-apo,\n    title = \"{AP}o-{VAE}: Text Generation in Hyperbolic Space\",\n    author = \"Dai, Shuyang  and\n      Gan, Zhe  and\n      Cheng, Yu  and\n      Tao, Chenyang  and\n      Carin, Lawrence  and\n      Liu, Jingjing\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.36/\",\n    doi = \"10.18653/v1/2021.naacl-main.36\",\n    pages = \"416--431\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.36.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.36/",
        "pdf_size": 3143257,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7809526320100721716&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Duke University; Microsoft Corporation; Microsoft Corporation; Duke University; Duke University; Microsoft Corporation",
        "aff_domain": "duke.edu;microsoft.com;microsoft.com;duke.edu;duke.edu;microsoft.com",
        "email": "duke.edu;microsoft.com;microsoft.com;duke.edu;duke.edu;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;1;0;0;1",
        "aff_unique_norm": "Duke University;Microsoft Corporation",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.duke.edu;https://www.microsoft.com",
        "aff_unique_abbr": "Duke;Microsoft",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.167",
        "title": "ASAP: A Chinese Review Dataset Towards Aspect Category Sentiment Analysis and Rating Prediction",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Sentiment analysis has attracted increasing attention in e-commerce. The sentiment polarities underlying user reviews are of great value for business intelligence. Aspect category sentiment analysis (ACSA) and review rating prediction (RP) are two essential tasks to detect the fine-to-coarse sentiment polarities. ACSA and RP are highly correlated and usually employed jointly in real-world e-commerce scenarios. While most public datasets are constructed for ACSA and RP separately, which may limit the further exploitation of both tasks. To address the problem and advance related researches, we present a large-scale Chinese restaurant review dataset ASAP including 46, 730 genuine reviews from a leading online-to-offline (O2O) e-commerce platform in China. Besides a 5-star scale rating, each review is manually annotated according to its sentiment polarities towards 18 pre-defined aspect categories. We hope the release of the dataset could shed some light on the field of sentiment analysis. Moreover, we propose an intuitive yet effective joint model for ACSA and RP. Experimental results demonstrate that the joint model outperforms state-of-the-art baselines on both tasks.",
        "author": "Jiahao Bu; Lei Ren; Shuang Zheng; Yang Yang; Jingang Wang; Fuzheng Zhang; Wei Wu",
        "authorids": "/j/jiahao-bu/; /l/lei-ren/; /s/shuang-zheng/; /y/yang-yang/; /j/jingang-wang/; /f/fuzheng-zhang/; /w/wei-wu/",
        "bibtex": "@inproceedings{bu-etal-2021-asap,\n    title = \"{ASAP}: A {C}hinese Review Dataset Towards Aspect Category Sentiment Analysis and Rating Prediction\",\n    author = \"Bu, Jiahao  and\n      Ren, Lei  and\n      Zheng, Shuang  and\n      Yang, Yang  and\n      Wang, Jingang  and\n      Zhang, Fuzheng  and\n      Wu, Wei\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.167/\",\n    doi = \"10.18653/v1/2021.naacl-main.167\",\n    pages = \"2069--2079\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.167.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.167/",
        "pdf_size": 3981988,
        "gs_citation": 81,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4393492656923979464&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Meituan, Beijing, China; Meituan, Beijing, China; Meituan, Beijing, China + School of Economics and Management, Dalian University of Technology, Dalian, China; Meituan, Beijing, China; Meituan, Beijing, China; Meituan, Beijing, China; Meituan, Beijing, China",
        "aff_domain": "meituan.com;meituan.com;meituan.com;meituan.com;meituan.com;meituan.com;meituan.com",
        "email": "meituan.com;meituan.com;meituan.com;meituan.com;meituan.com;meituan.com;meituan.com",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0+1;0;0;0;0",
        "aff_unique_norm": "Meituan;Dalian University of Technology",
        "aff_unique_dep": ";School of Economics and Management",
        "aff_unique_url": "https://www.meituan.com;http://en.dlut.edu.cn/",
        "aff_unique_abbr": "Meituan;DUT",
        "aff_campus_unique_index": "0;0;0+1;0;0;0;0",
        "aff_campus_unique": "Beijing;Dalian",
        "aff_country_unique_index": "0;0;0+0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2021.naacl-main.412",
        "title": "AVA: an Automatic eValuation Approach for Question Answering Systems",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We introduce AVA, an automatic evaluation approach for Question Answering, which given a set of questions associated with Gold Standard answers (references), can estimate system Accuracy. AVA uses Transformer-based language models to encode question, answer, and reference texts. This allows for effectively assessing answer correctness using similarity between the reference and an automatic answer, biased towards the question semantics. To design, train, and test AVA, we built multiple large training, development, and test sets on public and industrial benchmarks. Our innovative solutions achieve up to 74.7% F1 score in predicting human judgment for single answers. Additionally, AVA can be used to evaluate the overall system Accuracy with an error lower than 7% at 95% of confidence when measured on several QA systems.",
        "author": "Thuy Vu; Alessandro Moschitti",
        "authorids": "/t/thuy-vu/; /a/alessandro-moschitti/",
        "bibtex": "@inproceedings{vu-moschitti-2021-ava,\n    title = \"{AVA}: an Automatic e{V}aluation Approach for Question Answering Systems\",\n    author = \"Vu, Thuy  and\n      Moschitti, Alessandro\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.412/\",\n    doi = \"10.18653/v1/2021.naacl-main.412\",\n    pages = \"5223--5233\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.412.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.412/",
        "pdf_size": 525136,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3172844472661618784&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Amazon Alexa AI, Manhattan Beach, CA, USA; Amazon Alexa AI, Manhattan Beach, CA, USA",
        "aff_domain": "amazon.com;amazon.com",
        "email": "amazon.com;amazon.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Amazon Alexa AI",
        "aff_unique_dep": "AI",
        "aff_unique_url": "https://www.amazon.com/alexa",
        "aff_unique_abbr": "Amazon Alexa AI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Manhattan Beach",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.353",
        "title": "Ab Antiquo: Neural Proto-language Reconstruction",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Historical linguists have identified regularities in the process of historic sound change. The comparative method utilizes those regularities to reconstruct proto-words based on observed forms in daughter languages. Can this process be efficiently automated? We address the task of proto-word reconstruction, in which the model is exposed to cognates in contemporary daughter languages, and has to predict the proto word in the ancestor language. We provide a novel dataset for this task, encompassing over 8,000 comparative entries, and show that neural sequence models outperform conventional methods applied to this task so far. Error analysis reveals a variability in the ability of neural model to capture different phonological changes, correlating with the complexity of the changes. Analysis of learned embeddings reveals the models learn phonologically meaningful generalizations, corresponding to well-attested phonological shifts documented by historical linguistics.",
        "author": "Carlo Meloni; Shauli Ravfogel; Yoav Goldberg",
        "authorids": "/c/carlo-meloni/; /s/shauli-ravfogel/; /y/yoav-goldberg/",
        "bibtex": "@inproceedings{meloni-etal-2021-ab,\n    title = \"Ab Antiquo: Neural Proto-language Reconstruction\",\n    author = \"Meloni, Carlo  and\n      Ravfogel, Shauli  and\n      Goldberg, Yoav\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.353/\",\n    doi = \"10.18653/v1/2021.naacl-main.353\",\n    pages = \"4460--4473\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.353.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.353/",
        "pdf_size": 828166,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1480508838517388014&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Comparative Language Science, University of Zurich; Computer Science Department, Bar Ilan University + Allen Institute for Arti\ufb01cial Intelligence; Computer Science Department, Bar Ilan University + Allen Institute for Arti\ufb01cial Intelligence",
        "aff_domain": "mail.tau.ac.il;gmail.com;gmail.com",
        "email": "mail.tau.ac.il;gmail.com;gmail.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1+2;1+2",
        "aff_unique_norm": "University of Zurich;Bar Ilan University;Allen Institute for Artificial Intelligence",
        "aff_unique_dep": "Department of Comparative Language Science;Computer Science Department;Artificial Intelligence",
        "aff_unique_url": "https://www.unizh.ch;https://www.biu.ac.il;https://allenai.org",
        "aff_unique_abbr": "UZH;BIU;AI2",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1+2;1+2",
        "aff_country_unique": "Switzerland;Israel;United States"
    },
    {
        "id": "2021.naacl-main.4",
        "title": "Abstract Meaning Representation Guided Graph Encoding and Decoding for Joint Information Extraction",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The tasks of Rich Semantic Parsing, such as Abstract Meaning Representation (AMR), share similar goals with Information Extraction (IE) to convert natural language texts into structured semantic representations. To take advantage of such similarity, we propose a novel AMR-guided framework for joint information extraction to discover entities, relations, and events with the help of a pre-trained AMR parser. Our framework consists of two novel components: 1) an AMR based semantic graph aggregator to let the candidate entity and event trigger nodes collect neighborhood information from AMR graph for passing message among related knowledge elements; 2) an AMR guided graph decoder to extract knowledge elements based on the order decided by the hierarchical structures in AMR. Experiments on multiple datasets have shown that the AMR graph encoder and decoder have provided significant gains and our approach has achieved new state-of-the-art performance on all IE subtasks.",
        "author": "Zixuan Zhang; Heng Ji",
        "authorids": "/z/zixuan-zhang/; /h/heng-ji/",
        "bibtex": "@inproceedings{zhang-ji-2021-abstract,\n    title = \"{A}bstract {M}eaning {R}epresentation Guided Graph Encoding and Decoding for Joint Information Extraction\",\n    author = \"Zhang, Zixuan  and\n      Ji, Heng\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.4/\",\n    doi = \"10.18653/v1/2021.naacl-main.4\",\n    pages = \"39--49\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.4.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.4/",
        "pdf_size": 1170222,
        "gs_citation": 97,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11104484986789674588&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Computer Science Department, University of Illinois at Urbana-Champaign; Computer Science Department, University of Illinois at Urbana-Champaign",
        "aff_domain": "illinois.edu;illinois.edu",
        "email": "illinois.edu;illinois.edu",
        "github": "https://github.com/zhangzx-uiuc/AMR-IE",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Illinois at Urbana-Champaign",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://illinois.edu",
        "aff_unique_abbr": "UIUC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Urbana-Champaign",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.239",
        "title": "Action-Based Conversations Dataset: A Corpus for Building More In-Depth Task-Oriented Dialogue Systems",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Existing goal-oriented dialogue datasets focus mainly on identifying slots and values. However, customer support interactions in reality often involve agents following multi-step procedures derived from explicitly-defined company policies as well. To study customer service dialogue systems in more realistic settings, we introduce the Action-Based Conversations Dataset (ABCD), a fully-labeled dataset with over 10K human-to-human dialogues containing 55 distinct user intents requiring unique sequences of actions constrained by policies to achieve task success. We propose two additional dialog tasks, Action State Tracking and Cascading Dialogue Success, and establish a series of baselines involving large-scale, pre-trained language models on this dataset. Empirical results demonstrate that while more sophisticated networks outperform simpler models, a considerable gap (50.8% absolute accuracy) still exists to reach human-level performance on ABCD.",
        "author": "Derek Chen; Howard Chen; Yi Yang; Alexander Lin; Zhou Yu",
        "authorids": "/d/derek-chen/; /h/howard-chen/; /y/yi-yang/; /a/alexander-lin/; /z/zhou-yu/",
        "bibtex": "@inproceedings{chen-etal-2021-action,\n    title = \"Action-Based Conversations Dataset: A Corpus for Building More In-Depth Task-Oriented Dialogue Systems\",\n    author = \"Chen, Derek  and\n      Chen, Howard  and\n      Yang, Yi  and\n      Lin, Alexander  and\n      Yu, Zhou\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.239/\",\n    doi = \"10.18653/v1/2021.naacl-main.239\",\n    pages = \"3002--3017\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.239.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.239/",
        "pdf_size": 1437635,
        "gs_citation": 76,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14873553613643149112&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "ASAPP, New York, NY 10007; ASAPP, New York, NY 10007; ASAPP, New York, NY 10007; ASAPP, New York, NY 10007; Columbia University, NY",
        "aff_domain": "asapp.com;asapp.com;asapp.com;asapp.com;columbia.edu",
        "email": "asapp.com;asapp.com;asapp.com;asapp.com;columbia.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;1",
        "aff_unique_norm": "ASAPP;Columbia University",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://www.columbia.edu",
        "aff_unique_abbr": ";Columbia",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";New York",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.159",
        "title": "Active2 Learning: Actively reducing redundancies in Active Learning methods for Sequence Tagging and Machine Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "While deep learning is a powerful tool for natural language processing (NLP) problems, successful solutions to these problems rely heavily on large amounts of annotated samples. However, manually annotating data is expensive and time-consuming. Active Learning (AL) strategies reduce the need for huge volumes of labeled data by iteratively selecting a small number of examples for manual annotation based on their estimated utility in training the given model. In this paper, we argue that since AL strategies choose examples independently, they may potentially select similar examples, all of which may not contribute significantly to the learning process. Our proposed approach, Active2 Learning (A2L), actively adapts to the deep learning model being trained to eliminate such redundant examples chosen by an AL strategy. We show that A2L is widely applicable by using it in conjunction with several different AL strategies and NLP tasks. We empirically demonstrate that the proposed approach is further able to reduce the data requirements of state-of-the-art AL strategies by \u2248 3-25% on an absolute scale on multiple NLP tasks while achieving the same performance with virtually no additional computation overhead.",
        "author": "Rishi Hazra; Parag Dutta; Shubham Gupta; Mohammed Abdul Qaathir; Ambedkar Dukkipati",
        "authorids": "/r/rishi-hazra/; /p/parag-dutta/; /s/shubham-gupta/; /m/mohammed-abdul-qaathir/; /a/ambedkar-dukkipati/",
        "bibtex": "@inproceedings{hazra-etal-2021-active2,\n    title = \"Active$^2$ Learning: Actively reducing redundancies in Active Learning methods for Sequence Tagging and Machine Translation\",\n    author = \"Hazra, Rishi  and\n      Dutta, Parag  and\n      Gupta, Shubham  and\n      Qaathir, Mohammed Abdul  and\n      Dukkipati, Ambedkar\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.159/\",\n    doi = \"10.18653/v1/2021.naacl-main.159\",\n    pages = \"1982--1995\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.159.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.159/",
        "pdf_size": 4876091,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18222958916845749055&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 9,
        "aff": "Indian Institute of Science, Bangalore; Indian Institute of Science, Bangalore; Indian Institute of Science, Bangalore; Indian Institute of Science, Bangalore; Indian Institute of Science, Bangalore",
        "aff_domain": "iisc.ac.in;iisc.ac.in;iisc.ac.in;iisc.ac.in;iisc.ac.in",
        "email": "iisc.ac.in;iisc.ac.in;iisc.ac.in;iisc.ac.in;iisc.ac.in",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Indian Institute of Science",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iisc.ac.in",
        "aff_unique_abbr": "IISc",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Bangalore",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2021.naacl-demos.12",
        "title": "ActiveAnno: General-Purpose Document-Level Annotation Tool with Active Learning Integration",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "ActiveAnno is an annotation tool focused on document-level annotation tasks developed both for industry and research settings. It is designed to be a general-purpose tool with a wide variety of use cases. It features a modern and responsive web UI for creating annotation projects, conducting annotations, adjudicating disagreements, and analyzing annotation results. ActiveAnno embeds a highly configurable and interactive user interface. The tool also integrates a RESTful API that enables integration into other software systems, including an API for machine learning integration. ActiveAnno is built with extensible design and easy deployment in mind, all to enable users to perform annotation tasks with high efficiency and high-quality annotation results.",
        "author": "Max Wiechmann; Seid Muhie Yimam; Chris Biemann",
        "authorids": "/m/max-wiechmann/; /s/seid-muhie-yimam/; /c/chris-biemann/",
        "bibtex": "@inproceedings{wiechmann-etal-2021-activeanno,\n    title = \"{A}ctive{A}nno: General-Purpose Document-Level Annotation Tool with Active Learning Integration\",\n    author = \"Wiechmann, Max  and\n      Yimam, Seid Muhie  and\n      Biemann, Chris\",\n    editor = \"Sil, Avi  and\n      Lin, Xi Victoria\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-demos.12/\",\n    doi = \"10.18653/v1/2021.naacl-demos.12\",\n    pages = \"99--105\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-demos.12.pdf",
        "site": "https://aclanthology.org/2021.naacl-demos.12/",
        "pdf_size": 798420,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15457786230017137614&as_sdt=40005&sciodt=0,10&hl=en",
        "gs_version_total": 5,
        "aff": "Language Technology group, Universit\u00e4t Hamburg, Germany; Language Technology group, Universit\u00e4t Hamburg, Germany; Language Technology group, Universit\u00e4t Hamburg, Germany",
        "aff_domain": "maxwiechmann.de;informatik.uni-hamburg.de;informatik.uni-hamburg.de",
        "email": "maxwiechmann.de;informatik.uni-hamburg.de;informatik.uni-hamburg.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Universit\u00e4t Hamburg",
        "aff_unique_dep": "Language Technology group",
        "aff_unique_url": "https://www.uni-hamburg.de",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2021.naacl-industry.33",
        "title": "Ad Headline Generation using Self-Critical Masked Language Model",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "For any E-commerce website it is a nontrivial problem to build enduring advertisements that attract shoppers. It is hard to pass the creative quality bar of the website, especially at a large scale. We thus propose a programmatic solution to generate product advertising headlines using retail content. We propose a state of the art application of Reinforcement Learning (RL) Policy gradient methods on Transformer (Vaswani et al., 2017) based Masked Language Models (Devlin et al., 2019). Our method creates the advertising headline by jointly conditioning on multiple products that a seller wishes to advertise. We demonstrate that our method outperforms existing Transformer and LSTM + RL methods in overlap metrics and quality audits. We also show that our model generated headlines outperform human submitted headlines in terms of both grammar and creative quality as determined by audits.",
        "author": "Yashal Shakti Kanungo; Sumit Negi; Aruna Rajan",
        "authorids": "/y/yashal-shakti-kanungo/; /s/sumit-negi/; /a/aruna-rajan/",
        "bibtex": "@inproceedings{kanungo-etal-2021-ad,\n    title = \"Ad Headline Generation using Self-Critical Masked Language Model\",\n    author = \"Kanungo, Yashal Shakti  and\n      Negi, Sumit  and\n      Rajan, Aruna\",\n    editor = \"Kim, Young-bum  and\n      Li, Yunyao  and\n      Rambow, Owen\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-industry.33/\",\n    doi = \"10.18653/v1/2021.naacl-industry.33\",\n    pages = \"263--271\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-industry.33.pdf",
        "site": "https://aclanthology.org/2021.naacl-industry.33/",
        "pdf_size": 1813512,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14263084914333212743&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Amazon; Amazon; Amazon",
        "aff_domain": "amazon.com;amazon.com;amazon.com",
        "email": "amazon.com;amazon.com;amazon.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Amazon.com, Inc.",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.amazon.com",
        "aff_unique_abbr": "Amazon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.471",
        "title": "AdaptSum: Towards Low-Resource Domain Adaptation for Abstractive Summarization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "State-of-the-art abstractive summarization models generally rely on extensive labeled data, which lowers their generalization ability on domains where such data are not available. In this paper, we present a study of domain adaptation for the abstractive summarization task across six diverse target domains in a low-resource setting. Specifically, we investigate the second phase of pre-training on large-scale generative models under three different settings: 1) source domain pre-training; 2) domain-adaptive pre-training; and 3) task-adaptive pre-training. Experiments show that the effectiveness of pre-training is correlated with the similarity between the pre-training data and the target domain task. Moreover, we find that continuing pre-training could lead to the pre-trained model\u2019s catastrophic forgetting, and a learning method with less forgetting can alleviate this issue. Furthermore, results illustrate that a huge gap still exists between the low-resource and high-resource settings, which highlights the need for more advanced domain adaptation methods for the abstractive summarization task.",
        "author": "Tiezheng Yu; Zihan Liu; Pascale Fung",
        "authorids": "/t/tiezheng-yu/; /z/zihan-liu/; /p/pascale-fung/",
        "bibtex": "@inproceedings{yu-etal-2021-adaptsum,\n    title = \"{A}dapt{S}um: Towards Low-Resource Domain Adaptation for Abstractive Summarization\",\n    author = \"Yu, Tiezheng  and\n      Liu, Zihan  and\n      Fung, Pascale\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.471/\",\n    doi = \"10.18653/v1/2021.naacl-main.471\",\n    pages = \"5892--5904\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.471.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.471/",
        "pdf_size": 388956,
        "gs_citation": 99,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7699495858640479574&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Center for Arti\ufb01cial Intelligence Research (CAiRE) + Department of Electronic and Computer Engineering + The Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong; Center for Arti\ufb01cial Intelligence Research (CAiRE) + Department of Electronic and Computer Engineering + The Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong; Center for Arti\ufb01cial Intelligence Research (CAiRE) + Department of Electronic and Computer Engineering + The Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong",
        "aff_domain": "connect.ust.hk;connect.ust.hk;ece.ust.hk",
        "email": "connect.ust.hk;connect.ust.hk;ece.ust.hk",
        "github": "https://github.com/TysonYu/AdaptSum",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1+2;0+1+2;0+1+2",
        "aff_unique_norm": "Center for Arti\ufb01cial Intelligence Research;University of California, Santa Barbara;The Hong Kong University of Science and Technology",
        "aff_unique_dep": "Arti\ufb01cial Intelligence Research;Department of Electronic and Computer Engineering;",
        "aff_unique_url": ";https://www.ece.ucsb.edu;https://www.ust.hk",
        "aff_unique_abbr": "CAiRE;UCSB ECE;HKUST",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "1+2;1+2;1+2",
        "aff_country_unique": ";United States;China"
    },
    {
        "id": "2021.naacl-main.288",
        "title": "Adaptable and Interpretable Neural MemoryOver Symbolic Knowledge",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Past research has demonstrated that large neural language models (LMs) encode surprising amounts of factual information: however, augmenting or modifying this information requires modifying a corpus and retraining, which is computationally expensive. To address this problem, we develop a neural LM that includes an interpretable neuro-symbolic KB in the form of a \u201cfact memory\u201d. Each element of the fact memory is formed from a triple of vectors, where each vector corresponds to a KB entity or relation. Our LM improves performance on knowledge-intensive question-answering tasks, sometimes dramatically, including a 27 point increase in one setting of WebQuestionsSP over a state-of-the-art open-book model, despite using 5% of the parameters. Most interestingly, we demonstrate that the model can be modified, without any re-training, by updating the fact memory.",
        "author": "Pat Verga; Haitian Sun; Livio Baldini Soares; William Cohen",
        "authorids": "/p/pat-verga/; /h/haitian-sun/; /l/livio-baldini-soares/; /w/william-cohen/",
        "bibtex": "@inproceedings{verga-etal-2021-adaptable,\n    title = \"Adaptable and Interpretable Neural {M}emory{O}ver Symbolic Knowledge\",\n    author = \"Verga, Pat  and\n      Sun, Haitian  and\n      Baldini Soares, Livio  and\n      Cohen, William\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.288/\",\n    doi = \"10.18653/v1/2021.naacl-main.288\",\n    pages = \"3678--3691\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.288.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.288/",
        "pdf_size": 590610,
        "gs_citation": 72,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7342941987849281333&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Google Research; Google Research; Google Research; Google Research",
        "aff_domain": "google.com;google.com;google.com;google.com",
        "email": "google.com;google.com;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google Research",
        "aff_unique_url": "https://research.google",
        "aff_unique_abbr": "Google Research",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Mountain View",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.378",
        "title": "Adapting BERT for Continual Learning of a Sequence of Aspect Sentiment Classification Tasks",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "This paper studies continual learning (CL) of a sequence of aspect sentiment classification (ASC) tasks. Although some CL techniques have been proposed for document sentiment classification, we are not aware of any CL work on ASC. A CL system that incrementally learns a sequence of ASC tasks should address the following two issues: (1) transfer knowledge learned from previous tasks to the new task to help it learn a better model, and (2) maintain the performance of the models for previous tasks so that they are not forgotten. This paper proposes a novel capsule network based model called B-CL to address these issues. B-CL markedly improves the ASC performance on both the new task and the old tasks via forward and backward knowledge transfer. The effectiveness of B-CL is demonstrated through extensive experiments.",
        "author": "Zixuan Ke; Hu Xu; Bing Liu",
        "authorids": "/z/zixuan-ke/; /h/hu-xu/; /b/bing-liu/",
        "bibtex": "@inproceedings{ke-etal-2021-adapting,\n    title = \"Adapting {BERT} for Continual Learning of a Sequence of Aspect Sentiment Classification Tasks\",\n    author = \"Ke, Zixuan  and\n      Xu, Hu  and\n      Liu, Bing\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.378/\",\n    doi = \"10.18653/v1/2021.naacl-main.378\",\n    pages = \"4746--4755\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.378.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.378/",
        "pdf_size": 636720,
        "gs_citation": 86,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7195228133281844015&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science, University of Illinois at Chicago; Facebook AI Research; Department of Computer Science, University of Illinois at Chicago",
        "aff_domain": "uic.edu;fb.com;uic.edu",
        "email": "uic.edu;fb.com;uic.edu",
        "github": "https://github.com/ZixuanKe/PyContinual",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Illinois at Chicago;Facebook",
        "aff_unique_dep": "Department of Computer Science;Facebook AI Research",
        "aff_unique_url": "https://www.uic.edu;https://research.facebook.com",
        "aff_unique_abbr": "UIC;FAIR",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Chicago;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.361",
        "title": "Adapting Coreference Resolution for Processing Violent Death Narratives",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Coreference resolution is an important compo-nent in analyzing narrative text from admin-istrative data (e.g., clinical or police sources).However, existing coreference models trainedon general language corpora suffer from poortransferability due to domain gaps, especiallywhen they are applied to gender-inclusive datawith lesbian, gay, bisexual, and transgender(LGBT) individuals. In this paper, we an-alyzed the challenges of coreference resolu-tion in an exemplary form of administrativetext written in English: violent death nar-ratives from the USA\u2019s Centers for DiseaseControl\u2019s (CDC) National Violent Death Re-porting System. We developed a set of dataaugmentation rules to improve model perfor-mance using a probabilistic data programmingframework. Experiments on narratives froman administrative database, as well as existinggender-inclusive coreference datasets, demon-strate the effectiveness of data augmentationin training coreference models that can betterhandle text data about LGBT individuals.",
        "author": "Ankith Uppunda; Susan Cochran; Jacob Foster; Alina Arseniev-Koehler; Vickie Mays; Kai-Wei Chang",
        "authorids": "/a/ankith-uppunda/; /s/susan-cochran/; /j/jacob-foster/; /a/alina-arseniev-koehler/; /v/vickie-mays/; /k/kai-wei-chang/",
        "bibtex": "@inproceedings{uppunda-etal-2021-adapting,\n    title = \"Adapting Coreference Resolution for Processing Violent Death Narratives\",\n    author = \"Uppunda, Ankith  and\n      Cochran, Susan  and\n      Foster, Jacob  and\n      Arseniev-Koehler, Alina  and\n      Mays, Vickie  and\n      Chang, Kai-Wei\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.361/\",\n    doi = \"10.18653/v1/2021.naacl-main.361\",\n    pages = \"4553--4559\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.361.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.361/",
        "pdf_size": 274684,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12255790387603379263&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "University of California, Los Angeles; University of California, Los Angeles; University of California, Los Angeles; University of California, Los Angeles; University of California, Los Angeles; University of California, Los Angeles",
        "aff_domain": "ucla.edu;ucla.edu;ucla.edu;ucla.edu;ucla.edu;cs.ucla.edu",
        "email": "ucla.edu;ucla.edu;ucla.edu;ucla.edu;ucla.edu;cs.ucla.edu",
        "github": "",
        "project": "https://www.thetrevorproject.org/resources/preventing-suicide/facts-about-suicide/",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "University of California, Los Angeles",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ucla.edu",
        "aff_unique_abbr": "UCLA",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.124",
        "title": "Adding Chit-Chat to Enhance Task-Oriented Dialogues",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Existing dialogue corpora and models are typically designed under two disjoint motives: while task-oriented systems focus on achieving functional goals (e.g., booking hotels), open-domain chatbots aim at making socially engaging conversations. In this work, we propose to integrate both types of systems by Adding Chit-Chat to ENhance Task-ORiented dialogues (ACCENTOR), with the goal of making virtual assistant conversations more engaging and interactive. Specifically, we propose a Human <-> AI collaborative data collection approach for generating diverse chit-chat responses to augment task-oriented dialogues with minimal annotation effort. We then present our new chit-chat-based annotations to 23.8K dialogues from two popular task-oriented datasets (Schema-Guided Dialogue and MultiWOZ 2.1) and demonstrate their advantage over the originals via human evaluation. Lastly, we propose three new models for adding chit-chat to task-oriented dialogues, explicitly trained to predict user goals and to generate contextually relevant chit-chat responses. Automatic and human evaluations show that, compared with the state-of-the-art task-oriented baseline, our models can code-switch between task and chit-chat to be more engaging, interesting, knowledgeable, and humanlike, while maintaining competitive task performance.",
        "author": "Kai Sun; Seungwhan Moon; Paul Crook; Stephen Roller; Becka Silvert; Bing Liu; Zhiguang Wang; Honglei Liu; Eunjoon Cho; Claire Cardie",
        "authorids": "/k/kai-sun/; /s/seungwhan-moon/; /p/paul-a-crook/; /s/stephen-roller/; /b/becka-silvert/; /b/bing-liu/; /z/zhiguang-wang/; /h/honglei-liu/; /e/eunjoon-cho/; /c/claire-cardie/",
        "bibtex": "@inproceedings{sun-etal-2021-adding,\n    title = \"Adding Chit-Chat to Enhance Task-Oriented Dialogues\",\n    author = \"Sun, Kai  and\n      Moon, Seungwhan  and\n      Crook, Paul  and\n      Roller, Stephen  and\n      Silvert, Becka  and\n      Liu, Bing  and\n      Wang, Zhiguang  and\n      Liu, Honglei  and\n      Cho, Eunjoon  and\n      Cardie, Claire\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.124/\",\n    doi = \"10.18653/v1/2021.naacl-main.124\",\n    pages = \"1570--1583\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.124.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.124/",
        "pdf_size": 605981,
        "gs_citation": 89,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13058518855620010553&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Cornell University; Facebook; Facebook; Facebook AI Research; Facebook; Facebook; Facebook; Facebook; Facebook; Cornell University",
        "aff_domain": "cornell.edu;fb.com; ; ; ; ; ; ; ; ",
        "email": "cornell.edu;fb.com; ; ; ; ; ; ; ; ",
        "github": "https://github.com/facebookresearch/accentor",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0;1;1;2;1;1;1;1;1;0",
        "aff_unique_norm": "Cornell University;Facebook, Inc.;Facebook",
        "aff_unique_dep": ";;Facebook AI Research",
        "aff_unique_url": "https://www.cornell.edu;https://www.facebook.com;https://research.facebook.com",
        "aff_unique_abbr": "Cornell;FB;FAIR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-industry.11",
        "title": "Addressing the Vulnerability of NMT in Input Perturbations",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Neural Machine Translation (NMT) has achieved significant breakthrough in performance but is known to suffer vulnerability to input perturbations. As real input noise is difficult to predict during training, robustness is a big issue for system deployment. In this paper, we improve the robustness of NMT models by reducing the effect of noisy words through a Context-Enhanced Reconstruction (CER) approach. CER trains the model to resist noise in two steps: (1) perturbation step that breaks the naturalness of input sequence with made-up words; (2) reconstruction step that defends the noise propagation by generating better and more robust contextual representation. Experimental results on Chinese-English (ZH-EN) and French-English (FR-EN) translation tasks demonstrate robustness improvement on both news and social media text. Further fine-tuning experiments on social media text show our approach can converge at a higher position and provide a better adaptation.",
        "author": "Weiwen Xu; Ai Ti Aw; Yang Ding; Kui Wu; Shafiq Joty",
        "authorids": "/w/weiwen-xu/; /a/aiti-aw/; /y/yang-ding/; /k/kui-wu/; /s/shafiq-joty/",
        "bibtex": "@inproceedings{xu-etal-2021-addressing,\n    title = \"Addressing the Vulnerability of {NMT} in Input Perturbations\",\n    author = \"Xu, Weiwen  and\n      Aw, Ai Ti  and\n      Ding, Yang  and\n      Wu, Kui  and\n      Joty, Shafiq\",\n    editor = \"Kim, Young-bum  and\n      Li, Yunyao  and\n      Rambow, Owen\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-industry.11/\",\n    doi = \"10.18653/v1/2021.naacl-industry.11\",\n    pages = \"80--88\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-industry.11.pdf",
        "site": "https://aclanthology.org/2021.naacl-industry.11/",
        "pdf_size": 782129,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=151040005226867931&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Institute for Infocomm Research, A*STAR + The Chinese University of Hong Kong; Institute for Infocomm Research, A*STAR; Institute for Infocomm Research, A*STAR; Institute for Infocomm Research, A*STAR; Nanyang Technological University",
        "aff_domain": "se.cuhk.edu.hk;i2r.a-star.edu.sg;i2r.a-star.edu.sg;i2r.a-star.edu.sg;ntu.edu.sg",
        "email": "se.cuhk.edu.hk;i2r.a-star.edu.sg;i2r.a-star.edu.sg;i2r.a-star.edu.sg;ntu.edu.sg",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0;0;0;2",
        "aff_unique_norm": "Institute for Infocomm Research;The Chinese University of Hong Kong;Nanyang Technological University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.i2r.a-star.edu.sg;https://www.cuhk.edu.hk;https://www.ntu.edu.sg",
        "aff_unique_abbr": "I2R;CUHK;NTU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "0+1;0;0;0;0",
        "aff_country_unique": "Singapore;China"
    },
    {
        "id": "2021.naacl-main.379",
        "title": "Adversarial Learning for Zero-Shot Stance Detection on Social Media",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Stance detection on social media can help to identify and understand slanted news or commentary in everyday life. In this work, we propose a new model for zero-shot stance detection on Twitter that uses adversarial learning to generalize across topics. Our model achieves state-of-the-art performance on a number of unseen test topics with minimal computational costs. In addition, we extend zero-shot stance detection to topics not previously considered, highlighting future directions for zero-shot transfer.",
        "author": "Emily Allaway; Malavika Srikanth; Kathleen McKeown",
        "authorids": "/e/emily-allaway/; /m/malavika-srikanth/; /k/kathleen-mckeown/",
        "bibtex": "https://aclanthology.org/2021.naacl-main.379.bib",
        "pdf": "https://aclanthology.org/2021.naacl-main.379.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.379/",
        "gs_citation": 106,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8831492227878665954&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3
    },
    {
        "id": "2021.naacl-main.447",
        "title": "Adversarial Self-Supervised Learning for Out-of-Domain Detection",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Detecting out-of-domain (OOD) intents is crucial for the deployed task-oriented dialogue system. Previous unsupervised OOD detection methods only extract discriminative features of different in-domain intents while supervised counterparts can directly distinguish OOD and in-domain intents but require extensive labeled OOD data. To combine the benefits of both types, we propose a self-supervised contrastive learning framework to model discriminative semantic features of both in-domain intents and OOD intents from unlabeled data. Besides, we introduce an adversarial augmentation neural module to improve the efficiency and robustness of contrastive learning. Experiments on two public benchmark datasets show that our method can consistently outperform the baselines with a statistically significant margin.",
        "author": "Zhiyuan Zeng; Keqing He; Yuanmeng Yan; Hong Xu; Weiran Xu",
        "authorids": "/z/zhiyuan-zeng/; /k/keqing-he/; /y/yuanmeng-yan/; /h/hong-xu/; /w/weiran-xu/",
        "bibtex": "@inproceedings{zeng-etal-2021-adversarial,\n    title = \"Adversarial Self-Supervised Learning for Out-of-Domain Detection\",\n    author = \"Zeng, Zhiyuan  and\n      He, Keqing  and\n      Yan, Yuanmeng  and\n      Xu, Hong  and\n      Xu, Weiran\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.447/\",\n    doi = \"10.18653/v1/2021.naacl-main.447\",\n    pages = \"5631--5639\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.447.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.447/",
        "pdf_size": 455172,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12239398444779303217&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Pattern Recognition & Intelligent System Laboratory, Beijing University of Posts and Telecommunications, Beijing, China; Pattern Recognition & Intelligent System Laboratory, Beijing University of Posts and Telecommunications, Beijing, China + Meituan Group, Beijing, China; Pattern Recognition & Intelligent System Laboratory, Beijing University of Posts and Telecommunications, Beijing, China; Pattern Recognition & Intelligent System Laboratory, Beijing University of Posts and Telecommunications, Beijing, China; Pattern Recognition & Intelligent System Laboratory, Beijing University of Posts and Telecommunications, Beijing, China",
        "aff_domain": "bupt.edu.cn;meituan.com;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn",
        "email": "bupt.edu.cn;meituan.com;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn",
        "github": "https://github.com/parZival27/Adversarial-Self-Supervised-Out-of-Domain-Detection",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0+1;0;0;0",
        "aff_unique_norm": "Beijing University of Posts and Telecommunications;Meituan Group",
        "aff_unique_dep": "Pattern Recognition & Intelligent System Laboratory;",
        "aff_unique_url": "http://www.bupt.edu.cn/;https://www.meituan.com",
        "aff_unique_abbr": "BUPT;Meituan",
        "aff_campus_unique_index": "0;0+0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0+0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2021.naacl-demos.15",
        "title": "Alexa Conversations: An Extensible Data-driven Approach for Building Task-oriented Dialogue Systems",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "Traditional goal-oriented dialogue systems rely on various components such as natural language understanding, dialogue state tracking, policy learning and response generation. Training each component requires annotations which are hard to obtain for every new domain, limiting scalability of such systems. Similarly, rule-based dialogue systems require extensive writing and maintenance of rules and do not scale either. End-to-End dialogue systems, on the other hand, do not require module-specific annotations but need a large amount of data for training. To overcome these problems, in this demo, we present Alexa Conversations, a new approach for building goal-oriented dialogue systems that is scalable, extensible as well as data efficient. The components of this system are trained in a data-driven manner, but instead of collecting annotated conversations for training, we generate them using a novel dialogue simulator based on a few seed dialogues and specifications of APIs and entities provided by the developer. Our approach provides out-of-the-box support for natural conversational phenomenon like entity sharing across turns or users changing their mind during conversation without requiring developers to provide any such dialogue flows. We exemplify our approach using a simple pizza ordering task and showcase its value in reducing the developer burden for creating a robust experience. Finally, we evaluate our system using a typical movie ticket booking task integrated with live APIs and show that the dialogue simulator is an essential component of the system that leads to over 50% improvement in turn-level action signature prediction accuracy.",
        "author": "Anish Acharya; Suranjit Adhikari; Sanchit Agarwal; Vincent Auvray; Nehal Belgamwar; Arijit Biswas; Shubhra Chandra; Tagyoung Chung; Maryam Fazel-Zarandi; Raefer Gabriel; Shuyang Gao; Rahul Goel; Dilek Hakkani-Tur; Jan Jezabek; Abhay Jha; Jiun-Yu Kao; Prakash Krishnan; Peter Ku; Anuj Goyal; Chien-Wei Lin; Qing Liu; Arindam Mandal; Angeliki Metallinou; Vishal Naik; Yi Pan; Shachi Paul; Vittorio Perera; Abhishek Sethi; Minmin Shen; Nikko Strom; Eddie Wang",
        "authorids": "/a/anish-acharya/; /s/suranjit-adhikari/; /s/sanchit-agarwal/; /v/vincent-auvray/; /n/nehal-belgamwar/; /a/arijit-biswas/; /s/shubhra-chandra/; /t/tagyoung-chung/; /m/maryam-fazel-zarandi/; /r/raefer-gabriel/; /s/shuyang-gao/; /r/rahul-goel/; /d/dilek-hakkani-tur/; /j/jan-jezabek/; /a/abhay-jha/; /j/jiun-yu-kao/; /p/prakash-krishnan/; /p/peter-ku/; /a/anuj-goyal/; /c/chien-wei-lin/; /q/qing-liu/; /a/arindam-mandal/; /a/angeliki-metallinou/; /v/vishal-naik/; /y/yi-pan/; /s/shachi-paul/; /v/vittorio-perera/; /a/abhishek-sethi/; /m/minmin-shen/; /n/nikko-strom/; /e/eddie-wang/",
        "bibtex": "@inproceedings{acharya-etal-2021-alexa,\n    title = \"{A}lexa Conversations: An Extensible Data-driven Approach for Building Task-oriented Dialogue Systems\",\n    author = \"Acharya, Anish  and\n      Adhikari, Suranjit  and\n      Agarwal, Sanchit  and\n      Auvray, Vincent  and\n      Belgamwar, Nehal  and\n      Biswas, Arijit  and\n      Chandra, Shubhra  and\n      Chung, Tagyoung  and\n      Fazel-Zarandi, Maryam  and\n      Gabriel, Raefer  and\n      Gao, Shuyang  and\n      Goel, Rahul  and\n      Hakkani-Tur, Dilek  and\n      Jezabek, Jan  and\n      Jha, Abhay  and\n      Kao, Jiun-Yu  and\n      Krishnan, Prakash  and\n      Ku, Peter  and\n      Goyal, Anuj  and\n      Lin, Chien-Wei  and\n      Liu, Qing  and\n      Mandal, Arindam  and\n      Metallinou, Angeliki  and\n      Naik, Vishal  and\n      Pan, Yi  and\n      Paul, Shachi  and\n      Perera, Vittorio  and\n      Sethi, Abhishek  and\n      Shen, Minmin  and\n      Strom, Nikko  and\n      Wang, Eddie\",\n    editor = \"Sil, Avi  and\n      Lin, Xi Victoria\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-demos.15/\",\n    doi = \"10.18653/v1/2021.naacl-demos.15\",\n    pages = \"125--132\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-demos.15.pdf",
        "site": "https://aclanthology.org/2021.naacl-demos.15/",
        "pdf_size": 472442,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12362401163406971137&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Amazon Alexa AI, Sunnyvale, California, USA\u2020; Amazon Alexa AI, Sunnyvale, California, USA\u2020; Amazon Alexa AI, Sunnyvale, California, USA\u2020; Amazon Alexa AI, Sunnyvale, California, USA\u2020; Amazon Alexa AI, Sunnyvale, California, USA\u2020; Amazon Alexa AI, Sunnyvale, California, USA\u2020; Amazon Alexa AI, Sunnyvale, California, USA\u2020; Amazon Alexa AI, Sunnyvale, California, USA\u2020; Amazon Alexa AI, Sunnyvale, California, USA\u2020; Amazon Alexa AI, Sunnyvale, California, USA\u2020; Amazon Alexa AI, Sunnyvale, California, USA\u2020; Amazon Alexa AI, Sunnyvale, California, USA\u2020; Amazon Alexa AI, Sunnyvale, California, USA\u2020; Amazon Alexa AI, Sunnyvale, California, USA\u2020; Amazon Alexa AI, Sunnyvale, California, USA\u2020; Amazon Alexa AI, Sunnyvale, California, USA\u2020; Amazon Alexa AI, Sunnyvale, California, USA\u2020; Amazon Alexa AI, Sunnyvale, California, USA\u2020; Amazon Alexa AI, Sunnyvale, California, USA\u2020; Amazon Alexa AI, Sunnyvale, California, USA\u2020; Amazon Alexa AI, Sunnyvale, California, USA\u2020; Amazon Alexa AI, Sunnyvale, California, USA\u2020; Amazon Alexa AI, Sunnyvale, California, USA\u2020; Amazon Alexa AI, Sunnyvale, California, USA\u2020; Amazon Alexa AI, Sunnyvale, California, USA\u2020; Amazon Alexa AI, Sunnyvale, California, USA\u2020; Amazon Alexa AI, Sunnyvale, California, USA\u2020; Amazon Alexa AI, Sunnyvale, California, USA\u2020; Amazon Alexa AI, Sunnyvale, California, USA\u2020; Amazon Alexa AI, Sunnyvale, California, USA\u2020; Amazon Alexa AI, Sunnyvale, California, USA\u2020",
        "aff_domain": ";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;",
        "email": ";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;",
        "github": "",
        "project": "https://tinyurl.com/y3lowd34",
        "author_num": 31,
        "aff_unique_index": "0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Amazon Alexa AI",
        "aff_unique_dep": "AI",
        "aff_unique_url": "https://www.amazon.com/alexa",
        "aff_unique_abbr": "Amazon Alexa AI",
        "aff_campus_unique_index": "0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0",
        "aff_campus_unique": "Sunnyvale",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.154",
        "title": "Align-Refine: Non-Autoregressive Speech Recognition via Iterative Realignment",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Non-autoregressive encoder-decoder models greatly improve decoding speed over autoregressive models, at the expense of generation quality. To mitigate this, iterative decoding models repeatedly infill or refine the proposal of a non-autoregressive model. However, editing at the level of output sequences limits model flexibility. We instead propose *iterative realignment*, which by refining latent alignments allows more flexible edits in fewer steps. Our model, Align-Refine, is an end-to-end Transformer which iteratively realigns connectionist temporal classification (CTC) alignments. On the WSJ dataset, Align-Refine matches an autoregressive baseline with a 14x decoding speedup; on LibriSpeech, we reach an LM-free test-other WER of 9.0% (19% relative improvement on comparable work) in three iterations. We release our code at https://github.com/amazon-research/align-refine.",
        "author": "Ethan A. Chi; Julian Salazar; Katrin Kirchhoff",
        "authorids": "/e/ethan-a-chi/; /j/julian-salazar/; /k/katrin-kirchhoff/",
        "bibtex": "@inproceedings{chi-etal-2021-align,\n    title = \"Align-Refine: Non-Autoregressive Speech Recognition via Iterative Realignment\",\n    author = \"Chi, Ethan A.  and\n      Salazar, Julian  and\n      Kirchhoff, Katrin\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.154/\",\n    doi = \"10.18653/v1/2021.naacl-main.154\",\n    pages = \"1920--1927\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.154.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.154/",
        "pdf_size": 853340,
        "gs_citation": 64,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3845520401925519748&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Stanford University*; Amazon AWS AI; Amazon AWS AI",
        "aff_domain": "cs.stanford.edu;amazon.com;amazon.com",
        "email": "cs.stanford.edu;amazon.com;amazon.com",
        "github": "https://github.com/amazon-research/align-refine",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Stanford University;Amazon",
        "aff_unique_dep": ";Amazon Web Services AI",
        "aff_unique_url": "https://www.stanford.edu;https://aws.amazon.com",
        "aff_unique_abbr": "Stanford;AWS",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.307",
        "title": "Almost Free Semantic Draft for Neural Machine Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Translation quality can be improved by global information from the required target sentence because the decoder can understand both past and future information. However, the model needs additional cost to produce and consider such global information. In this work, to inject global information but also save cost, we present an efficient method to sample and consider a semantic draft as global information from semantic space for decoding with almost free of cost. Unlike other successful adaptations, we do not have to perform an EM-like process that repeatedly samples a possible semantic from the semantic space. Empirical experiments show that the presented method can achieve competitive performance in common language pairs with a clear advantage in inference efficiency. We will open all our source code on GitHub.",
        "author": "Xi Ai; Bin Fang",
        "authorids": "/x/xi-ai/; /b/bin-fang/",
        "bibtex": "@inproceedings{ai-fang-2021-almost,\n    title = \"Almost Free Semantic Draft for Neural Machine Translation\",\n    author = \"Ai, Xi  and\n      Fang, Bin\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.307/\",\n    doi = \"10.18653/v1/2021.naacl-main.307\",\n    pages = \"3931--3941\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.307.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.307/",
        "pdf_size": 608577,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12577300756521989488&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "College of Computer Science, Chongqing University; College of Computer Science, Chongqing University",
        "aff_domain": "gmail.com;cqu.edu.cn",
        "email": "gmail.com;cqu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Chongqing University",
        "aff_unique_dep": "College of Computer Science",
        "aff_unique_url": "https://www.cqu.edu.cn",
        "aff_unique_abbr": "CQU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2021.naacl-industry.21",
        "title": "An Architecture for Accelerated Large-Scale Inference of Transformer-Based Language Models",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "This work demonstrates the development process of a machine learning architecture for inference that can scale to a large volume of requests. We used a BERT model that was fine-tuned for emotion analysis, returning a probability distribution of emotions given a paragraph. The model was deployed as a gRPC service on Kubernetes. Apache Spark was used to perform inference in batches by calling the service. We encountered some performance and concurrency challenges and created solutions to achieve faster running time. Starting with 200 successful inference requests per minute, we were able to achieve as high as 18 thousand successful requests per minute with the same batch job resource allocation. As a result, we successfully stored emotion probabilities for 95 million paragraphs within 96 hours.",
        "author": "Amir Ganiev; Colton Chapin; Anderson De Andrade; Chen Liu",
        "authorids": "/a/amir-ganiev/; /c/colton-chapin/; /a/anderson-de-andrade/; /c/chen-liu/",
        "bibtex": "@inproceedings{ganiev-etal-2021-architecture,\n    title = \"An Architecture for Accelerated Large-Scale Inference of Transformer-Based Language Models\",\n    author = \"Ganiev, Amir  and\n      Chapin, Colton  and\n      De Andrade, Anderson  and\n      Liu, Chen\",\n    editor = \"Kim, Young-bum  and\n      Li, Yunyao  and\n      Rambow, Owen\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-industry.21/\",\n    doi = \"10.18653/v1/2021.naacl-industry.21\",\n    pages = \"163--169\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-industry.21.pdf",
        "site": "https://aclanthology.org/2021.naacl-industry.21/",
        "pdf_size": 221196,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8777953819012316070&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Wattpad; Wattpad; Wattpad; Wattpad",
        "aff_domain": "mail.utoronto.ca;wattpad.com;wattpad.com;mail.utoronto.ca",
        "email": "mail.utoronto.ca;wattpad.com;wattpad.com;mail.utoronto.ca",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Wattpad Inc.",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.wattpad.com",
        "aff_unique_abbr": "Wattpad",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2021.naacl-industry.17",
        "title": "An Emotional Comfort Framework for Improving User Satisfaction in E-Commerce Customer Service Chatbots",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "E-commerce has grown substantially over the last several years, and chatbots for intelligent customer service are concurrently drawing attention. We presented AliMe Assist, a Chinese intelligent assistant designed for creating an innovative online shopping experience in E-commerce. Based on question answering (QA), AliMe Assist offers assistance service, customer service, and chatting service. According to the survey of user studies and the real online testing, emotional comfort of customers\u2019 negative emotions, which make up more than 5% of whole number of customer visits on AliMe, is a key point for providing considerate service. In this paper, we propose a framework to obtain proper answer to customers\u2019 emotional questions. The framework takes emotion classification model as a core, and final answer selection is based on topic classification and text matching. Our experiments on real online systems show that the framework is very promising.",
        "author": "Shuangyong Song; Chao Wang; Haiqing Chen; Huan Chen",
        "authorids": "/s/shuangyong-song/; /c/chao-wang/; /h/haiqing-chen/; /h/huan-chen/",
        "bibtex": "@inproceedings{song-etal-2021-emotional,\n    title = \"An Emotional Comfort Framework for Improving User Satisfaction in {E}-Commerce Customer Service Chatbots\",\n    author = \"Song, Shuangyong  and\n      Wang, Chao  and\n      Chen, Haiqing  and\n      Chen, Huan\",\n    editor = \"Kim, Young-bum  and\n      Li, Yunyao  and\n      Rambow, Owen\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-industry.17/\",\n    doi = \"10.18653/v1/2021.naacl-industry.17\",\n    pages = \"130--137\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-industry.17.pdf",
        "site": "https://aclanthology.org/2021.naacl-industry.17/",
        "pdf_size": 780794,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14118508672578217876&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Alibaba Groups, Hangzhou 311121, China; Alibaba Groups, Hangzhou 311121, China; Alibaba Groups, Hangzhou 311121, China; Alibaba Groups, Hangzhou 311121, China",
        "aff_domain": "alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com",
        "email": "alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Alibaba Group",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.alibaba.com",
        "aff_unique_abbr": "Alibaba",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Hangzhou",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2021.naacl-main.75",
        "title": "An Empirical Comparison of Instance Attribution Methods for NLP",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Widespread adoption of deep models has motivated a pressing need for approaches to interpret network outputs and to facilitate model debugging. Instance attribution methods constitute one means of accomplishing these goals by retrieving training instances that (may have) led to a particular prediction. Influence functions (IF; Koh and Liang 2017) provide machinery for doing this by quantifying the effect that perturbing individual train instances would have on a specific test prediction. However, even approximating the IF is computationally expensive, to the degree that may be prohibitive in many cases. Might simpler approaches (e.g., retrieving train examples most similar to a given test point) perform comparably? In this work, we evaluate the degree to which different potential instance attribution agree with respect to the importance of training samples. We find that simple retrieval methods yield training instances that differ from those identified via gradient-based methods (such as IFs), but that nonetheless exhibit desirable characteristics similar to more complex attribution methods. Code for all methods and experiments in this paper is available at: https://github.com/successar/instance_attributions_NLP.",
        "author": "Pouya Pezeshkpour; Sarthak Jain; Byron Wallace; Sameer Singh",
        "authorids": "/p/pouya-pezeshkpour/; /s/sarthak-jain/; /b/byron-c-wallace/; /s/sameer-singh/",
        "bibtex": "@inproceedings{pezeshkpour-etal-2021-empirical,\n    title = \"An Empirical Comparison of Instance Attribution Methods for {NLP}\",\n    author = \"Pezeshkpour, Pouya  and\n      Jain, Sarthak  and\n      Wallace, Byron  and\n      Singh, Sameer\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.75/\",\n    doi = \"10.18653/v1/2021.naacl-main.75\",\n    pages = \"967--975\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.75.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.75/",
        "pdf_size": 635230,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14151304797425828894&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "University of California, Irvine; Northeastern University; Northeastern University; University of California, Irvine",
        "aff_domain": "uci.edu;northeastern.edu;northeastern.edu;uci.edu",
        "email": "uci.edu;northeastern.edu;northeastern.edu;uci.edu",
        "github": "https://github.com/successar/instance_attributions_NLP",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "University of California, Irvine;Northeastern University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uci.edu;https://www.northeastern.edu",
        "aff_unique_abbr": "UCI;NEU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Irvine;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.294",
        "title": "An Empirical Investigation of Bias in the Multimodal Analysis of Financial Earnings Calls",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Volatility prediction is complex due to the stock market\u2019s stochastic nature. Existing research focuses on the textual elements of financial disclosures like earnings calls transcripts to forecast stock volatility and risk, but ignores the rich acoustic features in the company executives\u2019 speech. Recently, new multimodal approaches that leverage the verbal and vocal cues of speakers in financial disclosures significantly outperform previous state-of-the-art approaches demonstrating the benefits of multimodality and speech. However, the financial realm is still plagued with a severe underrepresentation of various communities spanning diverse demographics, gender, and native speech. While multimodal models are better risk forecasters, it is imperative to also investigate the potential bias that these models may learn from the speech signals of company executives. In this work, we present the first study to discover the gender bias in multimodal volatility prediction due to gender-sensitive audio features and fewer female executives in earnings calls of one of the world\u2019s biggest stock indexes, the S&P 500 index. We quantitatively analyze bias as error disparity and investigate the sources of this bias. Our results suggest that multimodal neural financial models accentuate gender-based stereotypes.",
        "author": "Ramit Sawhney; Arshiya Aggarwal; Rajiv Ratn Shah",
        "authorids": "/r/ramit-sawhney/; /a/arshiya-aggarwal/; /r/rajiv-shah/",
        "bibtex": "@inproceedings{sawhney-etal-2021-empirical,\n    title = \"An Empirical Investigation of Bias in the Multimodal Analysis of Financial Earnings Calls\",\n    author = \"Sawhney, Ramit  and\n      Aggarwal, Arshiya  and\n      Shah, Rajiv Ratn\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.294/\",\n    doi = \"10.18653/v1/2021.naacl-main.294\",\n    pages = \"3751--3757\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.294.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.294/",
        "pdf_size": 517483,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9706128556959415124&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "IIIT Delhi; Adobe India; IIIT Delhi",
        "aff_domain": "iiitd.ac.in;adobe.com;iiitd.ac.in",
        "email": "iiitd.ac.in;adobe.com;iiitd.ac.in",
        "github": "https://github.com/midas-research/multimodal-bias-naacl",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "International Institute of Information Technology, Delhi;Adobe Systems Incorporated",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.iiitdelhi.ac.in;https://www.adobe.com/in/",
        "aff_unique_abbr": "IIIT-D;Adobe",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Delhi;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2021.naacl-industry.32",
        "title": "An Empirical Study of Generating Texts for Search Engine Advertising",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Although there are many studies on neural language generation (NLG), few trials are put into the real world, especially in the advertising domain. Generating ads with NLG models can help copywriters in their creation. However, few studies have adequately evaluated the effect of generated ads with actual serving included because it requires a large amount of training data and a particular environment. In this paper, we demonstrate a practical use case of generating ad-text with an NLG model. Specially, we show how to improve the ads\u2019 impact, deploy models to a product, and evaluate the generated ads.",
        "author": "Hidetaka Kamigaito; Peinan Zhang; Hiroya Takamura; Manabu Okumura",
        "authorids": "/h/hidetaka-kamigaito/; /p/peinan-zhang/; /h/hiroya-takamura/; /m/manabu-okumura/",
        "bibtex": "@inproceedings{kamigaito-etal-2021-empirical,\n    title = \"An Empirical Study of Generating Texts for Search Engine Advertising\",\n    author = \"Kamigaito, Hidetaka  and\n      Zhang, Peinan  and\n      Takamura, Hiroya  and\n      Okumura, Manabu\",\n    editor = \"Kim, Young-bum  and\n      Li, Yunyao  and\n      Rambow, Owen\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-industry.32/\",\n    doi = \"10.18653/v1/2021.naacl-industry.32\",\n    pages = \"255--262\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-industry.32.pdf",
        "site": "https://aclanthology.org/2021.naacl-industry.32/",
        "pdf_size": 825344,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13728907603528895776&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Tokyo Institute of Technology; CyberAgent, Inc.; National Institute of Advanced Industrial Science and Technology (AIST); Tokyo Institute of Technology",
        "aff_domain": "lr.pi.titech.ac.jp;cyberagent.co.jp;aist.go.jp;lr.pi.titech.ac.jp",
        "email": "lr.pi.titech.ac.jp;cyberagent.co.jp;aist.go.jp;lr.pi.titech.ac.jp",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Tokyo Institute of Technology;CyberAgent;National Institute of Advanced Industrial Science and Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.titech.ac.jp;https://www.cyberagent.co.jp;https://www.aist.go.jp",
        "aff_unique_abbr": "Titech;CyberAgent;AIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2021.naacl-main.396",
        "title": "An Empirical Study on Neural Keyphrase Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent years have seen a flourishing of neural keyphrase generation (KPG) works, including the release of several large-scale datasets and a host of new models to tackle them. Model performance on KPG tasks has increased significantly with evolving deep learning research. However, there lacks a comprehensive comparison among different model designs, and a thorough investigation on related factors that may affect a KPG system\u2019s generalization performance. In this empirical study, we aim to fill this gap by providing extensive experimental results and analyzing the most crucial factors impacting the generalizability of KPG models. We hope this study can help clarify some of the uncertainties surrounding the KPG task and facilitate future research on this topic.",
        "author": "Rui Meng; Xingdi Yuan; Tong Wang; Sanqiang Zhao; Adam Trischler; Daqing He",
        "authorids": "/r/rui-meng/; /x/xingdi-yuan/; /t/tong-wang/; /s/sanqiang-zhao/; /a/adam-trischler/; /d/daqing-he/",
        "bibtex": "@inproceedings{meng-etal-2021-empirical,\n    title = \"An Empirical Study on Neural Keyphrase Generation\",\n    author = \"Meng, Rui  and\n      Yuan, Xingdi  and\n      Wang, Tong  and\n      Zhao, Sanqiang  and\n      Trischler, Adam  and\n      He, Daqing\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.396/\",\n    doi = \"10.18653/v1/2021.naacl-main.396\",\n    pages = \"4985--5007\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.396.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.396/",
        "pdf_size": 1382125,
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4400507525145136087&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "School of Computing and Information, University of Pittsburgh\u2020; Microsoft Research, Montr\u00e9al\u2021; Microsoft Research, Montr\u00e9al\u2021; School of Computing and Information, University of Pittsburgh\u2020; Microsoft Research, Montr\u00e9al\u2021; School of Computing and Information, University of Pittsburgh\u2020",
        "aff_domain": "pitt.edu; ; ; ; ; ",
        "email": "pitt.edu; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;1;0;1;0",
        "aff_unique_norm": "University of Pittsburgh;Microsoft Research",
        "aff_unique_dep": "School of Computing and Information;",
        "aff_unique_url": "https://www.pitt.edu;https://www.microsoft.com/en-us/research/group/microsoft-research-montreal",
        "aff_unique_abbr": "Pitt;MSR",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Montr\u00e9al",
        "aff_country_unique_index": "0;1;1;0;1;0",
        "aff_country_unique": "United States;Canada"
    },
    {
        "id": "2021.naacl-main.114",
        "title": "Annotating and Modeling Fine-grained Factuality in Summarization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent pre-trained abstractive summarization systems have started to achieve credible performance, but a major barrier to their use in practice is their propensity to output summaries that are not faithful to the input and that contain factual errors. While a number of annotated datasets and statistical models for assessing factuality have been explored, there is no clear picture of what errors are most important to target or where current techniques are succeeding and failing. We explore both synthetic and human-labeled data sources for training models to identify factual errors in summarization, and study factuality at the word-, dependency-, and sentence-level. Our observations are threefold. First, exhibited factual errors differ significantly across datasets, and commonly-used training sets of simple synthetic errors do not reflect errors made on abstractive datasets like XSum. Second, human-labeled data with fine-grained annotations provides a more effective training signal than sentence-level annotations or synthetic data. Finally, we show that our best factuality detection model enables training of more factual XSum summarization models by allowing us to identify non-factual tokens in the training data.",
        "author": "Tanya Goyal; Greg Durrett",
        "authorids": "/t/tanya-goyal/; /g/greg-durrett/",
        "bibtex": "@inproceedings{goyal-durrett-2021-annotating,\n    title = \"Annotating and Modeling Fine-grained Factuality in Summarization\",\n    author = \"Goyal, Tanya  and\n      Durrett, Greg\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.114/\",\n    doi = \"10.18653/v1/2021.naacl-main.114\",\n    pages = \"1449--1462\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.114.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.114/",
        "pdf_size": 2079504,
        "gs_citation": 162,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12795779435332634163&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Computer Science, The University of Texas at Austin; Department of Computer Science, The University of Texas at Austin",
        "aff_domain": "utexas.edu;cs.utexas.edu",
        "email": "utexas.edu;cs.utexas.edu",
        "github": "https://github.com/tagoyal/factuality-datasets",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "The University of Texas at Austin",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.23",
        "title": "Answering Product-Questions by Utilizing Questions from Other Contextually Similar Products",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Predicting the answer to a product-related question is an emerging field of research that recently attracted a lot of attention. Answering subjective and opinion-based questions is most challenging due to the dependency on customer generated content. Previous works mostly focused on review-aware answer prediction; however, these approaches fail for new or unpopular products, having no (or only a few) reviews at hand. In this work, we propose a novel and complementary approach for predicting the answer for such questions, based on the answers for similar questions asked on similar products. We measure the contextual similarity between products based on the answers they provide for the same question. A mixture-of-expert framework is used to predict the answer by aggregating the answers from contextually similar products. Empirical results demonstrate that our model outperforms strong baselines on some segments of questions, namely those that have roughly ten or more similar resolved questions in the corpus. We additionally publish two large-scale datasets used in this work, one is of similar product question pairs, and the second is of product question-answer pairs.",
        "author": "Ohad Rozen; David Carmel; Avihai Mejer; Vitaly Mirkis; Yftah Ziser",
        "authorids": "/o/ohad-rozen/; /d/david-carmel/; /a/avihai-mejer/; /v/vitaly-mirkis/; /y/yftah-ziser/",
        "bibtex": "@inproceedings{rozen-etal-2021-answering,\n    title = \"Answering Product-Questions by Utilizing Questions from Other Contextually Similar Products\",\n    author = \"Rozen, Ohad  and\n      Carmel, David  and\n      Mejer, Avihai  and\n      Mirkis, Vitaly  and\n      Ziser, Yftah\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.23/\",\n    doi = \"10.18653/v1/2021.naacl-main.23\",\n    pages = \"242--253\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.23.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.23/",
        "pdf_size": 462586,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10599993374732346613&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Computer Science Department, Bar-Ilan University, Ramat-Gan, Israel; Amazon; Amazon; Amazon; Facebook",
        "aff_domain": "gmail.com;amazon.com;amazon.com;amazon.com;fb.com",
        "email": "gmail.com;amazon.com;amazon.com;amazon.com;fb.com",
        "github": "",
        "project": "https://registry.opendata.aws",
        "author_num": 5,
        "aff_unique_index": "0;1;1;1;2",
        "aff_unique_norm": "Bar-Ilan University;Amazon.com, Inc.;Facebook, Inc.",
        "aff_unique_dep": "Computer Science Department;;",
        "aff_unique_url": "https://www.biu.ac.il;https://www.amazon.com;https://www.facebook.com",
        "aff_unique_abbr": "BIU;Amazon;FB",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Ramat-Gan;",
        "aff_country_unique_index": "0;1;1;1;1",
        "aff_country_unique": "Israel;United States"
    },
    {
        "id": "2021.naacl-main.168",
        "title": "Are NLP Models really able to Solve Simple Math Word Problems?",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The problem of designing NLP solvers for math word problems (MWP) has seen sustained research activity and steady gains in the test accuracy. Since existing solvers achieve high performance on the benchmark datasets for elementary level MWPs containing one-unknown arithmetic word problems, such problems are often considered \u201csolved\u201d with the bulk of research attention moving to more complex MWPs. In this paper, we restrict our attention to English MWPs taught in grades four and lower. We provide strong evidence that the existing MWP solvers rely on shallow heuristics to achieve high performance on the benchmark datasets. To this end, we show that MWP solvers that do not have access to the question asked in the MWP can still solve a large fraction of MWPs. Similarly, models that treat MWPs as bag-of-words can also achieve surprisingly high accuracy. Further, we introduce a challenge dataset, SVAMP, created by applying carefully chosen variations over examples sampled from existing datasets. The best accuracy achieved by state-of-the-art models is substantially lower on SVAMP, thus showing that much remains to be done even for the simplest of the MWPs.",
        "author": "Arkil Patel; Satwik Bhattamishra; Navin Goyal",
        "authorids": "/a/arkil-patel/; /s/satwik-bhattamishra/; /n/navin-goyal/",
        "bibtex": "@inproceedings{patel-etal-2021-nlp,\n    title = \"Are {NLP} Models really able to Solve Simple Math Word Problems?\",\n    author = \"Patel, Arkil  and\n      Bhattamishra, Satwik  and\n      Goyal, Navin\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.168/\",\n    doi = \"10.18653/v1/2021.naacl-main.168\",\n    pages = \"2080--2094\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.168.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.168/",
        "pdf_size": 359153,
        "gs_citation": 779,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1721262773591457347&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Microsoft Research India; Microsoft Research India; Microsoft Research India",
        "aff_domain": "gmail.com;microsoft.com;microsoft.com",
        "email": "gmail.com;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Microsoft Research",
        "aff_unique_dep": "Microsoft Research India",
        "aff_unique_url": "https://www.microsoft.com/en-us/research/group/microsoft-research-india",
        "aff_unique_abbr": "MSR India",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2021.naacl-main.340",
        "title": "Ask what\u2019s missing and what\u2019s useful: Improving Clarification Question Generation using Global Knowledge",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The ability to generate clarification questions i.e., questions that identify useful missing information in a given context, is important in reducing ambiguity. Humans use previous experience with similar contexts to form a global view and compare it to the given context to ascertain what is missing and what is useful in the context. Inspired by this, we propose a model for clarification question generation where we first identify what is missing by taking a difference between the global and the local view and then train a model to identify what is useful and generate a question about it. Our model outperforms several baselines as judged by both automatic metrics and humans.",
        "author": "Bodhisattwa Prasad Majumder; Sudha Rao; Michel Galley; Julian McAuley",
        "authorids": "/b/bodhisattwa-prasad-majumder/; /s/sudha-rao/; /m/michel-galley/; /j/julian-mcauley/",
        "bibtex": "@inproceedings{majumder-etal-2021-ask,\n    title = \"Ask what{'}s missing and what{'}s useful: Improving Clarification Question Generation using Global Knowledge\",\n    author = \"Majumder, Bodhisattwa Prasad  and\n      Rao, Sudha  and\n      Galley, Michel  and\n      McAuley, Julian\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.340/\",\n    doi = \"10.18653/v1/2021.naacl-main.340\",\n    pages = \"4300--4312\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.340.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.340/",
        "pdf_size": 2850688,
        "gs_citation": 51,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9784424145767984958&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Department of Computer Science and Engineering, UC San Diego + Microsoft Research, Redmond; Microsoft Research, Redmond; Microsoft Research, Redmond; Department of Computer Science and Engineering, UC San Diego",
        "aff_domain": "eng.ucsd.edu;microsoft.com;microsoft.com;eng.ucsd.edu",
        "email": "eng.ucsd.edu;microsoft.com;microsoft.com;eng.ucsd.edu",
        "github": "https://github.com/microsoft/clarification-qgen-globalinfo",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;1;1;0",
        "aff_unique_norm": "University of California, San Diego;Microsoft Research",
        "aff_unique_dep": "Department of Computer Science and Engineering;",
        "aff_unique_url": "https://www.ucsd.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "UCSD;MSR",
        "aff_campus_unique_index": "0+1;1;1;0",
        "aff_campus_unique": "San Diego;Redmond",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.34",
        "title": "Aspect-Controlled Neural Argument Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We rely on arguments in our daily lives to deliver our opinions and base them on evidence, making them more convincing in turn. However, finding and formulating arguments can be challenging. In this work, we present the Arg-CTRL - a language model for argument generation that can be controlled to generate sentence-level arguments for a given topic, stance, and aspect. We define argument aspect detection as a necessary method to allow this fine-granular control and crowdsource a dataset with 5,032 arguments annotated with aspects. Our evaluation shows that the Arg-CTRL is able to generate high-quality, aspect-specific arguments, applicable to automatic counter-argument generation. We publish the model weights and all datasets and code to train the Arg-CTRL.",
        "author": "Benjamin Schiller; Johannes Daxenberger; Iryna Gurevych",
        "authorids": "/b/benjamin-schiller/; /j/johannes-daxenberger/; /i/iryna-gurevych/",
        "bibtex": "@inproceedings{schiller-etal-2021-aspect,\n    title = \"Aspect-Controlled Neural Argument Generation\",\n    author = \"Schiller, Benjamin  and\n      Daxenberger, Johannes  and\n      Gurevych, Iryna\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.34/\",\n    doi = \"10.18653/v1/2021.naacl-main.34\",\n    pages = \"380--396\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.34.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.34/",
        "pdf_size": 541989,
        "gs_citation": 81,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10078942564165604568&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3
    },
    {
        "id": "2021.naacl-main.231",
        "title": "Aspect-based Sentiment Analysis with Type-aware Graph Convolutional Networks and Layer Ensemble",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "It is popular that neural graph-based models are applied in existing aspect-based sentiment analysis (ABSA) studies for utilizing word relations through dependency parses to facilitate the task with better semantic guidance for analyzing context and aspect words. However, most of these studies only leverage dependency relations without considering their dependency types, and are limited in lacking efficient mechanisms to distinguish the important relations as well as learn from different layers of graph based models. To address such limitations, in this paper, we propose an approach to explicitly utilize dependency types for ABSA with type-aware graph convolutional networks (T-GCN), where attention is used in T-GCN to distinguish different edges (relations) in the graph and attentive layer ensemble is proposed to comprehensively learn from different layers of T-GCN. The validity and effectiveness of our approach are demonstrated in the experimental results, where state-of-the-art performance is achieved on six English benchmark datasets. Further experiments are conducted to analyze the contributions of each component in our approach and illustrate how different layers in T-GCN help ABSA with quantitative and qualitative analysis.",
        "author": "Yuanhe Tian; Guimin Chen; Yan Song",
        "authorids": "/y/yuanhe-tian/; /g/guimin-chen/; /y/yan-song/",
        "bibtex": "@inproceedings{tian-etal-2021-aspect,\n    title = \"Aspect-based Sentiment Analysis with Type-aware Graph Convolutional Networks and Layer Ensemble\",\n    author = \"Tian, Yuanhe  and\n      Chen, Guimin  and\n      Song, Yan\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.231/\",\n    doi = \"10.18653/v1/2021.naacl-main.231\",\n    pages = \"2910--2922\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.231.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.231/",
        "pdf_size": 980013,
        "gs_citation": 195,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9084205665931390214&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "University of Washington; Shenzhen Research Institute of Big Data + The Chinese University of Hong Kong (Shenzhen); The Chinese University of Hong Kong (Shenzhen)",
        "aff_domain": "uw.edu;foxmail.com;cuhk.edu.cn",
        "email": "uw.edu;foxmail.com;cuhk.edu.cn",
        "github": "https://github.com/cuhksz-nlp/ASA-TGCN",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1+2;2",
        "aff_unique_norm": "University of Washington;Shenzhen Research Institute of Big Data;The Chinese University of Hong Kong",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.washington.edu;http://www.sribd.cn;https://www.cuhk.edu.cn",
        "aff_unique_abbr": "UW;;CUHK",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Shenzhen",
        "aff_country_unique_index": "0;1+1;1",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2021.naacl-main.91",
        "title": "Assessing Reference-Free Peer Evaluation for Machine Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Reference-free evaluation has the potential to make machine translation evaluation substantially more scalable, allowing us to pivot easily to new languages or domains. It has been recently shown that the probabilities given by a large, multilingual model can achieve state of the art results when used as a reference-free metric. We experiment with various modifications to this model, and demonstrate that by scaling it up we can match the performance of BLEU. We analyze various potential weaknesses of the approach, and find that it is surprisingly robust and likely to offer reasonable performance across a broad spectrum of domains and different system qualities.",
        "author": "Sweta Agrawal; George Foster; Markus Freitag; Colin Cherry",
        "authorids": "/s/sweta-agrawal/; /g/george-foster/; /m/markus-freitag/; /c/colin-cherry/",
        "bibtex": "@inproceedings{agrawal-etal-2021-assessing,\n    title = \"Assessing Reference-Free Peer Evaluation for Machine Translation\",\n    author = \"Agrawal, Sweta  and\n      Foster, George  and\n      Freitag, Markus  and\n      Cherry, Colin\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.91/\",\n    doi = \"10.18653/v1/2021.naacl-main.91\",\n    pages = \"1158--1171\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.91.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.91/",
        "pdf_size": 512961,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1138293632044635246&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science, University of Maryland; Google Research; Google Research; Google Research",
        "aff_domain": "umd.edu;google.com;google.com;google.com",
        "email": "umd.edu;google.com;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "University of Maryland;Google",
        "aff_unique_dep": "Department of Computer Science;Google Research",
        "aff_unique_url": "https://www/umd.edu;https://research.google",
        "aff_unique_abbr": "UMD;Google Research",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.397",
        "title": "Attention Head Masking for Inference Time Content Selection in Abstractive Summarization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "How can we effectively inform content selection in Transformer-based abstractive summarization models? In this work, we present a simple-yet-effective attention head masking technique, which is applied on encoder-decoder attentions to pinpoint salient content at inference time. Using attention head masking, we are able to reveal the relation between encoder-decoder attentions and content selection behaviors of summarization models. We then demonstrate its effectiveness on three document summarization datasets based on both in-domain and cross-domain settings. Importantly, our models outperform prior state-of-the-art models on CNN/Daily Mail and New York Times datasets. Moreover, our inference-time masking technique is also data-efficient, requiring only 20% of the training samples to outperform BART fine-tuned on the full CNN/DailyMail dataset.",
        "author": "Shuyang Cao; Lu Wang",
        "authorids": "/s/shuyang-cao/; /l/lu-wang/",
        "bibtex": "@inproceedings{cao-wang-2021-attention,\n    title = \"Attention Head Masking for Inference Time Content Selection in Abstractive Summarization\",\n    author = \"Cao, Shuyang  and\n      Wang, Lu\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.397/\",\n    doi = \"10.18653/v1/2021.naacl-main.397\",\n    pages = \"5008--5016\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.397.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.397/",
        "pdf_size": 800368,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5152755470969533604&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Computer Science and Engineering, University of Michigan; Computer Science and Engineering, University of Michigan",
        "aff_domain": "umich.edu;umich.edu",
        "email": "umich.edu;umich.edu",
        "github": "https://shuyangcao.github.io/projects/inference_head_masking",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "Computer Science and Engineering",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Ann Arbor",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.28",
        "title": "Augmented SBERT: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "There are two approaches for pairwise sentence scoring: Cross-encoders, which perform full-attention over the input pair, and Bi-encoders, which map each input independently to a dense vector space. While cross-encoders often achieve higher performance, they are too slow for many practical use cases. Bi-encoders, on the other hand, require substantial training data and fine-tuning over the target task to achieve competitive performance. We present a simple yet efficient data augmentation strategy called Augmented SBERT, where we use the cross-encoder to label a larger set of input pairs to augment the training data for the bi-encoder. We show that, in this process, selecting the sentence pairs is non-trivial and crucial for the success of the method. We evaluate our approach on multiple tasks (in-domain) as well as on a domain adaptation task. Augmented SBERT achieves an improvement of up to 6 points for in-domain and of up to 37 points for domain adaptation tasks compared to the original bi-encoder performance.",
        "author": "Nandan Thakur; Nils Reimers; Johannes Daxenberger; Iryna Gurevych",
        "authorids": "/n/nandan-thakur/; /n/nils-reimers/; /j/johannes-daxenberger/; /i/iryna-gurevych/",
        "bibtex": "@inproceedings{thakur-etal-2021-augmented,\n    title = \"Augmented {SBERT}: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks\",\n    author = \"Thakur, Nandan  and\n      Reimers, Nils  and\n      Daxenberger, Johannes  and\n      Gurevych, Iryna\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.28/\",\n    doi = \"10.18653/v1/2021.naacl-main.28\",\n    pages = \"296--310\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.28.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.28/",
        "pdf_size": 735411,
        "gs_citation": 296,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6499475848780461584&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4
    },
    {
        "id": "2021.naacl-main.446",
        "title": "Augmenting Knowledge-grounded Conversations with Sequential Knowledge Transition",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Knowledge data are massive and widespread in the real-world, which can serve as good external sources to enrich conversations. However, in knowledge-grounded conversations, current models still lack the fine-grained control over knowledge selection and integration with dialogues, which finally leads to the knowledge-irrelevant response generation problems: 1) knowledge selection merely relies on the dialogue context, ignoring the inherent knowledge transitions along with conversation flows; 2) the models often over-fit during training, resulting with incoherent response by referring to unrelated tokens from specific knowledge content in the testing phase; 3) although response is generated upon the dialogue history and knowledge, the models often tend to overlook the selected knowledge, and hence generates knowledge-irrelevant response. To address these problems, we proposed to explicitly model the knowledge transition in sequential multi-turn conversations by abstracting knowledge into topic tags. Besides, to fully utilizing the selected knowledge in generative process, we propose pre-training a knowledge-aware response generator to pay more attention on the selected knowledge. In particular, a sequential knowledge transition model equipped with a pre-trained knowledge-aware response generator (SKT-KG) formulates the high-level knowledge transition and fully utilizes the limited knowledge data. Experimental results on both structured and unstructured knowledge-grounded dialogue benchmarks indicate that our model achieves better performance over baseline models.",
        "author": "Haolan Zhan; Hainan Zhang; Hongshen Chen; Zhuoye Ding; Yongjun Bao; Yanyan Lan",
        "authorids": "/h/haolan-zhan/; /h/hainan-zhang/; /h/hongshen-chen/; /z/zhuoye-ding/; /y/yongjun-bao/; /y/yanyan-lan/",
        "bibtex": "@inproceedings{zhan-etal-2021-augmenting,\n    title = \"Augmenting Knowledge-grounded Conversations with Sequential Knowledge Transition\",\n    author = \"Zhan, Haolan  and\n      Zhang, Hainan  and\n      Chen, Hongshen  and\n      Ding, Zhuoye  and\n      Bao, Yongjun  and\n      Lan, Yanyan\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.446/\",\n    doi = \"10.18653/v1/2021.naacl-main.446\",\n    pages = \"5621--5630\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.446.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.446/",
        "pdf_size": 922396,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3423468742625967430&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Institute of Software, Chinese Academy of Sciences, Beijing, China; Data Science Lab, JD.com, Beijing, China; Data Science Lab, JD.com, Beijing, China; Data Science Lab, JD.com, Beijing, China; Data Science Lab, JD.com, Beijing, China; Institute for AI Industry Research, Tsinghua University, Beijing, China",
        "aff_domain": "gmail.com;jd.com;chenhongshen.com;jd.com; ;tsinghua.edu.cn",
        "email": "gmail.com;jd.com;chenhongshen.com;jd.com; ;tsinghua.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;1;1;1;2",
        "aff_unique_norm": "Chinese Academy of Sciences;JD.com;Tsinghua University",
        "aff_unique_dep": "Institute of Software;Data Science Lab;Institute for AI Industry Research",
        "aff_unique_url": "http://www.ios.ac.cn;https://www.jd.com;https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "CAS;JD;Tsinghua",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2021.naacl-industry.14",
        "title": "Autocorrect in the Process of Translation \u2014 Multi-task Learning Improves Dialogue Machine Translation",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Automatic translation of dialogue texts is a much needed demand in many real life scenarios. However, the currently existing neural machine translation delivers unsatisfying results. In this paper, we conduct a deep analysis of a dialogue corpus and summarize three major issues on dialogue translation, including pronoun dropping (), punctuation dropping (), and typos (). In response to these challenges, we propose a joint learning method to identify omission and typo, and utilize context to translate dialogue utterances. To properly evaluate the performance, we propose a manually annotated dataset with 1,931 Chinese-English parallel utterances from 300 dialogues as a benchmark testbed for dialogue translation. Our experiments show that the proposed method improves translation quality by 3.2 BLEU over the baselines. It also elevates the recovery rate of omitted pronouns from 26.09% to 47.16%. We will publish the code and dataset publicly at https://xxx.xx.",
        "author": "Tao Wang; Chengqi Zhao; Mingxuan Wang; Lei Li; Deyi Xiong",
        "authorids": "/t/tao-wang/; /c/chengqi-zhao/; /m/mingxuan-wang/; /l/lei-li/; /d/deyi-xiong/",
        "bibtex": "@inproceedings{wang-etal-2021-autocorrect,\n    title = \"Autocorrect in the Process of Translation {---} Multi-task Learning Improves Dialogue Machine Translation\",\n    author = \"Wang, Tao  and\n      Zhao, Chengqi  and\n      Wang, Mingxuan  and\n      Li, Lei  and\n      Xiong, Deyi\",\n    editor = \"Kim, Young-bum  and\n      Li, Yunyao  and\n      Rambow, Owen\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-industry.14/\",\n    doi = \"10.18653/v1/2021.naacl-industry.14\",\n    pages = \"105--112\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-industry.14.pdf",
        "site": "https://aclanthology.org/2021.naacl-industry.14/",
        "pdf_size": 506481,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5277950541291868793&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "ByteDance AI Lab + School of Computer Science and Technology, Soochow University, Suzhou, China; ByteDance AI Lab; ByteDance AI Lab; ByteDance AI Lab; College of Intelligence and Computing, Tianjin University, Tianjin, China",
        "aff_domain": "bytedance.com;bytedance.com;bytedance.com;bytedance.com;tju.edu.cn",
        "email": "bytedance.com;bytedance.com;bytedance.com;bytedance.com;tju.edu.cn",
        "github": "https://github.com/rgwt123/DialogueMT",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0;0;0;2",
        "aff_unique_norm": "ByteDance;Soochow University;Tianjin University",
        "aff_unique_dep": "AI Lab;School of Computer Science and Technology;College of Intelligence and Computing",
        "aff_unique_url": "https://www.bytedance.com;http://www.soochow.edu.cn;http://www.tju.edu.cn",
        "aff_unique_abbr": "ByteDance;;Tianjin University",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Suzhou;Tianjin",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2021.naacl-main.175",
        "title": "Automatic Classification of Neutralization Techniques in the Narrative of Climate Change Scepticism",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Neutralisation techniques, e.g. denial of responsibility and denial of victim, are used in the narrative of climate change scepticism to justify lack of action or to promote an alternative view. We first draw on social science to introduce the problem to the community of nlp, present the granularity of the coding schema and then collect manual annotations of neutralised techniques in text relating to climate change, and experiment with supervised and semi- supervised BERT-based models.",
        "author": "Shraey Bhatia; Jey Han Lau; Timothy Baldwin",
        "authorids": "/s/shraey-bhatia/; /j/jey-han-lau/; /t/timothy-baldwin/",
        "bibtex": "@inproceedings{bhatia-etal-2021-automatic,\n    title = \"Automatic Classification of Neutralization Techniques in the Narrative of Climate Change Scepticism\",\n    author = \"Bhatia, Shraey  and\n      Lau, Jey Han  and\n      Baldwin, Timothy\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.175/\",\n    doi = \"10.18653/v1/2021.naacl-main.175\",\n    pages = \"2167--2175\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.175.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.175/",
        "pdf_size": 318809,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8451342580992042085&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "School of Computing and Information Systems, The University of Melbourne; School of Computing and Information Systems, The University of Melbourne; School of Computing and Information Systems, The University of Melbourne",
        "aff_domain": "gmail.com;gmail.com;ldwin.net",
        "email": "gmail.com;gmail.com;ldwin.net",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "The University of Melbourne",
        "aff_unique_dep": "School of Computing and Information Systems",
        "aff_unique_url": "https://www.unimelb.edu.au",
        "aff_unique_abbr": "UniMelb",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Melbourne",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "2021.naacl-main.9",
        "title": "Automatic Generation of Contrast Sets from Scene Graphs: Probing the Compositional Consistency of GQA",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent works have shown that supervised models often exploit data artifacts to achieve good test scores while their performance severely degrades on samples outside their training distribution. Contrast sets (Gardneret al., 2020) quantify this phenomenon by perturbing test samples in a minimal way such that the output label is modified. While most contrast sets were created manually, requiring intensive annotation effort, we present a novel method which leverages rich semantic input representation to automatically generate contrast sets for the visual question answering task. Our method computes the answer of perturbed questions, thus vastly reducing annotation cost and enabling thorough evaluation of models\u2019 performance on various semantic aspects (e.g., spatial or relational reasoning). We demonstrate the effectiveness of our approach on the GQA dataset and its semantic scene graph image representation. We find that, despite GQA\u2019s compositionality and carefully balanced label distribution, two high-performing models drop 13-17% in accuracy compared to the original test set. Finally, we show that our automatic perturbation can be applied to the training set to mitigate the degradation in performance, opening the door to more robust models.",
        "author": "Yonatan Bitton; Gabriel Stanovsky; Roy Schwartz; Michael Elhadad",
        "authorids": "/y/yonatan-bitton/; /g/gabriel-stanovsky/; /r/roy-schwartz/; /m/michael-elhadad/",
        "bibtex": "@inproceedings{bitton-etal-2021-automatic,\n    title = \"Automatic Generation of Contrast Sets from Scene Graphs: Probing the Compositional Consistency of {GQA}\",\n    author = \"Bitton, Yonatan  and\n      Stanovsky, Gabriel  and\n      Schwartz, Roy  and\n      Elhadad, Michael\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.9/\",\n    doi = \"10.18653/v1/2021.naacl-main.9\",\n    pages = \"94--105\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.9.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.9/",
        "pdf_size": 7114149,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12969377687545138701&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "School of Computer Science and Engineering, The Hebrew University of Jerusalem, Jerusalem, Israel; School of Computer Science and Engineering, The Hebrew University of Jerusalem, Jerusalem, Israel; School of Computer Science and Engineering, The Hebrew University of Jerusalem, Jerusalem, Israel; Department of Computer Science, Ben Gurion University, Beer Sheva, Israel",
        "aff_domain": "cs.huji.ac.il;cs.huji.ac.il;cs.huji.ac.il;cs.bgu.ac.il",
        "email": "cs.huji.ac.il;cs.huji.ac.il;cs.huji.ac.il;cs.bgu.ac.il",
        "github": "https://github.com/yonatanbitton/AutoGenOfContrastSetsFromSceneGraphs",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "The Hebrew University of Jerusalem;Ben Gurion University",
        "aff_unique_dep": "School of Computer Science and Engineering;Department of Computer Science",
        "aff_unique_url": "http://www.huji.ac.il;https://www.bgu.ac.il",
        "aff_unique_abbr": "HUJI;BGU",
        "aff_campus_unique_index": "0;0;0;1",
        "aff_campus_unique": "Jerusalem;Beer Sheva",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "2021.naacl-main.423",
        "title": "BBAEG: Towards BERT-based Biomedical Adversarial Example Generation for Text Classification",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Healthcare predictive analytics aids medical decision-making, diagnosis prediction and drug review analysis. Therefore, prediction accuracy is an important criteria which also necessitates robust predictive language models. However, the models using deep learning have been proven vulnerable towards insignificantly perturbed input instances which are less likely to be misclassified by humans. Recent efforts of generating adversaries using rule-based synonyms and BERT-MLMs have been witnessed in general domain, but the ever-increasing biomedical literature poses unique challenges. We propose BBAEG (Biomedical BERT-based Adversarial Example Generation), a black-box attack algorithm for biomedical text classification, leveraging the strengths of both domain-specific synonym replacement for biomedical named entities and BERT-MLM predictions, spelling variation and number replacement. Through automatic and human evaluation on two datasets, we demonstrate that BBAEG performs stronger attack with better language fluency, semantic coherence as compared to prior work.",
        "author": "Ishani Mondal",
        "authorids": "/i/ishani-mondal/",
        "bibtex": "@inproceedings{mondal-2021-bbaeg,\n    title = \"{BBAEG}: Towards {BERT}-based Biomedical Adversarial Example Generation for Text Classification\",\n    author = \"Mondal, Ishani\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.423/\",\n    doi = \"10.18653/v1/2021.naacl-main.423\",\n    pages = \"5378--5384\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.423.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.423/",
        "pdf_size": 261401,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3291371540440923750&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1
    },
    {
        "id": "2021.naacl-main.14",
        "title": "Backtranslation Feedback Improves User Confidence in MT, Not Quality",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Translating text into a language unknown to the text\u2019s author, dubbed outbound translation, is a modern need for which the user experience has significant room for improvement, beyond the basic machine translation facility. We demonstrate this by showing three ways in which user confidence in the outbound translation, as well as its overall final quality, can be affected: backward translation, quality estimation (with alignment) and source paraphrasing. In this paper, we describe an experiment on outbound translation from English to Czech and Estonian. We examine the effects of each proposed feedback module and further focus on how the quality of machine translation systems influence these findings and the user perception of success. We show that backward translation feedback has a mixed effect on the whole process: it increases user confidence in the produced translation, but not the objective quality.",
        "author": "Vil\u00e9m Zouhar; Michal Nov\u00e1k; Mat\u00fa\u0161 \u017dilinec; Ond\u0159ej Bojar; Mateo Obreg\u00f3n; Robin L. Hill; Fr\u00e9d\u00e9ric Blain; Marina Fomicheva; Lucia Specia; Lisa Yankovskaya",
        "authorids": "/v/vilem-zouhar/; /m/michal-novak/; /m/matus-zilinec/; /o/ondrej-bojar/; /m/mateo-obregon/; /r/robin-l-hill/; /f/frederic-blain/; /m/marina-fomicheva/; /l/lucia-specia/; /l/lisa-yankovskaya/",
        "bibtex": "@inproceedings{zouhar-etal-2021-backtranslation,\n    title = \"Backtranslation Feedback Improves User Confidence in {MT}, Not Quality\",\n    author = \"Zouhar, Vil{\\'e}m  and\n      Nov{\\'a}k, Michal  and\n      {\\v{Z}}ilinec, Mat{\\'u}{\\v{s}}  and\n      Bojar, Ond{\\v{r}}ej  and\n      Obreg{\\'o}n, Mateo  and\n      Hill, Robin L.  and\n      Blain, Fr{\\'e}d{\\'e}ric  and\n      Fomicheva, Marina  and\n      Specia, Lucia  and\n      Yankovskaya, Lisa\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.14/\",\n    doi = \"10.18653/v1/2021.naacl-main.14\",\n    pages = \"151--161\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.14.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.14/",
        "pdf_size": 493333,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11718121040120977465&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Charles University, Czech Republic; Charles University, Czech Republic; Charles University, Czech Republic; Charles University, Czech Republic; University of Edinburgh; University of Edinburgh; University of Shef\ufb01eld + University of Wolverhampton, United Kingdom; University of Shef\ufb01eld; University of Shef\ufb01eld; University of Tartu, Estonia",
        "aff_domain": "ufal.cuni.cz;ufal.cuni.cz;ufal.cuni.cz;ufal.cuni.cz;ed.ac.uk;ed.ac.uk;wlv.ac.uk;sheffield.ac.uk;sheffield.ac.uk;ut.ee",
        "email": "ufal.cuni.cz;ufal.cuni.cz;ufal.cuni.cz;ufal.cuni.cz;ed.ac.uk;ed.ac.uk;wlv.ac.uk;sheffield.ac.uk;sheffield.ac.uk;ut.ee",
        "github": "github.com/zouharvi/ptakopet",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0;0;0;0;1;1;2+3;2;2;4",
        "aff_unique_norm": "Charles University;University of Edinburgh;University of Sheffield;University of Wolverhampton;University of Tartu",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://www.cuni.cz;https://www.ed.ac.uk;https://www.sheffield.ac.uk;https://www.wolverhampton.ac.uk;https://www.ut.ee",
        "aff_unique_abbr": "Charles University;Edinburgh;Sheffield;Wolverhampton;UT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;1;1;1+1;1;1;2",
        "aff_country_unique": "Czech Republic;United Kingdom;Estonia"
    },
    {
        "id": "2021.naacl-main.165",
        "title": "Be Careful about Poisoned Word Embeddings: Exploring the Vulnerability of the Embedding Layers in NLP Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent studies have revealed a security threat to natural language processing (NLP) models, called the Backdoor Attack. Victim models can maintain competitive performance on clean samples while behaving abnormally on samples with a specific trigger word inserted. Previous backdoor attacking methods usually assume that attackers have a certain degree of data knowledge, either the dataset which users would use or proxy datasets for a similar task, for implementing the data poisoning procedure. However, in this paper, we find that it is possible to hack the model in a data-free way by modifying one single word embedding vector, with almost no accuracy sacrificed on clean samples. Experimental results on sentiment analysis and sentence-pair classification tasks show that our method is more efficient and stealthier. We hope this work can raise the awareness of such a critical security risk hidden in the embedding layers of NLP models. Our code is available at https://github.com/lancopku/Embedding-Poisoning.",
        "author": "Wenkai Yang; Lei Li; Zhiyuan Zhang; Xuancheng Ren; Xu Sun; Bin He",
        "authorids": "/w/wenkai-yang/; /l/lei-li/; /z/zhiyuan-zhang/; /x/xuancheng-ren/; /x/xu-sun/; /b/bin-he/",
        "bibtex": "@inproceedings{yang-etal-2021-careful,\n    title = \"Be Careful about Poisoned Word Embeddings: Exploring the Vulnerability of the Embedding Layers in {NLP} Models\",\n    author = \"Yang, Wenkai  and\n      Li, Lei  and\n      Zhang, Zhiyuan  and\n      Ren, Xuancheng  and\n      Sun, Xu  and\n      He, Bin\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.165/\",\n    doi = \"10.18653/v1/2021.naacl-main.165\",\n    pages = \"2048--2058\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.165.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.165/",
        "pdf_size": 559338,
        "gs_citation": 171,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7789098190159855863&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Center for Data Science, Peking University; MOE Key Laboratory of Computational Linguistics, School of EECS, Peking University; MOE Key Laboratory of Computational Linguistics, School of EECS, Peking University; MOE Key Laboratory of Computational Linguistics, School of EECS, Peking University; Center for Data Science, Peking University + MOE Key Laboratory of Computational Linguistics, School of EECS, Peking University; Huawei Noah\u2019s Ark Lab",
        "aff_domain": "stu.pku.edu.cn;stu.pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn;huawei.com",
        "email": "stu.pku.edu.cn;stu.pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn;huawei.com",
        "github": "https://github.com/lancopku/Embedding-Poisoning",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0+0;1",
        "aff_unique_norm": "Peking University;Huawei",
        "aff_unique_dep": "Center for Data Science;Noah\u2019s Ark Lab",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.huawei.com",
        "aff_unique_abbr": "PKU;Huawei",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0;0;0;0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2021.naacl-industry.38",
        "title": "Benchmarking Commercial Intent Detection Services with Practice-Driven Evaluations",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Intent detection is a key component of modern goal-oriented dialog systems that accomplish a user task by predicting the intent of users\u2019 text input. There are three primary challenges in designing robust and accurate intent detection models. First, typical intent detection models require a large amount of labeled data to achieve high accuracy. Unfortunately, in practical scenarios it is more common to find small, unbalanced, and noisy datasets. Secondly, even with large training data, the intent detection models can see a different distribution of test data when being deployed in the real world, leading to poor accuracy. Finally, a practical intent detection model must be computationally efficient in both training and single query inference so that it can be used continuously and re-trained frequently. We benchmark intent detection methods on a variety of datasets. Our results show that Watson Assistant\u2019s intent detection model outperforms other commercial solutions and is comparable to large pretrained language models while requiring only a fraction of computational resources and training data. Watson Assistant demonstrates a higher degree of robustness when the training and test distributions differ.",
        "author": "Haode Qi; Lin Pan; Atin Sood; Abhishek Shah; Ladislav Kunc; Mo Yu; Saloni Potdar",
        "authorids": "/h/haode-qi/; /l/lin-pan/; /a/atin-sood/; /a/abhishek-shah/; /l/ladislav-kunc/; /m/mo-yu/; /s/saloni-potdar/",
        "bibtex": "@inproceedings{qi-etal-2021-benchmarking,\n    title = \"Benchmarking Commercial Intent Detection Services with Practice-Driven Evaluations\",\n    author = \"Qi, Haode  and\n      Pan, Lin  and\n      Sood, Atin  and\n      Shah, Abhishek  and\n      Kunc, Ladislav  and\n      Yu, Mo  and\n      Potdar, Saloni\",\n    editor = \"Kim, Young-bum  and\n      Li, Yunyao  and\n      Rambow, Owen\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-industry.38/\",\n    doi = \"10.18653/v1/2021.naacl-industry.38\",\n    pages = \"304--310\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-industry.38.pdf",
        "site": "https://aclanthology.org/2021.naacl-industry.38/",
        "pdf_size": 261747,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9967861989729742141&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "IBM Watson; IBM Watson; IBM Watson; IBM Watson; IBM Watson; MIT-IBM Watson AI Lab; IBM Watson",
        "aff_domain": "ibm.com;us.ibm.com;us.ibm.com;ibm.com;ibm.com;us.ibm.com;us.ibm.com",
        "email": "ibm.com;us.ibm.com;us.ibm.com;ibm.com;ibm.com;us.ibm.com;us.ibm.com",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;1;0",
        "aff_unique_norm": "IBM;Massachusetts Institute of Technology",
        "aff_unique_dep": "IBM Watson;IBM Watson AI Lab",
        "aff_unique_url": "https://www.ibm.com/watson;https://www.mitibmwatsonailab.org",
        "aff_unique_abbr": "IBM Watson;MIT-IBM AI Lab",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.271",
        "title": "Better Feature Integration for Named Entity Recognition",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "It has been shown that named entity recognition (NER) could benefit from incorporating the long-distance structured information captured by dependency trees. We believe this is because both types of features - the contextual information captured by the linear sequences and the structured information captured by the dependency trees may complement each other. However, existing approaches largely focused on stacking the LSTM and graph neural networks such as graph convolutional networks (GCNs) for building improved NER models, where the exact interaction mechanism between the two types of features is not very clear, and the performance gain does not appear to be significant. In this work, we propose a simple and robust solution to incorporate both types of features with our Synergized-LSTM (Syn-LSTM), which clearly captures how the two types of features interact. We conduct extensive experiments on several standard datasets across four languages. The results demonstrate that the proposed model achieves better performance than previous approaches while requiring fewer parameters. Our further analysis demonstrates that our model can capture longer dependencies compared with strong baselines.",
        "author": "Lu Xu; Zhanming Jie; Wei Lu; Lidong Bing",
        "authorids": "/l/lu-xu/; /z/zhanming-jie/; /w/wei-lu/; /l/lidong-bing/",
        "bibtex": "@inproceedings{xu-etal-2021-better,\n    title = \"Better Feature Integration for Named Entity Recognition\",\n    author = \"Xu, Lu  and\n      Jie, Zhanming  and\n      Lu, Wei  and\n      Bing, Lidong\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.271/\",\n    doi = \"10.18653/v1/2021.naacl-main.271\",\n    pages = \"3457--3469\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.271.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.271/",
        "pdf_size": 500428,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1077914306139596804&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "StatNLP Research Group, Singapore University of Technology and Design+DAMO Academy, Alibaba Group; StatNLP Research Group, Singapore University of Technology and Design+ByteDance; StatNLP Research Group, Singapore University of Technology and Design; DAMO Academy, Alibaba Group",
        "aff_domain": "mymail.sutd.edu.sg;bytedance.com;sutd.edu.sg;alibaba-inc.com",
        "email": "mymail.sutd.edu.sg;bytedance.com;sutd.edu.sg;alibaba-inc.com",
        "github": "https://github.com/xuuuluuu/SynLSTM-for-NER",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0+2;0;1",
        "aff_unique_norm": "Singapore University of Technology and Design;Alibaba Group;ByteDance",
        "aff_unique_dep": "StatNLP Research Group;DAMO Academy;",
        "aff_unique_url": "https://www.sutd.edu.sg;https://www.alibaba-group.com;https://www.bytedance.com",
        "aff_unique_abbr": "SUTD;Alibaba;ByteDance",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0+1;0;1",
        "aff_country_unique": "Singapore;China"
    },
    {
        "id": "2021.naacl-main.204",
        "title": "Beyond Black & White: Leveraging Annotator Disagreement via Soft-Label Multi-Task Learning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Supervised learning assumes that a ground truth label exists. However, the reliability of this ground truth depends on human annotators, who often disagree. Prior work has shown that this disagreement can be helpful in training models. We propose a novel method to incorporate this disagreement as information: in addition to the standard error computation, we use soft-labels (i.e., probability distributions over the annotator labels) as an auxiliary task in a multi-task neural network. We measure the divergence between the predictions and the target soft-labels with several loss-functions and evaluate the models on various NLP tasks. We find that the soft-label prediction auxiliary task reduces the penalty for errors on ambiguous entities, and thereby mitigates overfitting. It significantly improves performance across tasks, beyond the standard approach and prior work.",
        "author": "Tommaso Fornaciari; Alexandra Uma; Silviu Paun; Barbara Plank; Dirk Hovy; Massimo Poesio",
        "authorids": "/t/tommaso-fornaciari/; /a/alexandra-uma/; /s/silviu-paun/; /b/barbara-plank/; /d/dirk-hovy/; /m/massimo-poesio/",
        "bibtex": "@inproceedings{fornaciari-etal-2021-beyond,\n    title = \"Beyond Black {\\&} White: Leveraging Annotator Disagreement via Soft-Label Multi-Task Learning\",\n    author = \"Fornaciari, Tommaso  and\n      Uma, Alexandra  and\n      Paun, Silviu  and\n      Plank, Barbara  and\n      Hovy, Dirk  and\n      Poesio, Massimo\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.204/\",\n    doi = \"10.18653/v1/2021.naacl-main.204\",\n    pages = \"2591--2597\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.204.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.204/",
        "pdf_size": 242880,
        "gs_citation": 132,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9101139989055780196&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Universit `a Bocconi; IT University of Copenhagen; Queen Mary University; Universit `a Bocconi; Queen Mary University; Queen Mary University",
        "aff_domain": "unibocconi.it;itu.dk;qmul.ac.uk;unibocconi.it;qmul.ac.uk;qmul.ac.uk",
        "email": "unibocconi.it;itu.dk;qmul.ac.uk;unibocconi.it;qmul.ac.uk;qmul.ac.uk",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;2;0;2;2",
        "aff_unique_norm": "Universit\u00e0 Bocconi;IT University of Copenhagen;Queen Mary University of London",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.unibocconi.eu;https://itu.dk;https://www.qmul.ac.uk",
        "aff_unique_abbr": "Unibo;ITU;QMUL",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";London",
        "aff_country_unique_index": "0;1;2;0;2;2",
        "aff_country_unique": "Italy;Denmark;United Kingdom"
    },
    {
        "id": "2021.naacl-main.295",
        "title": "Beyond Fair Pay: Ethical Implications of NLP Crowdsourcing",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The use of crowdworkers in NLP research is growing rapidly, in tandem with the exponential increase in research production in machine learning and AI. Ethical discussion regarding the use of crowdworkers within the NLP research community is typically confined in scope to issues related to labor conditions such as fair pay. We draw attention to the lack of ethical considerations related to the various tasks performed by workers, including labeling, evaluation, and production. We find that the Final Rule, the common ethical framework used by researchers, did not anticipate the use of online crowdsourcing platforms for data collection, resulting in gaps between the spirit and practice of human-subjects ethics in NLP research. We enumerate common scenarios where crowdworkers performing NLP tasks are at risk of harm. We thus recommend that researchers evaluate these risks by considering the three ethical principles set up by the Belmont Report. We also clarify some common misconceptions regarding the Institutional Review Board (IRB) application. We hope this paper will serve to reopen the discussion within our community regarding the ethical use of crowdworkers.",
        "author": "Boaz Shmueli; Jan Fell; Soumya Ray; Lun-Wei Ku",
        "authorids": "/b/boaz-shmueli/; /j/jan-fell/; /s/soumya-ray/; /l/lun-wei-ku/",
        "bibtex": "@inproceedings{shmueli-etal-2021-beyond,\n    title = \"Beyond Fair Pay: Ethical Implications of {NLP} Crowdsourcing\",\n    author = \"Shmueli, Boaz  and\n      Fell, Jan  and\n      Ray, Soumya  and\n      Ku, Lun-Wei\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.295/\",\n    doi = \"10.18653/v1/2021.naacl-main.295\",\n    pages = \"3758--3769\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.295.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.295/",
        "pdf_size": 300732,
        "gs_citation": 109,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16169693950219340382&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Social Networks and Human-Centered Computing, TIGP, Academia Sinica + Institute of Service Science, National Tsing Hua University + Institute of Information Science, Academia Sinica; Institute of Service Science, National Tsing Hua University; Institute of Service Science, National Tsing Hua University; Institute of Information Science, Academia Sinica",
        "aff_domain": "iis.sinica.edu.tw; ; ; ",
        "email": "iis.sinica.edu.tw; ; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1+0;1;1;0",
        "aff_unique_norm": "Academia Sinica;National Tsing Hua University",
        "aff_unique_dep": "Social Networks and Human-Centered Computing, TIGP;Institute of Service Science",
        "aff_unique_url": "https://www.sinica.edu.tw;https://www.nthu.edu.tw",
        "aff_unique_abbr": "Academia Sinica;NTHU",
        "aff_campus_unique_index": "0+0+0;0;0;0",
        "aff_campus_unique": "Taiwan",
        "aff_country_unique_index": "0+0+0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2021.naacl-main.172",
        "title": "Blow the Dog Whistle: A Chinese Dataset for Cant Understanding with Common Sense and World Knowledge",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Cant is important for understanding advertising, comedies and dog-whistle politics. However, computational research on cant is hindered by a lack of available datasets. In this paper, we propose a large and diverse Chinese dataset for creating and understanding cant from a computational linguistics perspective. We formulate a task for cant understanding and provide both quantitative and qualitative analysis for tested word embedding similarity and pretrained language models. Experiments suggest that such a task requires deep language understanding, common sense, and world knowledge and thus can be a good testbed for pretrained language models and help models perform better on other tasks.",
        "author": "Canwen Xu; Wangchunshu Zhou; Tao Ge; Ke Xu; Julian McAuley; Furu Wei",
        "authorids": "/c/canwen-xu/; /w/wangchunshu-zhou/; /t/tao-ge/; /k/ke-xu/; /j/julian-mcauley/; /f/furu-wei/",
        "bibtex": "@inproceedings{xu-etal-2021-blow,\n    title = \"Blow the Dog Whistle: A {C}hinese Dataset for Cant Understanding with Common Sense and World Knowledge\",\n    author = \"Xu, Canwen  and\n      Zhou, Wangchunshu  and\n      Ge, Tao  and\n      Xu, Ke  and\n      McAuley, Julian  and\n      Wei, Furu\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.172/\",\n    doi = \"10.18653/v1/2021.naacl-main.172\",\n    pages = \"2139--2145\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.172.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.172/",
        "pdf_size": 1730235,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2116676093357783871&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "University of California, San Diego+Microsoft Research Asia; Beihang University+Microsoft Research Asia; Microsoft Research Asia; Beihang University; University of California, San Diego+Microsoft Research Asia; Microsoft Research Asia",
        "aff_domain": "ucsd.edu;ucsd.edu;microsoft.com;microsoft.com;buaa.edu.cn;nlsde.buaa.edu.cn",
        "email": "ucsd.edu;ucsd.edu;microsoft.com;microsoft.com;buaa.edu.cn;nlsde.buaa.edu.cn",
        "github": "https://github.com/JetRunner/dogwhistle",
        "project": "https://competitions.codalab.org/competitions/30451",
        "author_num": 6,
        "aff_unique_index": "0+1;2+1;1;2;0+1;1",
        "aff_unique_norm": "University of California, San Diego;Microsoft Research;Beihang University",
        "aff_unique_dep": ";Research;",
        "aff_unique_url": "https://www.ucsd.edu;https://www.microsoft.com/en-us/research/group/asia;http://www.buaa.edu.cn/",
        "aff_unique_abbr": "UCSD;MSR Asia;BUAA",
        "aff_campus_unique_index": "0+1;1;1;0+1;1",
        "aff_campus_unique": "San Diego;Asia;",
        "aff_country_unique_index": "0+1;1+1;1;1;0+1;1",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2021.naacl-industry.7",
        "title": "Bootstrapping a Music Voice Assistant with Weak Supervision",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "One of the first building blocks to create a voice assistant relates to the task of tagging entities or attributes in user queries. This can be particularly challenging when entities are in the tenth of millions, as is the case of e.g. music catalogs. Training slot tagging models at an industrial scale requires large quantities of accurately labeled user queries, which are often hard and costly to gather. On the other hand, voice assistants typically collect plenty of unlabeled queries that often remain unexploited. This paper presents a weakly-supervised methodology to label large amounts of voice query logs, enhanced with a manual filtering step. Our experimental evaluations show that slot tagging models trained on weakly-supervised data outperform models trained on hand-annotated or synthetic data, at a lower cost. Further, manual filtering of weakly-supervised data leads to a very significant reduction in Sentence Error Rate, while allowing us to drastically reduce human curation efforts from weeks to hours, with respect to hand-annotation of queries. The method is applied to successfully bootstrap a slot tagging system for a major music streaming service that currently serves several tens of thousands of daily voice queries.",
        "author": "Sergio Oramas; Massimo Quadrana; Fabien Gouyon",
        "authorids": "/s/sergio-oramas/; /m/massimo-quadrana/; /f/fabien-gouyon/",
        "bibtex": "@inproceedings{oramas-etal-2021-bootstrapping,\n    title = \"Bootstrapping a Music Voice Assistant with Weak Supervision\",\n    author = \"Oramas, Sergio  and\n      Quadrana, Massimo  and\n      Gouyon, Fabien\",\n    editor = \"Kim, Young-bum  and\n      Li, Yunyao  and\n      Rambow, Owen\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-industry.7/\",\n    doi = \"10.18653/v1/2021.naacl-industry.7\",\n    pages = \"49--55\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-industry.7.pdf",
        "site": "https://aclanthology.org/2021.naacl-industry.7/",
        "pdf_size": 296003,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14256895236700068397&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3
    },
    {
        "id": "2021.naacl-main.235",
        "title": "Bot-Adversarial Dialogue for Safe Conversational Agents",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Conversational agents trained on large unlabeled corpora of human interactions will learn patterns and mimic behaviors therein, which include offensive or otherwise toxic behavior. We introduce a new human-and-model-in-the-loop framework for evaluating the toxicity of such models, and compare a variety of existing methods in both the cases of non-adversarial and adversarial users that expose their weaknesses. We then go on to propose two novel methods for safe conversational agents, by either training on data from our new human-and-model-in-the-loop framework in a two-stage system, or \u201dbaking-in\u201d safety to the generative model itself. We find our new techniques are (i) safer than existing models; while (ii) maintaining usability metrics such as engagingness relative to state-of-the-art chatbots. In contrast, we expose serious safety issues in existing standard systems like GPT2, DialoGPT, and BlenderBot.",
        "author": "Jing Xu; Da Ju; Margaret Li; Y-Lan Boureau; Jason Weston; Emily Dinan",
        "authorids": "/j/jing-xu/; /d/da-ju/; /m/margaret-li/; /y/y-lan-boureau/; /j/jason-weston/; /e/emily-dinan/",
        "bibtex": "@inproceedings{xu-etal-2021-bot,\n    title = \"Bot-Adversarial Dialogue for Safe Conversational Agents\",\n    author = \"Xu, Jing  and\n      Ju, Da  and\n      Li, Margaret  and\n      Boureau, Y-Lan  and\n      Weston, Jason  and\n      Dinan, Emily\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.235/\",\n    doi = \"10.18653/v1/2021.naacl-main.235\",\n    pages = \"2950--2968\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.235.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.235/",
        "pdf_size": 1336165,
        "gs_citation": 165,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11144401181243342983&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Facebook AI Research; Facebook AI Research; University of Washington; Facebook AI Research; Facebook AI Research; Facebook AI Research",
        "aff_domain": "fb.com;fb.com;cs.washington.edu;fb.com;fb.com;fb.com",
        "email": "fb.com;fb.com;cs.washington.edu;fb.com;fb.com;fb.com",
        "github": "",
        "project": "https://parl.ai/projects/safety_recipes/",
        "author_num": 6,
        "aff_unique_index": "0;0;1;0;0;0",
        "aff_unique_norm": "Facebook;University of Washington",
        "aff_unique_dep": "Facebook AI Research;",
        "aff_unique_url": "https://research.facebook.com;https://www.washington.edu",
        "aff_unique_abbr": "FAIR;UW",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.464",
        "title": "Breadth First Reasoning Graph for Multi-hop Question Answering",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recently Graph Neural Network (GNN) has been used as a promising tool in multi-hop question answering task. However, the unnecessary updations and simple edge constructions prevent an accurate answer span extraction in a more direct and interpretable way. In this paper, we propose a novel model of Breadth First Reasoning Graph (BFR-Graph), which presents a new message passing way that better conforms to the reasoning process. In BFR-Graph, the reasoning message is required to start from the question node and pass to the next sentences node hop by hop until all the edges have been passed, which can effectively prevent each node from over-smoothing or being updated multiple times unnecessarily. To introduce more semantics, we also define the reasoning graph as a weighted graph with considering the number of co-occurrence entities and the distance between sentences. Then we present a more direct and interpretable way to aggregate scores from different levels of granularity based on the GNN. On HotpotQA leaderboard, the proposed BFR-Graph achieves state-of-the-art on answer span prediction.",
        "author": "Yongjie Huang; Meng Yang",
        "authorids": "/y/yongjie-huang/; /m/meng-yang/",
        "bibtex": "@inproceedings{huang-yang-2021-breadth,\n    title = \"Breadth First Reasoning Graph for Multi-hop Question Answering\",\n    author = \"Huang, Yongjie  and\n      Yang, Meng\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.464/\",\n    doi = \"10.18653/v1/2021.naacl-main.464\",\n    pages = \"5810--5821\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.464.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.464/",
        "pdf_size": 7916889,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4626157505392367549&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "School of Computer Science and Engineering, Sun Yat-sen University / Guangzhou, China + Key Laboratory of Machine Intelligence and Advanced Computing, Sun Yat-sen University, Ministry of Education / China; School of Computer Science and Engineering, Sun Yat-sen University / Guangzhou, China + Key Laboratory of Machine Intelligence and Advanced Computing, Sun Yat-sen University, Ministry of Education / China",
        "aff_domain": "mail2.sysu.edu.cn;mail.sysu.edu.cn",
        "email": "mail2.sysu.edu.cn;mail.sysu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+0;0+0",
        "aff_unique_norm": "Sun Yat-sen University",
        "aff_unique_dep": "School of Computer Science and Engineering",
        "aff_unique_url": "http://www.sysu.edu.cn",
        "aff_unique_abbr": "SYSU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Guangzhou;",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2021.naacl-main.131",
        "title": "Bridging Resolution: Making Sense of the State of the Art",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "While Yu and Poesio (2020) have recently demonstrated the superiority of their neural multi-task learning (MTL) model to rule-based approaches for bridging anaphora resolution, there is little understanding of (1) how it is better than the rule-based approaches (e.g., are the two approaches making similar or complementary mistakes?) and (2) what should be improved. To shed light on these issues, we (1) propose a hybrid rule-based and MTL approach that would enable a better understanding of their comparative strengths and weaknesses; and (2) perform a manual analysis of the errors made by the MTL model.",
        "author": "Hideo Kobayashi; Vincent Ng",
        "authorids": "/h/hideo-kobayashi/; /v/vincent-ng/",
        "bibtex": "@inproceedings{kobayashi-ng-2021-bridging,\n    title = \"Bridging Resolution: Making Sense of the State of the Art\",\n    author = \"Kobayashi, Hideo  and\n      Ng, Vincent\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.131/\",\n    doi = \"10.18653/v1/2021.naacl-main.131\",\n    pages = \"1652--1659\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.131.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.131/",
        "pdf_size": 189581,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=436872175941569722&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Human Language Technology Research Institute, University of Texas at Dallas; Human Language Technology Research Institute, University of Texas at Dallas",
        "aff_domain": "hlt.utdallas.edu;hlt.utdallas.edu",
        "email": "hlt.utdallas.edu;hlt.utdallas.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Texas at Dallas",
        "aff_unique_dep": "Human Language Technology Research Institute",
        "aff_unique_url": "https://www.utdallas.edu",
        "aff_unique_abbr": "UT Dallas",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Dallas",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.289",
        "title": "CLEVR_HYP: A Challenge Dataset and Baselines for Visual Question Answering with Hypothetical Actions over Images",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Most existing research on visual question answering (VQA) is limited to information explicitly present in an image or a video. In this paper, we take visual understanding to a higher level where systems are challenged to answer questions that involve mentally simulating the hypothetical consequences of performing specific actions in a given scenario. Towards that end, we formulate a vision-language question answering task based on the CLEVR (Johnson et. al., 2017) dataset. We then modify the best existing VQA methods and propose baseline solvers for this task. Finally, we motivate the development of better vision-language models by providing insights about the capability of diverse architectures to perform joint reasoning over image-text modality. Our dataset setup scripts and codes will be made publicly available at https://github.com/shailaja183/clevr_hyp.",
        "author": "Shailaja Keyur Sampat; Akshay Kumar; Yezhou Yang; Chitta Baral",
        "authorids": "/s/shailaja-keyur-sampat/; /a/akshay-kumar/; /y/yezhou-yang/; /c/chitta-baral/",
        "bibtex": "@inproceedings{sampat-etal-2021-clevr,\n    title = \"{CLEVR}{\\_}{HYP}: A Challenge Dataset and Baselines for Visual Question Answering with Hypothetical Actions over Images\",\n    author = \"Sampat, Shailaja Keyur  and\n      Kumar, Akshay  and\n      Yang, Yezhou  and\n      Baral, Chitta\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.289/\",\n    doi = \"10.18653/v1/2021.naacl-main.289\",\n    pages = \"3692--3709\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.289.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.289/",
        "pdf_size": 6711230,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2894082497923967377&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Arizona State Universiy, USA; Arizona State Universiy, USA; Arizona State Universiy, USA; Arizona State Universiy, USA",
        "aff_domain": "asu.edu;asu.edu;asu.edu;asu.edu",
        "email": "asu.edu;asu.edu;asu.edu;asu.edu",
        "github": "https://github.com/shailaja183/clevr_hyp",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Arizona State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.asu.edu",
        "aff_unique_abbr": "ASU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.241",
        "title": "COIL: Revisit Exact Lexical Match in Information Retrieval with Contextualized Inverted List",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Classical information retrieval systems such as BM25 rely on exact lexical match and can carry out search efficiently with inverted list index. Recent neural IR models shifts towards soft matching all query document terms, but they lose the computation efficiency of exact match systems. This paper presents COIL, a contextualized exact match retrieval architecture, where scoring is based on overlapping query document tokens\u2019 contextualized representations. The new architecture stores contextualized token representations in inverted lists, bringing together the efficiency of exact match and the representation power of deep language models. Our experimental results show COIL outperforms classical lexical retrievers and state-of-the-art deep LM retrievers with similar or smaller latency.",
        "author": "Luyu Gao; Zhuyun Dai; Jamie Callan",
        "authorids": "/l/luyu-gao/; /z/zhuyun-dai/; /j/jamie-callan/",
        "bibtex": "@inproceedings{gao-etal-2021-coil,\n    title = \"{COIL}: Revisit Exact Lexical Match in Information Retrieval with Contextualized Inverted List\",\n    author = \"Gao, Luyu  and\n      Dai, Zhuyun  and\n      Callan, Jamie\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.241/\",\n    doi = \"10.18653/v1/2021.naacl-main.241\",\n    pages = \"3030--3042\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.241.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.241/",
        "pdf_size": 696244,
        "gs_citation": 260,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12719228274140069355&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Language Technologies Institute, Carnegie Mellon University; Language Technologies Institute, Carnegie Mellon University; Language Technologies Institute, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "github": "https://github.com/luyug/COIL",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Language Technologies Institute",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-demos.8",
        "title": "COVID-19 Literature Knowledge Graph Construction and Drug Repurposing Report Generation",
        "track": "main",
        "status": "System Demonstrations",
        "award": true,
        "abstract": "To combat COVID-19, both clinicians and scientists need to digest the vast amount of relevant biomedical knowledge in literature to understand the disease mechanism and the related biological functions. We have developed a novel and comprehensive knowledge discovery framework, COVID-KG to extract fine-grained multimedia knowledge elements (entities, relations and events) from scientific literature. We then exploit the constructed multimedia knowledge graphs (KGs) for question answering and report generation, using drug repurposing as a case study. Our framework also provides detailed contextual sentences, subfigures, and knowledge subgraphs as evidence. All of the data, KGs, reports.",
        "author": "Qingyun Wang; Manling Li; Xuan Wang; Nikolaus Parulian; Guangxing Han; Jiawei Ma; Jingxuan Tu; Ying Lin; Ranran Haoran Zhang; Weili Liu; Aabhas Chauhan; Yingjun Guan; Bangzheng Li; Ruisong Li; Xiangchen Song; Yi Fung; Heng Ji; Jiawei Han; Shih-Fu Chang; James Pustejovsky; Jasmine Rah; David Liem; Ahmed ELsayed; Martha Palmer; Clare Voss; Cynthia Schneider; Boyan Onyshkevych",
        "authorids": "/q/qingyun-wang/; /m/manling-li/; /x/xuan-wang/; /n/nikolaus-parulian/; /g/guangxing-han/; /j/jiawei-ma/; /j/jingxuan-tu/; /y/ying-lin/; /r/ranran-haoran-zhang/; /w/weili-liu/; /a/aabhas-chauhan/; /y/yingjun-guan/; /b/bangzheng-li/; /r/ruisong-li/; /x/xiangchen-song/; /y/yi-fung/; /h/heng-ji/; /j/jiawei-han/; /s/shih-fu-chang/; /j/james-pustejovsky/; /j/jasmine-rah/; /d/david-liem/; /a/ahmed-elsayed/; /m/martha-palmer/; /c/clare-voss/; /c/cynthia-schneider/; /b/boyan-onyshkevych/",
        "bibtex": "@inproceedings{wang-etal-2021-covid,\n    title = \"{COVID}-19 Literature Knowledge Graph Construction and Drug Repurposing Report Generation\",\n    author = \"Wang, Qingyun  and\n      Li, Manling  and\n      Wang, Xuan  and\n      Parulian, Nikolaus  and\n      Han, Guangxing  and\n      Ma, Jiawei  and\n      Tu, Jingxuan  and\n      Lin, Ying  and\n      Zhang, Ranran Haoran  and\n      Liu, Weili  and\n      Chauhan, Aabhas  and\n      Guan, Yingjun  and\n      Li, Bangzheng  and\n      Li, Ruisong  and\n      Song, Xiangchen  and\n      Fung, Yi  and\n      Ji, Heng  and\n      Han, Jiawei  and\n      Chang, Shih-Fu  and\n      Pustejovsky, James  and\n      Rah, Jasmine  and\n      Liem, David  and\n      ELsayed, Ahmed  and\n      Palmer, Martha  and\n      Voss, Clare  and\n      Schneider, Cynthia  and\n      Onyshkevych, Boyan\",\n    editor = \"Sil, Avi  and\n      Lin, Xi Victoria\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-demos.8/\",\n    doi = \"10.18653/v1/2021.naacl-demos.8\",\n    pages = \"66--77\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-demos.8.pdf",
        "site": "https://aclanthology.org/2021.naacl-demos.8/",
        "pdf_size": 2233889,
        "gs_citation": 165,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1160437947728156482&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": "University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; Columbia University; Columbia University; Brandeis University; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; Columbia University; Brandeis University; University of Washington; University of California, Los Angeles; Colorado University; Colorado University; Army Research Lab; QS29; Department of Defense",
        "aff_domain": "illinois.edu;illinois.edu;columbia.edu; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ",
        "email": "illinois.edu;illinois.edu;columbia.edu; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ",
        "github": "",
        "project": "http://blender.cs.illinois.edu/covid19/",
        "author_num": 27,
        "aff_unique_index": "0;0;0;0;1;1;2;0;0;0;0;0;0;0;0;0;0;0;1;2;3;4;5;5;6;8",
        "aff_unique_norm": "University of Illinois at Urbana-Champaign;Columbia University;Brandeis University;University of Washington;University of California, Los Angeles;University of Colorado;Army Research Laboratory;;United States Department of Defense",
        "aff_unique_dep": ";;;;;;;;Department of Defense",
        "aff_unique_url": "https://illinois.edu;https://www.columbia.edu;https://www.brandeis.edu;https://www.washington.edu;https://www.ucla.edu;https://www.colorado.edu;https://www.arl.army.mil;;https://www.defense.gov",
        "aff_unique_abbr": "UIUC;Columbia;Brandeis;UW;UCLA;CU;ARL;;DoD",
        "aff_campus_unique_index": "0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;2",
        "aff_campus_unique": "Urbana-Champaign;;Los Angeles",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "2021.naacl-main.173",
        "title": "COVID-19 Named Entity Recognition for Vietnamese",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The current COVID-19 pandemic has lead to the creation of many corpora that facilitate NLP research and downstream applications to help fight the pandemic. However, most of these corpora are exclusively for English. As the pandemic is a global problem, it is worth creating COVID-19 related datasets for languages other than English. In this paper, we present the first manually-annotated COVID-19 domain-specific dataset for Vietnamese. Particularly, our dataset is annotated for the named entity recognition (NER) task with newly-defined entity types that can be used in other future epidemics. Our dataset also contains the largest number of entities compared to existing Vietnamese NER datasets. We empirically conduct experiments using strong baselines on our dataset, and find that: automatic Vietnamese word segmentation helps improve the NER results and the highest performances are obtained by fine-tuning pre-trained language models where the monolingual model PhoBERT for Vietnamese (Nguyen and Nguyen, 2020) produces higher results than the multilingual model XLM-R (Conneau et al., 2020). We publicly release our dataset at: https://github.com/VinAIResearch/PhoNER_COVID19",
        "author": "Thinh Hung Truong; Mai Hoang Dao; Dat Quoc Nguyen",
        "authorids": "/t/thinh-hung-truong/; /m/mai-hoang-dao/; /d/dat-quoc-nguyen/",
        "bibtex": "@inproceedings{truong-etal-2021-covid,\n    title = \"{COVID}-19 Named Entity Recognition for {V}ietnamese\",\n    author = \"Truong, Thinh Hung  and\n      Dao, Mai Hoang  and\n      Nguyen, Dat Quoc\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.173/\",\n    doi = \"10.18653/v1/2021.naacl-main.173\",\n    pages = \"2146--2153\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.173.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.173/",
        "pdf_size": 190942,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10887017084294407826&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "VinAI Research, Hanoi, Vietnam; VinAI Research, Hanoi, Vietnam; VinAI Research, Hanoi, Vietnam",
        "aff_domain": "vinai.io;vinai.io;vinai.io",
        "email": "vinai.io;vinai.io;vinai.io",
        "github": "https://github.com/VinAIResearch/PhoNER_COVID19",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "VinAI Research",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.vin.ai",
        "aff_unique_abbr": "VinAI",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Hanoi",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Vietnam"
    },
    {
        "id": "2021.naacl-main.265",
        "title": "CREAD: Combined Resolution of Ellipses and Anaphora in Dialogues",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Anaphora and ellipses are two common phenomena in dialogues. Without resolving referring expressions and information omission, dialogue systems may fail to generate consistent and coherent responses. Traditionally, anaphora is resolved by coreference resolution and ellipses by query rewrite. In this work, we propose a novel joint learning framework of modeling coreference resolution and query rewriting for complex, multi-turn dialogue understanding. Given an ongoing dialogue between a user and a dialogue assistant, for the user query, our joint learning model first predicts coreference links between the query and the dialogue context, and then generates a self-contained rewritten user query. To evaluate our model, we annotate a dialogue based coreference resolution dataset, MuDoCo, with rewritten queries. Results show that the performance of query rewrite can be substantially boosted (+2.3% F1) with the aid of coreference modeling. Furthermore, our joint model outperforms the state-of-the-art coreference resolution model (+2% F1) on this dataset.",
        "author": "Bo-Hsiang Tseng; Shruti Bhargava; Jiarui Lu; Joel Ruben Antony Moniz; Dhivya Piraviperumal; Lin Li; Hong Yu",
        "authorids": "/b/bo-hsiang-tseng/; /s/shruti-bhargava/; /j/jiarui-lu/; /j/joel-ruben-antony-moniz/; /d/dhivya-piraviperumal/; /l/lin-li/; /h/hong-yu/",
        "bibtex": "https://aclanthology.org/2021.naacl-main.265.bib",
        "pdf": "https://aclanthology.org/2021.naacl-main.265.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.265/",
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4562394037205915135&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7
    },
    {
        "id": "2021.naacl-main.254",
        "title": "CaSiNo: A Corpus of Campsite Negotiation Dialogues for Automatic Negotiation Systems",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Automated systems that negotiate with humans have broad applications in pedagogy and conversational AI. To advance the development of practical negotiation systems, we present CaSiNo: a novel corpus of over a thousand negotiation dialogues in English. Participants take the role of campsite neighbors and negotiate for food, water, and firewood packages for their upcoming trip. Our design results in diverse and linguistically rich negotiations while maintaining a tractable, closed-domain environment. Inspired by the literature in human-human negotiations, we annotate persuasion strategies and perform correlation analysis to understand how the dialogue behaviors are associated with the negotiation performance. We further propose and evaluate a multi-task framework to recognize these strategies in a given utterance. We find that multi-task learning substantially improves the performance for all strategy labels, especially for the ones that are the most skewed. We release the dataset, annotations, and the code to propel future work in human-machine negotiations: https://github.com/kushalchawla/CaSiNo",
        "author": "Kushal Chawla; Jaysa Ramirez; Rene Clever; Gale Lucas; Jonathan May; Jonathan Gratch",
        "authorids": "/k/kushal-chawla/; /j/jaysa-ramirez/; /r/rene-clever/; /g/gale-lucas/; /j/jonathan-may/; /j/jonathan-gratch/",
        "bibtex": "@inproceedings{chawla-etal-2021-casino,\n    title = \"{C}a{S}i{N}o: A Corpus of Campsite Negotiation Dialogues for Automatic Negotiation Systems\",\n    author = \"Chawla, Kushal  and\n      Ramirez, Jaysa  and\n      Clever, Rene  and\n      Lucas, Gale  and\n      May, Jonathan  and\n      Gratch, Jonathan\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.254/\",\n    doi = \"10.18653/v1/2021.naacl-main.254\",\n    pages = \"3167--3185\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.254.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.254/",
        "pdf_size": 1764766,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5356555890364899259&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "https://github.com/kushalchawla/CaSiNo",
        "project": "",
        "author_num": 6
    },
    {
        "id": "2021.naacl-main.209",
        "title": "Can Latent Alignments Improve Autoregressive Machine Translation?",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Latent alignment objectives such as CTC and AXE significantly improve non-autoregressive machine translation models. Can they improve autoregressive models as well? We explore the possibility of training autoregressive machine translation models with latent alignment objectives, and observe that, in practice, this approach results in degenerate models. We provide a theoretical explanation for these empirical results, and prove that latent alignment objectives are incompatible with teacher forcing.",
        "author": "Adi Haviv; Lior Vassertail; Omer Levy",
        "authorids": "/a/adi-haviv/; /l/lior-vassertail/; /o/omer-levy/",
        "bibtex": "@inproceedings{haviv-etal-2021-latent,\n    title = \"Can Latent Alignments Improve Autoregressive Machine Translation?\",\n    author = \"Haviv, Adi  and\n      Vassertail, Lior  and\n      Levy, Omer\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.209/\",\n    doi = \"10.18653/v1/2021.naacl-main.209\",\n    pages = \"2637--2641\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.209.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.209/",
        "pdf_size": 380094,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1487966750594385993&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "The Blavatnik School of Computer Science, Tel Aviv University; The Blavatnik School of Computer Science, Tel Aviv University; The Blavatnik School of Computer Science, Tel Aviv University",
        "aff_domain": "cs.tau.ac.il;cs.tau.ac.il;cs.tau.ac.il",
        "email": "cs.tau.ac.il;cs.tau.ac.il;cs.tau.ac.il",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Tel Aviv University",
        "aff_unique_dep": "Blavatnik School of Computer Science",
        "aff_unique_url": "https://www.tau.ac.il",
        "aff_unique_abbr": "TAU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Tel Aviv",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "2021.naacl-main.96",
        "title": "Capturing Row and Column Semantics in Transformer Based Question Answering over Tables",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Transformer based architectures are recently used for the task of answering questions over tables. In order to improve the accuracy on this task, specialized pre-training techniques have been developed and applied on millions of open-domain web tables. In this paper, we propose two novel approaches demonstrating that one can achieve superior performance on table QA task without even using any of these specialized pre-training techniques. The first model, called RCI interaction, leverages a transformer based architecture that independently classifies rows and columns to identify relevant cells. While this model yields extremely high accuracy at finding cell values on recent benchmarks, a second model we propose, called RCI representation, provides a significant efficiency advantage for online QA systems over tables by materializing embeddings for existing tables. Experiments on recent benchmarks prove that the proposed methods can effectively locate cell values on tables (up to ~98% Hit@1 accuracy on WikiSQL lookup questions). Also, the interaction model outperforms the state-of-the-art transformer based approaches, pre-trained on very large table corpora (TAPAS and TaBERT), achieving ~3.4% and ~18.86% additional precision improvement on the standard WikiSQL benchmark.",
        "author": "Michael Glass; Mustafa Canim; Alfio Gliozzo; Saneem Chemmengath; Vishwajeet Kumar; Rishav Chakravarti; Avi Sil; Feifei Pan; Samarth Bharadwaj; Nicolas Rodolfo Fauceglia",
        "authorids": "/m/michael-glass/; /m/mustafa-canim/; /a/alfio-gliozzo/; /s/saneem-chemmengath/; /v/vishwajeet-kumar/; /r/rishav-chakravarti/; /a/avirup-sil/; /f/feifei-pan/; /s/samarth-bharadwaj/; /n/nicolas-rodolfo-fauceglia/",
        "bibtex": "@inproceedings{glass-etal-2021-capturing,\n    title = \"Capturing Row and Column Semantics in Transformer Based Question Answering over Tables\",\n    author = \"Glass, Michael  and\n      Canim, Mustafa  and\n      Gliozzo, Alfio  and\n      Chemmengath, Saneem  and\n      Kumar, Vishwajeet  and\n      Chakravarti, Rishav  and\n      Sil, Avi  and\n      Pan, Feifei  and\n      Bharadwaj, Samarth  and\n      Fauceglia, Nicolas Rodolfo\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.96/\",\n    doi = \"10.18653/v1/2021.naacl-main.96\",\n    pages = \"1212--1224\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.96.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.96/",
        "pdf_size": 519816,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6266838735075952401&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 10,
        "aff": "IBM Research AI+Rensselaer Polytechnic Institute; IBM Research AI+Rensselaer Polytechnic Institute; IBM Research AI+Rensselaer Polytechnic Institute; IBM Research AI+Rensselaer Polytechnic Institute; IBM Research AI+Rensselaer Polytechnic Institute; IBM Research AI+Rensselaer Polytechnic Institute; IBM Research AI+Rensselaer Polytechnic Institute; Rensselaer Polytechnic Institute; IBM Research AI+Rensselaer Polytechnic Institute; IBM Research AI+Rensselaer Polytechnic Institute",
        "aff_domain": "us.ibm.com;us.ibm.com;us.ibm.com;in.ibm.com;in.ibm.com;us.ibm.com;us.ibm.com;rpi.edu;in.ibm.com;ibm.com",
        "email": "us.ibm.com;us.ibm.com;us.ibm.com;in.ibm.com;in.ibm.com;us.ibm.com;us.ibm.com;rpi.edu;in.ibm.com;ibm.com",
        "github": "https://github.com/IBM/row-column-intersection",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0+1;0+1;0+1;0+1;0+1;0+1;0+1;1;0+1;0+1",
        "aff_unique_norm": "IBM Research;Rensselaer Polytechnic Institute",
        "aff_unique_dep": "AI;",
        "aff_unique_url": "https://www.ibm.com/research;https://www.rpi.edu",
        "aff_unique_abbr": "IBM;RPI",
        "aff_campus_unique_index": ";;;;;;;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;0+0;0+0;0+0;0;0+0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.297",
        "title": "Case Study: Deontological Ethics in NLP",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent work in natural language processing (NLP) has focused on ethical challenges such as understanding and mitigating bias in data and algorithms; identifying objectionable content like hate speech, stereotypes and offensive language; and building frameworks for better system design and data handling practices. However, there has been little discussion about the ethical foundations that underlie these efforts. In this work, we study one ethical theory, namely deontological ethics, from the perspective of NLP. In particular, we focus on the generalization principle and the respect for autonomy through informed consent. We provide four case studies to demonstrate how these principles can be used with NLP systems. We also recommend directions to avoid the ethical issues in these systems.",
        "author": "Shrimai Prabhumoye; Brendon Boldt; Ruslan Salakhutdinov; Alan W Black",
        "authorids": "/s/shrimai-prabhumoye/; /b/brendon-boldt/; /r/ruslan-salakhutdinov/; /a/alan-w-black/",
        "bibtex": "@inproceedings{prabhumoye-etal-2021-case,\n    title = \"Case Study: Deontological Ethics in {NLP}\",\n    author = \"Prabhumoye, Shrimai  and\n      Boldt, Brendon  and\n      Salakhutdinov, Ruslan  and\n      Black, Alan W\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.297/\",\n    doi = \"10.18653/v1/2021.naacl-main.297\",\n    pages = \"3784--3798\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.297.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.297/",
        "pdf_size": 643661,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6558555850812177315&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "School of Computer Science, Carnegie Mellon University; School of Computer Science, Carnegie Mellon University; School of Computer Science, Carnegie Mellon University; School of Computer Science, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.323",
        "title": "Causal Effects of Linguistic Properties",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We consider the problem of using observational data to estimate the causal effects of linguistic properties. For example, does writing a complaint politely lead to a faster response time? How much will a positive product review increase sales? This paper addresses two technical challenges related to the problem before developing a practical method. First, we formalize the causal quantity of interest as the effect of a writer\u2019s intent, and establish the assumptions necessary to identify this from observational data. Second, in practice, we only have access to noisy proxies for the linguistic properties of interest\u2014e.g., predictions from classifiers and lexicons. We propose an estimator for this setting and prove that its bias is bounded when we perform an adjustment for the text. Based on these results, we introduce TextCause, an algorithm for estimating causal effects of linguistic properties. The method leverages (1) distant supervision to improve the quality of noisy proxies, and (2) a pre-trained language model (BERT) to adjust for the text. We show that the proposed method outperforms related approaches when estimating the effect of Amazon review sentiment on semi-simulated sales figures. Finally, we present an applied case study investigating the effects of complaint politeness on bureaucratic response times.",
        "author": "Reid Pryzant; Dallas Card; Dan Jurafsky; Victor Veitch; Dhanya Sridhar",
        "authorids": "/r/reid-pryzant/; /d/dallas-card/; /d/dan-jurafsky/; /v/victor-veitch/; /d/dhanya-sridhar/",
        "bibtex": "@inproceedings{pryzant-etal-2021-causal,\n    title = \"Causal Effects of Linguistic Properties\",\n    author = \"Pryzant, Reid  and\n      Card, Dallas  and\n      Jurafsky, Dan  and\n      Veitch, Victor  and\n      Sridhar, Dhanya\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.323/\",\n    doi = \"10.18653/v1/2021.naacl-main.323\",\n    pages = \"4095--4109\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.323.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.323/",
        "pdf_size": 1072045,
        "gs_citation": 61,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12229831986851858914&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Stanford University; Stanford University; Stanford University; University of Chicago; Columbia University",
        "aff_domain": "stanford.edu;stanford.edu;stanford.edu;gmail.com;columbia.edu",
        "email": "stanford.edu;stanford.edu;stanford.edu;gmail.com;columbia.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;1;2",
        "aff_unique_norm": "Stanford University;University of Chicago;Columbia University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.stanford.edu;https://www.uchicago.edu;https://www.columbia.edu",
        "aff_unique_abbr": "Stanford;UChicago;Columbia",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.87",
        "title": "Certified Robustness to Word Substitution Attack with Differential Privacy",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The robustness and security of natural language processing (NLP) models are significantly important in real-world applications. In the context of text classification tasks, adversarial examples can be designed by substituting words with synonyms under certain semantic and syntactic constraints, such that a well-trained model will give a wrong prediction. Therefore, it is crucial to develop techniques to provide a rigorous and provable robustness guarantee against such attacks. In this paper, we propose WordDP to achieve certified robustness against word substitution at- tacks in text classification via differential privacy (DP). We establish the connection between DP and adversarial robustness for the first time in the text domain and propose a conceptual exponential mechanism-based algorithm to formally achieve the robustness. We further present a practical simulated exponential mechanism that has efficient inference with certified robustness. We not only provide a rigorous analytic derivation of the certified condition but also experimentally compare the utility of WordDP with existing defense algorithms. The results show that WordDP achieves higher accuracy and more than 30X efficiency improvement over the state-of-the-art certified robustness mechanism in typical text classification tasks.",
        "author": "Wenjie Wang; Pengfei Tang; Jian Lou; Li Xiong",
        "authorids": "/w/wenjie-wang/; /p/pengfei-tang/; /j/jian-lou/; /l/li-xiong/",
        "bibtex": "@inproceedings{wang-etal-2021-certified,\n    title = \"Certified Robustness to Word Substitution Attack with Differential Privacy\",\n    author = \"Wang, Wenjie  and\n      Tang, Pengfei  and\n      Lou, Jian  and\n      Xiong, Li\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.87/\",\n    doi = \"10.18653/v1/2021.naacl-main.87\",\n    pages = \"1102--1112\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.87.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.87/",
        "pdf_size": 1980506,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14339678284971800167&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4
    },
    {
        "id": "2021.naacl-main.199",
        "title": "Challenging distributional models with a conceptual network of philosophical terms",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Computational linguistic research on language change through distributional semantic (DS) models has inspired researchers from fields such as philosophy and literary studies, who use these methods for the exploration and comparison of comparatively small datasets traditionally analyzed by close reading. Research on methods for small data is still in early stages and it is not clear which methods achieve the best results. We investigate the possibilities and limitations of using distributional semantic models for analyzing philosophical data by means of a realistic use-case. We provide a ground truth for evaluation created by philosophy experts and a blueprint for using DS models in a sound methodological setup. We compare three methods for creating specialized models from small datasets. Though the models do not perform well enough to directly support philosophers yet, we find that models designed for small data yield promising directions for future work.",
        "author": "Yvette Oortwijn; Jelke Bloem; Pia Sommerauer; Francois Meyer; Wei Zhou; Antske Fokkens",
        "authorids": "/y/yvette-oortwijn/; /j/jelke-bloem/; /p/pia-sommerauer/; /f/francois-meyer/; /w/wei-zhou/; /a/antske-fokkens/",
        "bibtex": "@inproceedings{oortwijn-etal-2021-challenging,\n    title = \"Challenging distributional models with a conceptual network of philosophical terms\",\n    author = \"Oortwijn, Yvette  and\n      Bloem, Jelke  and\n      Sommerauer, Pia  and\n      Meyer, Francois  and\n      Zhou, Wei  and\n      Fokkens, Antske\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.199/\",\n    doi = \"10.18653/v1/2021.naacl-main.199\",\n    pages = \"2511--2522\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.199.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.199/",
        "pdf_size": 286935,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6965188937001502468&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Institute for Logic, Language and Computation, University of Amsterdam\u2663; Institute for Logic, Language and Computation, University of Amsterdam\u2663; Computational Linguistics & Text Mining Lab, Vrije Universiteit Amstersterdam\u2217; Institute for Logic, Language and Computation, University of Amsterdam\u2663; Computational Linguistics & Text Mining Lab, Vrije Universiteit Amstersterdam\u2217+Department of Mathematics and Computer Science, Eindhoven University of Technology\u2020; Computational Linguistics & Text Mining Lab, Vrije Universiteit Amstersterdam\u2217",
        "aff_domain": "uva.nl;uva.nl;gmail.com;gmail.com;vu.nl;vu.nl",
        "email": "uva.nl;uva.nl;gmail.com;gmail.com;vu.nl;vu.nl",
        "github": "https://github.com/YOortwijn/",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;1;0;1+2;1",
        "aff_unique_norm": "University of Amsterdam;Vrije Universiteit Amsterdam;Eindhoven University of Technology",
        "aff_unique_dep": "Institute for Logic, Language and Computation;Computational Linguistics & Text Mining Lab;Department of Mathematics and Computer Science",
        "aff_unique_url": "https://www.uva.nl;https://www.vu.nl;https://www.tue.nl",
        "aff_unique_abbr": "UvA;VU Amsterdam;TU/e",
        "aff_campus_unique_index": "0;0;0;0;0+1;0",
        "aff_campus_unique": "Amsterdam;Eindhoven",
        "aff_country_unique_index": "0;0;0;0;0+0;0",
        "aff_country_unique": "Netherlands"
    },
    {
        "id": "2021.naacl-main.279",
        "title": "Choose Your Own Adventure: Paired Suggestions in Collaborative Writing for Evaluating Story Generation Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Story generation is an open-ended and subjective task, which poses a challenge for evaluating story generation models. We present Choose Your Own Adventure, a collaborative writing setup for pairwise model evaluation. Two models generate suggestions to people as they write a short story; we ask writers to choose one of the two suggestions, and we observe which model\u2019s suggestions they prefer. The setup also allows further analysis based on the revisions people make to the suggestions. We show that these measures, combined with automatic metrics, provide an informative picture of the models\u2019 performance, both in cases where the differences in generation methods are small (nucleus vs. top-k sampling) and large (GPT2 vs. Fusion models).",
        "author": "Elizabeth Clark; Noah A. Smith",
        "authorids": "/e/elizabeth-clark/; /n/noah-a-smith/",
        "bibtex": "@inproceedings{clark-smith-2021-choose,\n    title = \"Choose Your Own Adventure: Paired Suggestions in Collaborative Writing for Evaluating Story Generation Models\",\n    author = \"Clark, Elizabeth  and\n      Smith, Noah A.\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.279/\",\n    doi = \"10.18653/v1/2021.naacl-main.279\",\n    pages = \"3566--3575\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.279.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.279/",
        "pdf_size": 2402181,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3089558531378647694&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Paul G. Allen School of Computer Science & Engineering, University of Washington + Allen Institute for Artificial Intelligence; Paul G. Allen School of Computer Science & Engineering, University of Washington",
        "aff_domain": "cs.washington.edu;cs.washington.edu",
        "email": "cs.washington.edu;cs.washington.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;0",
        "aff_unique_norm": "University of Washington;Allen Institute for Artificial Intelligence",
        "aff_unique_dep": "Paul G. Allen School of Computer Science & Engineering;",
        "aff_unique_url": "https://www.cs.washington.edu;https://allenai.org",
        "aff_unique_abbr": "UW;AI2",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Seattle;",
        "aff_country_unique_index": "0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.267",
        "title": "Clipping Loops for Sample-Efficient Dialogue Policy Optimisation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Training dialogue agents requires a large number of interactions with users: agents have no idea about which responses are bad among a lengthy dialogue. In this paper, we propose loop-clipping policy optimisation (LCPO) to eliminate useless responses. LCPO consists of two stages: loop clipping and advantage clipping. In loop clipping, we clip off useless responses (called loops) from dialogue history (called trajectories). The clipped trajectories are more succinct than the original ones, and the estimation of state-value is more accurate. Second, in advantage clipping, we estimate and clip the advantages of useless responses and normal ones separately. The clipped advantage distinguish useless actions from others and reduce the probabilities of useless actions efficiently. In experiments on Cambridge Restaurant Dialogue System, LCPO uses only 260 training dialogues to achieve 80% success rate, while PPO baseline requires 2160 dialogues. Besides, LCPO receives 3.7/5 scores in human evaluation where the agent interactively collects 100 real-user dialogues in training phase.",
        "author": "Yen-Chen Wu; Carl Edward Rasmussen",
        "authorids": "/y/yen-chen-wu/; /c/carl-edward-rasmussen/",
        "bibtex": "@inproceedings{wu-rasmussen-2021-clipping,\n    title = \"Clipping Loops for Sample-Efficient Dialogue Policy Optimisation\",\n    author = \"Wu, Yen-Chen  and\n      Rasmussen, Carl Edward\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.267/\",\n    doi = \"10.18653/v1/2021.naacl-main.267\",\n    pages = \"3420--3428\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.267.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.267/",
        "pdf_size": 659186,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15277562448803727753&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "University of Cambridge; University of Cambridge",
        "aff_domain": "cam.ac.uk;cam.ac.uk",
        "email": "cam.ac.uk;cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Cambridge",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cam.ac.uk",
        "aff_unique_abbr": "Cambridge",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2021.naacl-main.205",
        "title": "Clustering-based Inference for Biomedical Entity Linking",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Due to large number of entities in biomedical knowledge bases, only a small fraction of entities have corresponding labelled training data. This necessitates entity linking models which are able to link mentions of unseen entities using learned representations of entities. Previous approaches link each mention independently, ignoring the relationships within and across documents between the entity mentions. These relations can be very useful for linking mentions in biomedical text where linking decisions are often difficult due mentions having a generic or a highly specialized form. In this paper, we introduce a model in which linking decisions can be made not merely by linking to a knowledge base entity but also by grouping multiple mentions together via clustering and jointly making linking predictions. In experiments on the largest publicly available biomedical dataset, we improve the best independent prediction for entity linking by 3.0 points of accuracy, and our clustering-based inference model further improves entity linking by 2.3 points.",
        "author": "Rico Angell; Nicholas Monath; Sunil Mohan; Nishant Yadav; Andrew McCallum",
        "authorids": "/r/rico-angell/; /n/nicholas-monath/; /s/sunil-mohan/; /n/nishant-yadav/; /a/andrew-mccallum/",
        "bibtex": "@inproceedings{angell-etal-2021-clustering,\n    title = \"Clustering-based Inference for Biomedical Entity Linking\",\n    author = \"Angell, Rico  and\n      Monath, Nicholas  and\n      Mohan, Sunil  and\n      Yadav, Nishant  and\n      McCallum, Andrew\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.205/\",\n    doi = \"10.18653/v1/2021.naacl-main.205\",\n    pages = \"2598--2608\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.205.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.205/",
        "pdf_size": 1641262,
        "gs_citation": 70,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1806404590572730252&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "College of Information and Computer Sciences, University of Massachusetts Amherst; College of Information and Computer Sciences, University of Massachusetts Amherst; Chan Zuckerberg Initiative; College of Information and Computer Sciences, University of Massachusetts Amherst; College of Information and Computer Sciences, University of Massachusetts Amherst",
        "aff_domain": "cs.umass.edu;cs.umass.edu;chanzuckerberg.com;cs.umass.edu;cs.umass.edu",
        "email": "cs.umass.edu;cs.umass.edu;chanzuckerberg.com;cs.umass.edu;cs.umass.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "University of Massachusetts Amherst;Chan Zuckerberg Initiative",
        "aff_unique_dep": "College of Information and Computer Sciences;",
        "aff_unique_url": "https://www.umass.edu;https://www.chanzuckerberg.com",
        "aff_unique_abbr": "UMass Amherst;CZI",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Amherst;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.331",
        "title": "CoRT: Complementary Rankings from Transformers",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Many recent approaches towards neural information retrieval mitigate their computational costs by using a multi-stage ranking pipeline. In the first stage, a number of potentially relevant candidates are retrieved using an efficient retrieval model such as BM25. Although BM25 has proven decent performance as a first-stage ranker, it tends to miss relevant passages. In this context we propose CoRT, a simple neural first-stage ranking model that leverages contextual representations from pretrained language models such as BERT to complement term-based ranking functions while causing no significant delay at query time. Using the MS MARCO dataset, we show that CoRT significantly increases the candidate recall by complementing BM25 with missing candidates. Consequently, we find subsequent re-rankers achieve superior results with less candidates. We further demonstrate that passage retrieval using CoRT can be realized with surprisingly low latencies.",
        "author": "Marco Wrzalik; Dirk Krechel",
        "authorids": "/m/marco-wrzalik/; /d/dirk-krechel/",
        "bibtex": "@inproceedings{wrzalik-krechel-2021-cort,\n    title = \"{C}o{RT}: Complementary Rankings from Transformers\",\n    author = \"Wrzalik, Marco  and\n      Krechel, Dirk\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.331/\",\n    doi = \"10.18653/v1/2021.naacl-main.331\",\n    pages = \"4194--4204\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.331.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.331/",
        "pdf_size": 528823,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7283466564152336805&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "RheinMain University of Applied Sciences, Germany; RheinMain University of Applied Sciences, Germany",
        "aff_domain": "hs-rm.de;hs-rm.de",
        "email": "hs-rm.de;hs-rm.de",
        "github": "https://github.com/lavis-nlp/CoRT",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "RheinMain University of Applied Sciences",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.rheinmain.de",
        "aff_unique_abbr": "RMUAS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2021.naacl-main.282",
        "title": "Code-Mixing on Sesame Street: Dawn of the Adversarial Polyglots",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Multilingual models have demonstrated impressive cross-lingual transfer performance. However, test sets like XNLI are monolingual at the example level. In multilingual communities, it is common for polyglots to code-mix when conversing with each other. Inspired by this phenomenon, we present two strong black-box adversarial attacks (one word-level, one phrase-level) for multilingual models that push their ability to handle code-mixed sentences to the limit. The former uses bilingual dictionaries to propose perturbations and translations of the clean example for sense disambiguation. The latter directly aligns the clean example with its translations before extracting phrases as perturbations. Our phrase-level attack has a success rate of 89.75% against XLM-R-large, bringing its average accuracy of 79.85 down to 8.18 on XNLI. Finally, we propose an efficient adversarial training scheme that trains in the same number of steps as the original model and show that it creates more language-invariant representations, improving clean and robust accuracy in the absence of lexical overlap without degrading performance on the original examples.",
        "author": "Samson Tan; Shafiq Joty",
        "authorids": "/s/samson-tan/; /s/shafiq-joty/",
        "bibtex": "@inproceedings{tan-joty-2021-code,\n    title = \"Code-Mixing on Sesame Street: Dawn of the Adversarial Polyglots\",\n    author = \"Tan, Samson  and\n      Joty, Shafiq\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.282/\",\n    doi = \"10.18653/v1/2021.naacl-main.282\",\n    pages = \"3596--3616\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.282.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.282/",
        "pdf_size": 14621760,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14193067390219981781&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Salesforce Research Asia\u266eNational University of Singapore; Salesforce Research Asia\u2021Nanyang Technological University",
        "aff_domain": "salesforce.com;salesforce.com",
        "email": "salesforce.com;salesforce.com",
        "github": "github.com/salesforce/adversarial-polyglots",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Salesforce Research Asia;Nanyang Technological University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://research.salesforce.com;https://www.ntu.edu.sg",
        "aff_unique_abbr": "Salesforce Research Asia;NTU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "2021.naacl-industry.31",
        "title": "Coherent and Concise Radiology Report Generation via Context Specific Image Representations and Orthogonal Sentence States",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Neural models for text generation are often designed in an end-to-end fashion, typically with zero control over intermediate computations, limiting their practical usability in downstream applications. In this work, we incorporate explicit means into neural models to ensure topical continuity, informativeness and content diversity of generated radiology reports. For the purpose we propose a method to compute image representations specific to each sentential context and eliminate redundant content by exploiting diverse sentence states. We conduct experiments to generate radiology reports from medical images of chest x-rays using MIMIC-CXR. Our model outperforms baselines by up to 18% and 29% respective in the evaluation for informativeness and content ordering respectively, relative on objective metrics and 16% on human evaluation.",
        "author": "Litton J Kurisinkel; Ai Ti Aw; Nancy F Chen",
        "authorids": "/l/litton-j-kurisinkel/; /a/aiti-aw/; /n/nancy-f-chen/",
        "bibtex": "@inproceedings{j-kurisinkel-etal-2021-coherent,\n    title = \"Coherent and Concise Radiology Report Generation via Context Specific Image Representations and Orthogonal Sentence States\",\n    author = \"J Kurisinkel, Litton  and\n      Aw, Ai Ti  and\n      Chen, Nancy F\",\n    editor = \"Kim, Young-bum  and\n      Li, Yunyao  and\n      Rambow, Owen\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-industry.31/\",\n    doi = \"10.18653/v1/2021.naacl-industry.31\",\n    pages = \"246--254\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-industry.31.pdf",
        "site": "https://aclanthology.org/2021.naacl-industry.31/",
        "pdf_size": 448873,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16535806835670116032&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Institute for Infocomm Research, A*STAR, Singapore; Institute for Infocomm Research, A*STAR, Singapore; Institute for Infocomm Research, A*STAR, Singapore",
        "aff_domain": "i2r.a-star.edu.sg;i2r.a-star.edu.sg;i2r.a-star.edu.sg",
        "email": "i2r.a-star.edu.sg;i2r.a-star.edu.sg;i2r.a-star.edu.sg",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Institute for Infocomm Research",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.i2r.a-star.edu.sg",
        "aff_unique_abbr": "I2R",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "2021.naacl-industry.36",
        "title": "Combining Weakly Supervised ML Techniques for Low-Resource NLU",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Recent advances in transfer learning have improved the performance of virtual assistants considerably. Nevertheless, creating sophisticated voice-enabled applications for new domains remains a challenge, and meager training data is often a key bottleneck. Accordingly, unsupervised learning and SSL (semi-supervised learning) techniques continue to be of vital importance. While a number of such methods have been explored previously in isolation, in this paper we investigate the synergistic use of a number of weakly supervised techniques with a view to improving NLU (Natural Language Understanding) accuracy in low-resource settings. We explore three different approaches incorporating anonymized, unlabeled and automatically transcribed user utterances into the training process, two focused on data augmentation via SSL and another one focused on unsupervised and transfer learning. We show promising results, obtaining gains that range from 4.73% to 7.65% relative improvements on semantic error rate for each individual approach. Moreover, the combination of all three methods together yields a relative improvement of 11.77% over our current baseline model. Our methods are applicable to any new domain with minimal training data, and can be deployed over time into a cycle of continual learning.",
        "author": "Victor Soto; Konstantine Arkoudas",
        "authorids": "/v/victor-soto/; /k/konstantine-arkoudas/",
        "bibtex": "@inproceedings{soto-arkoudas-2021-combining,\n    title = \"Combining Weakly Supervised {ML} Techniques for Low-Resource {NLU}\",\n    author = \"Soto, Victor  and\n      Arkoudas, Konstantine\",\n    editor = \"Kim, Young-bum  and\n      Li, Yunyao  and\n      Rambow, Owen\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-industry.36/\",\n    doi = \"10.18653/v1/2021.naacl-industry.36\",\n    pages = \"288--295\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-industry.36.pdf",
        "site": "https://aclanthology.org/2021.naacl-industry.36/",
        "pdf_size": 280869,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14911381844658413452&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Amazon Alexa AI; Amazon Alexa AI",
        "aff_domain": "amazon.com;amazon.com",
        "email": "amazon.com;amazon.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Amazon",
        "aff_unique_dep": "Alexa AI",
        "aff_unique_url": "https://www.amazon.com",
        "aff_unique_abbr": "Amazon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.225",
        "title": "Compositional Generalization for Neural Semantic Parsing via Span-level Supervised Attention",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We describe a span-level supervised attention loss that improves compositional generalization in semantic parsers. Our approach builds on existing losses that encourage attention maps in neural sequence-to-sequence models to imitate the output of classical word alignment algorithms. Where past work has used word-level alignments, we focus on spans; borrowing ideas from phrase-based machine translation, we align subtrees in semantic parses to spans of input sentences, and encourage neural attention mechanisms to mimic these alignments. This method improves the performance of transformers, RNNs, and structured decoders on three benchmarks of compositional generalization.",
        "author": "Pengcheng Yin; Hao Fang; Graham Neubig; Adam Pauls; Emmanouil Antonios Platanios; Yu Su; Sam Thomson; Jacob Andreas",
        "authorids": "/p/pengcheng-yin/; /h/hao-fang/; /g/graham-neubig/; /a/adam-pauls/; /e/emmanouil-antonios-platanios/; /y/yu-su/; /s/sam-thomson/; /j/jacob-andreas/",
        "bibtex": "@inproceedings{yin-etal-2021-compositional,\n    title = \"Compositional Generalization for Neural Semantic Parsing via Span-level Supervised Attention\",\n    author = \"Yin, Pengcheng  and\n      Fang, Hao  and\n      Neubig, Graham  and\n      Pauls, Adam  and\n      Platanios, Emmanouil Antonios  and\n      Su, Yu  and\n      Thomson, Sam  and\n      Andreas, Jacob\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.225/\",\n    doi = \"10.18653/v1/2021.naacl-main.225\",\n    pages = \"2810--2823\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.225.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.225/",
        "pdf_size": 2204745,
        "gs_citation": 63,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2986246110805327950&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Carnegie Mellon University\u2660; Microsoft Semantic Machines\u2663; Carnegie Mellon University\u2660; Microsoft Semantic Machines\u2663; Microsoft Semantic Machines\u2663; Microsoft Semantic Machines\u2663; Microsoft Semantic Machines\u2663; Microsoft Semantic Machines\u2663",
        "aff_domain": "cs.cmu.edu;microsoft.com;cs.cmu.edu;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "email": "cs.cmu.edu;microsoft.com;cs.cmu.edu;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;1;0;1;1;1;1;1",
        "aff_unique_norm": "Carnegie Mellon University;Microsoft",
        "aff_unique_dep": ";Semantic Machines",
        "aff_unique_url": "https://www.cmu.edu;https://www.microsoft.com",
        "aff_unique_abbr": "CMU;Microsoft",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.264",
        "title": "ConVEx: Data-Efficient and Few-Shot Slot Labeling",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We propose ConVEx (Conversational Value Extractor), an efficient pretraining and fine-tuning neural approach for slot-labeling dialog tasks. Instead of relying on more general pretraining objectives from prior work (e.g., language modeling, response selection), ConVEx\u2019s pretraining objective, a novel pairwise cloze task using Reddit data, is well aligned with its intended usage on sequence labeling tasks. This enables learning domain-specific slot labelers by simply fine-tuning decoding layers of the pretrained general-purpose sequence labeling model, while the majority of the pretrained model\u2019s parameters are kept frozen. We report state-of-the-art performance of ConVEx across a range of diverse domains and data sets for dialog slot-labeling, with the largest gains in the most challenging, few-shot setups. We believe that ConVEx\u2019s reduced pretraining times (i.e., only 18 hours on 12 GPUs) and cost, along with its efficient fine-tuning and strong performance, promise wider portability and scalability for data-efficient sequence-labeling tasks in general.",
        "author": "Matthew Henderson; Ivan Vuli\u0107",
        "authorids": "/m/matthew-henderson/; /i/ivan-vulic/",
        "bibtex": "@inproceedings{henderson-vulic-2021-convex,\n    title = \"{ConVEx}: Data-Efficient and Few-Shot Slot Labeling\",\n    author = \"Henderson, Matthew  and\n      Vuli{\\'c}, Ivan\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.264/\",\n    doi = \"10.18653/v1/2021.naacl-main.264\",\n    pages = \"3375--3389\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.264.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.264/",
        "pdf_size": 700427,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6974833312480597485&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "PolyAI Ltd, London, UK; PolyAI Ltd, London, UK + Language Technology Lab, University of Cambridge, UK",
        "aff_domain": "polyai.com;polyai.com",
        "email": "polyai.com;polyai.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "PolyAI Ltd;University of Cambridge",
        "aff_unique_dep": ";Language Technology Lab",
        "aff_unique_url": "https://www.poly.ai;https://www.cam.ac.uk",
        "aff_unique_abbr": "PolyAI;Cambridge",
        "aff_campus_unique_index": "0;0+1",
        "aff_campus_unique": "London;Cambridge",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2021.naacl-main.13",
        "title": "Concealed Data Poisoning Attacks on NLP Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Adversarial attacks alter NLP model predictions by perturbing test-time inputs. However, it is much less understood whether, and how, predictions can be manipulated with small, concealed changes to the training data. In this work, we develop a new data poisoning attack that allows an adversary to control model predictions whenever a desired trigger phrase is present in the input. For instance, we insert 50 poison examples into a sentiment model\u2019s training set that causes the model to frequently predict Positive whenever the input contains \u201cJames Bond\u201d. Crucially, we craft these poison examples using a gradient-based procedure so that they do not mention the trigger phrase. We also apply our poison attack to language modeling (\u201cApple iPhone\u201d triggers negative generations) and machine translation (\u201ciced coffee\u201d mistranslated as \u201chot coffee\u201d). We conclude by proposing three defenses that can mitigate our attack at some cost in prediction accuracy or extra human annotation.",
        "author": "Eric Wallace; Tony Zhao; Shi Feng; Sameer Singh",
        "authorids": "/e/eric-wallace/; /t/tony-zhao/; /s/shi-feng/; /s/sameer-singh/",
        "bibtex": "@inproceedings{wallace-etal-2021-concealed,\n    title = \"Concealed Data Poisoning Attacks on {NLP} Models\",\n    author = \"Wallace, Eric  and\n      Zhao, Tony  and\n      Feng, Shi  and\n      Singh, Sameer\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.13/\",\n    doi = \"10.18653/v1/2021.naacl-main.13\",\n    pages = \"139--150\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.13.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.13/",
        "pdf_size": 929703,
        "gs_citation": 196,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17405161573017692947&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "UC Berkeley; UC Berkeley; University of Maryland; UC Irvine",
        "aff_domain": "berkeley.edu;berkeley.edu;cs.umd.edu;uci.edu",
        "email": "berkeley.edu;berkeley.edu;cs.umd.edu;uci.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;2",
        "aff_unique_norm": "University of California, Berkeley;University of Maryland;University of California, Irvine",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.berkeley.edu;https://www/umd.edu;https://www.uci.edu",
        "aff_unique_abbr": "UC Berkeley;UMD;UCI",
        "aff_campus_unique_index": "0;0;2",
        "aff_campus_unique": "Berkeley;;Irvine",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.356",
        "title": "Constrained Multi-Task Learning for Event Coreference Resolution",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We propose a neural event coreference model in which event coreference is jointly trained with five tasks: trigger detection, entity coreference, anaphoricity determination, realis detection, and argument extraction. To guide the learning of this complex model, we incorporate cross-task consistency constraints into the learning process as soft constraints via designing penalty functions. In addition, we propose the novel idea of viewing entity coreference and event coreference as a single coreference task, which we believe is a step towards a unified model of coreference resolution. The resulting model achieves state-of-the-art results on the KBP 2017 event coreference dataset.",
        "author": "Jing Lu; Vincent Ng",
        "authorids": "/j/jing-lu/; /v/vincent-ng/",
        "bibtex": "@inproceedings{lu-ng-2021-constrained,\n    title = \"Constrained Multi-Task Learning for Event Coreference Resolution\",\n    author = \"Lu, Jing  and\n      Ng, Vincent\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.356/\",\n    doi = \"10.18653/v1/2021.naacl-main.356\",\n    pages = \"4504--4514\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.356.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.356/",
        "pdf_size": 590798,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12242821577090281037&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2
    },
    {
        "id": "2021.naacl-main.373",
        "title": "Constructing Taxonomies from Pretrained Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We present a method for constructing taxonomic trees (e.g., WordNet) using pretrained language models. Our approach is composed of two modules, one that predicts parenthood relations and another that reconciles those pairwise predictions into trees. The parenthood prediction module produces likelihood scores for each potential parent-child pair, creating a graph of parent-child relation scores. The tree reconciliation module treats the task as a graph optimization problem and outputs the maximum spanning tree of this graph. We train our model on subtrees sampled from WordNet, and test on nonoverlapping WordNet subtrees. We show that incorporating web-retrieved glosses can further improve performance. On the task of constructing subtrees of English WordNet, the model achieves 66.7 ancestor F1, a 20.0% relative increase over the previous best published result on this task. In addition, we convert the original English dataset into nine other languages using Open Multilingual WordNet and extend our results across these languages.",
        "author": "Catherine Chen; Kevin Lin; Dan Klein",
        "authorids": "/c/catherine-chen-ucberkley/; /k/kevin-lin/; /d/dan-klein/",
        "bibtex": "@inproceedings{chen-etal-2021-constructing,\n    title = \"Constructing Taxonomies from Pretrained Language Models\",\n    author = \"Chen, Catherine  and\n      Lin, Kevin  and\n      Klein, Dan\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.373/\",\n    doi = \"10.18653/v1/2021.naacl-main.373\",\n    pages = \"4687--4700\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.373.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.373/",
        "pdf_size": 635693,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3191240942085944332&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "University of California, Berkeley; University of California, Berkeley; University of California, Berkeley",
        "aff_domain": "berkeley.edu;berkeley.edu;berkeley.edu",
        "email": "berkeley.edu;berkeley.edu;berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.126",
        "title": "Context Tracking Network: Graph-based Context Modeling for Implicit Discourse Relation Recognition",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Implicit discourse relation recognition (IDRR) aims to identify logical relations between two adjacent sentences in the discourse. Existing models fail to fully utilize the contextual information which plays an important role in interpreting each local sentence. In this paper, we thus propose a novel graph-based Context Tracking Network (CT-Net) to model the discourse context for IDRR. The CT-Net firstly converts the discourse into the paragraph association graph (PAG), where each sentence tracks their closely related context from the intricate discourse through different types of edges. Then, the CT-Net extracts contextual representation from the PAG through a specially designed cross-grained updating mechanism, which can effectively integrate both sentence-level and token-level contextual semantics. Experiments on PDTB 2.0 show that the CT-Net gains better performance than models that roughly model the context.",
        "author": "Yingxue Zhang; Fandong Meng; Peng Li; Ping Jian; Jie Zhou",
        "authorids": "/y/yingxue-zhang/; /f/fandong-meng/; /p/peng-li/; /p/ping-jian/; /j/jie-zhou/",
        "bibtex": "@inproceedings{zhang-etal-2021-context,\n    title = \"Context Tracking Network: Graph-based Context Modeling for Implicit Discourse Relation Recognition\",\n    author = \"Zhang, Yingxue  and\n      Meng, Fandong  and\n      Li, Peng  and\n      Jian, Ping  and\n      Zhou, Jie\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.126/\",\n    doi = \"10.18653/v1/2021.naacl-main.126\",\n    pages = \"1592--1599\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.126.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.126/",
        "pdf_size": 358941,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4964003861859286264&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Pattern Recognition Center, WeChat AI, Tencent Inc, China; Pattern Recognition Center, WeChat AI, Tencent Inc, China; Pattern Recognition Center, WeChat AI, Tencent Inc, China; Beijing Institute of Technology, China; Pattern Recognition Center, WeChat AI, Tencent Inc, China",
        "aff_domain": "tencent.com;tencent.com;tencent.com;bit.edu.cn;tencent.com",
        "email": "tencent.com;tencent.com;tencent.com;bit.edu.cn;tencent.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "Tencent Inc;Beijing Institute of Technology",
        "aff_unique_dep": "Pattern Recognition Center, WeChat AI;",
        "aff_unique_url": "https://www.tencent.com;http://www.bit.edu.cn/",
        "aff_unique_abbr": "Tencent;BIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2021.naacl-main.281",
        "title": "Context-Interactive Pre-Training for Document Machine Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Document machine translation aims to translate the source sentence into the target language in the presence of additional contextual information. However, it typically suffers from a lack of doc-level bilingual data. To remedy this, here we propose a simple yet effective context-interactive pre-training approach, which targets benefiting from external large-scale corpora. The proposed model performs inter sentence generation to capture the cross-sentence dependency within the target document, and cross sentence translation to make better use of valuable contextual information. Comprehensive experiments illustrate that our approach can achieve state-of-the-art performance on three benchmark datasets, which significantly outperforms a variety of baselines.",
        "author": "Pengcheng Yang; Pei Zhang; Boxing Chen; Jun Xie; Weihua Luo",
        "authorids": "/p/pengcheng-yang/; /p/pei-zhang/; /b/boxing-chen/; /j/jun-xie/; /w/weihua-luo/",
        "bibtex": "@inproceedings{yang-etal-2021-context,\n    title = \"Context-Interactive Pre-Training for Document Machine Translation\",\n    author = \"Yang, Pengcheng  and\n      Zhang, Pei  and\n      Chen, Boxing  and\n      Xie, Jun  and\n      Luo, Weihua\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.281/\",\n    doi = \"10.18653/v1/2021.naacl-main.281\",\n    pages = \"3589--3595\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.281.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.281/",
        "pdf_size": 368150,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12875578572639074411&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Machine Intelligence Technology Lab, Alibaba Group, Hangzhou, China; Machine Intelligence Technology Lab, Alibaba Group, Hangzhou, China; Machine Intelligence Technology Lab, Alibaba Group, Hangzhou, China; Machine Intelligence Technology Lab, Alibaba Group, Hangzhou, China; Machine Intelligence Technology Lab, Alibaba Group, Hangzhou, China",
        "aff_domain": "alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com",
        "email": "alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Alibaba Group",
        "aff_unique_dep": "Machine Intelligence Technology Lab",
        "aff_unique_url": "https://www.alibaba.com",
        "aff_unique_abbr": "Alibaba",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Hangzhou",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2021.naacl-main.461",
        "title": "Context-aware Decoder for Neural Machine Translation using a Target-side Document-Level Language Model",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Although many end-to-end context-aware neural machine translation models have been proposed to incorporate inter-sentential contexts in translation, these models can be trained only in domains where parallel documents with sentential alignments exist. We therefore present a simple method to perform context-aware decoding with any pre-trained sentence-level translation model by using a document-level language model. Our context-aware decoder is built upon sentence-level parallel data and target-side document-level monolingual data. From a theoretical viewpoint, our core contribution is the novel representation of contextual information using point-wise mutual information between context and the current sentence. We demonstrate the effectiveness of our method on English to Russian translation, by evaluating with BLEU and contrastive tests for context-aware translation.",
        "author": "Amane Sugiyama; Naoki Yoshinaga",
        "authorids": "/a/amane-sugiyama/; /n/naoki-yoshinaga/",
        "bibtex": "@inproceedings{sugiyama-yoshinaga-2021-context,\n    title = \"Context-aware Decoder for Neural Machine Translation using a Target-side Document-Level Language Model\",\n    author = \"Sugiyama, Amane  and\n      Yoshinaga, Naoki\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.461/\",\n    doi = \"10.18653/v1/2021.naacl-main.461\",\n    pages = \"5781--5791\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.461.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.461/",
        "pdf_size": 673019,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17274886950648583168&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "The University of Tokyo\u2217; Institute of Industrial Science, The University of Tokyo",
        "aff_domain": "tkl.iis.u-tokyo.ac.jp;iis.u-tokyo.ac.jp",
        "email": "tkl.iis.u-tokyo.ac.jp;iis.u-tokyo.ac.jp",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Tokyo;The University of Tokyo",
        "aff_unique_dep": ";Institute of Industrial Science",
        "aff_unique_url": "https://www.u-tokyo.ac.jp;https://www.iis.u-tokyo.ac.jp/en/",
        "aff_unique_abbr": "UTokyo;UTokyo",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Tokyo",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2021.naacl-industry.6",
        "title": "Contextual Domain Classification with Temporal Representations",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "In commercial dialogue systems, the Spoken Language Understanding (SLU) component tends to have numerous domains thus context is needed to help resolve ambiguities. Previous works that incorporate context for SLU have mostly focused on domains where context is limited to a few minutes. However, there are domains that have related context that could span up to hours and days. In this paper, we propose temporal representations that combine wall-clock second difference and turn order offset information to utilize both recent and distant context in a novel large-scale setup. Experiments on the Contextual Domain Classification (CDC) task with various encoder architectures show that temporal representations combining both information outperforms only one of the two. We further demonstrate that our contextual Transformer is able to reduce 13.04% of classification errors compared to a non-contextual baseline. We also conduct empirical analyses to study recent versus distant context and opportunities to lower deployment costs.",
        "author": "Tzu-Hsiang Lin; Yipeng Shi; Chentao Ye; Yang Fan; Weitong Ruan; Emre Barut; Wael Hamza; Chengwei Su",
        "authorids": "/t/tzu-hsiang-lin/; /y/yipeng-shi/; /c/chentao-ye/; /y/yang-fan/; /w/weitong-ruan/; /e/emre-barut/; /w/wael-hamza/; /c/chengwei-su/",
        "bibtex": "@inproceedings{lin-etal-2021-contextual,\n    title = \"Contextual Domain Classification with Temporal Representations\",\n    author = \"Lin, Tzu-Hsiang  and\n      Shi, Yipeng  and\n      Ye, Chentao  and\n      Fan, Yang  and\n      Ruan, Weitong  and\n      Barut, Emre  and\n      Hamza, Wael  and\n      Su, Chengwei\",\n    editor = \"Kim, Young-bum  and\n      Li, Yunyao  and\n      Rambow, Owen\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-industry.6/\",\n    doi = \"10.18653/v1/2021.naacl-industry.6\",\n    pages = \"41--48\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-industry.6.pdf",
        "site": "https://aclanthology.org/2021.naacl-industry.6/",
        "pdf_size": 343184,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17605176236529058651&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Amazon Alexa AI; Amazon Alexa AI; Amazon Alexa AI; Amazon Alexa AI; Amazon Alexa AI; Amazon Alexa AI; Amazon Alexa AI; Amazon Alexa AI",
        "aff_domain": "amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com",
        "email": "amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Amazon",
        "aff_unique_dep": "Alexa AI",
        "aff_unique_url": "https://www.amazon.com",
        "aff_unique_abbr": "Amazon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.400",
        "title": "Contextualized Perturbation for Textual Adversarial Attack",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Adversarial examples expose the vulnerabilities of natural language processing (NLP) models, and can be used to evaluate and improve their robustness. Existing techniques of generating such examples are typically driven by local heuristic rules that are agnostic to the context, often resulting in unnatural and ungrammatical outputs. This paper presents CLARE, a ContextuaLized AdversaRial Example generation model that produces fluent and grammatical outputs through a mask-then-infill procedure. CLARE builds on a pre-trained masked language model and modifies the inputs in a context-aware manner. We propose three contextualized perturbations, Replace, Insert and Merge, that allow for generating outputs of varied lengths. CLARE can flexibly combine these perturbations and apply them at any position in the inputs, and is thus able to attack the victim model more effectively with fewer edits. Extensive experiments and human evaluation demonstrate that CLARE outperforms the baselines in terms of attack success rate, textual similarity, fluency and grammaticality.",
        "author": "Dianqi Li; Yizhe Zhang; Hao Peng; Liqun Chen; Chris Brockett; Ming-Ting Sun; Bill Dolan",
        "authorids": "/d/dianqi-li/; /y/yizhe-zhang/; /h/hao-peng/; /l/liqun-chen/; /c/chris-brockett/; /m/ming-ting-sun/; /w/william-b-dolan/",
        "bibtex": "@inproceedings{li-etal-2021-contextualized,\n    title = \"Contextualized Perturbation for Textual Adversarial Attack\",\n    author = \"Li, Dianqi  and\n      Zhang, Yizhe  and\n      Peng, Hao  and\n      Chen, Liqun  and\n      Brockett, Chris  and\n      Sun, Ming-Ting  and\n      Dolan, Bill\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.400/\",\n    doi = \"10.18653/v1/2021.naacl-main.400\",\n    pages = \"5053--5069\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.400.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.400/",
        "pdf_size": 2051749,
        "gs_citation": 275,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14235085439673792398&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "University of Washington\u2660; Microsoft Research\u2666; University of Washington\u2660; Duke University\u2663; Microsoft Research\u2666; University of Washington\u2660; Microsoft Research\u2666",
        "aff_domain": "uw.edu;microsoft.com;cs.uw.edu;duke.edu;microsoft.com;uw.edu;microsoft.com",
        "email": "uw.edu;microsoft.com;cs.uw.edu;duke.edu;microsoft.com;uw.edu;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;0;2;1;0;1",
        "aff_unique_norm": "University of Washington;Microsoft Research;Duke University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.washington.edu;https://www.microsoft.com/en-us/research;https://www.duke.edu",
        "aff_unique_abbr": "UW;MSR;Duke",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.442",
        "title": "Contextualized and Generalized Sentence Representations by Contrastive Self-Supervised Learning: A Case Study on Discourse Relation Analysis",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We propose a method to learn contextualized and generalized sentence representations using contrastive self-supervised learning. In the proposed method, a model is given a text consisting of multiple sentences. One sentence is randomly selected as a target sentence. The model is trained to maximize the similarity between the representation of the target sentence with its context and that of the masked target sentence with the same context. Simultaneously, the model minimizes the similarity between the latter representation and the representation of a random sentence with the same context. We apply our method to discourse relation analysis in English and Japanese and show that it outperforms strong baseline methods based on BERT, XLNet, and RoBERTa.",
        "author": "Hirokazu Kiyomaru; Sadao Kurohashi",
        "authorids": "/h/hirokazu-kiyomaru/; /s/sadao-kurohashi/",
        "bibtex": "@inproceedings{kiyomaru-kurohashi-2021-contextualized,\n    title = \"Contextualized and Generalized Sentence Representations by Contrastive Self-Supervised Learning: A Case Study on Discourse Relation Analysis\",\n    author = \"Kiyomaru, Hirokazu  and\n      Kurohashi, Sadao\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.442/\",\n    doi = \"10.18653/v1/2021.naacl-main.442\",\n    pages = \"5578--5584\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.442.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.442/",
        "pdf_size": 779558,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9351173519634015268&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Graduate School of Informatics, Kyoto University; Graduate School of Informatics, Kyoto University",
        "aff_domain": "nlp.ist.i.kyoto-u.ac.jp;nlp.ist.i.kyoto-u.ac.jp",
        "email": "nlp.ist.i.kyoto-u.ac.jp;nlp.ist.i.kyoto-u.ac.jp",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Kyoto University",
        "aff_unique_dep": "Graduate School of Informatics",
        "aff_unique_url": "https://www.kyoto-u.ac.jp",
        "aff_unique_abbr": "Kyoto U",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Kyoto",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2021.naacl-main.310",
        "title": "Continual Learning for Neural Machine Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Neural machine translation (NMT) models are data-driven and require large-scale training corpus. In practical applications, NMT models are usually trained on a general domain corpus and then fine-tuned by continuing training on the in-domain corpus. However, this bears the risk of catastrophic forgetting that the performance on the general domain is decreased drastically. In this work, we propose a new continual learning framework for NMT models. We consider a scenario where the training is comprised of multiple stages and propose a dynamic knowledge distillation technique to alleviate the problem of catastrophic forgetting systematically. We also find that the bias exists in the output linear projection when fine-tuning on the in-domain corpus, and propose a bias-correction module to eliminate the bias. We conduct experiments on three representative settings of NMT application. Experimental results show that the proposed method achieves superior performance compared to baseline models in all settings.",
        "author": "Yue Cao; Hao-Ran Wei; Boxing Chen; Xiaojun Wan",
        "authorids": "/y/yue-cao/; /h/hao-ran-wei/; /b/boxing-chen/; /x/xiaojun-wan/",
        "bibtex": "@inproceedings{cao-etal-2021-continual,\n    title = \"Continual Learning for Neural Machine Translation\",\n    author = \"Cao, Yue  and\n      Wei, Hao-Ran  and\n      Chen, Boxing  and\n      Wan, Xiaojun\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.310/\",\n    doi = \"10.18653/v1/2021.naacl-main.310\",\n    pages = \"3964--3974\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.310.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.310/",
        "pdf_size": 440768,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7589683063777336020&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Wangxuan Institute of Computer Technology, Peking University + Center for Data Science, Peking University + The MOE Key Laboratory of Computational Linguistics, Peking University; Alibaba Group; Alibaba Group; Wangxuan Institute of Computer Technology, Peking University + Center for Data Science, Peking University + The MOE Key Laboratory of Computational Linguistics, Peking University",
        "aff_domain": "pku.edu.cn;alibaba-inc.com;alibaba-inc.com;pku.edu.cn",
        "email": "pku.edu.cn;alibaba-inc.com;alibaba-inc.com;pku.edu.cn",
        "github": "https://github.com/caoy1996/CL-NMT",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+0+0;1;1;0+0+0",
        "aff_unique_norm": "Peking University;Alibaba Group",
        "aff_unique_dep": "Wangxuan Institute of Computer Technology;",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.alibaba.com",
        "aff_unique_abbr": "PKU;Alibaba",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0+0+0;0;0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2021.naacl-main.218",
        "title": "Continual Learning for Text Classification with Information Disentanglement Based Regularization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Continual learning has become increasingly important as it enables NLP models to constantly learn and gain knowledge over time. Previous continual learning methods are mainly designed to preserve knowledge from previous tasks, without much emphasis on how to well generalize models to new tasks. In this work, we propose an information disentanglement based regularization method for continual learning on text classification. Our proposed method first disentangles text hidden spaces into representations that are generic to all tasks and representations specific to each individual task, and further regularizes these representations differently to better constrain the knowledge required to generalize. We also introduce two simple auxiliary tasks: next sentence prediction and task-id prediction, for learning better generic and specific representation spaces. Experiments conducted on large-scale benchmarks demonstrate the effectiveness of our method in continual text classification tasks with various sequences and lengths over state-of-the-art baselines. We have publicly released our code at https://github.com/GT-SALT/IDBR.",
        "author": "Yufan Huang; Yanzhe Zhang; Jiaao Chen; Xuezhi Wang; Diyi Yang",
        "authorids": "/y/yufan-huang/; /y/yanzhe-zhang/; /j/jiaao-chen/; /x/xuezhi-wang/; /d/diyi-yang/",
        "bibtex": "@inproceedings{huang-etal-2021-continual,\n    title = \"Continual Learning for Text Classification with Information Disentanglement Based Regularization\",\n    author = \"Huang, Yufan  and\n      Zhang, Yanzhe  and\n      Chen, Jiaao  and\n      Wang, Xuezhi  and\n      Yang, Diyi\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.218/\",\n    doi = \"10.18653/v1/2021.naacl-main.218\",\n    pages = \"2736--2746\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.218.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.218/",
        "pdf_size": 1107233,
        "gs_citation": 109,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9937070112602394370&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Georgia Institute of Technology; Georgia Institute of Technology+Zhejiang University; Georgia Institute of Technology; Google; Georgia Institute of Technology",
        "aff_domain": "gatech.edu;zju.edu.cn;gatech.edu;google.com;gatech.edu",
        "email": "gatech.edu;zju.edu.cn;gatech.edu;google.com;gatech.edu",
        "github": "https://github.com/GT-SALT/IDBR",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0+1;0;2;0",
        "aff_unique_norm": "Georgia Institute of Technology;Zhejiang University;Google",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.gatech.edu;https://www.zju.edu.cn;https://www.google.com",
        "aff_unique_abbr": "Georgia Tech;ZJU;Google",
        "aff_campus_unique_index": ";1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;0+1;0;0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2021.naacl-industry.8",
        "title": "Continuous Model Improvement for Language Understanding with Machine Translation",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Scaling conversational personal assistants to a multitude of languages puts high demands on collecting and labelling data, a setting in which cross-lingual learning techniques can help to reconcile the need for well-performing Natural Language Understanding (NLU) with a desideratum to support many languages without incurring unacceptable cost. In this work, we show that automatically annotating unlabeled utterances using Machine Translation in an offline fashion and adding them to the training data can improve performance for existing NLU features for low-resource languages, where a straightforward translate-test approach as considered in existing literature would fail the latency requirements of a live environment. We demonstrate the effectiveness of our method with intrinsic and extrinsic evaluation using a real-world commercial dialog system in German. Beyond an intrinsic evaluation, where 56% of the resulting automatically labeled utterances had a perfect match with ground-truth labels, we see significant performance improvements in an extrinsic evaluation settings when manual labeled data is available in small quantities.",
        "author": "Abdalghani Abujabal; Claudio Delli Bovi; Sungho Ryu; Turan Gojayev; Fabian Triefenbach; Yannick Versley",
        "authorids": "/a/abdalghani-abujabal/; /c/claudio-delli-bovi/; /s/sungho-ryu/; /t/turan-gojayev/; /f/fabian-triefenbach/; /y/yannick-versley/",
        "bibtex": "@inproceedings{abujabal-etal-2021-continuous,\n    title = \"Continuous Model Improvement for Language Understanding with Machine Translation\",\n    author = \"Abujabal, Abdalghani  and\n      Delli Bovi, Claudio  and\n      Ryu, Sungho  and\n      Gojayev, Turan  and\n      Triefenbach, Fabian  and\n      Versley, Yannick\",\n    editor = \"Kim, Young-bum  and\n      Li, Yunyao  and\n      Rambow, Owen\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-industry.8/\",\n    doi = \"10.18653/v1/2021.naacl-industry.8\",\n    pages = \"56--62\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-industry.8.pdf",
        "site": "https://aclanthology.org/2021.naacl-industry.8/",
        "pdf_size": 295524,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18231298959247131764&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Amazon Alexa AI, Germany; Amazon Alexa AI, Germany; Amazon Alexa AI, Germany; Amazon Alexa AI, Germany; Amazon Alexa AI, Germany; Amazon Alexa AI, Germany",
        "aff_domain": "amazon.de;amazon.de;amazon.de;amazon.de;amazon.de;amazon.de",
        "email": "amazon.de;amazon.de;amazon.de;amazon.de;amazon.de;amazon.de",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Amazon Alexa AI",
        "aff_unique_dep": "AI",
        "aff_unique_url": "https://www.amazon.de/alexia",
        "aff_unique_abbr": "Amazon Alexa AI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2021.naacl-main.277",
        "title": "Controllable Text Simplification with Explicit Paraphrasing",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Text Simplification improves the readability of sentences through several rewriting transformations, such as lexical paraphrasing, deletion, and splitting. Current simplification systems are predominantly sequence-to-sequence models that are trained end-to-end to perform all these operations simultaneously. However, such systems limit themselves to mostly deleting words and cannot easily adapt to the requirements of different target audiences. In this paper, we propose a novel hybrid approach that leverages linguistically-motivated rules for splitting and deletion, and couples them with a neural paraphrasing model to produce varied rewriting styles. We introduce a new data augmentation method to improve the paraphrasing capability of our model. Through automatic and manual evaluations, we show that our proposed model establishes a new state-of-the-art for the task, paraphrasing more often than the existing systems, and can control the degree of each simplification operation applied to the input texts.",
        "author": "Mounica Maddela; Fernando Alva-Manchego; Wei Xu",
        "authorids": "/m/mounica-maddela/; /f/fernando-alva-manchego/; /w/wei-xu/",
        "bibtex": "@inproceedings{maddela-etal-2021-controllable,\n    title = \"Controllable Text Simplification with Explicit Paraphrasing\",\n    author = \"Maddela, Mounica  and\n      Alva-Manchego, Fernando  and\n      Xu, Wei\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.277/\",\n    doi = \"10.18653/v1/2021.naacl-main.277\",\n    pages = \"3536--3553\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.277.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.277/",
        "pdf_size": 1459231,
        "gs_citation": 120,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6875250744020223317&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "School of Interactive Computing, Georgia Institute of Technology; Department of Computer Science, University of Sheffield; School of Interactive Computing, Georgia Institute of Technology",
        "aff_domain": "cc.gatech.edu;sheffield.ac.uk;cc.gatech.edu",
        "email": "cc.gatech.edu;sheffield.ac.uk;cc.gatech.edu",
        "github": "https://github.com/mounicam/controllable_simplification",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Georgia Institute of Technology;University of Sheffield",
        "aff_unique_dep": "School of Interactive Computing;Department of Computer Science",
        "aff_unique_url": "https://www.gatech.edu;https://www.sheffield.ac.uk",
        "aff_unique_abbr": "Georgia Tech;Sheffield",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Atlanta;",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "id": "2021.naacl-main.240",
        "title": "Controlling Dialogue Generation with Semantic Exemplars",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Dialogue systems pretrained with large language models generate locally coherent responses, but lack fine-grained control over responses necessary to achieve specific goals. A promising method to control response generation is exemplar-based generation, in which models edit exemplar responses that are retrieved from training data, or hand-written to strategically address discourse-level goals, to fit new dialogue contexts. We present an Exemplar-based Dialogue Generation model, EDGE, that uses the semantic frames present in exemplar responses to guide response generation. We show that controlling dialogue generation based on the semantic frames of exemplars improves the coherence of generated responses, while preserving semantic meaning and conversation goals present in exemplar responses.",
        "author": "Prakhar Gupta; Jeffrey Bigham; Yulia Tsvetkov; Amy Pavel",
        "authorids": "/p/prakhar-gupta/; /j/jeffrey-p-bigham/; /y/yulia-tsvetkov/; /a/amy-pavel/",
        "bibtex": "@inproceedings{gupta-etal-2021-controlling,\n    title = \"Controlling Dialogue Generation with Semantic Exemplars\",\n    author = \"Gupta, Prakhar  and\n      Bigham, Jeffrey  and\n      Tsvetkov, Yulia  and\n      Pavel, Amy\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.240/\",\n    doi = \"10.18653/v1/2021.naacl-main.240\",\n    pages = \"3018--3029\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.240.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.240/",
        "pdf_size": 791379,
        "gs_citation": 47,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11144609435161287808&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 9,
        "aff": "Language Technologies Institute, Carnegie Mellon University + Human-Computer Interaction Institute, Carnegie Mellon University; Language Technologies Institute, Carnegie Mellon University + Human-Computer Interaction Institute, Carnegie Mellon University; Language Technologies Institute, Carnegie Mellon University; Human-Computer Interaction Institute, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "github": "https://github.com/prakharguptaz/EDGE-exemplars",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+0;0+0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Language Technologies Institute",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0+0;0+0;0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0+0;0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-industry.24",
        "title": "Cost-effective Deployment of BERT Models in Serverless Environment",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "In this study, we demonstrate the viability of deploying BERT-style models to AWS Lambda in a production environment. Since the freely available pre-trained models are too large to be deployed in this environment, we utilize knowledge distillation and fine-tune the models on proprietary datasets for two real-world tasks: sentiment analysis and semantic textual similarity. As a result, we obtain models that are tuned for a specific domain and deployable in the serverless environment. The subsequent performance analysis shows that this solution does not only report latency levels acceptable for production use but that it is also a cost-effective alternative to small-to-medium size deployments of BERT models, all without any infrastructure overhead.",
        "author": "Marek Suppa; Katar\u00edna Bene\u0161ov\u00e1; Andrej \u0160vec",
        "authorids": "/m/marek-suppa/; /k/katarina-benesova/; /a/andrej-svec/",
        "bibtex": "@inproceedings{suppa-etal-2021-cost,\n    title = \"Cost-effective Deployment of {BERT} Models in Serverless Environment\",\n    author = \"Suppa, Marek  and\n      Bene{\\v{s}}ov{\\'a}, Katar{\\'i}na  and\n      {\\v{S}}vec, Andrej\",\n    editor = \"Kim, Young-bum  and\n      Li, Yunyao  and\n      Rambow, Owen\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-industry.24/\",\n    doi = \"10.18653/v1/2021.naacl-industry.24\",\n    pages = \"187--195\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-industry.24.pdf",
        "site": "https://aclanthology.org/2021.naacl-industry.24/",
        "pdf_size": 527817,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9248887799011095889&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Slido; Slido; Slido",
        "aff_domain": "slido.com;slido.com;slido.com",
        "email": "slido.com;slido.com;slido.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Slido",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.slido.com",
        "aff_unique_abbr": "Slido",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Unknown"
    },
    {
        "id": "2021.naacl-main.18",
        "title": "Counterfactual Data Augmentation for Neural Machine Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We propose a data augmentation method for neural machine translation. It works by interpreting language models and phrasal alignment causally. Specifically, it creates augmented parallel translation corpora by generating (path-specific) counterfactual aligned phrases. We generate these by sampling new source phrases from a masked language model, then sampling an aligned counterfactual target phrase by noting that a translation language model can be interpreted as a Gumbel-Max Structural Causal Model (Oberst and Sontag, 2019). Compared to previous work, our method takes both context and alignment into account to maintain the symmetry between source and target sequences. Experiments on IWSLT\u201915 English \u2192 Vietnamese, WMT\u201917 English \u2192 German, WMT\u201918 English \u2192 Turkish, and WMT\u201919 robust English \u2192 French show that the method can improve the performance of translation, backtranslation and translation robustness.",
        "author": "Qi Liu; Matt Kusner; Phil Blunsom",
        "authorids": "/q/qi-liu/; /m/matt-kusner/; /p/phil-blunsom/",
        "bibtex": "@inproceedings{liu-etal-2021-counterfactual,\n    title = \"Counterfactual Data Augmentation for Neural Machine Translation\",\n    author = \"Liu, Qi  and\n      Kusner, Matt  and\n      Blunsom, Phil\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.18/\",\n    doi = \"10.18653/v1/2021.naacl-main.18\",\n    pages = \"187--197\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.18.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.18/",
        "pdf_size": 1439213,
        "gs_citation": 60,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17650255543135408141&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "University of Oxford; University College London; DeepMind",
        "aff_domain": "cs.ox.ac.uk;ucl.ac.uk;deepmind.com",
        "email": "cs.ox.ac.uk;ucl.ac.uk;deepmind.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University of Oxford;University College London;DeepMind",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.ox.ac.uk;https://www.ucl.ac.uk;https://deepmind.com",
        "aff_unique_abbr": "Oxford;UCL;DeepMind",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2021.naacl-main.156",
        "title": "Counterfactual Supporting Facts Extraction for Explainable Medical Record Based Diagnosis with Graph Network",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Providing a reliable explanation for clinical diagnosis based on the Electronic Medical Record (EMR) is fundamental to the application of Artificial Intelligence in the medical field. Current methods mostly treat the EMR as a text sequence and provide explanations based on a precise medical knowledge base, which is disease-specific and difficult to obtain for experts in reality. Therefore, we propose a counterfactual multi-granularity graph supporting facts extraction (CMGE) method to extract supporting facts from irregular EMR itself without external knowledge bases in this paper. Specifically, we first structure the sequence of EMR into a hierarchical graph network and then obtain the causal relationship between multi-granularity features and diagnosis results through counterfactual intervention on the graph. Features having the strongest causal connection with the results provide interpretive support for the diagnosis. Experimental results on real Chinese EMR of the lymphedema demonstrate that our method can diagnose four types of EMR correctly, and can provide accurate supporting facts for the results. More importantly, the results on different diseases demonstrate the robustness of our approach, which represents the potential application in the medical field.",
        "author": "Haoran Wu; Wei Chen; Shuang Xu; Bo Xu",
        "authorids": "/h/haoran-wu/; /w/wei-chen/; /s/shuang-xu/; /b/bo-xu/",
        "bibtex": "@inproceedings{wu-etal-2021-counterfactual,\n    title = \"Counterfactual Supporting Facts Extraction for Explainable Medical Record Based Diagnosis with Graph Network\",\n    author = \"Wu, Haoran  and\n      Chen, Wei  and\n      Xu, Shuang  and\n      Xu, Bo\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.156/\",\n    doi = \"10.18653/v1/2021.naacl-main.156\",\n    pages = \"1942--1955\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.156.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.156/",
        "pdf_size": 1428411,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10717484899723801933&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 0,
        "aff": "Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China+School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, 100049, China; Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China; Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China; Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China+School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, 100049, China",
        "aff_domain": "ia.ac.cn;ia.ac.cn;ia.ac.cn;ia.ac.cn",
        "email": "ia.ac.cn;ia.ac.cn;ia.ac.cn;ia.ac.cn",
        "github": "https://github.com/CKRE/CMGE",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0;0;0+1",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Automation;School of Artificial Intelligence",
        "aff_unique_url": "http://www.ia.cas.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "CAS;UCAS",
        "aff_campus_unique_index": "0+0;0;0;0+0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0+0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2021.naacl-main.214",
        "title": "Cross-Lingual Word Embedding Refinement by \u21131 Norm Optimisation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Cross-Lingual Word Embeddings (CLWEs) encode words from two or more languages in a shared high-dimensional space in which vectors representing words with similar meaning (regardless of language) are closely located. Existing methods for building high-quality CLWEs learn mappings that minimise the \u21132 norm loss function. However, this optimisation objective has been demonstrated to be sensitive to outliers. Based on the more robust Manhattan norm (aka. \u21131 norm) goodness-of-fit criterion, this paper proposes a simple post-processing step to improve CLWEs. An advantage of this approach is that it is fully agnostic to the training process of the original CLWEs and can therefore be applied widely. Extensive experiments are performed involving ten diverse languages and embeddings trained on different corpora. Evaluation results based on bilingual lexicon induction and cross-lingual transfer for natural language inference tasks show that the \u21131 refinement substantially outperforms four state-of-the-art baselines in both supervised and unsupervised settings. It is therefore recommended that this strategy be adopted as a standard for CLWE methods.",
        "author": "Xutan Peng; Chenghua Lin; Mark Stevenson",
        "authorids": "/x/xutan-peng/; /c/chenghua-lin/; /m/mark-stevenson/",
        "bibtex": "@inproceedings{peng-etal-2021-cross,\n    title = \"Cross-Lingual Word Embedding Refinement by $\\ell_{1}$ Norm Optimisation\",\n    author = \"Peng, Xutan  and\n      Lin, Chenghua  and\n      Stevenson, Mark\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.214/\",\n    doi = \"10.18653/v1/2021.naacl-main.214\",\n    pages = \"2690--2701\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.214.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.214/",
        "pdf_size": 1380072,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5798572634414368107&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, The University of Shef\ufb01eld, UK; Department of Computer Science, The University of Shef\ufb01eld, UK; Department of Computer Science, The University of Shef\ufb01eld, UK",
        "aff_domain": "shef.ac.uk;shef.ac.uk;shef.ac.uk",
        "email": "shef.ac.uk;shef.ac.uk;shef.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "The University of Sheffield",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.sheffield.ac.uk",
        "aff_unique_abbr": "Sheffield",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2021.naacl-main.3",
        "title": "Cross-Task Instance Representation Interactions and Label Dependencies for Joint Information Extraction with Graph Convolutional Networks",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Existing works on information extraction (IE) have mainly solved the four main tasks separately (entity mention recognition, relation extraction, event trigger detection, and argument extraction), thus failing to benefit from inter-dependencies between tasks. This paper presents a novel deep learning model to simultaneously solve the four tasks of IE in a single model (called FourIE). Compared to few prior work on jointly performing four IE tasks, FourIE features two novel contributions to capture inter-dependencies between tasks. First, at the representation level, we introduce an interaction graph between instances of the four tasks that is used to enrich the prediction representation for one instance with those from related instances of other tasks. Second, at the label level, we propose a dependency graph for the information types in the four IE tasks that captures the connections between the types expressed in an input sentence. A new regularization mechanism is introduced to enforce the consistency between the golden and predicted type dependency graphs to improve representation learning. We show that the proposed model achieves the state-of-the-art performance for joint IE on both monolingual and multilingual learning settings with three different languages.",
        "author": "Minh Van Nguyen; Viet Dac Lai; Thien Huu Nguyen",
        "authorids": "/m/minh-van-nguyen/; /v/viet-dac-lai/; /t/thien-huu-nguyen/",
        "bibtex": "@inproceedings{nguyen-etal-2021-cross,\n    title = \"Cross-Task Instance Representation Interactions and Label Dependencies for Joint Information Extraction with Graph Convolutional Networks\",\n    author = \"Nguyen, Minh Van  and\n      Lai, Viet Dac  and\n      Nguyen, Thien Huu\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.3/\",\n    doi = \"10.18653/v1/2021.naacl-main.3\",\n    pages = \"27--38\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.3.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.3/",
        "pdf_size": 651863,
        "gs_citation": 75,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18332597809089239935&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Computer and Information Science, University of Oregon, Eugene, Oregon, USA; Department of Computer and Information Science, University of Oregon, Eugene, Oregon, USA; Department of Computer and Information Science, University of Oregon, Eugene, Oregon, USA",
        "aff_domain": "cs.uoregon.edu;cs.uoregon.edu;cs.uoregon.edu",
        "email": "cs.uoregon.edu;cs.uoregon.edu;cs.uoregon.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Oregon",
        "aff_unique_dep": "Department of Computer and Information Science",
        "aff_unique_url": "https://www.uoregon.edu",
        "aff_unique_abbr": "UO",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Eugene",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.285",
        "title": "Cross-lingual Cross-modal Pretraining for Multimodal Retrieval",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent pretrained vision-language models have achieved impressive performance on cross-modal retrieval tasks in English. Their success, however, heavily depends on the availability of many annotated image-caption datasets for pretraining, where the texts are not necessarily in English. Although we can utilize machine translation (MT) tools to translate non-English text to English, the performance still largely relies on MT\u2019s quality and may suffer from high latency problems in real-world applications. This paper proposes a new approach to learn cross-lingual cross-modal representations for matching images and their relevant captions in multiple languages. We seamlessly combine cross-lingual pretraining objectives and cross-modal pretraining objectives in a unified framework to learn image and text in a joint embedding space from available English image-caption data, monolingual and parallel corpus. We show that our approach achieves SOTA performance in retrieval tasks on two multimodal multilingual image caption benchmarks: Multi30k with German captions and MSCOCO with Japanese captions.",
        "author": "Hongliang Fei; Tan Yu; Ping Li",
        "authorids": "/h/hongliang-fei/; /t/tan-yu/; /p/ping-li/",
        "bibtex": "@inproceedings{fei-etal-2021-cross,\n    title = \"Cross-lingual Cross-modal Pretraining for Multimodal Retrieval\",\n    author = \"Fei, Hongliang  and\n      Yu, Tan  and\n      Li, Ping\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.285/\",\n    doi = \"10.18653/v1/2021.naacl-main.285\",\n    pages = \"3644--3650\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.285.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.285/",
        "pdf_size": 1613648,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14898732271269644444&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3
    },
    {
        "id": "2021.naacl-industry.12",
        "title": "Cross-lingual Supervision Improves Unsupervised Neural Machine Translation",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "We propose to improve unsupervised neural machine translation with cross-lingual supervision (), which utilizes supervision signals from high resource language pairs to improve the translation of zero-source languages. Specifically, for training En-Ro system without parallel corpus, we can leverage the corpus from En-Fr and En-De to collectively train the translation from one language into many languages under one model. % is based on multilingual models which require no changes to the standard unsupervised NMT. Simple and effective, significantly improves the translation quality with a big margin in the benchmark unsupervised translation tasks, and even achieves comparable performance to supervised NMT. In particular, on WMT\u201914 -tasks achieves 37.6 and 35.18 BLEU score, which is very close to the large scale supervised setting and on WMT\u201916 -tasks achieves 35.09 BLEU score which is even better than the supervised Transformer baseline.",
        "author": "Mingxuan Wang; Hongxiao Bai; Hai Zhao; Lei Li",
        "authorids": "/m/mingxuan-wang/; /h/hongxiao-bai/; /h/hai-zhao/; /l/lei-li/",
        "bibtex": "@inproceedings{wang-etal-2021-cross,\n    title = \"Cross-lingual Supervision Improves Unsupervised Neural Machine Translation\",\n    author = \"Wang, Mingxuan  and\n      Bai, Hongxiao  and\n      Zhao, Hai  and\n      Li, Lei\",\n    editor = \"Kim, Young-bum  and\n      Li, Yunyao  and\n      Rambow, Owen\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-industry.12/\",\n    doi = \"10.18653/v1/2021.naacl-industry.12\",\n    pages = \"89--96\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-industry.12.pdf",
        "site": "https://aclanthology.org/2021.naacl-industry.12/",
        "pdf_size": 1123798,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10805246621196526971&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "ByteDance AI Lab, Beijing, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University; ByteDance AI Lab, Beijing, China",
        "aff_domain": "bytedance.com;cs.sjtu.edu.cn;cs.sjtu.edu.cn;bytedance.com",
        "email": "bytedance.com;cs.sjtu.edu.cn;cs.sjtu.edu.cn;bytedance.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "ByteDance;Shanghai Jiao Tong University",
        "aff_unique_dep": "AI Lab;Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.bytedance.com;https://www.sjtu.edu.cn",
        "aff_unique_abbr": ";SJTU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2021.naacl-main.19",
        "title": "Cultural and Geographical Influences on Image Translatability of Words across Languages",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Neural Machine Translation (NMT) models have been observed to produce poor translations when there are few/no parallel sentences to train the models. In the absence of parallel data, several approaches have turned to the use of images to learn translations. Since images of words, e.g., horse may be unchanged across languages, translations can be identified via images associated with words in different languages that have a high degree of visual similarity. However, translating via images has been shown to improve upon text-only models only marginally. To better understand when images are useful for translation, we study image translatability of words, which we define as the translatability of words via images, by measuring intra- and inter-cluster similarities of image representations of words that are translations of each other. We find that images of words are not always invariant across languages, and that language pairs with shared culture, meaning having either a common language family, ethnicity or religion, have improved image translatability (i.e., have more similar images for similar words) compared to its converse, regardless of their geographic proximity. In addition, in line with previous works that show images help more in translating concrete words, we found that concrete words have improved image translatability compared to abstract ones.",
        "author": "Nikzad Khani; Isidora Tourni; Mohammad Sadegh Rasooli; Chris Callison-Burch; Derry Tanti Wijaya",
        "authorids": "/n/nikzad-khani/; /i/isidora-tourni/; /m/mohammad-sadegh-rasooli/; /c/chris-callison-burch/; /d/derry-tanti-wijaya/",
        "bibtex": "@inproceedings{khani-etal-2021-cultural,\n    title = \"Cultural and Geographical Influences on Image Translatability of Words across Languages\",\n    author = \"Khani, Nikzad  and\n      Tourni, Isidora  and\n      Rasooli, Mohammad Sadegh  and\n      Callison-Burch, Chris  and\n      Wijaya, Derry Tanti\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.19/\",\n    doi = \"10.18653/v1/2021.naacl-main.19\",\n    pages = \"198--209\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.19.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.19/",
        "pdf_size": 993659,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18341729206004648947&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Boston University; Boston University; University of Pennsylvania; University of Pennsylvania; Boston University",
        "aff_domain": "bu.edu;bu.edu;upenn.edu;upenn.edu;bu.edu",
        "email": "bu.edu;bu.edu;upenn.edu;upenn.edu;bu.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;1;0",
        "aff_unique_norm": "Boston University;University of Pennsylvania",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.bu.edu;https://www.upenn.edu",
        "aff_unique_abbr": "BU;UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.111",
        "title": "D2S: Document-to-Slide Generation Via Query-Based Text Summarization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Presentations are critical for communication in all areas of our lives, yet the creation of slide decks is often tedious and time-consuming. There has been limited research aiming to automate the document-to-slides generation process and all face a critical challenge: no publicly available dataset for training and benchmarking. In this work, we first contribute a new dataset, SciDuet, consisting of pairs of papers and their corresponding slides decks from recent years\u2019 NLP and ML conferences (e.g., ACL). Secondly, we present D2S, a novel system that tackles the document-to-slides task with a two-step approach: 1) Use slide titles to retrieve relevant and engaging text, figures, and tables; 2) Summarize the retrieved context into bullet points with long-form question answering. Our evaluation suggests that long-form QA outperforms state-of-the-art summarization baselines on both automated ROUGE metrics and qualitative human evaluation.",
        "author": "Edward Sun; Yufang Hou; Dakuo Wang; Yunfeng Zhang; Nancy X. R. Wang",
        "authorids": "/e/edward-sun/; /y/yufang-hou/; /d/dakuo-wang/; /y/yunfeng-zhang/; /n/nancy-x-r-wang/",
        "bibtex": "@inproceedings{sun-etal-2021-d2s,\n    title = \"{D}2{S}: Document-to-Slide Generation Via Query-Based Text Summarization\",\n    author = \"Sun, Edward  and\n      Hou, Yufang  and\n      Wang, Dakuo  and\n      Zhang, Yunfeng  and\n      Wang, Nancy X. R.\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.111/\",\n    doi = \"10.18653/v1/2021.naacl-main.111\",\n    pages = \"1405--1418\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.111.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.111/",
        "pdf_size": 755286,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16757894580450949882&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "University of Michigan; IBM Research Europe; IBM Research; IBM Research; IBM Research",
        "aff_domain": "umich.edu;ie.ibm.com;ibm.com;us.ibm.com;gmail.com",
        "email": "umich.edu;ie.ibm.com;ibm.com;us.ibm.com;gmail.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;2;2;2",
        "aff_unique_norm": "University of Michigan;IBM Research;IBM",
        "aff_unique_dep": ";Research;IBM Research",
        "aff_unique_url": "https://www.umich.edu;https://www.ibm.com/research/europe;https://www.ibm.com/research",
        "aff_unique_abbr": "UM;IBM Research Europe;IBM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0;0",
        "aff_country_unique": "United States;Unknown"
    },
    {
        "id": "2021.naacl-main.166",
        "title": "DA-Transformer: Distance-aware Transformer",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Transformer has achieved great success in the NLP field by composing various advanced models like BERT and GPT. However, Transformer and its existing variants may not be optimal in capturing token distances because the position or distance embeddings used by these methods usually cannot keep the precise information of real distances, which may not be beneficial for modeling the orders and relations of contexts. In this paper, we propose DA-Transformer, which is a distance-aware Transformer that can exploit the real distance. We propose to incorporate the real distances between tokens to re-scale the raw self-attention weights, which are computed by the relevance between attention query and key. Concretely, in different self-attention heads the relative distance between each pair of tokens is weighted by different learnable parameters, which control the different preferences on long- or short-term information of these heads. Since the raw weighted real distances may not be optimal for adjusting self-attention weights, we propose a learnable sigmoid function to map them into re-scaled coefficients that have proper ranges. We first clip the raw self-attention weights via the ReLU function to keep non-negativity and introduce sparsity, and then multiply them with the re-scaled coefficients to encode real distance information into self-attention. Extensive experiments on five benchmark datasets show that DA-Transformer can effectively improve the performance of many tasks and outperform the vanilla Transformer and its several variants.",
        "author": "Chuhan Wu; Fangzhao Wu; Yongfeng Huang",
        "authorids": "/c/chuhan-wu/; /f/fangzhao-wu/; /y/yongfeng-huang/",
        "bibtex": "@inproceedings{wu-etal-2021-da,\n    title = \"{DA}-Transformer: Distance-aware Transformer\",\n    author = \"Wu, Chuhan  and\n      Wu, Fangzhao  and\n      Huang, Yongfeng\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.166/\",\n    doi = \"10.18653/v1/2021.naacl-main.166\",\n    pages = \"2059--2068\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.166.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.166/",
        "pdf_size": 2852960,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1937690852046072304&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Electronic Engineering & BNRist, Tsinghua University, Beijing 100084, China; Microsoft Research Asia, Beijing 100080, China; Department of Electronic Engineering & BNRist, Tsinghua University, Beijing 100084, China",
        "aff_domain": "gmail.com;gmail.com;tsinghua.edu.cn",
        "email": "gmail.com;gmail.com;tsinghua.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Tsinghua University;Microsoft Research Asia",
        "aff_unique_dep": "Department of Electronic Engineering;Research",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.microsoft.com/en-us/research/group/asia",
        "aff_unique_abbr": "THU;MSRA",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2021.naacl-main.467",
        "title": "DAGN: Discourse-Aware Graph Network for Logical Reasoning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent QA with logical reasoning questions requires passage-level relations among the sentences. However, current approaches still focus on sentence-level relations interacting among tokens. In this work, we explore aggregating passage-level clues for solving logical reasoning QA by using discourse-based information. We propose a discourse-aware graph network (DAGN) that reasons relying on the discourse structure of the texts. The model encodes discourse information as a graph with elementary discourse units (EDUs) and discourse relations, and learns the discourse-aware features via a graph network for downstream QA tasks. Experiments are conducted on two logical reasoning QA datasets, ReClor and LogiQA, and our proposed DAGN achieves competitive results. The source code is available at https://github.com/Eleanor-H/DAGN.",
        "author": "Yinya Huang; Meng Fang; Yu Cao; Liwei Wang; Xiaodan Liang",
        "authorids": "/y/yinya-huang/; /m/meng-fang/; /y/yu-cao/; /l/liwei-wang/; /x/xiaodan-liang/",
        "bibtex": "@inproceedings{huang-etal-2021-dagn,\n    title = \"{DAGN}: Discourse-Aware Graph Network for Logical Reasoning\",\n    author = \"Huang, Yinya  and\n      Fang, Meng  and\n      Cao, Yu  and\n      Wang, Liwei  and\n      Liang, Xiaodan\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.467/\",\n    doi = \"10.18653/v1/2021.naacl-main.467\",\n    pages = \"5848--5855\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.467.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.467/",
        "pdf_size": 1111261,
        "gs_citation": 66,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6434939069898754221&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Shenzhen Campus of Sun Yat-sen University; Tencent Robotics X; School of Computer Science, The University of Sydney; The Chinese University of Hong Kong; Shenzhen Campus of Sun Yat-sen University",
        "aff_domain": "hotmail.com;tencent.com;uni.sydney.edu.au;cse.cuhk.edu.hk;gmail.com",
        "email": "hotmail.com;tencent.com;uni.sydney.edu.au;cse.cuhk.edu.hk;gmail.com",
        "github": "https://github.com/Eleanor-H/DAGN",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;2;3;0",
        "aff_unique_norm": "Sun Yat-sen University;Tencent;The University of Sydney;The Chinese University of Hong Kong",
        "aff_unique_dep": ";Tencent Robotics X;School of Computer Science;",
        "aff_unique_url": "http://www.sysu.edu.cn/;https://www.tencent.com;https://www.sydney.edu.au;https://www.cuhk.edu.hk",
        "aff_unique_abbr": "SYSU;Tencent Robotics X;USYD;CUHK",
        "aff_campus_unique_index": "0;2;3;0",
        "aff_campus_unique": "Shenzhen;;Sydney;Hong Kong SAR",
        "aff_country_unique_index": "0;0;1;0;0",
        "aff_country_unique": "China;Australia"
    },
    {
        "id": "2021.naacl-main.37",
        "title": "DART: Open-Domain Structured Data Record to Text Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We present DART, an open domain structured DAta Record to Text generation dataset with over 82k instances (DARTs). Data-to-text annotations can be a costly process, especially when dealing with tables which are the major source of structured data and contain nontrivial structures. To this end, we propose a procedure of extracting semantic triples from tables that encodes their structures by exploiting the semantic dependencies among table headers and the table title. Our dataset construction framework effectively merged heterogeneous sources from open domain semantic parsing and spoken dialogue systems by utilizing techniques including tree ontology annotation, question-answer pair to declarative sentence conversion, and predicate unification, all with minimum post-editing. We present systematic evaluation on DART as well as new state-of-the-art results on WebNLG 2017 to show that DART (1) poses new challenges to existing data-to-text datasets and (2) facilitates out-of-domain generalization. Our data and code can be found at https://github.com/Yale-LILY/dart.",
        "author": "Linyong Nan; Dragomir Radev; Rui Zhang; Amrit Rau; Abhinand Sivaprasad; Chiachun Hsieh; Xiangru Tang; Aadit Vyas; Neha Verma; Pranav Krishna; Yangxiaokang Liu; Nadia Irwanto; Jessica Pan; Faiaz Rahman; Ahmad Zaidi; Mutethia Mutuma; Yasin Tarabar; Ankit Gupta; Tao Yu; Yi Chern Tan; Xi Victoria Lin; Caiming Xiong; Richard Socher; Nazneen Fatema Rajani",
        "authorids": "/l/linyong-nan/; /d/dragomir-radev/; /r/rui-zhang/; /a/amrit-rau/; /a/abhinand-sivaprasad/; /c/chiachun-hsieh/; /x/xiangru-tang/; /a/aadit-vyas/; /n/neha-verma/; /p/pranav-krishna/; /y/yangxiaokang-liu/; /n/nadia-irwanto/; /j/jessica-pan/; /f/faiaz-rahman/; /a/ahmad-zaidi/; /m/mutethia-mutuma/; /y/yasin-tarabar/; /a/ankit-gupta/; /t/tao-yu/; /y/yi-chern-tan/; /x/xi-victoria-lin/; /c/caiming-xiong/; /r/richard-socher/; /n/nazneen-fatema-rajani/",
        "bibtex": "@inproceedings{nan-etal-2021-dart,\n    title = \"{DART}: Open-Domain Structured Data Record to Text Generation\",\n    author = \"Nan, Linyong  and\n      Radev, Dragomir  and\n      Zhang, Rui  and\n      Rau, Amrit  and\n      Sivaprasad, Abhinand  and\n      Hsieh, Chiachun  and\n      Tang, Xiangru  and\n      Vyas, Aadit  and\n      Verma, Neha  and\n      Krishna, Pranav  and\n      Liu, Yangxiaokang  and\n      Irwanto, Nadia  and\n      Pan, Jessica  and\n      Rahman, Faiaz  and\n      Zaidi, Ahmad  and\n      Mutuma, Mutethia  and\n      Tarabar, Yasin  and\n      Gupta, Ankit  and\n      Yu, Tao  and\n      Tan, Yi Chern  and\n      Lin, Xi Victoria  and\n      Xiong, Caiming  and\n      Socher, Richard  and\n      Rajani, Nazneen Fatema\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.37/\",\n    doi = \"10.18653/v1/2021.naacl-main.37\",\n    pages = \"432--447\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.37.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.37/",
        "pdf_size": 1925792,
        "gs_citation": 200,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1216103011597283164&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Yale University; Yale University+Salesforce Research; Penn State University; Yale University; Yale University; The University of Hong Kong; Yale University; Yale University; Yale University; MIT; Yale University; Yale University; Yale University; Yale University; Yale University; Yale University; Yale University; Yale University; Yale University; Yale University; Salesforce Research; Salesforce Research; Salesforce Research; Salesforce Research",
        "aff_domain": "yale.edu;yale.edu;psu.edu; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ;salesforce.com; ; ; ",
        "email": "yale.edu;yale.edu;psu.edu; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ;salesforce.com; ; ; ",
        "github": "https://github.com/Yale-LILY/dart",
        "project": "",
        "author_num": 24,
        "aff_unique_index": "0;0+1;2;0;0;3;0;0;0;4;0;0;0;0;0;0;0;0;0;0;1;1;1;1",
        "aff_unique_norm": "Yale University;Salesforce;Penn State University;The University of Hong Kong;Massachusetts Institute of Technology",
        "aff_unique_dep": ";Salesforce Research;;;",
        "aff_unique_url": "https://www.yale.edu;https://research.salesforce.com;https://www.psu.edu;https://www.hku.hk;https://web.mit.edu",
        "aff_unique_abbr": "Yale;Salesforce;PSU;HKU;MIT",
        "aff_campus_unique_index": ";1",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "0;0+0;0;0;0;1;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2021.naacl-main.25",
        "title": "DATE: Detecting Anomalies in Text via Self-Supervision of Transformers",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Leveraging deep learning models for Anomaly Detection (AD) has seen widespread use in recent years due to superior performances over traditional methods. Recent deep methods for anomalies in images learn better features of normality in an end-to-end self-supervised setting. These methods train a model to discriminate between different transformations applied to visual data and then use the output to compute an anomaly score. We use this approach for AD in text, by introducing a novel pretext task on text sequences. We learn our DATE model end-to-end, enforcing two independent and complementary self-supervision signals, one at the token-level and one at the sequence-level. Under this new task formulation, we show strong quantitative and qualitative results on the 20Newsgroups and AG News datasets. In the semi-supervised setting, we outperform state-of-the-art results by +13.5% and +6.9%, respectively (AUROC). In the unsupervised configuration, DATE surpasses all other methods even when 10% of its training data is contaminated with outliers (compared with 0% for the others).",
        "author": "Andrei Manolache; Florin Brad; Elena Burceanu",
        "authorids": "/a/andrei-manolache/; /f/florin-brad/; /e/elena-burceanu/",
        "bibtex": "@inproceedings{manolache-etal-2021-date,\n    title = \"{DATE}: Detecting Anomalies in Text via Self-Supervision of Transformers\",\n    author = \"Manolache, Andrei  and\n      Brad, Florin  and\n      Burceanu, Elena\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.25/\",\n    doi = \"10.18653/v1/2021.naacl-main.25\",\n    pages = \"267--277\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.25.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.25/",
        "pdf_size": 1853055,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13665245849141649941&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Bitdefender+University of Bucharest, Romania; Bitdefender+University of Bucharest, Romania; Bitdefender+University of Bucharest, Romania+Institute of Mathematics of the Romanian Academy",
        "aff_domain": "bitdefender.com;bitdefender.com;bitdefender.com",
        "email": "bitdefender.com;bitdefender.com;bitdefender.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;0+1;0+1+2",
        "aff_unique_norm": "Bitdefender;University of Bucharest;Romanian Academy",
        "aff_unique_dep": ";;Institute of Mathematics",
        "aff_unique_url": "https://www.bitdefender.com;https://www.unibuc.ro;https://www.math.ro/",
        "aff_unique_abbr": "Bitdefender;Unibuc;IMAR",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0+0",
        "aff_country_unique": "Romania"
    },
    {
        "id": "2021.naacl-main.88",
        "title": "DReCa: A General Task Augmentation Strategy for Few-Shot Natural Language Inference",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Meta-learning promises few-shot learners that can adapt to new distributions by repurposing knowledge acquired from previous training. However, we believe meta-learning has not yet succeeded in NLP due to the lack of a well-defined task distribution, leading to attempts that treat datasets as tasks. Such an ad hoc task distribution causes problems of quantity and quality. Since there\u2019s only a handful of datasets for any NLP problem, meta-learners tend to overfit their adaptation mechanism and, since NLP datasets are highly heterogeneous, many learning episodes have poor transfer between their support and query sets, which discourages the meta-learner from adapting. To alleviate these issues, we propose DReCA (Decomposing datasets into Reasoning Categories), a simple method for discovering and using latent reasoning categories in a dataset, to form additional high quality tasks. DReCA works by splitting examples into label groups, embedding them with a finetuned BERT model and then clustering each group into reasoning categories. Across four few-shot NLI problems, we demonstrate that using DReCA improves the accuracy of meta-learners by 1.5-4%",
        "author": "Shikhar Murty; Tatsunori B. Hashimoto; Christopher Manning",
        "authorids": "/s/shikhar-murty/; /t/tatsunori-b-hashimoto/; /c/christopher-d-manning/",
        "bibtex": "@inproceedings{murty-etal-2021-dreca,\n    title = \"{DR}e{C}a: A General Task Augmentation Strategy for Few-Shot Natural Language Inference\",\n    author = \"Murty, Shikhar  and\n      Hashimoto, Tatsunori B.  and\n      Manning, Christopher\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.88/\",\n    doi = \"10.18653/v1/2021.naacl-main.88\",\n    pages = \"1113--1125\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.88.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.88/",
        "pdf_size": 1824802,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6428009994325520236&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Computer Science Department, Stanford University; Computer Science Department, Stanford University; Computer Science Department, Stanford University",
        "aff_domain": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.15",
        "title": "Data Filtering using Cross-Lingual Word Embeddings",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Data filtering for machine translation (MT) describes the task of selecting a subset of a given, possibly noisy corpus with the aim to maximize the performance of an MT system trained on this selected data. Over the years, many different filtering approaches have been proposed. However, varying task definitions and data conditions make it difficult to draw a meaningful comparison. In the present work, we aim for a more systematic approach to the task at hand. First, we analyze the performance of language identification, a tool commonly used for data filtering in the MT community and identify specific weaknesses. Based on our findings, we then propose several novel methods for data filtering, based on cross-lingual word embeddings. We compare our approaches to one of the winning methods from the WMT 2018 shared task on parallel corpus filtering on three real-life, high resource MT tasks. We find that said method, which was performing very strong in the WMT shared task, does not perform well within our more realistic task conditions. While we find that our approaches come out at the top on all three tasks, different variants perform best on different tasks. Further experiments on the WMT 2020 shared task for parallel corpus filtering show that our methods achieve comparable results to the strongest submissions of this campaign.",
        "author": "Christian Herold; Jan Rosendahl; Joris Vanvinckenroye; Hermann Ney",
        "authorids": "/c/christian-herold/; /j/jan-rosendahl/; /j/joris-vanvinckenroye/; /h/hermann-ney/",
        "bibtex": "@inproceedings{herold-etal-2021-data,\n    title = \"Data Filtering using Cross-Lingual Word Embeddings\",\n    author = \"Herold, Christian  and\n      Rosendahl, Jan  and\n      Vanvinckenroye, Joris  and\n      Ney, Hermann\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.15/\",\n    doi = \"10.18653/v1/2021.naacl-main.15\",\n    pages = \"162--172\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.15.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.15/",
        "pdf_size": 470147,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12050896334093827459&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Human Language Technology and Pattern Recognition Group, Computer Science Department, RWTH Aachen University; Human Language Technology and Pattern Recognition Group, Computer Science Department, RWTH Aachen University; Human Language Technology and Pattern Recognition Group, Computer Science Department, RWTH Aachen University; Human Language Technology and Pattern Recognition Group, Computer Science Department, RWTH Aachen University",
        "aff_domain": "i6.informatik.rwth-aachen.de;i6.informatik.rwth-aachen.de;i6.informatik.rwth-aachen.de;i6.informatik.rwth-aachen.de",
        "email": "i6.informatik.rwth-aachen.de;i6.informatik.rwth-aachen.de;i6.informatik.rwth-aachen.de;i6.informatik.rwth-aachen.de",
        "github": "",
        "project": "http://opus.nlpl.eu",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "RWTH Aachen University",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.rwth-aachen.de",
        "aff_unique_abbr": "RWTH",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Aachen",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2021.naacl-main.360",
        "title": "Data and Model Distillation as a Solution for Domain-transferable Fact Verification",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "While neural networks produce state-of-the-art performance in several NLP tasks, they generally depend heavily on lexicalized information, which transfer poorly between domains. We present a combination of two strategies to mitigate this dependence on lexicalized information in fact verification tasks. We present a data distillation technique for delexicalization, which we then combine with a model distillation method to prevent aggressive data distillation. We show that by using our solution, not only does the performance of an existing state-of-the-art model remain at par with that of the model trained on a fully lexicalized data, but it also performs better than it when tested out of domain. We show that the technique we present encourages models to extract transferable facts from a given fact verification dataset.",
        "author": "Mitch Paul Mithun; Sandeep Suntwal; Mihai Surdeanu",
        "authorids": "/m/mitch-paul-mithun/; /s/sandeep-suntwal/; /m/mihai-surdeanu/",
        "bibtex": "@inproceedings{mithun-etal-2021-data,\n    title = \"Data and Model Distillation as a Solution for Domain-transferable Fact Verification\",\n    author = \"Mithun, Mitch Paul  and\n      Suntwal, Sandeep  and\n      Surdeanu, Mihai\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.360/\",\n    doi = \"10.18653/v1/2021.naacl-main.360\",\n    pages = \"4546--4552\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.360.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.360/",
        "pdf_size": 200198,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7287354818132436800&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "University of Arizona, Tucson, Arizona, USA; University of Arizona, Tucson, Arizona, USA; University of Arizona, Tucson, Arizona, USA",
        "aff_domain": "email.arizona.edu;email.arizona.edu;email.arizona.edu",
        "email": "email.arizona.edu;email.arizona.edu;email.arizona.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Arizona",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.arizona.edu",
        "aff_unique_abbr": "UA",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Tucson",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.193",
        "title": "DeCEMBERT: Learning from Noisy Instructional Videos via Dense Captions and Entropy Minimization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Leveraging large-scale unlabeled web videos such as instructional videos for pre-training followed by task-specific finetuning has become the de facto approach for many video-and-language tasks. However, these instructional videos are very noisy, the accompanying ASR narrations are often incomplete, and can be irrelevant to or temporally misaligned with the visual content, limiting the performance of the models trained on such data. To address these issues, we propose an improved video-and-language pre-training method that first adds automatically-extracted dense region captions from the video frames as auxiliary text input, to provide informative visual cues for learning better video and language associations. Second, to alleviate the temporal misalignment issue, our method incorporates an entropy minimization-based constrained attention loss, to encourage the model to automatically focus on the correct caption from a pool of candidate ASR captions. Our overall approach is named DeCEMBERT (Dense Captions and Entropy Minimization). Comprehensive experiments on three video-and-language tasks (text-to-video retrieval, video captioning, and video question answering) across five datasets demonstrate that our approach outperforms previous state-of-the-art methods. Ablation studies on pre-training and downstream tasks show that adding dense captions and constrained attention loss help improve the model performance. Lastly, we also provide attention visualization to show the effect of applying the proposed constrained attention loss.",
        "author": "Zineng Tang; Jie Lei; Mohit Bansal",
        "authorids": "/z/zineng-tang/; /j/jie-lei/; /m/mohit-bansal/",
        "bibtex": "@inproceedings{tang-etal-2021-decembert,\n    title = \"{D}e{CEMBERT}: Learning from Noisy Instructional Videos via Dense Captions and Entropy Minimization\",\n    author = \"Tang, Zineng  and\n      Lei, Jie  and\n      Bansal, Mohit\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.193/\",\n    doi = \"10.18653/v1/2021.naacl-main.193\",\n    pages = \"2415--2426\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.193.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.193/",
        "pdf_size": 2571626,
        "gs_citation": 68,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15692904504236659718&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Department of Computer Science, University of North Carolina at Chapel Hill; Department of Computer Science, University of North Carolina at Chapel Hill; Department of Computer Science, University of North Carolina at Chapel Hill",
        "aff_domain": "cs.unc.edu;cs.unc.edu;cs.unc.edu",
        "email": "cs.unc.edu;cs.unc.edu;cs.unc.edu",
        "github": "https://github.com/zinengtang/DeCEMBERT",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of North Carolina at Chapel Hill",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.unc.edu",
        "aff_unique_abbr": "UNC Chapel Hill",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Chapel Hill",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.437",
        "title": "Decompose, Fuse and Generate: A Formation-Informed Method for Chinese Definition Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In this paper, we tackle the task of Definition Generation (DG) in Chinese, which aims at automatically generating a definition for a word. Most existing methods take the source word as an indecomposable semantic unit. However, in parataxis languages like Chinese, word meanings can be composed using the word formation process, where a word (\u201c\u6843\u82b1\u201d, peach-blossom) is formed by formation components (\u201c\u6843\u201d, peach; \u201c\u82b1\u201d, flower) using a formation rule (Modifier-Head). Inspired by this process, we propose to enhance DG with word formation features. We build a formation-informed dataset, and propose a model DeFT, which Decomposes words into formation features, dynamically Fuses different features through a gating mechanism, and generaTes word definitions. Experimental results show that our method is both effective and robust.",
        "author": "Hua Zheng; Damai Dai; Lei Li; Tianyu Liu; Zhifang Sui; Baobao Chang; Yang Liu",
        "authorids": "/h/hua-zheng/; /d/damai-dai/; /l/lei-li/; /t/tianyu-liu/; /z/zhifang-sui/; /b/baobao-chang/; /y/yang-liu/",
        "bibtex": "@inproceedings{zheng-etal-2021-decompose,\n    title = \"Decompose, Fuse and Generate: A Formation-Informed Method for {C}hinese Definition Generation\",\n    author = \"Zheng, Hua  and\n      Dai, Damai  and\n      Li, Lei  and\n      Liu, Tianyu  and\n      Sui, Zhifang  and\n      Chang, Baobao  and\n      Liu, Yang\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.437/\",\n    doi = \"10.18653/v1/2021.naacl-main.437\",\n    pages = \"5524--5531\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.437.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.437/",
        "pdf_size": 531593,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1765339971128591591&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Department of Computer Science and Technology, Peking University+Key Lab of Computational Linguistics (MOE), Peking University+Peng Cheng Laboratory, China; Department of Computer Science and Technology, Peking University+Key Lab of Computational Linguistics (MOE), Peking University+Peng Cheng Laboratory, China; Department of Computer Science and Technology, Peking University+Key Lab of Computational Linguistics (MOE), Peking University; Department of Computer Science and Technology, Peking University+Key Lab of Computational Linguistics (MOE), Peking University; Department of Computer Science and Technology, Peking University+Key Lab of Computational Linguistics (MOE), Peking University+Peng Cheng Laboratory, China; Department of Computer Science and Technology, Peking University+Key Lab of Computational Linguistics (MOE), Peking University+Peng Cheng Laboratory, China; Department of Computer Science and Technology, Peking University+Key Lab of Computational Linguistics (MOE), Peking University+Peng Cheng Laboratory, China",
        "aff_domain": "pku.edu.cn;pku.edu.cn;stu.pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn",
        "email": "pku.edu.cn;pku.edu.cn;stu.pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn",
        "github": "https://github.com/Hunter-DDM/DeFT-naacl2021",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+0+1;0+0+1;0+0;0+0;0+0+1;0+0+1;0+0+1",
        "aff_unique_norm": "Peking University;Peng Cheng Laboratory",
        "aff_unique_dep": "Department of Computer Science and Technology;",
        "aff_unique_url": "http://www.pku.edu.cn;",
        "aff_unique_abbr": "Peking U;",
        "aff_campus_unique_index": ";;;;;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0+0;0+0+0;0+0;0+0;0+0+0;0+0+0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2021.naacl-main.468",
        "title": "Designing a Minimal Retrieve-and-Read System for Open-Domain Question Answering",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In open-domain question answering (QA), retrieve-and-read mechanism has the inherent benefit of interpretability and the easiness of adding, removing, or editing knowledge compared to the parametric approaches of closed-book QA models. However, it is also known to suffer from its large storage footprint due to its document corpus and index. Here, we discuss several orthogonal strategies to drastically reduce the footprint of a retrieve-and-read open-domain QA system by up to 160x. Our results indicate that retrieve-and-read can be a viable option even in a highly constrained serving environment such as edge devices, as we show that it can achieve better accuracy than a purely parametric model with comparable docker-level system size.",
        "author": "Sohee Yang; Minjoon Seo",
        "authorids": "/s/sohee-yang/; /m/minjoon-seo/",
        "bibtex": "@inproceedings{yang-seo-2021-designing,\n    title = \"Designing a Minimal Retrieve-and-Read System for Open-Domain Question Answering\",\n    author = \"Yang, Sohee  and\n      Seo, Minjoon\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.468/\",\n    doi = \"10.18653/v1/2021.naacl-main.468\",\n    pages = \"5856--5865\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.468.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.468/",
        "pdf_size": 507904,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2984105601308164263&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "KAIST AI+NA VER Corp.; KAIST AI",
        "aff_domain": "kaist.ac.kr;kaist.ac.kr",
        "email": "kaist.ac.kr;kaist.ac.kr",
        "github": "https://github.com/clovaai/minimal-rnr-qa",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;0",
        "aff_unique_norm": "Korea Advanced Institute of Science and Technology;NAVER Corporation",
        "aff_unique_dep": "KAIST AI;",
        "aff_unique_url": "https://www.kaist.edu;https://www.naver.com",
        "aff_unique_abbr": "KAIST;NAVER",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2021.naacl-main.190",
        "title": "Detoxifying Language Models Risks Marginalizing Minority Voices",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Language models (LMs) must be both safe and equitable to be responsibly deployed in practice. With safety in mind, numerous detoxification techniques (e.g., Dathathri et al. 2020; Krause et al. 2020) have been proposed to mitigate toxic LM generations. In this work, we show that these detoxification techniques hurt equity: they decrease the utility of LMs on language used by marginalized groups (e.g., African-American English and minority identity mentions). In particular, we perform automatic and human evaluations of text generation quality when LMs are conditioned on inputs with different dialects and group identifiers. We find that detoxification makes LMs more brittle to distribution shift, especially on language used by marginalized groups. We identify that these failures stem from detoxification methods exploiting spurious correlations in toxicity datasets. Overall, our results highlight the tension between the controllability and distributional robustness of LMs.",
        "author": "Albert Xu; Eshaan Pathak; Eric Wallace; Suchin Gururangan; Maarten Sap; Dan Klein",
        "authorids": "/a/albert-xu/; /e/eshaan-pathak/; /e/eric-wallace/; /s/suchin-gururangan/; /m/maarten-sap/; /d/dan-klein/",
        "bibtex": "@inproceedings{xu-etal-2021-detoxifying,\n    title = \"Detoxifying Language Models Risks Marginalizing Minority Voices\",\n    author = \"Xu, Albert  and\n      Pathak, Eshaan  and\n      Wallace, Eric  and\n      Gururangan, Suchin  and\n      Sap, Maarten  and\n      Klein, Dan\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.190/\",\n    doi = \"10.18653/v1/2021.naacl-main.190\",\n    pages = \"2390--2397\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.190.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.190/",
        "pdf_size": 603747,
        "gs_citation": 147,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11501726893040946550&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "UC Berkeley\u2666; UC Berkeley\u2666; UC Berkeley\u2666; University of Washington\u2660; University of Washington\u2660; UC Berkeley\u2666",
        "aff_domain": "berkeley.edu;berkeley.edu;berkeley.edu;cs.washington.edu;cs.washington.edu;berkeley.edu",
        "email": "berkeley.edu;berkeley.edu;berkeley.edu;cs.washington.edu;cs.washington.edu;berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;1;1;0",
        "aff_unique_norm": "University of California, Berkeley;University of Washington",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.berkeley.edu;https://www.washington.edu",
        "aff_unique_abbr": "UC Berkeley;UW",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-industry.28",
        "title": "Development of an Enterprise-Grade Contract Understanding System",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Contracts are arguably the most important type of business documents. Despite their significance in business, legal contract review largely remains an arduous, expensive and manual process. In this paper, we describe TECUS: a commercial system designed and deployed for contract understanding and used by a wide range of enterprise users for the past few years. We reflect on the challenges and design decisions when building TECUS. We also summarize the data science life cycle of TECUS and share lessons learned.",
        "author": "Arvind Agarwal; Laura Chiticariu; Poornima Chozhiyath Raman; Marina Danilevsky; Diman Ghazi; Ankush Gupta; Shanmukha Guttula; Yannis Katsis; Rajasekar Krishnamurthy; Yunyao Li; Shubham Mudgal; Vitobha Munigala; Nicholas Phan; Dhaval Sonawane; Sneha Srinivasan; Sudarshan R. Thitte; Mitesh Vasa; Ramiya Venkatachalam; Vinitha Yaski; Huaiyu Zhu",
        "authorids": "/a/arvind-agarwal/; /l/laura-chiticariu/; /p/poornima-chozhiyath-raman/; /m/marina-danilevsky/; /d/diman-ghazi/; /a/ankush-gupta/; /s/shanmukha-guttula/; /y/yannis-katsis/; /r/rajasekar-krishnamurthy/; /y/yunyao-li/; /s/shubham-mudgal/; /v/vitobha-munigala/; /n/nicholas-phan/; /d/dhaval-sonawane/; /s/sneha-srinivasan/; /s/sudarshan-r-thitte/; /m/mitesh-vasa/; /r/ramiya-venkatachalam/; /v/vinitha-yaski/; /h/huaiyu-zhu/",
        "bibtex": "@inproceedings{agarwal-etal-2021-development,\n    title = \"Development of an Enterprise-Grade Contract Understanding System\",\n    author = \"Agarwal, Arvind  and\n      Chiticariu, Laura  and\n      Chozhiyath Raman, Poornima  and\n      Danilevsky, Marina  and\n      Ghazi, Diman  and\n      Gupta, Ankush  and\n      Guttula, Shanmukha  and\n      Katsis, Yannis  and\n      Krishnamurthy, Rajasekar  and\n      Li, Yunyao  and\n      Mudgal, Shubham  and\n      Munigala, Vitobha  and\n      Phan, Nicholas  and\n      Sonawane, Dhaval  and\n      Srinivasan, Sneha  and\n      Thitte, Sudarshan R.  and\n      Vasa, Mitesh  and\n      Venkatachalam, Ramiya  and\n      Yaski, Vinitha  and\n      Zhu, Huaiyu\",\n    editor = \"Kim, Young-bum  and\n      Li, Yunyao  and\n      Rambow, Owen\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-industry.28/\",\n    doi = \"10.18653/v1/2021.naacl-industry.28\",\n    pages = \"222--229\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-industry.28.pdf",
        "site": "https://aclanthology.org/2021.naacl-industry.28/",
        "pdf_size": 4040430,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4654486257104511337&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "IBM Research - India; IBM Data and AI; IBM Data and AI; IBM Research - Almaden; Google LLC; IBM Research - India; IBM Research - India; IBM Research - Almaden; IBM Data and AI; IBM Research - Almaden; IBM Data and AI; IBM Research - India; Visa; IBM Data and AI; IBM Data and AI; IBM Data and AI; IBM Data and AI; IBM Data and AI; Amazon; IBM Research - Almaden",
        "aff_domain": "in.ibm.com;us.ibm.com;us.ibm.com;us.ibm.com;google.com;in.ibm.com;in.ibm.com;ibm.com;us.ibm.com;us.ibm.com;ibm.com;in.ibm.com;gmail.com;ibm.com;ibm.com;us.ibm.com;us.ibm.com;us.ibm.com;gmail.com;us.ibm.com",
        "email": "in.ibm.com;us.ibm.com;us.ibm.com;us.ibm.com;google.com;in.ibm.com;in.ibm.com;ibm.com;us.ibm.com;us.ibm.com;ibm.com;in.ibm.com;gmail.com;ibm.com;ibm.com;us.ibm.com;us.ibm.com;us.ibm.com;gmail.com;us.ibm.com",
        "github": "",
        "project": "",
        "author_num": 20,
        "aff_unique_index": "0;1;1;0;2;0;0;0;1;0;1;0;3;1;1;1;1;1;4;0",
        "aff_unique_norm": "IBM Research;IBM;Google;Visa Inc.;Amazon.com, Inc.",
        "aff_unique_dep": "Research;Data and AI;;;",
        "aff_unique_url": "https://www.ibm.com/research/in;https://www.ibm.com;https://www.google.com;https://www.visa.com;https://www.amazon.com",
        "aff_unique_abbr": "IBM;IBM;Google;Visa;Amazon",
        "aff_campus_unique_index": "1;2;1;1;1",
        "aff_campus_unique": ";Almaden;Mountain View",
        "aff_country_unique_index": "0;1;1;1;1;0;0;1;1;1;1;0;1;1;1;1;1;1;1;1",
        "aff_country_unique": "India;United States"
    },
    {
        "id": "2021.naacl-demos.4",
        "title": "DiSCoL: Toward Engaging Dialogue Systems through Conversational Line Guided Response Generation",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "Having engaging and informative conversations with users is the utmost goal for open-domain conversational systems. Recent advances in transformer-based language models and their applications to dialogue systems have succeeded to generate fluent and human-like responses. However, they still lack control over the generation process towards producing contentful responses and achieving engaging conversations. To achieve this goal, we present DiSCoL (Dialogue Systems through Coversational Line guided response generation). DiSCoL is an open-domain dialogue system that leverages conversational lines (briefly convlines) as controllable and informative content-planning elements to guide the generation model produce engaging and informative responses. Two primary modules in DiSCoL\u2019s pipeline are conditional generators trained for 1) predicting relevant and informative convlines for dialogue contexts and 2) generating high-quality responses conditioned on the predicted convlines. Users can also change the returned convlines to control the direction of the conversations towards topics that are more interesting for them. Through automatic and human evaluations, we demonstrate the efficiency of the convlines in producing engaging conversations.",
        "author": "Sarik Ghazarian; Zixi Liu; Tuhin Chakrabarty; Xuezhe Ma; Aram Galstyan; Nanyun Peng",
        "authorids": "/s/sarik-ghazarian/; /z/zixi-liu/; /t/tuhin-chakrabarty/; /x/xuezhe-ma/; /a/aram-galstyan/; /n/nanyun-peng/",
        "bibtex": "@inproceedings{ghazarian-etal-2021-discol,\n    title = \"{D}i{SC}o{L}: Toward Engaging Dialogue Systems through Conversational Line Guided Response Generation\",\n    author = \"Ghazarian, Sarik  and\n      Liu, Zixi  and\n      Chakrabarty, Tuhin  and\n      Ma, Xuezhe  and\n      Galstyan, Aram  and\n      Peng, Nanyun\",\n    editor = \"Sil, Avi  and\n      Lin, Xi Victoria\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-demos.4/\",\n    doi = \"10.18653/v1/2021.naacl-demos.4\",\n    pages = \"26--34\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-demos.4.pdf",
        "site": "https://aclanthology.org/2021.naacl-demos.4/",
        "pdf_size": 3761868,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=827161278389945538&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "University of Southern California / Information Sciences Institute; University of Southern California / Information Sciences Institute; Computer Science Department of Columbia University; University of Southern California / Information Sciences Institute; University of Southern California / Information Sciences Institute; Computer Science Department of University of California, Los Angeles",
        "aff_domain": "isi.edu;isi.edu;cs.columbia.edu;isi.edu;isi.edu;cs.ucla.edu",
        "email": "isi.edu;isi.edu;cs.columbia.edu;isi.edu;isi.edu;cs.ucla.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;1;0;0;2",
        "aff_unique_norm": "University of Southern California;Columbia University;University of California, Los Angeles",
        "aff_unique_dep": "Information Sciences Institute;Computer Science Department;Computer Science Department",
        "aff_unique_url": "https://www.usc.edu;https://www.columbia.edu;https://www.ucla.edu",
        "aff_unique_abbr": "USC;Columbia;UCLA",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.129",
        "title": "Did they answer? Subjective acts and intents in conversational discourse",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Discourse signals are often implicit, leaving it up to the interpreter to draw the required inferences. At the same time, discourse is embedded in a social context, meaning that interpreters apply their own assumptions and beliefs when resolving these inferences, leading to multiple, valid interpretations. However, current discourse data and frameworks ignore the social aspect, expecting only a single ground truth. We present the first discourse dataset with multiple and subjective interpretations of English conversation in the form of perceived conversation acts and intents. We carefully analyze our dataset and create computational models to (1) confirm our hypothesis that taking into account the bias of the interpreters leads to better predictions of the interpretations, (2) and show disagreements are nuanced and require a deeper understanding of the different contextual factors. We share our dataset and code at http://github.com/elisaF/subjective_discourse.",
        "author": "Elisa Ferracane; Greg Durrett; Junyi Jessy Li; Katrin Erk",
        "authorids": "/e/elisa-ferracane/; /g/greg-durrett/; /j/junyi-jessy-li/; /k/katrin-erk/",
        "bibtex": "@inproceedings{ferracane-etal-2021-answer,\n    title = \"Did they answer? Subjective acts and intents in conversational discourse\",\n    author = \"Ferracane, Elisa  and\n      Durrett, Greg  and\n      Li, Junyi Jessy  and\n      Erk, Katrin\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.129/\",\n    doi = \"10.18653/v1/2021.naacl-main.129\",\n    pages = \"1626--1644\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.129.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.129/",
        "pdf_size": 2001954,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2864386868255700714&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Linguistics; Department of Computer Science; Department of Linguistics; Department of Linguistics",
        "aff_domain": "ferracane.com;cs.utexas.edu;austin.utexas.edu;mail.utexas.edu",
        "email": "ferracane.com;cs.utexas.edu;austin.utexas.edu;mail.utexas.edu",
        "github": "http://github.com/elisaF/subjective_discourse",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "University Affiliation Not Specified;Unknown Institution",
        "aff_unique_dep": "Department of Linguistics;Department of Computer Science",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "2021.naacl-main.366",
        "title": "Differentiable Open-Ended Commonsense Reasoning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Current commonsense reasoning research focuses on developing models that use commonsense knowledge to answer multiple-choice questions. However, systems designed to answer multiple-choice questions may not be useful in applications that do not provide a small list of candidate answers to choose from. As a step towards making commonsense reasoning research more realistic, we propose to study open-ended commonsense reasoning (OpenCSR) \u2014 the task of answering a commonsense question without any pre-defined choices \u2014 using as a resource only a corpus of commonsense facts written in natural language. OpenCSR is challenging due to a large decision space, and because many questions require implicit multi-hop reasoning. As an approach to OpenCSR, we propose DrFact, an efficient Differentiable model for multi-hop Reasoning over knowledge Facts. To evaluate OpenCSR methods, we adapt several popular commonsense reasoning benchmarks, and collect multiple new answers for each test question via crowd-sourcing. Experiments show that DrFact outperforms strong baseline methods by a large margin.",
        "author": "Bill Yuchen Lin; Haitian Sun; Bhuwan Dhingra; Manzil Zaheer; Xiang Ren; William Cohen",
        "authorids": "/b/bill-yuchen-lin/; /h/haitian-sun/; /b/bhuwan-dhingra/; /m/manzil-zaheer/; /x/xiang-ren/; /w/william-cohen/",
        "bibtex": "@inproceedings{lin-etal-2021-differentiable,\n    title = \"Differentiable Open-Ended Commonsense Reasoning\",\n    author = \"Lin, Bill Yuchen  and\n      Sun, Haitian  and\n      Dhingra, Bhuwan  and\n      Zaheer, Manzil  and\n      Ren, Xiang  and\n      Cohen, William\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.366/\",\n    doi = \"10.18653/v1/2021.naacl-main.366\",\n    pages = \"4611--4625\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.366.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.366/",
        "pdf_size": 1440654,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9654512991323691865&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "University of Southern California+Google Research; Google Research; Google Research; Google Research; University of Southern California; Google Research",
        "aff_domain": "usc.edu;google.com;google.com;google.com;usc.edu;google.com",
        "email": "usc.edu;google.com;google.com;google.com;usc.edu;google.com",
        "github": "",
        "project": "https://open-csr.github.io/",
        "author_num": 6,
        "aff_unique_index": "0+1;1;1;1;0;1",
        "aff_unique_norm": "University of Southern California;Google",
        "aff_unique_dep": ";Google Research",
        "aff_unique_url": "https://www.usc.edu;https://research.google",
        "aff_unique_abbr": "USC;Google Research",
        "aff_campus_unique_index": "0+1;1;1;1;0;1",
        "aff_campus_unique": "Los Angeles;Mountain View",
        "aff_country_unique_index": "0+0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.401",
        "title": "DirectProbe: Studying Representations without Classifiers",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Understanding how linguistic structure is encoded in contextualized embedding could help explain their impressive performance across NLP. Existing approaches for probing them usually call for training classifiers and use the accuracy, mutual information, or complexity as a proxy for the representation\u2019s goodness. In this work, we argue that doing so can be unreliable because different representations may need different classifiers. We develop a heuristic, DirectProbe, that directly studies the geometry of a representation by building upon the notion of a version space for a task. Experiments with several linguistic tasks and contextualized embeddings show that, even without training classifiers, DirectProbe can shine lights on how an embedding space represents labels and also anticipate the classifier performance for the representation.",
        "author": "Yichu Zhou; Vivek Srikumar",
        "authorids": "/y/yichu-zhou/; /v/vivek-srikumar/",
        "bibtex": "@inproceedings{zhou-srikumar-2021-directprobe,\n    title = \"{D}irect{P}robe: Studying Representations without Classifiers\",\n    author = \"Zhou, Yichu  and\n      Srikumar, Vivek\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.401/\",\n    doi = \"10.18653/v1/2021.naacl-main.401\",\n    pages = \"5070--5083\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.401.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.401/",
        "pdf_size": 1465112,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13464604913097240678&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "School of Computing, University of Utah; School of Computing, University of Utah",
        "aff_domain": "cs.utah.edu;cs.utah.edu",
        "email": "cs.utah.edu;cs.utah.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Utah",
        "aff_unique_dep": "School of Computing",
        "aff_unique_url": "https://www.utah.edu",
        "aff_unique_abbr": "U of U",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Utah",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.301",
        "title": "Discourse Probing of Pretrained Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Existing work on probing of pretrained language models (LMs) has predominantly focused on sentence-level syntactic tasks. In this paper, we introduce document-level discourse probing to evaluate the ability of pretrained LMs to capture document-level relations. We experiment with 7 pretrained LMs, 4 languages, and 7 discourse probing tasks, and find BART to be overall the best model at capturing discourse \u2014 but only in its encoder, with BERT performing surprisingly well as the baseline model. Across the different models, there are substantial differences in which layers best capture discourse information, and large disparities between models.",
        "author": "Fajri Koto; Jey Han Lau; Timothy Baldwin",
        "authorids": "/f/fajri-koto/; /j/jey-han-lau/; /t/timothy-baldwin/",
        "bibtex": "@inproceedings{koto-etal-2021-discourse,\n    title = \"Discourse Probing of Pretrained Language Models\",\n    author = \"Koto, Fajri  and\n      Lau, Jey Han  and\n      Baldwin, Timothy\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.301/\",\n    doi = \"10.18653/v1/2021.naacl-main.301\",\n    pages = \"3849--3864\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.301.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.301/",
        "pdf_size": 1443303,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15422729124016522490&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "School of Computing and Information Systems, The University of Melbourne; School of Computing and Information Systems, The University of Melbourne; School of Computing and Information Systems, The University of Melbourne",
        "aff_domain": "student.unimelb.edu.au;gmail.com;unimelb.edu.au",
        "email": "student.unimelb.edu.au;gmail.com;unimelb.edu.au",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "The University of Melbourne",
        "aff_unique_dep": "School of Computing and Information Systems",
        "aff_unique_url": "https://www.unimelb.edu.au",
        "aff_unique_abbr": "UniMelb",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Melbourne",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "2021.naacl-industry.29",
        "title": "Discovering Better Model Architectures for Medical Query Understanding",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "In developing an online question-answering system for the medical domains, natural language inference (NLI) models play a central role in question matching and intention detection. However, which models are best for our datasets? Manually selecting or tuning a model is time-consuming. Thus we experiment with automatically optimizing the model architectures on the task at hand via neural architecture search (NAS). First, we formulate a novel architecture search space based on the previous NAS literature, supporting cross-sentence attention (cross-attn) modeling. Second, we propose to modify the ENAS method to accelerate and stabilize the search results. We conduct extensive experiments on our two medical NLI tasks. Results show that our system can easily outperform the classical baseline models. We compare different NAS methods and demonstrate our approach provides the best results.",
        "author": "Wei Zhu; Yuan Ni; Xiaoling Wang; Guotong Xie",
        "authorids": "/w/wei-zhu/; /y/yuan-ni/; /x/xiaoling-wang/; /g/guotong-xie/",
        "bibtex": "@inproceedings{zhu-etal-2021-discovering,\n    title = \"Discovering Better Model Architectures for Medical Query Understanding\",\n    author = \"Zhu, Wei  and\n      Ni, Yuan  and\n      Wang, Xiaoling  and\n      Xie, Guotong\",\n    editor = \"Kim, Young-bum  and\n      Li, Yunyao  and\n      Rambow, Owen\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-industry.29/\",\n    doi = \"10.18653/v1/2021.naacl-industry.29\",\n    pages = \"230--237\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-industry.29.pdf",
        "site": "https://aclanthology.org/2021.naacl-industry.29/",
        "pdf_size": 523833,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4864293013176112194&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "https://www.cbc.ca/news/health/covid-19-emergency-departments-canada-1.5510778",
        "author_num": 4
    },
    {
        "id": "2021.naacl-main.431",
        "title": "Discrete Argument Representation Learning for Interactive Argument Pair Identification",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In this paper, we focus on identifying interactive argument pairs from two posts with opposite stances to a certain topic. Considering opinions are exchanged from different perspectives of the discussing topic, we study the discrete representations for arguments to capture varying aspects in argumentation languages (e.g., the debate focus and the participant behavior). Moreover, we utilize hierarchical structure to model post-wise information incorporating contextual knowledge. Experimental results on the large-scale dataset collected from CMV show that our proposed framework can significantly outperform the competitive baselines. Further analyses reveal why our model yields superior performance and prove the usefulness of our learned representations.",
        "author": "Lu Ji; Zhongyu Wei; Jing Li; Qi Zhang; Xuanjing Huang",
        "authorids": "/l/lu-ji/; /z/zhongyu-wei/; /j/jing-li/; /q/qi-zhang/; /x/xuan-jing-huang/",
        "bibtex": "@inproceedings{ji-etal-2021-discrete,\n    title = \"Discrete Argument Representation Learning for Interactive Argument Pair Identification\",\n    author = \"Ji, Lu  and\n      Wei, Zhongyu  and\n      Li, Jing  and\n      Zhang, Qi  and\n      Huang, Xuanjing\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.431/\",\n    doi = \"10.18653/v1/2021.naacl-main.431\",\n    pages = \"5467--5478\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.431.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.431/",
        "pdf_size": 718638,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3425229222184519072&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "School of Computer Science, Fudan University, Shanghai, China; School of Data Science, Fudan University, China; Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China; School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China",
        "aff_domain": "fudan.edu.cn;fudan.edu.cn;polyu.edu.hk;fudan.edu.cn;fudan.edu.cn",
        "email": "fudan.edu.cn;fudan.edu.cn;polyu.edu.hk;fudan.edu.cn;fudan.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "Fudan University;The Hong Kong Polytechnic University",
        "aff_unique_dep": "School of Computer Science;Department of Computing",
        "aff_unique_url": "https://www.fudan.edu.cn;https://www.polyu.edu.hk",
        "aff_unique_abbr": "Fudan;PolyU",
        "aff_campus_unique_index": "0;2;0;0",
        "aff_campus_unique": "Shanghai;;Hong Kong",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2021.naacl-main.108",
        "title": "Disentangling Semantics and Syntax in Sentence Embeddings with Pre-trained Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Pre-trained language models have achieved huge success on a wide range of NLP tasks. However, contextual representations from pre-trained models contain entangled semantic and syntactic information, and therefore cannot be directly used to derive useful semantic sentence embeddings for some tasks. Paraphrase pairs offer an effective way of learning the distinction between semantics and syntax, as they naturally share semantics and often vary in syntax. In this work, we present ParaBART, a semantic sentence embedding model that learns to disentangle semantics and syntax in sentence embeddings obtained by pre-trained language models. ParaBART is trained to perform syntax-guided paraphrasing, based on a source sentence that shares semantics with the target paraphrase, and a parse tree that specifies the target syntax. In this way, ParaBART learns disentangled semantic and syntactic representations from their respective inputs with separate encoders. Experiments in English show that ParaBART outperforms state-of-the-art sentence embedding models on unsupervised semantic similarity tasks. Additionally, we show that our approach can effectively remove syntactic information from semantic sentence embeddings, leading to better robustness against syntactic variation on downstream semantic tasks.",
        "author": "James Y. Huang; Kuan-Hao Huang; Kai-Wei Chang",
        "authorids": "/j/james-y-huang/; /k/kuan-hao-huang/; /k/kai-wei-chang/",
        "bibtex": "@inproceedings{huang-etal-2021-disentangling,\n    title = \"Disentangling Semantics and Syntax in Sentence Embeddings with Pre-trained Language Models\",\n    author = \"Huang, James Y.  and\n      Huang, Kuan-Hao  and\n      Chang, Kai-Wei\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.108/\",\n    doi = \"10.18653/v1/2021.naacl-main.108\",\n    pages = \"1372--1379\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.108.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.108/",
        "pdf_size": 301971,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7446116245058658414&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "University of California, Los Angeles; University of California, Los Angeles; University of California, Los Angeles",
        "aff_domain": "cs.ucla.edu;cs.ucla.edu;cs.ucla.edu",
        "email": "cs.ucla.edu;cs.ucla.edu;cs.ucla.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Los Angeles",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ucla.edu",
        "aff_unique_abbr": "UCLA",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.2",
        "title": "Distantly Supervised Relation Extraction with Sentence Reconstruction and Knowledge Base Priors",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We propose a multi-task, probabilistic approach to facilitate distantly supervised relation extraction by bringing closer the representations of sentences that contain the same Knowledge Base pairs. To achieve this, we bias the latent space of sentences via a Variational Autoencoder (VAE) that is trained jointly with a relation classifier. The latent code guides the pair representations and influences sentence reconstruction. Experimental results on two datasets created via distant supervision indicate that multi-task learning results in performance benefits. Additional exploration of employing Knowledge Base priors into theVAE reveals that the sentence space can be shifted towards that of the Knowledge Base, offering interpretability and further improving results.",
        "author": "Fenia Christopoulou; Makoto Miwa; Sophia Ananiadou",
        "authorids": "/f/fenia-christopoulou/; /m/makoto-miwa/; /s/sophia-ananiadou/",
        "bibtex": "@inproceedings{christopoulou-etal-2021-distantly,\n    title = \"Distantly Supervised Relation Extraction with Sentence Reconstruction and Knowledge Base Priors\",\n    author = \"Christopoulou, Fenia  and\n      Miwa, Makoto  and\n      Ananiadou, Sophia\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.2/\",\n    doi = \"10.18653/v1/2021.naacl-main.2\",\n    pages = \"11--26\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.2.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.2/",
        "pdf_size": 2378044,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15337611791802515974&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "National Centre for Text Mining, Department of Computer Science, The University of Manchester, United Kingdom; Toyota Technological Institute, Nagoya, 468-8511, Japan + Artificial Intelligence Research Center, National Institute of Advanced Industrial Science and Technology, Japan; National Centre for Text Mining, Department of Computer Science, The University of Manchester, United Kingdom",
        "aff_domain": "manchester.ac.uk;toyota-ti.ac.jp;manchester.ac.uk",
        "email": "manchester.ac.uk;toyota-ti.ac.jp;manchester.ac.uk",
        "github": "https://github.com/fenchri/dsre-vae",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1+2;0",
        "aff_unique_norm": "The University of Manchester;Toyota Technological Institute;National Institute of Advanced Industrial Science and Technology",
        "aff_unique_dep": "Department of Computer Science;;Artificial Intelligence Research Center",
        "aff_unique_url": "https://www.manchester.ac.uk;https://www.tti.ac.jp;https://www.aist.go.jp",
        "aff_unique_abbr": "UoM;TTI;AIST",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Nagoya",
        "aff_country_unique_index": "0;1+1;0",
        "aff_country_unique": "United Kingdom;Japan"
    },
    {
        "id": "2021.naacl-main.315",
        "title": "Distantly Supervised Transformers For E-Commerce Product QA",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We propose a practical instant question answering (QA) system on product pages of e-commerce services, where for each user query, relevant community question answer (CQA) pairs are retrieved. User queries and CQA pairs differ significantly in language characteristics making relevance learning difficult. Our proposed transformer-based model learns a robust relevance function by jointly learning unified syntactic and semantic representations without the need for human labeled data. This is achieved by distantly supervising our model by distilling from predictions of a syntactic matching system on user queries and simultaneously training with CQA pairs. Training with CQA pairs helps our model learning semantic QA relevance and distant supervision enables learning of syntactic features as well as the nuances of user querying language. Additionally, our model encodes queries and candidate responses independently allowing offline candidate embedding generation thereby minimizing the need for real-time transformer model execution. Consequently, our framework is able to scale to large e-commerce QA traffic. Extensive evaluation on user queries shows that our framework significantly outperforms both syntactic and semantic baselines in offline as well as large scale online A/B setups of a popular e-commerce service.",
        "author": "Happy Mittal; Aniket Chakrabarti; Belhassen Bayar; Animesh Anant Sharma; Nikhil Rasiwasia",
        "authorids": "/h/happy-mittal/; /a/aniket-chakrabarti/; /b/belhassen-bayar/; /a/animesh-anant-sharma/; /n/nikhil-rasiwasia/",
        "bibtex": "@inproceedings{mittal-etal-2021-distantly,\n    title = \"Distantly Supervised Transformers For {E}-Commerce Product {QA}\",\n    author = \"Mittal, Happy  and\n      Chakrabarti, Aniket  and\n      Bayar, Belhassen  and\n      Sharma, Animesh Anant  and\n      Rasiwasia, Nikhil\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.315/\",\n    doi = \"10.18653/v1/2021.naacl-main.315\",\n    pages = \"4008--4017\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.315.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.315/",
        "pdf_size": 297596,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14385205751122537648&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "India Machine Learning+Amazon; India Machine Learning+Amazon; Digital Video+Amazon; Community Shopping+Amazon; India Machine Learning+Amazon",
        "aff_domain": "amazon.com;amazon.com;amazon.com;amazon.com;amazon.com",
        "email": "amazon.com;amazon.com;amazon.com;amazon.com;amazon.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0+1;2+1;3+1;0+1",
        "aff_unique_norm": "India Machine Learning;Amazon.com, Inc.;Digital Video;Community Shopping",
        "aff_unique_dep": ";;;",
        "aff_unique_url": ";https://www.amazon.com;;",
        "aff_unique_abbr": ";Amazon;;",
        "aff_campus_unique_index": ";;;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0+1;1;1;0+1",
        "aff_country_unique": "India;United States;"
    },
    {
        "id": "2021.naacl-main.207",
        "title": "Diversity-Aware Batch Active Learning for Dependency Parsing",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "While the predictive performance of modern statistical dependency parsers relies heavily on the availability of expensive expert-annotated treebank data, not all annotations contribute equally to the training of the parsers. In this paper, we attempt to reduce the number of labeled examples needed to train a strong dependency parser using batch active learning (AL). In particular, we investigate whether enforcing diversity in the sampled batches, using determinantal point processes (DPPs), can improve over their diversity-agnostic counterparts. Simulation experiments on an English newswire corpus show that selecting diverse batches with DPPs is superior to strong selection strategies that do not enforce batch diversity, especially during the initial stages of the learning process. Additionally, our diversity-aware strategy is robust under a corpus duplication setting, where diversity-agnostic sampling strategies exhibit significant degradation.",
        "author": "Tianze Shi; Adrian Benton; Igor Malioutov; Ozan \u0130rsoy",
        "authorids": "/t/tianze-shi/; /a/adrian-benton/; /i/igor-malioutov/; /o/ozan-irsoy/",
        "bibtex": "@inproceedings{shi-etal-2021-diversity,\n    title = \"Diversity-Aware Batch Active Learning for Dependency Parsing\",\n    author = \"Shi, Tianze  and\n      Benton, Adrian  and\n      Malioutov, Igor  and\n      {\\.I}rsoy, Ozan\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.207/\",\n    doi = \"10.18653/v1/2021.naacl-main.207\",\n    pages = \"2616--2626\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.207.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.207/",
        "pdf_size": 477371,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2951294572255074791&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Cornell University; Bloomberg L.P.; Bloomberg L.P.; Bloomberg L.P.",
        "aff_domain": "cs.cornell.edu;bloomberg.net;bloomberg.net;bloomberg.net",
        "email": "cs.cornell.edu;bloomberg.net;bloomberg.net;bloomberg.net",
        "github": "https://github.com/tzshi/dpp-al-parsing-naacl21",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "Cornell University;Bloomberg",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cornell.edu;https://www.bloomberg.com",
        "aff_unique_abbr": "Cornell;Bloomberg",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.435",
        "title": "Do RNN States Encode Abstract Phonological Alternations?",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Sequence-to-sequence models have delivered impressive results in word formation tasks such as morphological inflection, often learning to model subtle morphophonological details with limited training data. Despite the performance, the opacity of neural models makes it difficult to determine whether complex generalizations are learned, or whether a kind of separate rote memorization of each morphophonological process takes place. To investigate whether complex alternations are simply memorized or whether there is some level of generalization across related sound changes in a sequence-to-sequence model, we perform several experiments on Finnish consonant gradation\u2014a complex set of sound changes triggered in some words by certain suffixes. We find that our models often\u2014though not always\u2014encode 17 different consonant gradation processes in a handful of dimensions in the RNN. We also show that by scaling the activations in these dimensions we can control whether consonant gradation occurs and the direction of the gradation.",
        "author": "Miikka Silfverberg; Francis Tyers; Garrett Nicolai; Mans Hulden",
        "authorids": "/m/miikka-silfverberg/; /f/francis-tyers/; /g/garrett-nicolai/; /m/mans-hulden/",
        "bibtex": "@inproceedings{silfverberg-etal-2021-rnn,\n    title = \"Do {RNN} States Encode Abstract Phonological Alternations?\",\n    author = \"Silfverberg, Miikka  and\n      Tyers, Francis  and\n      Nicolai, Garrett  and\n      Hulden, Mans\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.435/\",\n    doi = \"10.18653/v1/2021.naacl-main.435\",\n    pages = \"5501--5513\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.435.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.435/",
        "pdf_size": 1434915,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11782277253105082486&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "University of British Columbia; Indiana University; University of British Columbia; University of Colorado",
        "aff_domain": "ubc.ca;iu.edu;ubc.ca;colorado.edu",
        "email": "ubc.ca;iu.edu;ubc.ca;colorado.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;2",
        "aff_unique_norm": "University of British Columbia;Indiana University;University of Colorado",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.ubc.ca;https://www.indiana.edu;https://www.colorado.edu",
        "aff_unique_abbr": "UBC;IU;CU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;1",
        "aff_country_unique": "Canada;United States"
    },
    {
        "id": "2021.naacl-main.11",
        "title": "Do Syntactic Probes Probe Syntax? Experiments with Jabberwocky Probing",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Analysing whether neural language models encode linguistic information has become popular in NLP. One method of doing so, which is frequently cited to support the claim that models like BERT encode syntax, is called probing; probes are small supervised models trained to extract linguistic information from another model\u2019s output. If a probe is able to predict a particular structure, it is argued that the model whose output it is trained on must have implicitly learnt to encode it. However, drawing a generalisation about a model\u2019s linguistic knowledge about a specific phenomena based on what a probe is able to learn may be problematic: in this work, we show that semantic cues in training data means that syntactic probes do not properly isolate syntax. We generate a new corpus of semantically nonsensical but syntactically well-formed Jabberwocky sentences, which we use to evaluate two probes trained on normal data. We train the probes on several popular language models (BERT, GPT-2, and RoBERTa), and find that in all settings they perform worse when evaluated on these data, for one probe by an average of 15.4 UUAS points absolute. Although in most cases they still outperform the baselines, their lead is reduced substantially, e.g. by 53% in the case of BERT for one probe. This begs the question: what empirical scores constitute knowing syntax?",
        "author": "Rowan Hall Maudslay; Ryan Cotterell",
        "authorids": "/r/rowan-hall-maudslay/; /r/ryan-cotterell/",
        "bibtex": "@inproceedings{hall-maudslay-cotterell-2021-syntactic,\n    title = \"Do Syntactic Probes Probe Syntax? Experiments with Jabberwocky Probing\",\n    author = \"Maudslay, Rowan Hall  and\n      Cotterell, Ryan\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.11/\",\n    doi = \"10.18653/v1/2021.naacl-main.11\",\n    pages = \"124--131\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.11.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.11/",
        "pdf_size": 434613,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11265705806471315016&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "University of Cambridge+ETH Z\u00fcrich; University of Cambridge+ETH Z\u00fcrich",
        "aff_domain": "cam.ac.uk;inf.ethz.ch",
        "email": "cam.ac.uk;inf.ethz.ch",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "University of Cambridge;ETH Z\u00fcrich",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cam.ac.uk;https://www.ethz.ch",
        "aff_unique_abbr": "Cambridge;ETHZ",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0+1;0+1",
        "aff_country_unique": "United Kingdom;Switzerland"
    },
    {
        "id": "2021.naacl-main.69",
        "title": "Document-Level Event Argument Extraction by Conditional Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Event extraction has long been treated as a sentence-level task in the IE community. We argue that this setting does not match human informative seeking behavior and leads to incomplete and uninformative extraction results. We propose a document-level neural event argument extraction model by formulating the task as conditional generation following event templates. We also compile a new document-level event extraction benchmark dataset WikiEvents which includes complete event and coreference annotation. On the task of argument extraction, we achieve an absolute gain of 7.6% F1 and 5.7% F1 over the next best model on the RAMS and WikiEvents dataset respectively. On the more challenging task of informative argument extraction, which requires implicit coreference reasoning, we achieve a 9.3% F1 gain over the best baseline. To demonstrate the portability of our model, we also create the first end-to-end zero-shot event extraction framework and achieve 97% of fully supervised model\u2019s trigger extraction performance and 82% of the argument extraction performance given only access to 10 out of the 33 types on ACE.",
        "author": "Sha Li; Heng Ji; Jiawei Han",
        "authorids": "/s/sha-li/; /h/heng-ji/; /j/jiawei-han/",
        "bibtex": "@inproceedings{li-etal-2021-document,\n    title = \"Document-Level Event Argument Extraction by Conditional Generation\",\n    author = \"Li, Sha  and\n      Ji, Heng  and\n      Han, Jiawei\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.69/\",\n    doi = \"10.18653/v1/2021.naacl-main.69\",\n    pages = \"894--908\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.69.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.69/",
        "pdf_size": 734056,
        "gs_citation": 334,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14123359146996621375&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "University of Illinois at Urbana-Champaign, IL, USA; University of Illinois at Urbana-Champaign, IL, USA; University of Illinois at Urbana-Champaign, IL, USA",
        "aff_domain": "illinois.edu;illinois.edu;illinois.edu",
        "email": "illinois.edu;illinois.edu;illinois.edu",
        "github": "https://github.com/raspberryice/gen-arg",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Illinois at Urbana-Champaign",
        "aff_unique_dep": "",
        "aff_unique_url": "https://illinois.edu",
        "aff_unique_abbr": "UIUC",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Urbana-Champaign",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.73",
        "title": "Does BERT Pretrained on Clinical Notes Reveal Sensitive Data?",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Large Transformers pretrained over clinical notes from Electronic Health Records (EHR) have afforded substantial gains in performance on predictive clinical tasks. The cost of training such models (and the necessity of data access to do so) coupled with their utility motivates parameter sharing, i.e., the release of pretrained models such as ClinicalBERT. While most efforts have used deidentified EHR, many researchers have access to large sets of sensitive, non-deidentified EHR with which they might train a BERT model (or similar). Would it be safe to release the weights of such a model if they did? In this work, we design a battery of approaches intended to recover Personal Health Information (PHI) from a trained BERT. Specifically, we attempt to recover patient names and conditions with which they are associated. We find that simple probing methods are not able to meaningfully extract sensitive information from BERT trained over the MIMIC-III corpus of EHR. However, more sophisticated \u201cattacks\u201d may succeed in doing so: To facilitate such research, we make our experimental setup and baseline probing models available at https://github.com/elehman16/exposing_patient_data_release.",
        "author": "Eric Lehman; Sarthak Jain; Karl Pichotta; Yoav Goldberg; Byron Wallace",
        "authorids": "/e/eric-lehman/; /s/sarthak-jain/; /k/karl-pichotta/; /y/yoav-goldberg/; /b/byron-c-wallace/",
        "bibtex": "@inproceedings{lehman-etal-2021-bert,\n    title = \"Does {BERT} Pretrained on Clinical Notes Reveal Sensitive Data?\",\n    author = \"Lehman, Eric  and\n      Jain, Sarthak  and\n      Pichotta, Karl  and\n      Goldberg, Yoav  and\n      Wallace, Byron\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.73/\",\n    doi = \"10.18653/v1/2021.naacl-main.73\",\n    pages = \"946--959\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.73.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.73/",
        "pdf_size": 587205,
        "gs_citation": 126,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3431096910224722524&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "MIT CSAIL+Northeastern University; Northeastern University; Memorial Sloan Kettering Cancer Center; Bar Ilan University / Ramat Gan, Israel+Allen Institute for Arti\ufb01cial Intelligence; Northeastern University",
        "aff_domain": "mit.edu;northeastern.edu; ;biu.ac.il;northeastern.edu",
        "email": "mit.edu;northeastern.edu; ;biu.ac.il;northeastern.edu",
        "github": "https://github.com/elehman16/exposing_patient_data_release",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;1;2;3+4;1",
        "aff_unique_norm": "Massachusetts Institute of Technology;Northeastern University;Memorial Sloan Kettering Cancer Center;Bar Ilan University;Allen Institute for Artificial Intelligence",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory;;;;Artificial Intelligence",
        "aff_unique_url": "https://www.csail.mit.edu;https://www.northeastern.edu;https://www.mskcc.org;https://www.biu.ac.il;https://allenai.org",
        "aff_unique_abbr": "MIT CSAIL;NEU;MSKCC;BIU;AI2",
        "aff_campus_unique_index": "0;2",
        "aff_campus_unique": "Cambridge;;Ramat Gan",
        "aff_country_unique_index": "0+0;0;0;1+0;0",
        "aff_country_unique": "United States;Israel"
    },
    {
        "id": "2021.naacl-main.367",
        "title": "Does Structure Matter? Encoding Documents for Machine Reading Comprehension",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Machine reading comprehension is a challenging task especially for querying documents with deep and interconnected contexts. Transformer-based methods have shown advanced performances on this task; however, most of them still treat documents as a flat sequence of tokens. This work proposes a new Transformer-based method that reads a document as tree slices. It contains two modules for identifying more relevant text passage and the best answer span respectively, which are not only jointly trained but also jointly consulted at inference time. Our evaluation results show that our proposed method outperforms several competitive baseline approaches on two datasets from varied domains.",
        "author": "Hui Wan; Song Feng; Chulaka Gunasekara; Siva Sankalp Patel; Sachindra Joshi; Luis Lastras",
        "authorids": "/h/hui-wan/; /s/song-feng/; /c/chulaka-gunasekara/; /s/siva-sankalp-patel/; /s/sachindra-joshi/; /l/luis-lastras/",
        "bibtex": "@inproceedings{wan-etal-2021-structure,\n    title = \"Does Structure Matter? Encoding Documents for Machine Reading Comprehension\",\n    author = \"Wan, Hui  and\n      Feng, Song  and\n      Gunasekara, Chulaka  and\n      Patel, Siva Sankalp  and\n      Joshi, Sachindra  and\n      Lastras, Luis\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.367/\",\n    doi = \"10.18653/v1/2021.naacl-main.367\",\n    pages = \"4626--4634\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.367.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.367/",
        "pdf_size": 501432,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8093945605115767167&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "IBM Research AI; IBM Research AI; IBM Research AI; IBM Research AI; IBM Research AI; IBM Research AI",
        "aff_domain": "us.ibm.com;us.ibm.com;ibm.com;ibm.com;in.ibm.com;us.ibm.com",
        "email": "us.ibm.com;us.ibm.com;ibm.com;ibm.com;in.ibm.com;us.ibm.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "IBM Research",
        "aff_unique_dep": "AI",
        "aff_unique_url": "https://www.ibm.com/research",
        "aff_unique_abbr": "IBM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.146",
        "title": "Does syntax matter? A strong baseline for Aspect-based Sentiment Analysis with RoBERTa",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Aspect-based Sentiment Analysis (ABSA), aiming at predicting the polarities for aspects, is a fine-grained task in the field of sentiment analysis. Previous work showed syntactic information, e.g. dependency trees, can effectively improve the ABSA performance. Recently, pre-trained models (PTMs) also have shown their effectiveness on ABSA. Therefore, the question naturally arises whether PTMs contain sufficient syntactic information for ABSA so that we can obtain a good ABSA model only based on PTMs. In this paper, we firstly compare the induced trees from PTMs and the dependency parsing trees on several popular models for the ABSA task, showing that the induced tree from fine-tuned RoBERTa (FT-RoBERTa) outperforms the parser-provided tree. The further analysis experiments reveal that the FT-RoBERTa Induced Tree is more sentiment-word-oriented and could benefit the ABSA task. The experiments also show that the pure RoBERTa-based model can outperform or approximate to the previous SOTA performances on six datasets across four languages since it implicitly incorporates the task-oriented syntactic information.",
        "author": "Junqi Dai; Hang Yan; Tianxiang Sun; Pengfei Liu; Xipeng Qiu",
        "authorids": "/j/junqi-dai/; /h/hang-yan/; /t/tianxiang-sun/; /p/pengfei-liu/; /x/xipeng-qiu/",
        "bibtex": "@inproceedings{dai-etal-2021-syntax,\n    title = \"Does syntax matter? A strong baseline for Aspect-based Sentiment Analysis with {R}o{BERT}a\",\n    author = \"Dai, Junqi  and\n      Yan, Hang  and\n      Sun, Tianxiang  and\n      Liu, Pengfei  and\n      Qiu, Xipeng\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.146/\",\n    doi = \"10.18653/v1/2021.naacl-main.146\",\n    pages = \"1816--1829\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.146.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.146/",
        "pdf_size": 573452,
        "gs_citation": 181,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18156544266608401351&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Shanghai Key Laboratory of Intelligent Information Processing, Fudan University + School of Computer Science, Fudan University; Shanghai Key Laboratory of Intelligent Information Processing, Fudan University + School of Computer Science, Fudan University; Shanghai Key Laboratory of Intelligent Information Processing, Fudan University + School of Computer Science, Fudan University; Carnegie Mellon University; Shanghai Key Laboratory of Intelligent Information Processing, Fudan University + School of Computer Science, Fudan University",
        "aff_domain": "fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;cs.cmu.edu;fudan.edu.cn",
        "email": "fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;cs.cmu.edu;fudan.edu.cn",
        "github": "https://github.com/ROGERDJQ/RoBERTaABSA",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+0;0+0;0+0;1;0+0",
        "aff_unique_norm": "Fudan University;Carnegie Mellon University",
        "aff_unique_dep": "Shanghai Key Laboratory of Intelligent Information Processing;",
        "aff_unique_url": "https://www.fudan.edu.cn;https://www.cmu.edu",
        "aff_unique_abbr": "Fudan;CMU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Shanghai;",
        "aff_country_unique_index": "0+0;0+0;0+0;1;0+0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2021.naacl-main.226",
        "title": "Domain Adaptation for Arabic Cross-Domain and Cross-Dialect Sentiment Analysis from Contextualized Word Embedding",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Finetuning deep pre-trained language models has shown state-of-the-art performances on a wide range of Natural Language Processing (NLP) applications. Nevertheless, their generalization performance drops under domain shift. In the case of Arabic language, diglossia makes building and annotating corpora for each dialect and/or domain a more challenging task. Unsupervised Domain Adaptation tackles this issue by transferring the learned knowledge from labeled source domain data to unlabeled target domain data. In this paper, we propose a new unsupervised domain adaptation method for Arabic cross-domain and cross-dialect sentiment analysis from Contextualized Word Embedding. Several experiments are performed adopting the coarse-grained and the fine-grained taxonomies of Arabic dialects. The obtained results show that our method yields very promising results and outperforms several domain adaptation methods for most of the evaluated datasets. On average, our method increases the performance by an improvement rate of 20.8% over the zero-shot transfer learning from BERT.",
        "author": "Abdellah El Mekki; Abdelkader El Mahdaouy; Ismail Berrada; Ahmed Khoumsi",
        "authorids": "/a/abdellah-el-mekki/; /a/abdelkader-el-mahdaouy/; /i/ismail-berrada/; /a/ahmed-khoumsi/",
        "bibtex": "@inproceedings{el-mekki-etal-2021-domain,\n    title = \"Domain Adaptation for {A}rabic Cross-Domain and Cross-Dialect Sentiment Analysis from Contextualized Word Embedding\",\n    author = \"El Mekki, Abdellah  and\n      El Mahdaouy, Abdelkader  and\n      Berrada, Ismail  and\n      Khoumsi, Ahmed\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.226/\",\n    doi = \"10.18653/v1/2021.naacl-main.226\",\n    pages = \"2824--2837\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.226.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.226/",
        "pdf_size": 579008,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9893476128912032047&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "School of Computer Sciences, Mohammed VI Polytechnic University, Morocco; School of Computer Sciences, Mohammed VI Polytechnic University, Morocco; School of Computer Sciences, Mohammed VI Polytechnic University, Morocco; Dept. Electrical & Computer Engineering, University of Sherbrooke, Canada",
        "aff_domain": "um6p.ma;um6p.ma;um6p.ma;usherbrooke.ca",
        "email": "um6p.ma;um6p.ma;um6p.ma;usherbrooke.ca",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Mohammed VI Polytechnic University;University of Sherbrooke",
        "aff_unique_dep": "School of Computer Sciences;Dept. Electrical & Computer Engineering",
        "aff_unique_url": ";https://www.usherbrooke.ca",
        "aff_unique_abbr": ";USherb",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "Morocco;Canada"
    },
    {
        "id": "2021.naacl-main.147",
        "title": "Domain Divergences: A Survey and Empirical Analysis",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Domain divergence plays a significant role in estimating the performance of a model in new domains. While there is a significant literature on divergence measures, researchers find it hard to choose an appropriate divergence for a given NLP application. We address this shortcoming by both surveying the literature and through an empirical study. We develop a taxonomy of divergence measures consisting of three classes \u2014 Information-theoretic, Geometric, and Higher-order measures and identify the relationships between them. Further, to understand the common use-cases of these measures, we recognise three novel applications \u2013 1) Data Selection, 2) Learning Representation, and 3) Decisions in the Wild \u2013 and use it to organise our literature. From this, we identify that Information-theoretic measures are prevalent for 1) and 3), and Higher-order measures are more common for 2). To further help researchers choose appropriate measures to predict drop in performance \u2013 an important aspect of Decisions in the Wild, we perform correlation analysis spanning 130 domain adaptation scenarios, 3 varied NLP tasks and 12 divergence measures identified from our survey. To calculate these divergences, we consider the current contextual word representations (CWR) and contrast with the older distributed representations. We find that traditional measures over word distributions still serve as strong baselines, while higher-order measures with CWR are effective.",
        "author": "Abhinav Ramesh Kashyap; Devamanyu Hazarika; Min-Yen Kan; Roger Zimmermann",
        "authorids": "/a/abhinav-ramesh-kashyap/; /d/devamanyu-hazarika/; /m/min-yen-kan/; /r/roger-zimmermann/",
        "bibtex": "@inproceedings{ramesh-kashyap-etal-2021-domain,\n    title = \"Domain Divergences: A Survey and Empirical Analysis\",\n    author = \"Ramesh Kashyap, Abhinav  and\n      Hazarika, Devamanyu  and\n      Kan, Min-Yen  and\n      Zimmermann, Roger\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.147/\",\n    doi = \"10.18653/v1/2021.naacl-main.147\",\n    pages = \"1830--1849\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.147.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.147/",
        "pdf_size": 15022664,
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6093798558352323118&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "National University of Singapore, Singapore; National University of Singapore, Singapore; National University of Singapore, Singapore; National University of Singapore, Singapore",
        "aff_domain": "comp.nus.edu.sg;comp.nus.edu.sg;comp.nus.edu.sg;comp.nus.edu.sg",
        "email": "comp.nus.edu.sg;comp.nus.edu.sg;comp.nus.edu.sg;comp.nus.edu.sg",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "National University of Singapore",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.nus.edu.sg",
        "aff_unique_abbr": "NUS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "2021.naacl-main.305",
        "title": "Double Perturbation: On the Robustness of Robustness and Counterfactual Bias Evaluation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Robustness and counterfactual bias are usually evaluated on a test dataset. However, are these evaluations robust? If the test dataset is perturbed slightly, will the evaluation results keep the same? In this paper, we propose a \u201cdouble perturbation\u201d framework to uncover model weaknesses beyond the test dataset. The framework first perturbs the test dataset to construct abundant natural sentences similar to the test data, and then diagnoses the prediction change regarding a single-word substitution. We apply this framework to study two perturbation-based approaches that are used to analyze models\u2019 robustness and counterfactual bias in English. (1) For robustness, we focus on synonym substitutions and identify vulnerable examples where prediction can be altered. Our proposed attack attains high success rates (96.0%-99.8%) in finding vulnerable examples on both original and robustly trained CNNs and Transformers. (2) For counterfactual bias, we focus on substituting demographic tokens (e.g., gender, race) and measure the shift of the expected prediction among constructed sentences. Our method is able to reveal the hidden model biases not directly shown in the test dataset. Our code is available at https://github.com/chong-z/nlp-second-order-attack.",
        "author": "Chong Zhang; Jieyu Zhao; Huan Zhang; Kai-Wei Chang; Cho-Jui Hsieh",
        "authorids": "/c/chong-zhang/; /j/jieyu-zhao/; /h/huan-zhang/; /k/kai-wei-chang/; /c/cho-jui-hsieh/",
        "bibtex": "@inproceedings{zhang-etal-2021-double,\n    title = \"Double Perturbation: On the Robustness of Robustness and Counterfactual Bias Evaluation\",\n    author = \"Zhang, Chong  and\n      Zhao, Jieyu  and\n      Zhang, Huan  and\n      Chang, Kai-Wei  and\n      Hsieh, Cho-Jui\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.305/\",\n    doi = \"10.18653/v1/2021.naacl-main.305\",\n    pages = \"3899--3916\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.305.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.305/",
        "pdf_size": 1127712,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14414146817453252054&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science, UCLA; Department of Computer Science, UCLA; Department of Computer Science, UCLA; Department of Computer Science, UCLA; Department of Computer Science, UCLA",
        "aff_domain": "cs.ucla.edu;cs.ucla.edu;huan-zhang.com;cs.ucla.edu;cs.ucla.edu",
        "email": "cs.ucla.edu;cs.ucla.edu;huan-zhang.com;cs.ucla.edu;cs.ucla.edu",
        "github": "https://github.com/chong-z/nlp-second-order-attack",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of California, Los Angeles",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ucla.edu",
        "aff_unique_abbr": "UCLA",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.103",
        "title": "DuoRAT: Towards Simpler Text-to-SQL Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent neural text-to-SQL models can effectively translate natural language questions to corresponding SQL queries on unseen databases. Working mostly on the Spider dataset, researchers have proposed increasingly sophisticated solutions to the problem. Contrary to this trend, in this paper we focus on simplifications. We begin by building DuoRAT, a re-implementation of the state-of-the-art RAT-SQL model that unlike RAT-SQL is using only relation-aware or vanilla transformers as the building blocks. We perform several ablation experiments using DuoRAT as the baseline model. Our experiments confirm the usefulness of some techniques and point out the redundancy of others, including structural SQL features and features that link the question with the schema.",
        "author": "Torsten Scholak; Raymond Li; Dzmitry Bahdanau; Harm de Vries; Chris Pal",
        "authorids": "/t/torsten-scholak/; /r/raymond-li/; /d/dzmitry-bahdanau/; /h/harm-de-vries/; /c/christopher-pal/",
        "bibtex": "@inproceedings{scholak-etal-2021-duorat,\n    title = \"{D}uo{RAT}: Towards Simpler Text-to-{SQL} Models\",\n    author = \"Scholak, Torsten  and\n      Li, Raymond  and\n      Bahdanau, Dzmitry  and\n      de Vries, Harm  and\n      Pal, Chris\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.103/\",\n    doi = \"10.18653/v1/2021.naacl-main.103\",\n    pages = \"1313--1321\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.103.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.103/",
        "pdf_size": 336547,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13381797499938481021&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Element AI, a ServiceNow company; Element AI, a ServiceNow company; Element AI, a ServiceNow company; Element AI, a ServiceNow company; Element AI, a ServiceNow company",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "https://github.com/ElementAI/duorat",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Element AI",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.elementai.com",
        "aff_unique_abbr": "Element AI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2021.naacl-main.324",
        "title": "Dynabench: Rethinking Benchmarking in NLP",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We introduce Dynabench, an open-source platform for dynamic dataset creation and model benchmarking. Dynabench runs in a web browser and supports human-and-model-in-the-loop dataset creation: annotators seek to create examples that a target model will misclassify, but that another person will not. In this paper, we argue that Dynabench addresses a critical need in our community: contemporary models quickly achieve outstanding performance on benchmark tasks but nonetheless fail on simple challenge examples and falter in real-world scenarios. With Dynabench, dataset creation, model development, and model assessment can directly inform each other, leading to more robust and informative benchmarks. We report on four initial NLP tasks, illustrating these concepts and highlighting the promise of the platform, and address potential objections to dynamic benchmarking as a new standard for the field.",
        "author": "Douwe Kiela; Max Bartolo; Yixin Nie; Divyansh Kaushik; Atticus Geiger; Zhengxuan Wu; Bertie Vidgen; Grusha Prasad; Amanpreet Singh; Pratik Ringshia; Zhiyi Ma; Tristan Thrush; Sebastian Riedel; Zeerak Waseem; Pontus Stenetorp; Robin Jia; Mohit Bansal; Christopher Potts; Adina Williams",
        "authorids": "/d/douwe-kiela/; /m/max-bartolo/; /y/yixin-nie/; /d/divyansh-kaushik/; /a/atticus-geiger/; /z/zhengxuan-wu/; /b/bertie-vidgen/; /g/grusha-prasad/; /a/amanpreet-singh/; /p/pratik-ringshia/; /z/zhiyi-ma/; /t/tristan-thrush/; /s/sebastian-riedel/; /z/zeerak-talat/; /p/pontus-stenetorp/; /r/robin-jia/; /m/mohit-bansal/; /c/christopher-potts/; /a/adina-williams/",
        "bibtex": "@inproceedings{kiela-etal-2021-dynabench,\n    title = \"Dynabench: Rethinking Benchmarking in {NLP}\",\n    author = \"Kiela, Douwe  and\n      Bartolo, Max  and\n      Nie, Yixin  and\n      Kaushik, Divyansh  and\n      Geiger, Atticus  and\n      Wu, Zhengxuan  and\n      Vidgen, Bertie  and\n      Prasad, Grusha  and\n      Singh, Amanpreet  and\n      Ringshia, Pratik  and\n      Ma, Zhiyi  and\n      Thrush, Tristan  and\n      Riedel, Sebastian  and\n      Waseem, Zeerak  and\n      Stenetorp, Pontus  and\n      Jia, Robin  and\n      Bansal, Mohit  and\n      Potts, Christopher  and\n      Williams, Adina\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.324/\",\n    doi = \"10.18653/v1/2021.naacl-main.324\",\n    pages = \"4110--4124\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.324.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.324/",
        "pdf_size": 480678,
        "gs_citation": 471,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11201738544723136629&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": ";;;;;;;;;;;;;;;;;;",
        "aff_domain": ";;;;;;;;;;;;;;;;;;",
        "email": ";;;;;;;;;;;;;;;;;;",
        "github": "",
        "project": "",
        "author_num": 19
    },
    {
        "id": "2021.naacl-main.293",
        "title": "Dynamically Disentangling Social Bias from Task-Oriented Representations with Adversarial Attack",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Representation learning is widely used in NLP for a vast range of tasks. However, representations derived from text corpora often reflect social biases. This phenomenon is pervasive and consistent across different neural models, causing serious concern. Previous methods mostly rely on a pre-specified, user-provided direction or suffer from unstable training. In this paper, we propose an adversarial disentangled debiasing model to dynamically decouple social bias attributes from the intermediate representations trained on the main task. We aim to denoise bias information while training on the downstream task, rather than completely remove social bias and pursue static unbiased representations. Experiments show the effectiveness of our method, both on the effect of debiasing and the main task performance.",
        "author": "Liwen Wang; Yuanmeng Yan; Keqing He; Yanan Wu; Weiran Xu",
        "authorids": "/l/liwen-wang/; /y/yuanmeng-yan/; /k/keqing-he/; /y/yanan-wu/; /w/weiran-xu/",
        "bibtex": "@inproceedings{wang-etal-2021-dynamically,\n    title = \"Dynamically Disentangling Social Bias from Task-Oriented Representations with Adversarial Attack\",\n    author = \"Wang, Liwen  and\n      Yan, Yuanmeng  and\n      He, Keqing  and\n      Wu, Yanan  and\n      Xu, Weiran\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.293/\",\n    doi = \"10.18653/v1/2021.naacl-main.293\",\n    pages = \"3740--3750\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.293.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.293/",
        "pdf_size": 837575,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2984075619236766084&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications + Meituan Group; Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications",
        "aff_domain": "bupt.edu.cn;bupt.edu.cn;meituan.com;bupt.edu.cn;bupt.edu.cn",
        "email": "bupt.edu.cn;bupt.edu.cn;meituan.com;bupt.edu.cn;bupt.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0+1;0;0",
        "aff_unique_norm": "Beijing University of Posts and Telecommunications;Meituan Group",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.bupt.edu.cn/;https://www.meituan.com",
        "aff_unique_abbr": "BUPT;Meituan",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0;0;0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2021.naacl-main.394",
        "title": "ENTRUST: Argument Reframing with Language Models and Entailment",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Framing involves the positive or negative presentation of an argument or issue depending on the audience and goal of the speaker. Differences in lexical framing, the focus of our work, can have large effects on peoples\u2019 opinions and beliefs. To make progress towards reframing arguments for positive effects, we create a dataset and method for this task. We use a lexical resource for \u201cconnotations\u201d to create a parallel corpus and propose a method for argument reframing that combines controllable text generation (positive connotation) with a post-decoding entailment component (same denotation). Our results show that our method is effective compared to strong baselines along the dimensions of fluency, meaning, and trustworthiness/reduction of fear.",
        "author": "Tuhin Chakrabarty; Christopher Hidey; Smaranda Muresan",
        "authorids": "/t/tuhin-chakrabarty/; /c/christopher-hidey/; /s/smaranda-muresan/",
        "bibtex": "@inproceedings{chakrabarty-etal-2021-entrust,\n    title = \"{ENTRUST}: Argument Reframing with Language Models and Entailment\",\n    author = \"Chakrabarty, Tuhin  and\n      Hidey, Christopher  and\n      Muresan, Smaranda\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.394/\",\n    doi = \"10.18653/v1/2021.naacl-main.394\",\n    pages = \"4958--4971\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.394.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.394/",
        "pdf_size": 343348,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5697190125108394428&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science, Columbia University1 + Data Science Institute, Columbia University2; Google3; Department of Computer Science, Columbia University1 + Data Science Institute, Columbia University2",
        "aff_domain": "cs.columbia.edu;gmail.com;cs.columbia.edu",
        "email": "cs.columbia.edu;gmail.com;cs.columbia.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+0;1;0+0",
        "aff_unique_norm": "Columbia University;Google",
        "aff_unique_dep": "Department of Computer Science;",
        "aff_unique_url": "https://www.columbia.edu;https://www.google.com",
        "aff_unique_abbr": "Columbia;Google",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.314",
        "title": "ER-AE: Differentially Private Text Generation for Authorship Anonymization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Most of privacy protection studies for textual data focus on removing explicit sensitive identifiers. However, personal writing style, as a strong indicator of the authorship, is often neglected. Recent studies, such as SynTF, have shown promising results on privacy-preserving text mining. However, their anonymization algorithm can only output numeric term vectors which are difficult for the recipients to interpret. We propose a novel text generation model with a two-set exponential mechanism for authorship anonymization. By augmenting the semantic information through a REINFORCE training reward function, the model can generate differentially private text that has a close semantic and similar grammatical structure to the original text while removing personal traits of the writing style. It does not assume any conditioned labels or paralleled text data for training. We evaluate the performance of the proposed model on the real-life peer reviews dataset and the Yelp review dataset. The result suggests that our model outperforms the state-of-the-art on semantic preservation, authorship obfuscation, and stylometric transformation.",
        "author": "Haohan Bo; Steven H. H. Ding; Benjamin C. M. Fung; Farkhund Iqbal",
        "authorids": "/h/haohan-bo/; /s/steven-h-h-ding/; /b/benjamin-c-m-fung/; /f/farkhund-iqbal/",
        "bibtex": "@inproceedings{bo-etal-2021-er,\n    title = \"{ER}-{AE}: Differentially Private Text Generation for Authorship Anonymization\",\n    author = \"Bo, Haohan  and\n      Ding, Steven H. H.  and\n      Fung, Benjamin C. M.  and\n      Iqbal, Farkhund\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.314/\",\n    doi = \"10.18653/v1/2021.naacl-main.314\",\n    pages = \"3997--4007\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.314.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.314/",
        "pdf_size": 492831,
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4987826677034018240&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "McGill University, Canada; Queen\u2019s University, Canada; McGill University, Canada; Zayed University, UAE",
        "aff_domain": "mail.mcgill.ca;cs.queensu.ca;mcgill.ca;zu.ac.ae",
        "email": "mail.mcgill.ca;cs.queensu.ca;mcgill.ca;zu.ac.ae",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;2",
        "aff_unique_norm": "McGill University;Queen's University;Zayed University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.mcgill.ca;https://www.queensu.ca;https://www.zu.ac.ae",
        "aff_unique_abbr": "McGill;Queen's U;ZU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "Canada;United Arab Emirates"
    },
    {
        "id": "2021.naacl-main.136",
        "title": "ERNIE-Gram: Pre-Training with Explicitly N-Gram Masked Language Modeling for Natural Language Understanding",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Coarse-grained linguistic information, such as named entities or phrases, facilitates adequately representation learning in pre-training. Previous works mainly focus on extending the objective of BERT\u2019s Masked Language Modeling (MLM) from masking individual tokens to contiguous sequences of n tokens. We argue that such contiguously masking method neglects to model the intra-dependencies and inter-relation of coarse-grained linguistic information. As an alternative, we propose ERNIE-Gram, an explicitly n-gram masking method to enhance the integration of coarse-grained information into pre-training. In ERNIE-Gram, n-grams are masked and predicted directly using explicit n-gram identities rather than contiguous sequences of n tokens. Furthermore, ERNIE-Gram employs a generator model to sample plausible n-gram identities as optional n-gram masks and predict them in both coarse-grained and fine-grained manners to enable comprehensive n-gram prediction and relation modeling. We pre-train ERNIE-Gram on English and Chinese text corpora and fine-tune on 19 downstream tasks. Experimental results show that ERNIE-Gram outperforms previous pre-training models like XLNet and RoBERTa by a large margin, and achieves comparable results with state-of-the-art methods. The source codes and pre-trained models have been released at https://github.com/PaddlePaddle/ERNIE.",
        "author": "Dongling Xiao; Yu-Kun Li; Han Zhang; Yu Sun; Hao Tian; Hua Wu; Haifeng Wang",
        "authorids": "/d/dongling-xiao/; /y/yu-kun-li/; /h/han-zhang/; /y/yu-sun/; /h/hao-tian/; /h/hua-wu/; /h/haifeng-wang/",
        "bibtex": "@inproceedings{xiao-etal-2021-ernie,\n    title = \"{ERNIE}-Gram: Pre-Training with Explicitly N-Gram Masked Language Modeling for Natural Language Understanding\",\n    author = \"Xiao, Dongling  and\n      Li, Yu-Kun  and\n      Zhang, Han  and\n      Sun, Yu  and\n      Tian, Hao  and\n      Wu, Hua  and\n      Wang, Haifeng\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.136/\",\n    doi = \"10.18653/v1/2021.naacl-main.136\",\n    pages = \"1702--1715\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.136.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.136/",
        "pdf_size": 4337688,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5665286699416068154&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Baidu Inc., China; Baidu Inc., China; Baidu Inc., China; Baidu Inc., China; Baidu Inc., China; Baidu Inc., China; Baidu Inc., China",
        "aff_domain": "baidu.com;baidu.com;baidu.com;baidu.com;baidu.com;baidu.com;baidu.com",
        "email": "baidu.com;baidu.com;baidu.com;baidu.com;baidu.com;baidu.com;baidu.com",
        "github": "https://github.com/PaddlePaddle/ERNIE",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "Baidu Inc.",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.baidu.com",
        "aff_unique_abbr": "Baidu",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2021.naacl-main.371",
        "title": "ESC: Redesigning WSD with Extractive Sense Comprehension",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Word Sense Disambiguation (WSD) is a historical NLP task aimed at linking words in contexts to discrete sense inventories and it is usually cast as a multi-label classification task. Recently, several neural approaches have employed sense definitions to better represent word meanings. Yet, these approaches do not observe the input sentence and the sense definition candidates all at once, thus potentially reducing the model performance and generalization power. We cope with this issue by reframing WSD as a span extraction problem \u2014 which we called Extractive Sense Comprehension (ESC) \u2014 and propose ESCHER, a transformer-based neural architecture for this new formulation. By means of an extensive array of experiments, we show that ESC unleashes the full potential of our model, leading it to outdo all of its competitors and to set a new state of the art on the English WSD task. In the few-shot scenario, ESCHER proves to exploit training data efficiently, attaining the same performance as its closest competitor while relying on almost three times fewer annotations. Furthermore, ESCHER can nimbly combine data annotated with senses from different lexical resources, achieving performances that were previously out of everyone\u2019s reach. The model along with data is available at https://github.com/SapienzaNLP/esc.",
        "author": "Edoardo Barba; Tommaso Pasini; Roberto Navigli",
        "authorids": "/e/edoardo-barba/; /t/tommaso-pasini/; /r/roberto-navigli/",
        "bibtex": "@inproceedings{barba-etal-2021-esc,\n    title = \"{ESC}: Redesigning {WSD} with Extractive Sense Comprehension\",\n    author = \"Barba, Edoardo  and\n      Pasini, Tommaso  and\n      Navigli, Roberto\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.371/\",\n    doi = \"10.18653/v1/2021.naacl-main.371\",\n    pages = \"4661--4672\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.371.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.371/",
        "pdf_size": 800136,
        "gs_citation": 109,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2113465352866782955&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Sapienza NLP Group, Department of Computer Science, Sapienza University of Rome; Department of Computer Science, University of Copenhagen + Sapienza University of Rome; Sapienza NLP Group, Department of Computer Science, Sapienza University of Rome",
        "aff_domain": "di.uniroma1.it;di.ku.dk;di.uniroma1.it",
        "email": "di.uniroma1.it;di.ku.dk;di.uniroma1.it",
        "github": "https://github.com/SapienzaNLP/esc",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1+0;0",
        "aff_unique_norm": "Sapienza University of Rome;University of Copenhagen",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www.uniroma1.it;https://www.ku.dk",
        "aff_unique_abbr": "Sapienza;UCPH",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Rome;",
        "aff_country_unique_index": "0;1+0;0",
        "aff_country_unique": "Italy;Denmark"
    },
    {
        "id": "2021.naacl-main.192",
        "title": "EaSe: A Diagnostic Tool for VQA based on Answer Diversity",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We propose EASE, a simple diagnostic tool for Visual Question Answering (VQA) which quantifies the difficulty of an image, question sample. EASE is based on the pattern of answers provided by multiple annotators to a given question. In particular, it considers two aspects of the answers: (i) their Entropy; (ii) their Semantic content. First, we prove the validity of our diagnostic to identify samples that are easy/hard for state-of-art VQA models. Second, we show that EASE can be successfully used to select the most-informative samples for training/fine-tuning. Crucially, only information that is readily available in any VQA dataset is used to compute its scores.",
        "author": "Shailza Jolly; Sandro Pezzelle; Moin Nabi",
        "authorids": "/s/shailza-jolly/; /s/sandro-pezzelle/; /m/moin-nabi/",
        "bibtex": "@inproceedings{jolly-etal-2021-ease,\n    title = \"{E}a{S}e: A Diagnostic Tool for {VQA} based on Answer Diversity\",\n    author = \"Jolly, Shailza  and\n      Pezzelle, Sandro  and\n      Nabi, Moin\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.192/\",\n    doi = \"10.18653/v1/2021.naacl-main.192\",\n    pages = \"2407--2414\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.192.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.192/",
        "pdf_size": 1355482,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2472494234498184931&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "TU Kaiserslautern, Germany+DFKI GmbH, Germany; ILLC, University of Amsterdam, Netherlands; SAP AI Research, Germany",
        "aff_domain": "dfki.de;uva.nl;sap.com",
        "email": "dfki.de;uva.nl;sap.com",
        "github": "github.com/shailzajolly/EaSe",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;2;3",
        "aff_unique_norm": "Technische Universit\u00e4t Kaiserslautern;Deutsches Forschungszentrum f\u00fcr K\u00fcnstliche Intelligenz GmbH;University of Amsterdam;SAP AI Research",
        "aff_unique_dep": ";;ILLC;AI Research",
        "aff_unique_url": "https://www.tu-kl.de;https://www.dfki.de;https://www.uva.nl;https://www.sap.com",
        "aff_unique_abbr": "TU Kaiserslautern;DFKI;UvA;SAP",
        "aff_campus_unique_index": ";1",
        "aff_campus_unique": ";Amsterdam",
        "aff_country_unique_index": "0+0;1;0",
        "aff_country_unique": "Germany;Netherlands"
    },
    {
        "id": "2021.naacl-main.221",
        "title": "Edge: Enriching Knowledge Graph Embeddings with External Text",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Knowledge graphs suffer from sparsity which degrades the quality of representations generated by various methods. While there is an abundance of textual information throughout the web and many existing knowledge bases, aligning information across these diverse data sources remains a challenge in the literature. Previous work has partially addressed this issue by enriching knowledge graph entities based on \u201chard\u201d co-occurrence of words present in the entities of the knowledge graphs and external text, while we achieve \u201csoft\u201d augmentation by proposing a knowledge graph enrichment and embedding framework named Edge. Given an original knowledge graph, we first generate a rich but noisy augmented graph using external texts in semantic and structural level. To distill the relevant knowledge and suppress the introduced noise, we design a graph alignment term in a shared embedding space between the original graph and augmented graph. To enhance the embedding learning on the augmented graph, we further regularize the locality relationship of target entity based on negative sampling. Experimental results on four benchmark datasets demonstrate the robustness and effectiveness of Edge in link prediction and node classification.",
        "author": "Saed Rezayi; Handong Zhao; Sungchul Kim; Ryan Rossi; Nedim Lipka; Sheng Li",
        "authorids": "/s/saed-rezayi/; /h/handong-zhao/; /s/sungchul-kim/; /r/ryan-rossi/; /n/nedim-lipka/; /s/sheng-li/",
        "bibtex": "@inproceedings{rezayi-etal-2021-edge,\n    title = \"Edge: Enriching Knowledge Graph Embeddings with External Text\",\n    author = \"Rezayi, Saed  and\n      Zhao, Handong  and\n      Kim, Sungchul  and\n      Rossi, Ryan  and\n      Lipka, Nedim  and\n      Li, Sheng\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.221/\",\n    doi = \"10.18653/v1/2021.naacl-main.221\",\n    pages = \"2767--2776\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.221.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.221/",
        "pdf_size": 2268495,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11387404645576999006&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Computer Science, University of Georgia, Athens, GA, USA; Adobe Research, San Jose, CA, USA; Adobe Research, San Jose, CA, USA; Adobe Research, San Jose, CA, USA; Adobe Research, San Jose, CA, USA; Department of Computer Science, University of Georgia, Athens, GA, USA",
        "aff_domain": "uga.edu;adobe.com;adobe.com;adobe.com;adobe.com;uga.edu",
        "email": "uga.edu;adobe.com;adobe.com;adobe.com;adobe.com;uga.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;1;1;1;0",
        "aff_unique_norm": "University of Georgia;Adobe Research",
        "aff_unique_dep": "Department of Computer Science;",
        "aff_unique_url": "https://www.uga.edu;https://research.adobe.com",
        "aff_unique_abbr": "UGA;Adobe",
        "aff_campus_unique_index": "0;1;1;1;1;0",
        "aff_campus_unique": "Athens;San Jose",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.112",
        "title": "Efficient Attentions for Long Document Summarization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The quadratic computational and memory complexities of large Transformers have limited their scalability for long document summarization. In this paper, we propose Hepos, a novel efficient encoder-decoder attention with head-wise positional strides to effectively pinpoint salient information from the source. We further conduct a systematic study of existing efficient self-attentions. Combined with Hepos, we are able to process ten times more tokens than existing models that use full attentions. For evaluation, we present a new dataset, GovReport, with significantly longer documents and summaries. Results show that our models produce significantly higher ROUGE scores than competitive comparisons, including new state-of-the-art results on PubMed. Human evaluation also shows that our models generate more informative summaries with fewer unfaithful errors.",
        "author": "Luyang Huang; Shuyang Cao; Nikolaus Parulian; Heng Ji; Lu Wang",
        "authorids": "/l/luyang-huang/; /s/shuyang-cao/; /n/nikolaus-parulian/; /h/heng-ji/; /l/lu-wang/",
        "bibtex": "@inproceedings{huang-etal-2021-efficient,\n    title = \"Efficient Attentions for Long Document Summarization\",\n    author = \"Huang, Luyang  and\n      Cao, Shuyang  and\n      Parulian, Nikolaus  and\n      Ji, Heng  and\n      Wang, Lu\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.112/\",\n    doi = \"10.18653/v1/2021.naacl-main.112\",\n    pages = \"1419--1436\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.112.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.112/",
        "pdf_size": 615594,
        "gs_citation": 303,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2313677618261620816&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Computer Science and Engineering, University of Michigan, Ann Arbor, MI; Computer Science and Engineering, University of Michigan, Ann Arbor, MI; Department of Computer Science, University of Illinois at Urbana-Champaign, IL; Department of Computer Science, University of Illinois at Urbana-Champaign, IL; Computer Science and Engineering, University of Michigan, Ann Arbor, MI",
        "aff_domain": "umich.edu;umich.edu;illinois.edu;illinois.edu;umich.edu",
        "email": "umich.edu;umich.edu;illinois.edu;illinois.edu;umich.edu",
        "github": "https://github.com/luyang-huang96/LongDocSum",
        "project": "https://gov-report-data.github.io",
        "author_num": 5,
        "aff_unique_index": "0;0;1;1;0",
        "aff_unique_norm": "University of Michigan;University of Illinois at Urbana-Champaign",
        "aff_unique_dep": "Computer Science and Engineering;Department of Computer Science",
        "aff_unique_url": "https://www.umich.edu;https://illinois.edu",
        "aff_unique_abbr": "UM;UIUC",
        "aff_campus_unique_index": "0;0;1;1;0",
        "aff_campus_unique": "Ann Arbor;Urbana-Champaign",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.380",
        "title": "Efficiently Summarizing Text and Graph Encodings of Multi-Document Clusters",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "This paper presents an efficient graph-enhanced approach to multi-document summarization (MDS) with an encoder-decoder Transformer model. This model is based on recent advances in pre-training both encoder and decoder on very large text data (Lewis et al., 2019), and it incorporates an efficient encoding mechanism (Beltagy et al., 2020) that avoids the quadratic memory growth typical for traditional Transformers. We show that this powerful combination not only scales to large input documents commonly found when summarizing news clusters; it also enables us to process additional input in the form of auxiliary graph representations, which we derive from the multi-document clusters. We present a mechanism to incorporate such graph information into the encoder-decoder model that was pre-trained on text only. Our approach leads to significant improvements on the Multi-News dataset, overall leading to an average 1.8 ROUGE score improvement over previous work (Li et al., 2020). We also show improvements in a transfer-only setup on the DUC-2004 dataset. The graph encodings lead to summaries that are more abstractive. Human evaluation shows that they are also more informative and factually more consistent with their input documents.",
        "author": "Ramakanth Pasunuru; Mengwen Liu; Mohit Bansal; Sujith Ravi; Markus Dreyer",
        "authorids": "/r/ramakanth-pasunuru/; /m/mengwen-liu/; /m/mohit-bansal/; /s/sujith-ravi/; /m/markus-dreyer/",
        "bibtex": "@inproceedings{pasunuru-etal-2021-efficiently,\n    title = \"Efficiently Summarizing Text and Graph Encodings of Multi-Document Clusters\",\n    author = \"Pasunuru, Ramakanth  and\n      Liu, Mengwen  and\n      Bansal, Mohit  and\n      Ravi, Sujith  and\n      Dreyer, Markus\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.380/\",\n    doi = \"10.18653/v1/2021.naacl-main.380\",\n    pages = \"4768--4779\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.380.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.380/",
        "pdf_size": 613267,
        "gs_citation": 74,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4592032237278527717&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "UNC Chapel Hill; Amazon Alexa; UNC Chapel Hill; Amazon Alexa; Amazon Alexa",
        "aff_domain": "cs.unc.edu;amazon.com;cs.unc.edu;amazon.com;amazon.com",
        "email": "cs.unc.edu;amazon.com;cs.unc.edu;amazon.com;amazon.com",
        "github": "https://github.com/amazon-research/BartGraphSumm",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;0;1;1",
        "aff_unique_norm": "University of North Carolina at Chapel Hill;Amazon",
        "aff_unique_dep": ";Amazon Alexa",
        "aff_unique_url": "https://www.unc.edu;https://www.amazon.com/alexa",
        "aff_unique_abbr": "UNC;Amazon Alexa",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Chapel Hill;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.230",
        "title": "Emotion-Infused Models for Explainable Psychological Stress Detection",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The problem of detecting psychological stress in online posts, and more broadly, of detecting people in distress or in need of help, is a sensitive application for which the ability to interpret models is vital. Here, we present work exploring the use of a semantically related task, emotion detection, for equally competent but more explainable and human-like psychological stress detection as compared to a black-box model. In particular, we explore the use of multi-task learning as well as emotion-based language model fine-tuning. With our emotion-infused models, we see comparable results to state-of-the-art BERT. Our analysis of the words used for prediction show that our emotion-infused models mirror psychological components of stress.",
        "author": "Elsbeth Turcan; Smaranda Muresan; Kathleen McKeown",
        "authorids": "/e/elsbeth-turcan/; /s/smaranda-muresan/; /k/kathleen-mckeown/",
        "bibtex": "@inproceedings{turcan-etal-2021-emotion,\n    title = \"Emotion-Infused Models for Explainable Psychological Stress Detection\",\n    author = \"Turcan, Elsbeth  and\n      Muresan, Smaranda  and\n      McKeown, Kathleen\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.230/\",\n    doi = \"10.18653/v1/2021.naacl-main.230\",\n    pages = \"2895--2909\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.230.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.230/",
        "pdf_size": 396586,
        "gs_citation": 56,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16294812807664890894&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Department of Computer Science, Columbia University1 + Data Science Institute, Columbia University2; Department of Computer Science, Columbia University1 + Data Science Institute, Columbia University2; Department of Computer Science, Columbia University1",
        "aff_domain": "cs.columbia.edu;cs.columbia.edu;cs.columbia.edu",
        "email": "cs.columbia.edu;cs.columbia.edu;cs.columbia.edu",
        "github": "github.com/eturcan/emotion-infused",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+0;0+0;0",
        "aff_unique_norm": "Columbia University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.columbia.edu",
        "aff_unique_abbr": "Columbia",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.357",
        "title": "Empirical Evaluation of Pre-trained Transformers for Human-Level NLP: The Role of Sample Size and Dimensionality",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In human-level NLP tasks, such as predicting mental health, personality, or demographics, the number of observations is often smaller than the standard 768+ hidden state sizes of each layer within modern transformer-based language models, limiting the ability to effectively leverage transformers. Here, we provide a systematic study on the role of dimension reduction methods (principal components analysis, factorization techniques, or multi-layer auto-encoders) as well as the dimensionality of embedding vectors and sample sizes as a function of predictive performance. We first find that fine-tuning large models with a limited amount of data pose a significant difficulty which can be overcome with a pre-trained dimension reduction regime. RoBERTa consistently achieves top performance in human-level tasks, with PCA giving benefit over other reduction methods in better handling users that write longer texts. Finally, we observe that a majority of the tasks achieve results comparable to the best performance with just 1/12 of the embedding dimensions.",
        "author": "Adithya V Ganesan; Matthew Matero; Aravind Reddy Ravula; Huy Vu; H. Andrew Schwartz",
        "authorids": "/a/adithya-v-ganesan/; /m/matthew-matero/; /a/aravind-reddy-ravula/; /h/huy-vu/; /h/h-andrew-schwartz/",
        "bibtex": "@inproceedings{v-ganesan-etal-2021-empirical,\n    title = \"Empirical Evaluation of Pre-trained Transformers for Human-Level {NLP}: The Role of Sample Size and Dimensionality\",\n    author = \"V Ganesan, Adithya  and\n      Matero, Matthew  and\n      Ravula, Aravind Reddy  and\n      Vu, Huy  and\n      Schwartz, H. Andrew\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.357/\",\n    doi = \"10.18653/v1/2021.naacl-main.357\",\n    pages = \"4515--4532\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.357.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.357/",
        "pdf_size": 5935100,
        "gs_citation": 47,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1718859590257315204&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science, Stony Brook University; Department of Computer Science, Stony Brook University; Department of Computer Science, Stony Brook University; Department of Computer Science, Stony Brook University; Department of Computer Science, Stony Brook University",
        "aff_domain": "cs.stonybrook.edu;cs.stonybrook.edu;cs.stonybrook.edu;cs.stonybrook.edu;cs.stonybrook.edu",
        "email": "cs.stonybrook.edu;cs.stonybrook.edu;cs.stonybrook.edu;cs.stonybrook.edu;cs.stonybrook.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Stony Brook University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.stonybrook.edu",
        "aff_unique_abbr": "SBU",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Stony Brook",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.24",
        "title": "EnSidNet: Enhanced Hybrid Siamese-Deep Network for grouping clinical trials into drug-development pathways",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Siamese Neural Networks have been widely used to perform similarity classification in multi-class settings. Their architecture can be used to group the clinical trials belonging to the same drug-development pathway along the several clinical trial phases. Here we present an approach for the unmet need of drug-development pathway reconstruction, based on an Enhanced hybrid Siamese-Deep Neural Network (EnSidNet). The proposed model demonstrates significant improvement above baselines in a 1-shot evaluation setting and in a classical similarity setting. EnSidNet can be an essential tool in a semi-supervised learning environment: by selecting clinical trials highly likely to belong to the same drug-development pathway it is possible to speed up the labelling process of human experts, allowing the check of a consistent volume of data, further used in the model\u2019s training dataset.",
        "author": "Lucia Pagani",
        "authorids": "/l/lucia-pagani/",
        "bibtex": "@inproceedings{pagani-2021-ensidnet,\n    title = \"{E}n{S}id{N}et: Enhanced Hybrid {S}iamese-Deep Network for grouping clinical trials into drug-development pathways\",\n    author = \"Pagani, Lucia\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.24/\",\n    doi = \"10.18653/v1/2021.naacl-main.24\",\n    pages = \"254--266\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.24.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.24/",
        "pdf_size": 687317,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:IjqZk44XjcIJ:scholar.google.com/&scioq=EnSidNet:+Enhanced+Hybrid+Siamese-Deep+Network+for+grouping+clinical+trials+into+drug-development+pathways&hl=en&as_sdt=0,33",
        "gs_version_total": 2,
        "aff": "Analytics Center of Excellence, IQVIA",
        "aff_domain": "iqvia.com",
        "email": "iqvia.com",
        "github": "",
        "project": "",
        "author_num": 1,
        "aff_unique_index": "0",
        "aff_unique_norm": "IQVIA",
        "aff_unique_dep": "Analytics Center of Excellence",
        "aff_unique_url": "https://www.iqvia.com",
        "aff_unique_abbr": "IQVIA",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.149",
        "title": "End-to-end ASR to jointly predict transcriptions and linguistic annotations",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We propose a Transformer-based sequence-to-sequence model for automatic speech recognition (ASR) capable of simultaneously transcribing and annotating audio with linguistic information such as phonemic transcripts or part-of-speech (POS) tags. Since linguistic information is important in natural language processing (NLP), the proposed ASR is especially useful for speech interface applications, including spoken dialogue systems and speech translation, which combine ASR and NLP. To produce linguistic annotations, we train the ASR system using modified training targets: each grapheme or multi-grapheme unit in the target transcript is followed by an aligned phoneme sequence and/or POS tag. Since our method has access to the underlying audio data, we can estimate linguistic annotations more accurately than pipeline approaches in which NLP-based methods are applied to a hypothesized ASR transcript. Experimental results on Japanese and English datasets show that the proposed ASR system is capable of simultaneously producing high-quality transcriptions and linguistic annotations.",
        "author": "Motoi Omachi; Yuya Fujita; Shinji Watanabe; Matthew Wiesner",
        "authorids": "/m/motoi-omachi/; /y/yuya-fujita/; /s/shinji-watanabe/; /m/matthew-wiesner/",
        "bibtex": "@inproceedings{omachi-etal-2021-end,\n    title = \"End-to-end {ASR} to jointly predict transcriptions and linguistic annotations\",\n    author = \"Omachi, Motoi  and\n      Fujita, Yuya  and\n      Watanabe, Shinji  and\n      Wiesner, Matthew\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.149/\",\n    doi = \"10.18653/v1/2021.naacl-main.149\",\n    pages = \"1861--1871\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.149.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.149/",
        "pdf_size": 592414,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14560185958518955958&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4
    },
    {
        "id": "2021.naacl-main.58",
        "title": "Enhancing Factual Consistency of Abstractive Summarization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Automatic abstractive summaries are found to often distort or fabricate facts in the article. This inconsistency between summary and original text has seriously impacted its applicability. We propose a fact-aware summarization model FASum to extract and integrate factual relations into the summary generation process via graph attention. We then design a factual corrector model FC to automatically correct factual errors from summaries generated by existing systems. Empirical results show that the fact-aware summarization can produce abstractive summaries with higher factual consistency compared with existing systems, and the correction model improves the factual consistency of given summaries via modifying only a few keywords.",
        "author": "Chenguang Zhu; William Hinthorn; Ruochen Xu; Qingkai Zeng; Michael Zeng; Xuedong Huang; Meng Jiang",
        "authorids": "/c/chenguang-zhu/; /w/william-hinthorn/; /r/ruochen-xu/; /q/qingkai-zeng/; /m/michael-zeng/; /x/xuedong-huang/; /m/meng-jiang/",
        "bibtex": "@inproceedings{zhu-etal-2021-enhancing,\n    title = \"Enhancing Factual Consistency of Abstractive Summarization\",\n    author = \"Zhu, Chenguang  and\n      Hinthorn, William  and\n      Xu, Ruochen  and\n      Zeng, Qingkai  and\n      Zeng, Michael  and\n      Huang, Xuedong  and\n      Jiang, Meng\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.58/\",\n    doi = \"10.18653/v1/2021.naacl-main.58\",\n    pages = \"718--733\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.58.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.58/",
        "pdf_size": 495706,
        "gs_citation": 185,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8308401772468719778&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Microsoft Cognitive Services Group; Microsoft Cognitive Services Group; Microsoft Cognitive Services Group; University of Notre Dame; Microsoft Cognitive Services Group; Microsoft Cognitive Services Group; University of Notre Dame",
        "aff_domain": "microsoft.com;microsoft.com;microsoft.com;nd.edu;microsoft.com;microsoft.com;nd.edu",
        "email": "microsoft.com;microsoft.com;microsoft.com;nd.edu;microsoft.com;microsoft.com;nd.edu",
        "github": "https://github.com/zcgzcgzcg1/FASum/",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;1;0;0;1",
        "aff_unique_norm": "Microsoft;University of Notre Dame",
        "aff_unique_dep": "Cognitive Services Group;",
        "aff_unique_url": "https://www.microsoft.com;https://www.nd.edu",
        "aff_unique_abbr": "Microsoft;Notre Dame",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.381",
        "title": "Enriching Transformers with Structured Tensor-Product Representations for Abstractive Summarization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Abstractive summarization, the task of generating a concise summary of input documents, requires: (1) reasoning over the source document to determine the salient pieces of information scattered across the long document, and (2) composing a cohesive text by reconstructing these salient facts into a shorter summary that faithfully reflects the complex relations connecting these facts. In this paper, we adapt TP-Transformer (Schlag et al., 2019), an architecture that enriches the original Transformer (Vaswani et al., 2017) with the explicitly compositional Tensor Product Representation (TPR), for the task of abstractive summarization. The key feature of our model is a structural bias that we introduce by encoding two separate representations for each token to represent the syntactic structure (with role vectors) and semantic content (with filler vectors) separately. The model then binds the role and filler vectors into the TPR as the layer output. We argue that the structured intermediate representations enable the model to take better control of the contents (salient facts) and structures (the syntax that connects the facts) when generating the summary. Empirically, we show that our TP-Transformer outperforms the Transformer and the original TP-Transformer significantly on several abstractive summarization datasets based on both automatic and human evaluations. On several syntactic and semantic probing tasks, we demonstrate the emergent structural information in the role vectors and the performance gain by information specificity of the role vectors and improved syntactic interpretability in the TPR layer outputs.(Code and models are available at https://github.com/jiangycTarheel/TPT-Summ)",
        "author": "Yichen Jiang; Asli Celikyilmaz; Paul Smolensky; Paul Soulos; Sudha Rao; Hamid Palangi; Roland Fernandez; Caitlin Smith; Mohit Bansal; Jianfeng Gao",
        "authorids": "/y/yichen-jiang/; /a/asli-celikyilmaz/; /p/paul-smolensky/; /p/paul-soulos/; /s/sudha-rao/; /h/hamid-palangi/; /r/roland-fernandez/; /c/caitlin-smith/; /m/mohit-bansal/; /j/jianfeng-gao/",
        "bibtex": "@inproceedings{jiang-etal-2021-enriching,\n    title = \"Enriching Transformers with Structured Tensor-Product Representations for Abstractive Summarization\",\n    author = \"Jiang, Yichen  and\n      Celikyilmaz, Asli  and\n      Smolensky, Paul  and\n      Soulos, Paul  and\n      Rao, Sudha  and\n      Palangi, Hamid  and\n      Fernandez, Roland  and\n      Smith, Caitlin  and\n      Bansal, Mohit  and\n      Gao, Jianfeng\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.381/\",\n    doi = \"10.18653/v1/2021.naacl-main.381\",\n    pages = \"4780--4793\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.381.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.381/",
        "pdf_size": 677442,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10806359198239051200&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "UNC Chapel Hill; Microsoft Research, Redmond; Microsoft Research, Redmond + Johns Hopkins University; Johns Hopkins University; Microsoft Research, Redmond; Microsoft Research, Redmond; Microsoft Research, Redmond; Johns Hopkins University; UNC Chapel Hill; Microsoft Research, Redmond",
        "aff_domain": "cs.unc.edu;microsoft.com;microsoft.com;jhu.edu;microsoft.com;microsoft.com;microsoft.com;jhu.edu;cs.unc.edu;microsoft.com",
        "email": "cs.unc.edu;microsoft.com;microsoft.com;jhu.edu;microsoft.com;microsoft.com;microsoft.com;jhu.edu;cs.unc.edu;microsoft.com",
        "github": "https://github.com/jiangycTarheel/TPT-Summ",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0;1;1+2;2;1;1;1;2;0;1",
        "aff_unique_norm": "University of North Carolina at Chapel Hill;Microsoft Research;Johns Hopkins University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.unc.edu;https://www.microsoft.com/en-us/research;https://www.jhu.edu",
        "aff_unique_abbr": "UNC;MSR;JHU",
        "aff_campus_unique_index": "0;1;1;1;1;1;0;1",
        "aff_campus_unique": "Chapel Hill;Redmond;",
        "aff_country_unique_index": "0;0;0+0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.262",
        "title": "Ensemble of MRR and NDCG models for Visual Dialog",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Assessing an AI agent that can converse in human language and understand visual content is challenging. Generation metrics, such as BLEU scores favor correct syntax over semantics. Hence a discriminative approach is often used, where an agent ranks a set of candidate options. The mean reciprocal rank (MRR) metric evaluates the model performance by taking into account the rank of a single human-derived answer. This approach, however, raises a new challenge: the ambiguity and synonymy of answers, for instance, semantic equivalence (e.g., \u2018yeah\u2019 and \u2018yes\u2019). To address this, the normalized discounted cumulative gain (NDCG) metric has been used to capture the relevance of all the correct answers via dense annotations. However, the NDCG metric favors the usually applicable uncertain answers such as \u2018I don\u2019t know.\u2019 Crafting a model that excels on both MRR and NDCG metrics is challenging. Ideally, an AI agent should answer a human-like reply and validate the correctness of any answer. To address this issue, we describe a two-step non-parametric ranking approach that can merge strong MRR and NDCG models. Using our approach, we manage to keep most MRR state-of-the-art performance (70.41% vs. 71.24%) and the NDCG state-of-the-art performance (72.16% vs. 75.35%). Moreover, our approach won the recent Visual Dialog 2020 challenge. Source code is available at https://github.com/idansc/mrr-ndcg.",
        "author": "Idan Schwartz",
        "authorids": "/i/idan-schwartz/",
        "bibtex": "@inproceedings{schwartz-2021-ensemble,\n    title = \"Ensemble of {MRR} and {NDCG} models for Visual Dialog\",\n    author = \"Schwartz, Idan\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.262/\",\n    doi = \"10.18653/v1/2021.naacl-main.262\",\n    pages = \"3272--3363\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.262.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.262/",
        "pdf_size": 13445552,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15011227088973439387&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "https://github.com/idansc/mrr-ndcg",
        "project": "",
        "author_num": 1
    },
    {
        "id": "2021.naacl-industry.4",
        "title": "Entity Resolution in Open-domain Conversations",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "In recent years, incorporating external knowledge for response generation in open-domain conversation systems has attracted great interest. To improve the relevancy of retrieved knowledge, we propose a neural entity linking (NEL) approach. Different from formal documents, such as news, conversational utterances are informal and multi-turn, which makes it more challenging to disambiguate the entities. Therefore, we present a context-aware named entity recognition model (NER) and entity resolution (ER) model to utilize dialogue context information. We conduct NEL experiments on three open-domain conversation datasets and validate that incorporating context information improves the performance of NER and ER models. The end-to-end NEL approach outperforms the baseline by 62.8% relatively in F1 metric. Furthermore, we verify that using external knowledge based on NEL benefits the neural response generation model.",
        "author": "Mingyue Shang; Tong Wang; Mihail Eric; Jiangning Chen; Jiyang Wang; Matthew Welch; Tiantong Deng; Akshay Grewal; Han Wang; Yue Liu; Yang Liu; Dilek Hakkani-Tur",
        "authorids": "/m/mingyue-shang/; /t/tong-wang/; /m/mihail-eric/; /j/jiangning-chen/; /j/jiyang-wang/; /m/matthew-welch/; /t/tiantong-deng/; /a/akshay-grewal/; /h/han-wang/; /y/yue-liu/; /y/yang-liu-icsi/; /d/dilek-hakkani-tur/",
        "bibtex": "@inproceedings{shang-etal-2021-entity,\n    title = \"Entity Resolution in Open-domain Conversations\",\n    author = \"Shang, Mingyue  and\n      Wang, Tong  and\n      Eric, Mihail  and\n      Chen, Jiangning  and\n      Wang, Jiyang  and\n      Welch, Matthew  and\n      Deng, Tiantong  and\n      Grewal, Akshay  and\n      Wang, Han  and\n      Liu, Yue  and\n      Liu, Yang  and\n      Hakkani-Tur, Dilek\",\n    editor = \"Kim, Young-bum  and\n      Li, Yunyao  and\n      Rambow, Owen\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-industry.4/\",\n    doi = \"10.18653/v1/2021.naacl-industry.4\",\n    pages = \"26--33\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-industry.4.pdf",
        "site": "https://aclanthology.org/2021.naacl-industry.4/",
        "pdf_size": 432684,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9611405023306197923&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": ";;;;;;;;;;;",
        "aff_domain": ";;;;;;;;;;;",
        "email": ";;;;;;;;;;;",
        "github": "",
        "project": "",
        "author_num": 12
    },
    {
        "id": "2021.naacl-main.399",
        "title": "Evaluating Saliency Methods for Neural Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Saliency methods are widely used to interpret neural network predictions, but different variants of saliency methods often disagree even on the interpretations of the same prediction made by the same model. In these cases, how do we identify when are these interpretations trustworthy enough to be used in analyses? To address this question, we conduct a comprehensive and quantitative evaluation of saliency methods on a fundamental category of NLP models: neural language models. We evaluate the quality of prediction interpretations from two perspectives that each represents a desirable property of these interpretations: plausibility and faithfulness. Our evaluation is conducted on four different datasets constructed from the existing human annotation of syntactic and semantic agreements, on both sentence-level and document-level. Through our evaluation, we identified various ways saliency methods could yield interpretations of low quality. We recommend that future work deploying such methods to neural language models should carefully validate their interpretations before drawing insights.",
        "author": "Shuoyang Ding; Philipp Koehn",
        "authorids": "/s/shuoyang-ding/; /p/philipp-koehn/",
        "bibtex": "@inproceedings{ding-koehn-2021-evaluating,\n    title = \"Evaluating Saliency Methods for Neural Language Models\",\n    author = \"Ding, Shuoyang  and\n      Koehn, Philipp\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.399/\",\n    doi = \"10.18653/v1/2021.naacl-main.399\",\n    pages = \"5034--5052\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.399.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.399/",
        "pdf_size": 653409,
        "gs_citation": 64,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17487152442546570658&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Center for Language and Speech Processing; Center for Language and Speech Processing",
        "aff_domain": "jhu.edu;jhu.edu",
        "email": "jhu.edu;jhu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Center for Language and Speech Processing",
        "aff_unique_dep": "Department of Language and Speech Processing",
        "aff_unique_url": "",
        "aff_unique_abbr": "CLSP",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.130",
        "title": "Evaluating the Impact of a Hierarchical Discourse Representation on Entity Coreference Resolution Performance",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent work on entity coreference resolution (CR) follows current trends in Deep Learning applied to embeddings and relatively simple task-related features. SOTA models do not make use of hierarchical representations of discourse structure. In this work, we leverage automatically constructed discourse parse trees within a neural approach and demonstrate a significant improvement on two benchmark entity coreference-resolution datasets. We explore how the impact varies depending upon the type of mention.",
        "author": "Sopan Khosla; James Fiacco; Carolyn Ros\u00e9",
        "authorids": "/s/sopan-khosla/; /j/james-fiacco/; /c/carolyn-rose/",
        "bibtex": "@inproceedings{khosla-etal-2021-evaluating,\n    title = \"Evaluating the Impact of a Hierarchical Discourse Representation on Entity Coreference Resolution Performance\",\n    author = \"Khosla, Sopan  and\n      Fiacco, James  and\n      Ros{\\'e}, Carolyn\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.130/\",\n    doi = \"10.18653/v1/2021.naacl-main.130\",\n    pages = \"1645--1651\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.130.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.130/",
        "pdf_size": 379072,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14788678439097135055&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Language Technologies Institute, Carnegie Mellon University, USA; Language Technologies Institute, Carnegie Mellon University, USA; Language Technologies Institute, Carnegie Mellon University, USA",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Language Technologies Institute",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.402",
        "title": "Evaluating the Values of Sources in Transfer Learning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Transfer learning that adapts a model trained on data-rich sources to low-resource targets has been widely applied in natural language processing (NLP). However, when training a transfer model over multiple sources, not every source is equally useful for the target. To better transfer a model, it is essential to understand the values of the sources. In this paper, we develop , an efficient source valuation framework for quantifying the usefulness of the sources (e.g., ) in transfer learning based on the Shapley value method. Experiments and comprehensive analyses on both cross-domain and cross-lingual transfers demonstrate that our framework is not only effective in choosing useful transfer sources but also the source values match the intuitive source-target similarity.",
        "author": "Md Rizwan Parvez; Kai-Wei Chang",
        "authorids": "/m/md-rizwan-parvez/; /k/kai-wei-chang/",
        "bibtex": "@inproceedings{parvez-chang-2021-evaluating,\n    title = \"Evaluating the Values of Sources in Transfer Learning\",\n    author = \"Parvez, Md Rizwan  and\n      Chang, Kai-Wei\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.402/\",\n    doi = \"10.18653/v1/2021.naacl-main.402\",\n    pages = \"5084--5116\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.402.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.402/",
        "pdf_size": 2276429,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2697810526849073738&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "University of California Los Angeles; University of California Los Angeles",
        "aff_domain": "cs.ucla.edu;cs.ucla.edu",
        "email": "cs.ucla.edu;cs.ucla.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Los Angeles",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ucla.edu",
        "aff_unique_abbr": "UCLA",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.374",
        "title": "Event Representation with Sequential, Semi-Supervised Discrete Variables",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Within the context of event modeling and understanding, we propose a new method for neural sequence modeling that takes partially-observed sequences of discrete, external knowledge into account. We construct a sequential neural variational autoencoder, which uses Gumbel-Softmax reparametrization within a carefully defined encoder, to allow for successful backpropagation during training. The core idea is to allow semi-supervised external discrete knowledge to guide, but not restrict, the variational latent parameters during training. Our experiments indicate that our approach not only outperforms multiple baselines and the state-of-the-art in narrative script induction, but also converges more quickly.",
        "author": "Mehdi Rezaee; Francis Ferraro",
        "authorids": "/m/mehdi-rezaee/; /f/francis-ferraro/",
        "bibtex": "@inproceedings{rezaee-ferraro-2021-event,\n    title = \"Event Representation with Sequential, Semi-Supervised Discrete Variables\",\n    author = \"Rezaee, Mehdi  and\n      Ferraro, Francis\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.374/\",\n    doi = \"10.18653/v1/2021.naacl-main.374\",\n    pages = \"4701--4716\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.374.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.374/",
        "pdf_size": 1288850,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1640340478578618306&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science, University of Maryland Baltimore County; Department of Computer Science, University of Maryland Baltimore County",
        "aff_domain": "umbc.edu;umbc.edu",
        "email": "umbc.edu;umbc.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Maryland, Baltimore County",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.umbc.edu",
        "aff_unique_abbr": "UMBC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Baltimore County",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.6",
        "title": "Event Time Extraction and Propagation via Graph Attention Networks",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Grounding events into a precise timeline is important for natural language understanding but has received limited attention in recent work. This problem is challenging due to the inherent ambiguity of language and the requirement for information propagation over inter-related events. This paper first formulates this problem based on a 4-tuple temporal representation used in entity slot filling, which allows us to represent fuzzy time spans more conveniently. We then propose a graph attention network-based approach to propagate temporal information over document-level event graphs constructed by shared entity arguments and temporal relations. To better evaluate our approach, we present a challenging new benchmark on the ACE2005 corpus, where more than 78% of events do not have time spans mentioned explicitly in their local contexts. The proposed approach yields an absolute gain of 7.0% in match rate over contextualized embedding approaches, and 16.3% higher match rate compared to sentence-level manual event time argument annotation.",
        "author": "Haoyang Wen; Yanru Qu; Heng Ji; Qiang Ning; Jiawei Han; Avi Sil; Hanghang Tong; Dan Roth",
        "authorids": "/h/haoyang-wen/; /y/yanru-qu/; /h/heng-ji/; /q/qiang-ning/; /j/jiawei-han/; /a/avirup-sil/; /h/hanghang-tong/; /d/dan-roth/",
        "bibtex": "@inproceedings{wen-etal-2021-event,\n    title = \"Event Time Extraction and Propagation via Graph Attention Networks\",\n    author = \"Wen, Haoyang  and\n      Qu, Yanru  and\n      Ji, Heng  and\n      Ning, Qiang  and\n      Han, Jiawei  and\n      Sil, Avi  and\n      Tong, Hanghang  and\n      Roth, Dan\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.6/\",\n    doi = \"10.18653/v1/2021.naacl-main.6\",\n    pages = \"62--73\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.6.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.6/",
        "pdf_size": 450428,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8241149372991594509&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; Amazon; University of Illinois at Urbana-Champaign; IBM Research AI; University of Illinois at Urbana-Champaign; University of Pennsylvania",
        "aff_domain": "illinois.edu;illinois.edu;illinois.edu;amazon.com;illinois.edu;us.ibm.com;illinois.edu;seas.upenn.edu",
        "email": "illinois.edu;illinois.edu;illinois.edu;amazon.com;illinois.edu;us.ibm.com;illinois.edu;seas.upenn.edu",
        "github": "https://github.com/wenhycs/NAACL2021-Event-Time-Extraction-and-Propagation-via-Graph-Attention-Networks",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;0;1;0;2;0;3",
        "aff_unique_norm": "University of Illinois at Urbana-Champaign;Amazon.com, Inc.;IBM Research;University of Pennsylvania",
        "aff_unique_dep": ";;AI;",
        "aff_unique_url": "https://illinois.edu;https://www.amazon.com;https://www.ibm.com/research;https://www.upenn.edu",
        "aff_unique_abbr": "UIUC;Amazon;IBM;UPenn",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Urbana-Champaign;",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-demos.7",
        "title": "EventPlus: A Temporal Event Understanding Pipeline",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "We present EventPlus, a temporal event understanding pipeline that integrates various state-of-the-art event understanding components including event trigger and type detection, event argument detection, event duration and temporal relation extraction. Event information, especially event temporal knowledge, is a type of common sense knowledge that helps people understand how stories evolve and provides predictive hints for future events. EventPlus as the first comprehensive temporal event understanding pipeline provides a convenient tool for users to quickly obtain annotations about events and their temporal information for any user-provided document. Furthermore, we show EventPlus can be easily adapted to other domains (e.g., biomedical domain). We make EventPlus publicly available to facilitate event-related information extraction and downstream applications.",
        "author": "Mingyu Derek Ma; Jiao Sun; Mu Yang; Kung-Hsiang Huang; Nuan Wen; Shikhar Singh; Rujun Han; Nanyun Peng",
        "authorids": "/m/mingyu-derek-ma/; /j/jiao-sun/; /m/mu-yang/; /k/kung-hsiang-huang/; /n/nuan-wen/; /s/shikhar-singh/; /r/rujun-han/; /n/nanyun-peng/",
        "bibtex": "@inproceedings{ma-etal-2021-eventplus,\n    title = \"{E}vent{P}lus: A Temporal Event Understanding Pipeline\",\n    author = \"Ma, Mingyu Derek  and\n      Sun, Jiao  and\n      Yang, Mu  and\n      Huang, Kung-Hsiang  and\n      Wen, Nuan  and\n      Singh, Shikhar  and\n      Han, Rujun  and\n      Peng, Nanyun\",\n    editor = \"Sil, Avi  and\n      Lin, Xi Victoria\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-demos.7/\",\n    doi = \"10.18653/v1/2021.naacl-demos.7\",\n    pages = \"56--65\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-demos.7.pdf",
        "site": "https://aclanthology.org/2021.naacl-demos.7/",
        "pdf_size": 1473408,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4010223815025152097&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Computer Science Department, University of California, Los Angeles; Information Sciences Institute, University of Southern California; Texas A&M University; Information Sciences Institute, University of Southern California; Information Sciences Institute, University of Southern California; Information Sciences Institute, University of Southern California; Information Sciences Institute, University of Southern California; Computer Science Department, University of California, Los Angeles + Information Sciences Institute, University of Southern California",
        "aff_domain": "cs.ucla.edu;usc.edu;tamu.edu;usc.edu;usc.edu;usc.edu;usc.edu;cs.ucla.edu",
        "email": "cs.ucla.edu;usc.edu;tamu.edu;usc.edu;usc.edu;usc.edu;usc.edu;cs.ucla.edu",
        "github": "https://github.com/PlusLabNLP/EventPlus",
        "project": "https://kairos-event.isi.edu",
        "author_num": 8,
        "aff_unique_index": "0;1;2;1;1;1;1;0+1",
        "aff_unique_norm": "University of California, Los Angeles;University of Southern California;Texas A&M University",
        "aff_unique_dep": "Computer Science Department;Information Sciences Institute;",
        "aff_unique_url": "https://www.ucla.edu;https://www.usc.edu;https://www.tamu.edu",
        "aff_unique_abbr": "UCLA;USC;TAMU",
        "aff_campus_unique_index": "0;0;0;0;0;0;0+0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.155",
        "title": "Everything Has a Cause: Leveraging Causal Inference in Legal Text Analysis",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Causal inference is the process of capturing cause-effect relationship among variables. Most existing works focus on dealing with structured data, while mining causal relationship among factors from unstructured data, like text, has been less examined, but is of great importance, especially in the legal domain. In this paper, we propose a novel Graph-based Causal Inference (GCI) framework, which builds causal graphs from fact descriptions without much human involvement and enables causal inference to facilitate legal practitioners to make proper decisions. We evaluate the framework on a challenging similar charge disambiguation task. Experimental results show that GCI can capture the nuance from fact descriptions among multiple confusing charges and provide explainable discrimination, especially in few-shot settings. We also observe that the causal knowledge contained in GCI can be effectively injected into powerful neural networks for better performance and interpretability.",
        "author": "Xiao Liu; Da Yin; Yansong Feng; Yuting Wu; Dongyan Zhao",
        "authorids": "/x/xiao-liu/; /d/da-yin/; /y/yansong-feng/; /y/yuting-wu/; /d/dongyan-zhao/",
        "bibtex": "@inproceedings{liu-etal-2021-everything,\n    title = \"Everything Has a Cause: Leveraging Causal Inference in Legal Text Analysis\",\n    author = \"Liu, Xiao  and\n      Yin, Da  and\n      Feng, Yansong  and\n      Wu, Yuting  and\n      Zhao, Dongyan\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.155/\",\n    doi = \"10.18653/v1/2021.naacl-main.155\",\n    pages = \"1928--1941\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.155.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.155/",
        "pdf_size": 290795,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3042895581193084936&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Wangxuan Institute of Computer Technology, Peking University, China+The MOE Key Laboratory of Computational Linguistics, Peking University, China; Computer Science Department, University of California, Los Angeles; Wangxuan Institute of Computer Technology, Peking University, China+The MOE Key Laboratory of Computational Linguistics, Peking University, China; Wangxuan Institute of Computer Technology, Peking University, China; Wangxuan Institute of Computer Technology, Peking University, China+The MOE Key Laboratory of Computational Linguistics, Peking University, China",
        "aff_domain": "pku.edu.cn;cs.ucla.edu;pku.edu.cn;pku.edu.cn;pku.edu.cn",
        "email": "pku.edu.cn;cs.ucla.edu;pku.edu.cn;pku.edu.cn;pku.edu.cn",
        "github": "https://github.com/xxxiaol/GCI/",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+0;1;0+0;0;0+0",
        "aff_unique_norm": "Peking University;University of California, Los Angeles",
        "aff_unique_dep": "Wangxuan Institute of Computer Technology;Computer Science Department",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.ucla.edu",
        "aff_unique_abbr": "PKU;UCLA",
        "aff_campus_unique_index": ";1;;",
        "aff_campus_unique": ";Los Angeles",
        "aff_country_unique_index": "0+0;1;0+0;0;0+0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2021.naacl-main.237",
        "title": "Example-Driven Intent Prediction with Observers",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "A key challenge of dialog systems research is to effectively and efficiently adapt to new domains. A scalable paradigm for adaptation necessitates the development of generalizable models that perform well in few-shot settings. In this paper, we focus on the intent classification problem which aims to identify user intents given utterances addressed to the dialog system. We propose two approaches for improving the generalizability of utterance classification models: (1) observers and (2) example-driven training. Prior work has shown that BERT-like models tend to attribute a significant amount of attention to the [CLS] token, which we hypothesize results in diluted representations. Observers are tokens that are not attended to, and are an alternative to the [CLS] token as a semantic representation of utterances. Example-driven training learns to classify utterances by comparing to examples, thereby using the underlying encoder as a sentence similarity model. These methods are complementary; improving the representation through observers allows the example-driven model to better measure sentence similarities. When combined, the proposed methods attain state-of-the-art results on three intent prediction datasets (banking77, clinc150, hwu64) in both the full data and few-shot (10 examples per intent) settings. Furthermore, we demonstrate that the proposed approach can transfer to new intents and across datasets without any additional training.",
        "author": "Shikib Mehri; Mihail Eric",
        "authorids": "/s/shikib-mehri/; /m/mihail-eric/",
        "bibtex": "@inproceedings{mehri-eric-2021-example,\n    title = \"Example-Driven Intent Prediction with Observers\",\n    author = \"Mehri, Shikib  and\n      Eric, Mihail\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.237/\",\n    doi = \"10.18653/v1/2021.naacl-main.237\",\n    pages = \"2979--2992\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.237.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.237/",
        "pdf_size": 631327,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7151554532293552748&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Language Technologies Institute, Carnegie Mellon University; Amazon Alexa AI",
        "aff_domain": "cs.cmu.edu;amazon.com",
        "email": "cs.cmu.edu;amazon.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Carnegie Mellon University;Amazon",
        "aff_unique_dep": "Language Technologies Institute;Alexa AI",
        "aff_unique_url": "https://www.cmu.edu;https://www.amazon.com",
        "aff_unique_abbr": "CMU;Amazon",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Pittsburgh;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.97",
        "title": "Explainable Multi-hop Verbal Reasoning Through Internal Monologue",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Many state-of-the-art (SOTA) language models have achieved high accuracy on several multi-hop reasoning problems. However, these approaches tend to not be interpretable because they do not make the intermediate reasoning steps explicit. Moreover, models trained on simpler tasks tend to fail when directly tested on more complex problems. We propose the Explainable multi-hop Verbal Reasoner (EVR) to solve these limitations by (a) decomposing multi-hop reasoning problems into several simple ones, and (b) using natural language to guide the intermediate reasoning hops. We implement EVR by extending the classic reasoning paradigm General Problem Solver (GPS) with a SOTA generative language model to generate subgoals and perform inference in natural language at each reasoning step. Evaluation of EVR on the RuleTaker synthetic question answering (QA) dataset shows that EVR achieves SOTA performance while being able to generate all reasoning steps in natural language. Furthermore, EVR generalizes better than other strong methods when trained on simpler tasks or less training data (up to 35.7% and 7.7% absolute improvement respectively).",
        "author": "Zhengzhong Liang; Steven Bethard; Mihai Surdeanu",
        "authorids": "/z/zhengzhong-liang/; /s/steven-bethard/; /m/mihai-surdeanu/",
        "bibtex": "@inproceedings{liang-etal-2021-explainable,\n    title = \"Explainable Multi-hop Verbal Reasoning Through Internal Monologue\",\n    author = \"Liang, Zhengzhong  and\n      Bethard, Steven  and\n      Surdeanu, Mihai\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.97/\",\n    doi = \"10.18653/v1/2021.naacl-main.97\",\n    pages = \"1225--1250\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.97.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.97/",
        "pdf_size": 576351,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12004370725979781927&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Computer Science Department, The University of Arizona, Tucson, AZ; School of Information, The University of Arizona, Tucson, AZ; Computer Science Department, The University of Arizona, Tucson, AZ",
        "aff_domain": "email.arizona.edu;email.arizona.edu;email.arizona.edu",
        "email": "email.arizona.edu;email.arizona.edu;email.arizona.edu",
        "github": "https://github.com/clulab/releases/tree/master/naacl2021-evr",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "The University of Arizona",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.arizona.edu",
        "aff_unique_abbr": "UArizona",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Tucson",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.306",
        "title": "Explaining Neural Network Predictions on Sentence Pairs via Learning Word-Group Masks",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Explaining neural network models is important for increasing their trustworthiness in real-world applications. Most existing methods generate post-hoc explanations for neural network models by identifying individual feature attributions or detecting interactions between adjacent features. However, for models with text pairs as inputs (e.g., paraphrase identification), existing methods are not sufficient to capture feature interactions between two texts and their simple extension of computing all word-pair interactions between two texts is computationally inefficient. In this work, we propose the Group Mask (GMASK) method to implicitly detect word correlations by grouping correlated words from the input text pair together and measure their contribution to the corresponding NLP tasks as a whole. The proposed method is evaluated with two different model architectures (decomposable attention model and BERT) across four datasets, including natural language inference and paraphrase identification tasks. Experiments show the effectiveness of GMASK in providing faithful explanations to these models.",
        "author": "Hanjie Chen; Song Feng; Jatin Ganhotra; Hui Wan; Chulaka Gunasekara; Sachindra Joshi; Yangfeng Ji",
        "authorids": "/h/hanjie-chen/; /s/song-feng/; /j/jatin-ganhotra/; /h/hui-wan/; /c/chulaka-gunasekara/; /s/sachindra-joshi/; /y/yangfeng-ji/",
        "bibtex": "@inproceedings{chen-etal-2021-explaining,\n    title = \"Explaining Neural Network Predictions on Sentence Pairs via Learning Word-Group Masks\",\n    author = \"Chen, Hanjie  and\n      Feng, Song  and\n      Ganhotra, Jatin  and\n      Wan, Hui  and\n      Gunasekara, Chulaka  and\n      Joshi, Sachindra  and\n      Ji, Yangfeng\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.306/\",\n    doi = \"10.18653/v1/2021.naacl-main.306\",\n    pages = \"3917--3930\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.306.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.306/",
        "pdf_size": 809240,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6935964815381006705&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science, University of Virginia, Charlottesville, V A, USA; IBM Research AI; IBM Research AI; IBM Research AI; IBM Research AI; IBM Research AI; Department of Computer Science, University of Virginia, Charlottesville, V A, USA",
        "aff_domain": "virginia.edu;us.ibm.com;us.ibm.com;us.ibm.com;ibm.com;in.ibm.com;virginia.edu",
        "email": "virginia.edu;us.ibm.com;us.ibm.com;us.ibm.com;ibm.com;in.ibm.com;virginia.edu",
        "github": "https://github.com/UVa-NLP/GMASK",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;1;1;1;1;0",
        "aff_unique_norm": "University of Virginia;IBM Research",
        "aff_unique_dep": "Department of Computer Science;AI",
        "aff_unique_url": "https://www.virginia.edu;https://www.ibm.com/research",
        "aff_unique_abbr": "UVA;IBM",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Charlottesville;",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.284",
        "title": "Explicit Alignment Objectives for Multilingual Bidirectional Encoders",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Pre-trained cross-lingual encoders such as mBERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) have proven impressively effective at enabling transfer-learning of NLP systems from high-resource languages to low-resource languages. This success comes despite the fact that there is no explicit objective to align the contextual embeddings of words/sentences with similar meanings across languages together in the same space. In this paper, we present a new method for learning multilingual encoders, AMBER (Aligned Multilingual Bidirectional EncodeR). AMBER is trained on additional parallel data using two explicit alignment objectives that align the multilingual representations at different granularities. We conduct experiments on zero-shot cross-lingual transfer learning for different tasks including sequence tagging, sentence retrieval and sentence classification. Experimental results on the tasks in the XTREME benchmark (Hu et al., 2020) show that AMBER obtains gains of up to 1.1 average F1 score on sequence tagging and up to 27.3 average accuracy on retrieval over the XLM-R-large model which has 3.2x the parameters of AMBER. Our code and models are available at http://github.com/junjiehu/amber.",
        "author": "Junjie Hu; Melvin Johnson; Orhan Firat; Aditya Siddhant; Graham Neubig",
        "authorids": "/j/junjie-hu/; /m/melvin-johnson/; /o/orhan-firat/; /a/aditya-siddhant/; /g/graham-neubig/",
        "bibtex": "@inproceedings{hu-etal-2021-explicit,\n    title = \"Explicit Alignment Objectives for Multilingual Bidirectional Encoders\",\n    author = \"Hu, Junjie  and\n      Johnson, Melvin  and\n      Firat, Orhan  and\n      Siddhant, Aditya  and\n      Neubig, Graham\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.284/\",\n    doi = \"10.18653/v1/2021.naacl-main.284\",\n    pages = \"3633--3643\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.284.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.284/",
        "pdf_size": 476970,
        "gs_citation": 50,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10249222096096978749&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Carnegie Mellon University+Google Research; Google Research; Google Research; Google Research; Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;google.com;google.com;google.com;cs.cmu.edu",
        "email": "cs.cmu.edu;google.com;google.com;google.com;cs.cmu.edu",
        "github": "http://github.com/junjiehu/amber",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;1;1;1;0",
        "aff_unique_norm": "Carnegie Mellon University;Google",
        "aff_unique_dep": ";Google Research",
        "aff_unique_url": "https://www.cmu.edu;https://research.google",
        "aff_unique_abbr": "CMU;Google Research",
        "aff_campus_unique_index": "1;1;1;1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.132",
        "title": "Explicitly Modeling Syntax in Language Models with Incremental Parsing and a Dynamic Oracle",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Syntax is fundamental to our thinking about language. Failing to capture the structure of input language could lead to generalization problems and over-parametrization. In the present work, we propose a new syntax-aware language model: Syntactic Ordered Memory (SOM). The model explicitly models the structure with an incremental parser and maintains the conditional probability setting of a standard language model (left-to-right). To train the incremental parser and avoid exposure bias, we also propose a novel dynamic oracle, so that SOM is more robust to wrong parsing decisions. Experiments show that SOM can achieve strong results in language modeling, incremental parsing, and syntactic generalization tests while using fewer parameters than other models.",
        "author": "Yikang Shen; Shawn Tan; Alessandro Sordoni; Siva Reddy; Aaron Courville",
        "authorids": "/y/yikang-shen/; /s/shawn-tan/; /a/alessandro-sordoni/; /s/siva-reddy/; /a/aaron-courville/",
        "bibtex": "@inproceedings{shen-etal-2021-explicitly,\n    title = \"Explicitly Modeling Syntax in Language Models with Incremental Parsing and a Dynamic Oracle\",\n    author = \"Shen, Yikang  and\n      Tan, Shawn  and\n      Sordoni, Alessandro  and\n      Reddy, Siva  and\n      Courville, Aaron\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.132/\",\n    doi = \"10.18653/v1/2021.naacl-main.132\",\n    pages = \"1660--1672\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.132.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.132/",
        "pdf_size": 1059023,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15395701025547881900&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 3,
        "aff": "Mila/Universit\u00e9 de Montr\u00e9al; Mila/Universit\u00e9 de Montr\u00e9al; Microsoft Research Montr\u00e9al; Mila/McGill University; Mila/Universit\u00e9 de Montr\u00e9al",
        "aff_domain": "umontreal.ca;mila.quebec; ; ; ",
        "email": "umontreal.ca;mila.quebec; ; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;2;0",
        "aff_unique_norm": "Universit\u00e9 de Montr\u00e9al;Microsoft Research;McGill University",
        "aff_unique_dep": "Mila;Microsoft Research;Mila",
        "aff_unique_url": "https://www.umontreal.ca;https://www.microsoft.com/en-us/research/group/microsoft-research-montreal;https://www.mcgill.ca",
        "aff_unique_abbr": "UdeM;MSR Montreal;McGill",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Montr\u00e9al;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2021.naacl-main.244",
        "title": "Exploring the Relationship Between Algorithm Performance, Vocabulary, and Run-Time in Text Classification",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Text classification is a significant branch of natural language processing, and has many applications including document classification and sentiment analysis. Unsurprisingly, those who do text classification are concerned with the run-time of their algorithms, many of which depend on the size of the corpus\u2019 vocabulary due to their bag-of-words representation. Although many studies have examined the effect of preprocessing techniques on vocabulary size and accuracy, none have examined how these methods affect a model\u2019s run-time. To fill this gap, we provide a comprehensive study that examines how preprocessing techniques affect the vocabulary size, model performance, and model run-time, evaluating ten techniques over four models and two datasets. We show that some individual methods can reduce run-time with no loss of accuracy, while some combinations of methods can trade 2-5% of the accuracy for up to a 65% reduction of run-time. Furthermore, some combinations of preprocessing techniques can even provide a 15% reduction in run-time while simultaneously improving model accuracy.",
        "author": "Wilson Fearn; Orion Weller; Kevin Seppi",
        "authorids": "/w/wilson-fearn/; /o/orion-weller/; /k/kevin-seppi/",
        "bibtex": "@inproceedings{fearn-etal-2021-exploring,\n    title = \"Exploring the Relationship Between Algorithm Performance, Vocabulary, and Run-Time in Text Classification\",\n    author = \"Fearn, Wilson  and\n      Weller, Orion  and\n      Seppi, Kevin\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.244/\",\n    doi = \"10.18653/v1/2021.naacl-main.244\",\n    pages = \"3069--3082\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.244.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.244/",
        "pdf_size": 285553,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:nACDlMVPk3IJ:scholar.google.com/&scioq=Exploring+the+Relationship+Between+Algorithm+Performance,+Vocabulary,+and+Run-Time+in+Text+Classification&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "Brigham Young University; Brigham Young University; Brigham Young University",
        "aff_domain": "gmail.com;byu.edu;byu.edu",
        "email": "gmail.com;byu.edu;byu.edu",
        "github": "https://github.com/wfearn/preprocessing-paper",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Brigham Young University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.byu.edu",
        "aff_unique_abbr": "BYU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.54",
        "title": "Extending Multi-Document Summarization Evaluation to the Interactive Setting",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Allowing users to interact with multi-document summarizers is a promising direction towards improving and customizing summary results. Different ideas for interactive summarization have been proposed in previous work but these solutions are highly divergent and incomparable. In this paper, we develop an end-to-end evaluation framework for interactive summarization, focusing on expansion-based interaction, which considers the accumulating information along a user session. Our framework includes a procedure of collecting real user sessions, as well as evaluation measures relying on summarization standards, but adapted to reflect interaction. All of our solutions and resources are available publicly as a benchmark, allowing comparison of future developments in interactive summarization, and spurring progress in its methodological evaluation. We demonstrate the use of our framework by evaluating and comparing baseline implementations that we developed for this purpose, which will serve as part of our benchmark. Our extensive experimentation and analysis motivate the proposed evaluation framework design and support its viability.",
        "author": "Ori Shapira; Ramakanth Pasunuru; Hadar Ronen; Mohit Bansal; Yael Amsterdamer; Ido Dagan",
        "authorids": "/o/ori-shapira/; /r/ramakanth-pasunuru/; /h/hadar-ronen/; /m/mohit-bansal/; /y/yael-amsterdamer/; /i/ido-dagan/",
        "bibtex": "@inproceedings{shapira-etal-2021-extending,\n    title = \"Extending Multi-Document Summarization Evaluation to the Interactive Setting\",\n    author = \"Shapira, Ori  and\n      Pasunuru, Ramakanth  and\n      Ronen, Hadar  and\n      Bansal, Mohit  and\n      Amsterdamer, Yael  and\n      Dagan, Ido\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.54/\",\n    doi = \"10.18653/v1/2021.naacl-main.54\",\n    pages = \"657--677\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.54.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.54/",
        "pdf_size": 563592,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4182629354939746220&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Bar-Ilan University; UNC Chapel Hill; Peres Academic Center; UNC Chapel Hill; Bar-Ilan University; Bar-Ilan University",
        "aff_domain": "gmail.com;cs.unc.edu;gmail.com;cs.unc.edu;cs.biu.ac.il;cs.biu.ac.il",
        "email": "gmail.com;cs.unc.edu;gmail.com;cs.unc.edu;cs.biu.ac.il;cs.biu.ac.il",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;2;1;0;0",
        "aff_unique_norm": "Bar-Ilan University;University of North Carolina at Chapel Hill;Peres Academic Center",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.biu.ac.il;https://www.unc.edu;",
        "aff_unique_abbr": "BIU;UNC;",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Chapel Hill",
        "aff_country_unique_index": "0;1;0;1;0;0",
        "aff_country_unique": "Israel;United States"
    },
    {
        "id": "2021.naacl-main.355",
        "title": "Extracting a Knowledge Base of Mechanisms from COVID-19 Papers",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The COVID-19 pandemic has spawned a diverse body of scientific literature that is challenging to navigate, stimulating interest in automated tools to help find useful knowledge. We pursue the construction of a knowledge base (KB) of mechanisms\u2014a fundamental concept across the sciences, which encompasses activities, functions and causal relations, ranging from cellular processes to economic impacts. We extract this information from the natural language of scientific papers by developing a broad, unified schema that strikes a balance between relevance and breadth. We annotate a dataset of mechanisms with our schema and train a model to extract mechanism relations from papers. Our experiments demonstrate the utility of our KB in supporting interdisciplinary scientific search over COVID-19 literature, outperforming the prominent PubMed search in a study with clinical experts. Our search engine, dataset and code are publicly available.",
        "author": "Tom Hope; Aida Amini; David Wadden; Madeleine van Zuylen; Sravanthi Parasa; Eric Horvitz; Daniel Weld; Roy Schwartz; Hannaneh Hajishirzi",
        "authorids": "/t/tom-hope/; /a/aida-amini/; /d/david-wadden/; /m/madeleine-van-zuylen/; /s/sravanthi-parasa/; /e/eric-horvitz/; /d/daniel-s-weld/; /r/roy-schwartz/; /h/hannaneh-hajishirzi/",
        "bibtex": "@inproceedings{hope-etal-2021-extracting,\n    title = \"Extracting a Knowledge Base of Mechanisms from {COVID}-19 Papers\",\n    author = \"Hope, Tom  and\n      Amini, Aida  and\n      Wadden, David  and\n      van Zuylen, Madeleine  and\n      Parasa, Sravanthi  and\n      Horvitz, Eric  and\n      Weld, Daniel  and\n      Schwartz, Roy  and\n      Hajishirzi, Hannaneh\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.355/\",\n    doi = \"10.18653/v1/2021.naacl-main.355\",\n    pages = \"4489--4503\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.355.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.355/",
        "pdf_size": 1799466,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12671731348701799372&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Paul G. Allen School for Computer Science & Engineering, University of Washington+Allen Institute for Arti\ufb01cial Intelligence; Paul G. Allen School for Computer Science & Engineering, University of Washington+Allen Institute for Arti\ufb01cial Intelligence; Paul G. Allen School for Computer Science & Engineering, University of Washington; Allen Institute for Arti\ufb01cial Intelligence; Swedish Medical Group; Microsoft Research; Paul G. Allen School for Computer Science & Engineering, University of Washington+Allen Institute for Arti\ufb01cial Intelligence; The Hebrew University of Jerusalem, Israel; Paul G. Allen School for Computer Science & Engineering, University of Washington+Allen Institute for Arti\ufb01cial Intelligence",
        "aff_domain": "allenai.org;allenai.org;allenai.org; ; ; ;allenai.org; ;allenai.org",
        "email": "allenai.org;allenai.org;allenai.org; ; ; ;allenai.org; ;allenai.org",
        "github": "",
        "project": "https://covidmechanisms.apps.allenai.org/",
        "author_num": 9,
        "aff_unique_index": "0+1;0+1;0;1;2;3;0+1;4;0+1",
        "aff_unique_norm": "University of Washington;Allen Institute for Artificial Intelligence;Swedish Medical Group;Microsoft Corporation;The Hebrew University of Jerusalem",
        "aff_unique_dep": "Paul G. Allen School for Computer Science & Engineering;Artificial Intelligence;;Microsoft Research;",
        "aff_unique_url": "https://www.cs.washington.edu;https://allenai.org;;https://www.microsoft.com/en-us/research;https://www.huji.ac.il",
        "aff_unique_abbr": "UW;AI2;;MSR;HUJI",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Seattle;",
        "aff_country_unique_index": "0+0;0+0;0;0;1;0;0+0;2;0+0",
        "aff_country_unique": "United States;Sweden;Israel"
    },
    {
        "id": "2021.naacl-demos.5",
        "title": "FITAnnotator: A Flexible and Intelligent Text Annotation System",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "In this paper, we introduce FITAnnotator, a generic web-based tool for efficient text annotation. Benefiting from the fully modular architecture design, FITAnnotator provides a systematic solution for the annotation of a variety of natural language processing tasks, including classification, sequence tagging and semantic role annotation, regardless of the language. Three kinds of interfaces are developed to annotate instances, evaluate annotation quality and manage the annotation task for annotators, reviewers and managers, respectively. FITAnnotator also gives intelligent annotations by introducing task-specific assistant to support and guide the annotators based on active learning and incremental learning strategies. This assistant is able to effectively update from the annotator feedbacks and easily handle the incremental labeling scenarios.",
        "author": "Yanzeng Li; Bowen Yu; Li Quangang; Tingwen Liu",
        "authorids": "/y/yanzeng-li/; /b/bowen-yu/; /l/li-quangang/; /t/tingwen-liu/",
        "bibtex": "@inproceedings{li-etal-2021-fitannotator,\n    title = \"{FITA}nnotator: A Flexible and Intelligent Text Annotation System\",\n    author = \"Li, Yanzeng  and\n      Yu, Bowen  and\n      Quangang, Li  and\n      Liu, Tingwen\",\n    editor = \"Sil, Avi  and\n      Lin, Xi Victoria\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-demos.5/\",\n    doi = \"10.18653/v1/2021.naacl-demos.5\",\n    pages = \"35--41\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-demos.5.pdf",
        "site": "https://aclanthology.org/2021.naacl-demos.5/",
        "pdf_size": 1148833,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3897086372769358066&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Institute of Information Engineering, Chinese Academy of Sciences; School of Cyber Security, University of Chinese Academy of Sciences; Institute of Information Engineering, Chinese Academy of Sciences; School of Cyber Security, University of Chinese Academy of Sciences",
        "aff_domain": "iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn",
        "email": "iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn",
        "github": "",
        "project": "https://vimeo.com/499446008",
        "author_num": 4,
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Information Engineering;School of Cyber Security",
        "aff_unique_url": "http://www.cas.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "CAS;UCAS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2021.naacl-main.222",
        "title": "FLIN: A Flexible Natural Language Interface for Web Navigation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "AI assistants can now carry out tasks for users by directly interacting with website UIs. Current semantic parsing and slot-filling techniques cannot flexibly adapt to many different websites without being constantly re-trained. We propose FLIN, a natural language interface for web navigation that maps user commands to concept-level actions (rather than low-level UI actions), thus being able to flexibly adapt to different websites and handle their transient nature. We frame this as a ranking problem: given a user command and a webpage, FLIN learns to score the most relevant navigation instruction (involving action and parameter values). To train and evaluate FLIN, we collect a dataset using nine popular websites from three domains. Our results show that FLIN was able to adapt to new websites in a given domain.",
        "author": "Sahisnu Mazumder; Oriana Riva",
        "authorids": "/s/sahisnu-mazumder/; /o/oriana-riva/",
        "bibtex": "@inproceedings{mazumder-riva-2021-flin,\n    title = \"{FLIN}: A Flexible Natural Language Interface for Web Navigation\",\n    author = \"Mazumder, Sahisnu  and\n      Riva, Oriana\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.222/\",\n    doi = \"10.18653/v1/2021.naacl-main.222\",\n    pages = \"2777--2788\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.222.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.222/",
        "pdf_size": 1510384,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8048275031501669695&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science, University of Illinois at Chicago, USA; Microsoft Research, Redmond, USA",
        "aff_domain": "gmail.com;microsoft.com",
        "email": "gmail.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Illinois at Chicago;Microsoft Research",
        "aff_unique_dep": "Department of Computer Science;",
        "aff_unique_url": "https://www.uic.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "UIC;MSR",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Chicago;Redmond",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.276",
        "title": "FUDGE: Controlled Text Generation With Future Discriminators",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We propose Future Discriminators for Generation (FUDGE), a flexible and modular method for controlled text generation. Given a pre-existing model G for generating text from a distribution of interest, FUDGE enables conditioning on a desired attribute a (for example, formality) while requiring access only to G\u2019s output logits. FUDGE learns an attribute predictor operating on a partial sequence, and uses this predictor\u2019s outputs to adjust G\u2019s original probabilities. We show that FUDGE models terms corresponding to a Bayesian decomposition of the conditional distribution of G given attribute a. Moreover, FUDGE can easily compose predictors for multiple desired attributes. We evaluate FUDGE on three tasks \u2014 couplet completion in poetry, topic control in language generation, and formality change in machine translation \u2014 and observe gains in all three tasks.",
        "author": "Kevin Yang; Dan Klein",
        "authorids": "/k/kevin-yang/; /d/dan-klein/",
        "bibtex": "@inproceedings{yang-klein-2021-fudge,\n    title = \"{FUDGE}: Controlled Text Generation With Future Discriminators\",\n    author = \"Yang, Kevin  and\n      Klein, Dan\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.276/\",\n    doi = \"10.18653/v1/2021.naacl-main.276\",\n    pages = \"3511--3535\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.276.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.276/",
        "pdf_size": 631912,
        "gs_citation": 342,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10827445999786051497&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "UC Berkeley; UC Berkeley",
        "aff_domain": "berkeley.edu;berkeley.edu",
        "email": "berkeley.edu;berkeley.edu",
        "github": "https://github.com/yangkevin2/naacl-2021-fudge-controlled-generation",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.398",
        "title": "Factual Probing Is [MASK]: Learning vs. Learning to Recall",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Petroni et al. (2019) demonstrated that it is possible to retrieve world facts from a pre-trained language model by expressing them as cloze-style prompts and interpret the model\u2019s prediction accuracy as a lower bound on the amount of factual information it encodes. Subsequent work has attempted to tighten the estimate by searching for better prompts, using a disjoint set of facts as training data. In this work, we make two complementary contributions to better understand these factual probing techniques. First, we propose OptiPrompt, a novel and efficient method which directly optimizes in continuous embedding space. We find this simple method is able to predict an additional 6.4% of facts in the LAMA benchmark. Second, we raise a more important question: Can we really interpret these probing results as a lower bound? Is it possible that these prompt-search methods learn from the training data too? We find, somewhat surprisingly, that the training data used by these methods contains certain regularities of the underlying fact distribution, and all the existing prompt methods, including ours, are able to exploit them for better fact prediction. We conduct a set of control experiments to disentangle \u201clearning\u201d from \u201clearning to recall\u201d, providing a more detailed picture of what different prompts can reveal about pre-trained language models.",
        "author": "Zexuan Zhong; Dan Friedman; Danqi Chen",
        "authorids": "/z/zexuan-zhong/; /d/dan-friedman/; /d/danqi-chen/",
        "bibtex": "@inproceedings{zhong-etal-2021-factual,\n    title = \"Factual Probing Is [{MASK}]: Learning vs. Learning to Recall\",\n    author = \"Zhong, Zexuan  and\n      Friedman, Dan  and\n      Chen, Danqi\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.398/\",\n    doi = \"10.18653/v1/2021.naacl-main.398\",\n    pages = \"5017--5033\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.398.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.398/",
        "pdf_size": 880559,
        "gs_citation": 427,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7642081788035823041&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science, Princeton University; Department of Computer Science, Princeton University; Department of Computer Science, Princeton University",
        "aff_domain": "cs.princeton.edu;cs.princeton.edu;cs.princeton.edu",
        "email": "cs.princeton.edu;cs.princeton.edu;cs.princeton.edu",
        "github": "https://github.com/princeton-nlp/OptiPrompt",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Princeton University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.princeton.edu",
        "aff_unique_abbr": "Princeton",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.245",
        "title": "Faithfully Explainable Recommendation via Neural Logic Reasoning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Knowledge graphs (KG) have become increasingly important to endow modern recommender systems with the ability to generate traceable reasoning paths to explain the recommendation process. However, prior research rarely considers the faithfulness of the derived explanations to justify the decision-making process. To the best of our knowledge, this is the first work that models and evaluates faithfully explainable recommendation under the framework of KG reasoning. Specifically, we propose neural logic reasoning for explainable recommendation (LOGER) by drawing on interpretable logical rules to guide the path-reasoning process for explanation generation. We experiment on three large-scale datasets in the e-commerce domain, demonstrating the effectiveness of our method in delivering high-quality recommendations as well as ascertaining the faithfulness of the derived explanation.",
        "author": "Yaxin Zhu; Yikun Xian; Zuohui Fu; Gerard de Melo; Yongfeng Zhang",
        "authorids": "/y/yaxin-zhu/; /y/yikun-xian/; /z/zuohui-fu/; /g/gerard-de-melo/; /y/yongfeng-zhang/",
        "bibtex": "@inproceedings{zhu-etal-2021-faithfully,\n    title = \"Faithfully Explainable Recommendation via Neural Logic Reasoning\",\n    author = \"Zhu, Yaxin  and\n      Xian, Yikun  and\n      Fu, Zuohui  and\n      de Melo, Gerard  and\n      Zhang, Yongfeng\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.245/\",\n    doi = \"10.18653/v1/2021.naacl-main.245\",\n    pages = \"3083--3090\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.245.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.245/",
        "pdf_size": 566293,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7315790023796964651&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Rutgers University, New Brunswick; Rutgers University, New Brunswick; Rutgers University, New Brunswick; Rutgers University, New Brunswick; Rutgers University, New Brunswick",
        "aff_domain": "rutgers.edu;rutgers.edu;rutgers.edu;rutgers.edu;rutgers.edu",
        "email": "rutgers.edu;rutgers.edu;rutgers.edu;rutgers.edu;rutgers.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Rutgers University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.rutgers.edu",
        "aff_unique_abbr": "Rutgers",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "New Brunswick",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.27",
        "title": "Fast and Scalable Dialogue State Tracking with Explicit Modular Decomposition",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We present a fast and scalable architecture called Explicit Modular Decomposition (EMD), in which we incorporate both classification-based and extraction-based methods and design four modules (for clas- sification and sequence labelling) to jointly extract dialogue states. Experimental results based on the MultiWoz 2.0 dataset validates the superiority of our proposed model in terms of both complexity and scalability when compared to the state-of-the-art methods, especially in the scenario of multi-domain dialogues entangled with many turns of utterances.",
        "author": "Dingmin Wang; Chenghua Lin; Qi Liu; Kam-Fai Wong",
        "authorids": "/d/dingmin-wang/; /c/chenghua-lin/; /q/qi-liu/; /k/kam-fai-wong/",
        "bibtex": "@inproceedings{wang-etal-2021-fast,\n    title = \"Fast and Scalable Dialogue State Tracking with Explicit Modular Decomposition\",\n    author = \"Wang, Dingmin  and\n      Lin, Chenghua  and\n      Liu, Qi  and\n      Wong, Kam-Fai\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.27/\",\n    doi = \"10.18653/v1/2021.naacl-main.27\",\n    pages = \"289--295\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.27.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.27/",
        "pdf_size": 464882,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12621057539355856890&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science, University of Oxford, UK; Department of Computer Science, The University of Shef\ufb01eld, UK; Department of Computer Science, University of Oxford, UK; The Chinese University of Hong Kong, Hong Kong SAR",
        "aff_domain": "cs.ox.ac.uk;shef.ac.uk;cs.ox.ac.uk;se.cuhk.edu.hk",
        "email": "cs.ox.ac.uk;shef.ac.uk;cs.ox.ac.uk;se.cuhk.edu.hk",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;2",
        "aff_unique_norm": "University of Oxford;The University of Sheffield;The Chinese University of Hong Kong",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science;",
        "aff_unique_url": "https://www.ox.ac.uk;https://www.sheffield.ac.uk;https://www.cuhk.edu.hk",
        "aff_unique_abbr": "Oxford;Sheffield;CUHK",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "United Kingdom;China"
    },
    {
        "id": "2021.naacl-main.434",
        "title": "Few-Shot Text Classification with Triplet Networks, Data Augmentation, and Curriculum Learning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Few-shot text classification is a fundamental NLP task in which a model aims to classify text into a large number of categories, given only a few training examples per category. This paper explores data augmentation\u2014a technique particularly suitable for training with limited data\u2014for this few-shot, highly-multiclass text classification setting. On four diverse text classification tasks, we find that common data augmentation techniques can improve the performance of triplet networks by up to 3.0% on average. To further boost performance, we present a simple training strategy called curriculum data augmentation, which leverages curriculum learning by first training on only original examples and then introducing augmented data as training progresses. We explore a two-stage and a gradual schedule, and find that, compared with standard single-stage training, curriculum data augmentation trains faster, improves performance, and remains robust to high amounts of noising from augmentation.",
        "author": "Jason Wei; Chengyu Huang; Soroush Vosoughi; Yu Cheng; Shiqi Xu",
        "authorids": "/j/jason-wei/; /c/chengyu-huang/; /s/soroush-vosoughi/; /y/yu-cheng/; /s/shiqi-xu/",
        "bibtex": "@inproceedings{wei-etal-2021-shot,\n    title = \"Few-Shot Text Classification with Triplet Networks, Data Augmentation, and Curriculum Learning\",\n    author = \"Wei, Jason  and\n      Huang, Chengyu  and\n      Vosoughi, Soroush  and\n      Cheng, Yu  and\n      Xu, Shiqi\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.434/\",\n    doi = \"10.18653/v1/2021.naacl-main.434\",\n    pages = \"5493--5500\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.434.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.434/",
        "pdf_size": 890135,
        "gs_citation": 69,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10149515634354772501&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5
    },
    {
        "id": "2021.naacl-main.59",
        "title": "Few-shot Intent Classification and Slot Filling with Retrieved Examples",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Few-shot learning arises in important practical scenarios, such as when a natural language understanding system needs to learn new semantic labels for an emerging, resource-scarce domain. In this paper, we explore retrieval-based methods for intent classification and slot filling tasks in few-shot settings. Retrieval-based methods make predictions based on labeled examples in the retrieval index that are similar to the input, and thus can adapt to new domains simply by changing the index without having to retrain the model. However, it is non-trivial to apply such methods on tasks with a complex label space like slot filling. To this end, we propose a span-level retrieval method that learns similar contextualized representations for spans with the same label via a novel batch-softmax objective. At inference time, we use the labels of the retrieved spans to construct the final structure with the highest aggregated score. Our method outperforms previous systems in various few-shot settings on the CLINC and SNIPS benchmarks.",
        "author": "Dian Yu; Luheng He; Yuan Zhang; Xinya Du; Panupong Pasupat; Qi Li",
        "authorids": "/d/dian-yu/; /l/luheng-he/; /y/yuan-zhang/; /x/xinya-du/; /p/panupong-pasupat/; /q/qi-li/",
        "bibtex": "@inproceedings{yu-etal-2021-shot,\n    title = \"Few-shot Intent Classification and Slot Filling with Retrieved Examples\",\n    author = \"Yu, Dian  and\n      He, Luheng  and\n      Zhang, Yuan  and\n      Du, Xinya  and\n      Pasupat, Panupong  and\n      Li, Qi\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.59/\",\n    doi = \"10.18653/v1/2021.naacl-main.59\",\n    pages = \"734--749\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.59.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.59/",
        "pdf_size": 453730,
        "gs_citation": 60,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4256841659698156532&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "University of California, Davis; Google Research; Google Research; Cornell University + Google Research; Google Research; Google Assistant",
        "aff_domain": "ucdavis.edu;google.com;google.com;cornell.edu;google.com;google.com",
        "email": "ucdavis.edu;google.com;google.com;cornell.edu;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;1;2+1;1;1",
        "aff_unique_norm": "University of California, Davis;Google;Cornell University",
        "aff_unique_dep": ";Google Research;",
        "aff_unique_url": "https://www.ucdavis.edu;https://research.google;https://www.cornell.edu",
        "aff_unique_abbr": "UC Davis;Google Research;Cornell",
        "aff_campus_unique_index": "0;1;1;1;1;1",
        "aff_campus_unique": "Davis;Mountain View;",
        "aff_country_unique_index": "0;0;0;0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.140",
        "title": "Field Embedding: A Unified Grain-Based Framework for Word Representation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Word representations empowered with additional linguistic information have been widely studied and proved to outperform traditional embeddings. Current methods mainly focus on learning embeddings for words while embeddings of linguistic information (referred to as grain embeddings) are discarded after the learning. This work proposes a framework field embedding to jointly learn both word and grain embeddings by incorporating morphological, phonetic, and syntactical linguistic fields. The framework leverages an innovative fine-grained pipeline that integrates multiple linguistic fields and produces high-quality grain sequences for learning supreme word representations. A novel algorithm is also designed to learn embeddings for words and grains by capturing information that is contained within each field and that is shared across them. Experimental results of lexical tasks and downstream natural language processing tasks illustrate that our framework can learn better word embeddings and grain embeddings. Qualitative evaluations show grain embeddings effectively capture the semantic information.",
        "author": "Junjie Luo; Xi Chen; Jichao Sun; Yuejia Xiang; Ningyu Zhang; Xiang Wan",
        "authorids": "/j/junjie-luo/; /x/xi-chen/; /j/jichao-sun/; /y/yuejia-xiang/; /n/ningyu-zhang/; /x/xiang-wan/",
        "bibtex": "@inproceedings{luo-etal-2021-field,\n    title = \"Field Embedding: A Unified Grain-Based Framework for Word Representation\",\n    author = \"Luo, Junjie  and\n      Chen, Xi  and\n      Sun, Jichao  and\n      Xiang, Yuejia  and\n      Zhang, Ningyu  and\n      Wan, Xiang\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.140/\",\n    doi = \"10.18653/v1/2021.naacl-main.140\",\n    pages = \"1754--1762\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.140.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.140/",
        "pdf_size": 1172444,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6703348532171035482&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Department of Computer Science, Tsinghua University; Department of Computer Science, University of Illinois Urbana-Champaign; Department of Computer Science, University of Illinois Urbana-Champaign; Department of Computer Science, Tsinghua University; School of Informatics, Computing, and Engineering, Indiana University Bloomington; Department of Computer Science, Wayne State University",
        "aff_domain": "tsinghua.edu.cn;tsinghua.edu.cn;illinois.edu;tsinghua.edu.cn;indiana.edu;wayne.edu",
        "email": "tsinghua.edu.cn;tsinghua.edu.cn;illinois.edu;tsinghua.edu.cn;indiana.edu;wayne.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;1;0;2;3",
        "aff_unique_norm": "Tsinghua University;University of Illinois Urbana-Champaign;Indiana University Bloomington;Wayne State University",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science;School of Informatics, Computing, and Engineering;Department of Computer Science",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://illinois.edu;https://www.iu.edu;https://wayne.edu",
        "aff_unique_abbr": "THU;UIUC;IU;WSU",
        "aff_campus_unique_index": "1;1;2",
        "aff_campus_unique": ";Urbana-Champaign;Bloomington",
        "aff_country_unique_index": "0;1;1;0;1;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2021.naacl-main.349",
        "title": "Finding Concept-specific Biases in Form\u2013Meaning Associations",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "This work presents an information-theoretic operationalisation of cross-linguistic non-arbitrariness. It is not a new idea that there are small, cross-linguistic associations between the forms and meanings of words. For instance, it has been claimed (Blasi et al., 2016) that the word for \u201ctongue\u201d is more likely than chance to contain the phone [l]. By controlling for the influence of language family and geographic proximity within a very large concept-aligned, cross-lingual lexicon, we extend methods previously used to detect within language non-arbitrariness (Pimentel et al., 2019) to measure cross-linguistic associations. We find that there is a significant effect of non-arbitrariness, but it is unsurprisingly small (less than 0.5% on average according to our information-theoretic estimate). We also provide a concept-level analysis which shows that a quarter of the concepts considered in our work exhibit a significant level of cross-linguistic non-arbitrariness. In sum, the paper provides new methods to detect cross-linguistic associations at scale, and confirms their effects are minor.",
        "author": "Tiago Pimentel; Brian Roark; S\u00f8ren Wichmann; Ryan Cotterell; Dami\u00e1n Blasi",
        "authorids": "/t/tiago-pimentel/; /b/brian-roark/; /s/soren-wichmann/; /r/ryan-cotterell/; /d/damian-blasi/",
        "bibtex": "@inproceedings{pimentel-etal-2021-finding,\n    title = \"Finding Concept-specific Biases in Form{--}Meaning Associations\",\n    author = \"Pimentel, Tiago  and\n      Roark, Brian  and\n      Wichmann, S{\\o}ren  and\n      Cotterell, Ryan  and\n      Blasi, Dami{\\'a}n\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.349/\",\n    doi = \"10.18653/v1/2021.naacl-main.349\",\n    pages = \"4416--4425\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.349.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.349/",
        "pdf_size": 1895465,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9682050525891960269&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "University of Cambridge; Google; Kazan Federal University; ETH Z\u00fcrich; Harvard University + MPI for Evolutionary Anthropology + HSE University",
        "aff_domain": "cam.ac.uk;google.com;gmail.com;inf.ethz.ch;fas.harvard.edu",
        "email": "cam.ac.uk;google.com;gmail.com;inf.ethz.ch;fas.harvard.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;2;3;4+5+6",
        "aff_unique_norm": "University of Cambridge;Google;Kazan Federal University;ETH Z\u00fcrich;Harvard University;Max Planck Institute for Evolutionary Anthropology;Higher School of Economics",
        "aff_unique_dep": ";;;;;Evolutionary Anthropology;",
        "aff_unique_url": "https://www.cam.ac.uk;https://www.google.com;http://kpfu.ru;https://www.ethz.ch;https://www.harvard.edu;https://www.eva.mpg.de;https://hse.ru",
        "aff_unique_abbr": "Cambridge;Google;KFU;ETHZ;Harvard;MPI-EVA;HSE",
        "aff_campus_unique_index": "0;1;",
        "aff_campus_unique": "Cambridge;Mountain View;",
        "aff_country_unique_index": "0;1;2;3;1+4+2",
        "aff_country_unique": "United Kingdom;United States;Russia;Switzerland;Germany"
    },
    {
        "id": "2021.naacl-main.84",
        "title": "Fine-Tuning Pre-trained Language Model with Weak Supervision: A Contrastive-Regularized Self-Training Approach",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Fine-tuned pre-trained language models (LMs) have achieved enormous success in many natural language processing (NLP) tasks, but they still require excessive labeled data in the fine-tuning stage. We study the problem of fine-tuning pre-trained LMs using only weak supervision, without any labeled data. This problem is challenging because the high capacity of LMs makes them prone to overfitting the noisy labels generated by weak supervision. To address this problem, we develop a contrastive self-training framework, COSINE, to enable fine-tuning LMs with weak supervision. Underpinned by contrastive regularization and confidence-based reweighting, our framework gradually improves model fitting while effectively suppressing error propagation. Experiments on sequence, token, and sentence pair classification tasks show that our model outperforms the strongest baseline by large margins and achieves competitive performance with fully-supervised fine-tuning methods. Our implementation is available on https://github.com/yueyu1030/COSINE.",
        "author": "Yue Yu; Simiao Zuo; Haoming Jiang; Wendi Ren; Tuo Zhao; Chao Zhang",
        "authorids": "/y/yue-yu/; /s/simiao-zuo/; /h/haoming-jiang/; /w/wendi-ren/; /t/tuo-zhao/; /c/chao-zhang-tu/",
        "bibtex": "@inproceedings{yu-etal-2021-fine,\n    title = \"Fine-Tuning Pre-trained Language Model with Weak Supervision: A Contrastive-Regularized Self-Training Approach\",\n    author = \"Yu, Yue  and\n      Zuo, Simiao  and\n      Jiang, Haoming  and\n      Ren, Wendi  and\n      Zhao, Tuo  and\n      Zhang, Chao\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.84/\",\n    doi = \"10.18653/v1/2021.naacl-main.84\",\n    pages = \"1063--1077\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.84.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.84/",
        "pdf_size": 1927400,
        "gs_citation": 148,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8998549078948832750&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Georgia Institute of Technology, Atlanta, GA, USA; Georgia Institute of Technology, Atlanta, GA, USA; Georgia Institute of Technology, Atlanta, GA, USA; Georgia Institute of Technology, Atlanta, GA, USA; Georgia Institute of Technology, Atlanta, GA, USA; Georgia Institute of Technology, Atlanta, GA, USA",
        "aff_domain": "gatech.edu;gatech.edu;gatech.edu;gatech.edu;gatech.edu;gatech.edu",
        "email": "gatech.edu;gatech.edu;gatech.edu;gatech.edu;gatech.edu;gatech.edu",
        "github": "https://github.com/yueyu1030/COSINE",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Georgia Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.gatech.edu",
        "aff_unique_abbr": "Georgia Tech",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Atlanta",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.122",
        "title": "Fine-grained Post-training for Improving Retrieval-based Dialogue Systems",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Retrieval-based dialogue systems display an outstanding performance when pre-trained language models are used, which includes bidirectional encoder representations from transformers (BERT). During the multi-turn response selection, BERT focuses on training the relationship between the context with multiple utterances and the response. However, this method of training is insufficient when considering the relations between each utterance in the context. This leads to a problem of not completely understanding the context flow that is required to select a response. To address this issue, we propose a new fine-grained post-training method that reflects the characteristics of the multi-turn dialogue. Specifically, the model learns the utterance level interactions by training every short context-response pair in a dialogue session. Furthermore, by using a new training objective, the utterance relevance classification, the model understands the semantic relevance and coherence between the dialogue utterances. Experimental results show that our model achieves new state-of-the-art with significant margins on three benchmark datasets. This suggests that the fine-grained post-training method is highly effective for the response selection task.",
        "author": "Janghoon Han; Taesuk Hong; Byoungjae Kim; Youngjoong Ko; Jungyun Seo",
        "authorids": "/j/janghoon-han/; /t/taesuk-hong/; /b/byoungjae-kim/; /y/youngjoong-ko/; /j/jungyun-seo/",
        "bibtex": "@inproceedings{han-etal-2021-fine,\n    title = \"Fine-grained Post-training for Improving Retrieval-based Dialogue Systems\",\n    author = \"Han, Janghoon  and\n      Hong, Taesuk  and\n      Kim, Byoungjae  and\n      Ko, Youngjoong  and\n      Seo, Jungyun\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.122/\",\n    doi = \"10.18653/v1/2021.naacl-main.122\",\n    pages = \"1549--1558\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.122.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.122/",
        "pdf_size": 976074,
        "gs_citation": 88,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2745681913893948746&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science and Engineering, Sogang University+LG AI Research; Department of Computer Science and Engineering, Sogang University; Department of Computer Science and Engineering, Sogang University; Department of Computer Science and Engineering, Sungkyunkwan University; Department of Computer Science and Engineering, Sogang University",
        "aff_domain": "lgresearch.ai;sogang.ac.kr;sogang.ac.kr;skku.edu;sogang.ac.kr",
        "email": "lgresearch.ai;sogang.ac.kr;sogang.ac.kr;skku.edu;sogang.ac.kr",
        "github": "https://github.com/hanjanghoon/BERT_FP",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0;0;2;0",
        "aff_unique_norm": "Sogang University;LG AI Research;Sungkyunkwan University",
        "aff_unique_dep": "Department of Computer Science and Engineering;;Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.sogang.ac.kr;https://www.lgaires.com;https://www.sungkyunkwan.ac.kr",
        "aff_unique_abbr": "Sogang;LG AI;SKKU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2021.naacl-main.243",
        "title": "Fine-tuning Encoders for Improved Monolingual and Zero-shot Polylingual Neural Topic Modeling",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Neural topic models can augment or replace bag-of-words inputs with the learned representations of deep pre-trained transformer-based word prediction models. One added benefit when using representations from multilingual models is that they facilitate zero-shot polylingual topic modeling. However, while it has been widely observed that pre-trained embeddings should be fine-tuned to a given task, it is not immediately clear what supervision should look like for an unsupervised task such as topic modeling. Thus, we propose several methods for fine-tuning encoders to improve both monolingual and zero-shot polylingual neural topic modeling. We consider fine-tuning on auxiliary tasks, constructing a new topic classification task, integrating the topic classification objective directly into topic model training, and continued pre-training. We find that fine-tuning encoder representations on topic classification and integrating the topic classification task directly into topic modeling improves topic quality, and that fine-tuning encoder representations on any task is the most important factor for facilitating cross-lingual transfer.",
        "author": "Aaron Mueller; Mark Dredze",
        "authorids": "/a/aaron-mueller/; /m/mark-dredze/",
        "bibtex": "@inproceedings{mueller-dredze-2021-fine,\n    title = \"Fine-tuning Encoders for Improved Monolingual and Zero-shot Polylingual Neural Topic Modeling\",\n    author = \"Mueller, Aaron  and\n      Dredze, Mark\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.243/\",\n    doi = \"10.18653/v1/2021.naacl-main.243\",\n    pages = \"3054--3068\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.243.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.243/",
        "pdf_size": 1516586,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4597649823789924682&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science, Johns Hopkins University; Department of Computer Science, Johns Hopkins University",
        "aff_domain": "jhu.edu;cs.jhu.edu",
        "email": "jhu.edu;cs.jhu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Johns Hopkins University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.jhu.edu",
        "aff_unique_abbr": "JHU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.259",
        "title": "FlowPrior: Learning Expressive Priors for Latent Variable Sentence Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Variational autoencoders (VAEs) are widely used for latent variable modeling of text. We focus on variations that learn expressive prior distributions over the latent variable. We find that existing training strategies are not effective for learning rich priors, so we propose adding the importance-sampled log marginal likelihood as a second term to the standard VAE objective to help when learning the prior. Doing so improves results for all priors evaluated, including a novel choice for sentence VAEs based on normalizing flows (NF). Priors parameterized with NF are no longer constrained to a specific distribution family, allowing a more flexible way to encode the data distribution. Our model, which we call FlowPrior, shows a substantial improvement in language modeling tasks compared to strong baselines. We demonstrate that FlowPrior learns an expressive prior with analysis and several forms of evaluation involving generation.",
        "author": "Xiaoan Ding; Kevin Gimpel",
        "authorids": "/x/xiaoan-ding/; /k/kevin-gimpel/",
        "bibtex": "@inproceedings{ding-gimpel-2021-flowprior,\n    title = \"{F}low{P}rior: Learning Expressive Priors for Latent Variable Sentence Models\",\n    author = \"Ding, Xiaoan  and\n      Gimpel, Kevin\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.259/\",\n    doi = \"10.18653/v1/2021.naacl-main.259\",\n    pages = \"3242--3258\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.259.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.259/",
        "pdf_size": 1887529,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2807613446747300201&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "University of Chicago; Toyota Technological Institute at Chicago",
        "aff_domain": "uchicago.edu;ttic.edu",
        "email": "uchicago.edu;ttic.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Chicago;Toyota Technological Institute at Chicago",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uchicago.edu;https://www.tti-chicago.org",
        "aff_unique_abbr": "UChicago;TTI Chicago",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Chicago",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.338",
        "title": "Focused Attention Improves Document-Grounded Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Document grounded generation is the task of using the information provided in a document to improve text generation. This work focuses on two different document grounded generation tasks: Wikipedia Update Generation task and Dialogue response generation. Our work introduces two novel adaptations of large scale pre-trained encoder-decoder models focusing on building context driven representation of the document and enabling specific attention to the information in the document. Additionally, we provide a stronger BART baseline for these tasks. Our proposed techniques outperform existing methods on both automated (at least 48% increase in BLEU-4 points) and human evaluation for closeness to reference and relevance to the document. Furthermore, we perform comprehensive manual inspection of the generated output and categorize errors to provide insights into future directions in modeling these tasks.",
        "author": "Shrimai Prabhumoye; Kazuma Hashimoto; Yingbo Zhou; Alan W Black; Ruslan Salakhutdinov",
        "authorids": "/s/shrimai-prabhumoye/; /k/kazuma-hashimoto/; /y/yingbo-zhou/; /a/alan-w-black/; /r/ruslan-salakhutdinov/",
        "bibtex": "@inproceedings{prabhumoye-etal-2021-focused,\n    title = \"Focused Attention Improves Document-Grounded Generation\",\n    author = \"Prabhumoye, Shrimai  and\n      Hashimoto, Kazuma  and\n      Zhou, Yingbo  and\n      Black, Alan W  and\n      Salakhutdinov, Ruslan\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.338/\",\n    doi = \"10.18653/v1/2021.naacl-main.338\",\n    pages = \"4274--4287\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.338.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.338/",
        "pdf_size": 813636,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5101140296498089666&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Carnegie Mellon University; Salesforce Research; Salesforce Research; Carnegie Mellon University; Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;salesforce.com; ; ; ",
        "email": "cs.cmu.edu;salesforce.com; ; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;1;0;0",
        "aff_unique_norm": "Carnegie Mellon University;Salesforce",
        "aff_unique_dep": ";Salesforce Research",
        "aff_unique_url": "https://www.cmu.edu;https://research.salesforce.com",
        "aff_unique_abbr": "CMU;Salesforce",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.32",
        "title": "Fool Me Twice: Entailment from Wikipedia Gamification",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We release FoolMeTwice (FM2 for short), a large dataset of challenging entailment pairs collected through a fun multi-player game. Gamification encourages adversarial examples, drastically lowering the number of examples that can be solved using \u201cshortcuts\u201d compared to other popular entailment datasets. Players are presented with two tasks. The first task asks the player to write a plausible claim based on the evidence from a Wikipedia page. The second one shows two plausible claims written by other players, one of which is false, and the goal is to identify it before the time runs out. Players \u201cpay\u201d to see clues retrieved from the evidence pool: the more evidence the player needs, the harder the claim. Game-play between motivated players leads to diverse strategies for crafting claims, such as temporal inference and diverting to unrelated evidence, and results in higher quality data for the entailment and evidence retrieval tasks. We open source the dataset and the game code.",
        "author": "Julian Eisenschlos; Bhuwan Dhingra; Jannis Bulian; Benjamin B\u00f6rschinger; Jordan Boyd-Graber",
        "authorids": "/j/julian-eisenschlos/; /b/bhuwan-dhingra/; /j/jannis-bulian/; /b/benjamin-borschinger/; /j/jordan-boyd-graber/",
        "bibtex": "@inproceedings{eisenschlos-etal-2021-fool,\n    title = \"Fool Me Twice: Entailment from {W}ikipedia Gamification\",\n    author = {Eisenschlos, Julian  and\n      Dhingra, Bhuwan  and\n      Bulian, Jannis  and\n      B{\\\"o}rschinger, Benjamin  and\n      Boyd-Graber, Jordan},\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.32/\",\n    doi = \"10.18653/v1/2021.naacl-main.32\",\n    pages = \"352--365\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.32.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.32/",
        "pdf_size": 1386438,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6742163215746027422&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Google Research; Google Research; Google Research; Google Research; CS, iSchool, UMIACS, LSC University of Maryland",
        "aff_domain": "google.com;google.com;google.com;google.com;umiacs.umd.edu",
        "email": "google.com;google.com;google.com;google.com;umiacs.umd.edu",
        "github": "https://github.com/google-research/fool-me-twice",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;1",
        "aff_unique_norm": "Google;University of Maryland",
        "aff_unique_dep": "Google Research;Computer Science",
        "aff_unique_url": "https://research.google;https://www.umd.edu",
        "aff_unique_abbr": "Google Research;UMD",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Mountain View;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.174",
        "title": "Framing Unpacked: A Semi-Supervised Interpretable Multi-View Model of Media Frames",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Understanding how news media frame political issues is important due to its impact on public attitudes, yet hard to automate. Computational approaches have largely focused on classifying the frame of a full news article while framing signals are often subtle and local. Furthermore, automatic news analysis is a sensitive domain, and existing classifiers lack transparency in their predictions. This paper addresses both issues with a novel semi-supervised model, which jointly learns to embed local information about the events and related actors in a news article through an auto-encoding framework, and to leverage this signal for document-level frame classification. Our experiments show that: our model outperforms previous models of frame prediction; we can further improve performance with unlabeled training data leveraging the semi-supervised nature of our model; and the learnt event and actor embeddings intuitively corroborate the document-level predictions, providing a nuanced and interpretable article frame representation.",
        "author": "Shima Khanehzar; Trevor Cohn; Gosia Mikolajczak; Andrew Turpin; Lea Frermann",
        "authorids": "/s/shima-khanehzar/; /t/trevor-cohn/; /g/gosia-mikolajczak/; /a/andrew-turpin/; /l/lea-frermann/",
        "bibtex": "@inproceedings{khanehzar-etal-2021-framing,\n    title = \"Framing Unpacked: A Semi-Supervised Interpretable Multi-View Model of Media Frames\",\n    author = \"Khanehzar, Shima  and\n      Cohn, Trevor  and\n      Mikolajczak, Gosia  and\n      Turpin, Andrew  and\n      Frermann, Lea\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.174/\",\n    doi = \"10.18653/v1/2021.naacl-main.174\",\n    pages = \"2154--2166\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.174.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.174/",
        "pdf_size": 523099,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11719409051109206559&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "School of Computing and Information Systems; School of Computing and Information Systems; School of Computing and Information Systems+School of Social and Political Sciences; School of Computing and Information Systems; School of Computing and Information Systems",
        "aff_domain": "student.unimelb.edu.au;unimelb.edu.au;unimelb.edu.au;unimelb.edu.au;unimelb.edu.au",
        "email": "student.unimelb.edu.au;unimelb.edu.au;unimelb.edu.au;unimelb.edu.au;unimelb.edu.au",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0+1;0;0",
        "aff_unique_norm": "School of Computing and Information Systems;University of Cambridge",
        "aff_unique_dep": "Computing and Information Systems;School of Social and Political Sciences",
        "aff_unique_url": ";https://www.cam.ac.uk",
        "aff_unique_abbr": ";Cambridge",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United Kingdom"
    },
    {
        "id": "2021.naacl-main.197",
        "title": "From Masked Language Modeling to Translation: Non-English Auxiliary Tasks Improve Zero-shot Spoken Language Understanding",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The lack of publicly available evaluation data for low-resource languages limits progress in Spoken Language Understanding (SLU). As key tasks like intent classification and slot filling require abundant training data, it is desirable to reuse existing data in high-resource languages to develop models for low-resource scenarios. We introduce xSID, a new benchmark for cross-lingual (x) Slot and Intent Detection in 13 languages from 6 language families, including a very low-resource dialect. To tackle the challenge, we propose a joint learning approach, with English SLU training data and non-English auxiliary tasks from raw text, syntax and translation for transfer. We study two setups which differ by type and language coverage of the pre-trained embeddings. Our results show that jointly learning the main tasks with masked language modeling is effective for slots, while machine translation transfer works best for intent classification.",
        "author": "Rob van der Goot; Ibrahim Sharaf; Aizhan Imankulova; Ahmet \u00dcst\u00fcn; Marija Stepanovi\u0107; Alan Ramponi; Siti Oryza Khairunnisa; Mamoru Komachi; Barbara Plank",
        "authorids": "/r/rob-van-der-goot/; /i/ibrahim-sharaf/; /a/aizhan-imankulova/; /a/ahmet-ustun/; /m/marija-stepanovic/; /a/alan-ramponi/; /s/siti-oryza-khairunnisa/; /m/mamoru-komachi/; /b/barbara-plank/",
        "bibtex": "@inproceedings{van-der-goot-etal-2021-masked,\n    title = \"From Masked Language Modeling to Translation: Non-{E}nglish Auxiliary Tasks Improve Zero-shot Spoken Language Understanding\",\n    author = {van der Goot, Rob  and\n      Sharaf, Ibrahim  and\n      Imankulova, Aizhan  and\n      {\\\"U}st{\\\"u}n, Ahmet  and\n      Stepanovi{\\'c}, Marija  and\n      Ramponi, Alan  and\n      Khairunnisa, Siti Oryza  and\n      Komachi, Mamoru  and\n      Plank, Barbara},\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.197/\",\n    doi = \"10.18653/v1/2021.naacl-main.197\",\n    pages = \"2479--2497\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.197.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.197/",
        "pdf_size": 561813,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17064580047792582801&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "IT University of Copenhagen; Factmata; Tokyo Metropolitan University; University of Groningen; IT University of Copenhagen; University of Trento + Fondazione The Microsoft Research \u2013 University of Trento Centre for Computational and Systems Biology (COSBI); Tokyo Metropolitan University; Tokyo Metropolitan University; IT University of Copenhagen",
        "aff_domain": "itu.dk; ; ; ; ; ; ; ;itu.dk",
        "email": "itu.dk; ; ; ; ; ; ; ;itu.dk",
        "github": "",
        "project": "https://bitbucket.org/robvanderg/xsid",
        "author_num": 9,
        "aff_unique_index": "0;1;2;3;0;4+5;2;2;0",
        "aff_unique_norm": "IT University of Copenhagen;Factmata;Tokyo Metropolitan University;University of Groningen;University of Trento;Microsoft Research - University of Trento",
        "aff_unique_dep": ";;;;;Centre for Computational and Systems Biology (COSBI)",
        "aff_unique_url": "https://itu.dk;https://www.factmata.com;https://www.tmuc.ac.jp;https://www.rug.nl;https://www.unitn.it;https://www.microsoft.com/en-us/research/group/cosbi",
        "aff_unique_abbr": "ITU;;TMU;RUG;UniTN;COSBI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;2;3;0;4+4;2;2;0",
        "aff_country_unique": "Denmark;United Kingdom;Japan;Netherlands;Italy"
    },
    {
        "id": "2021.naacl-main.433",
        "title": "Frustratingly Easy Edit-based Linguistic Steganography with a Masked Language Model",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "With advances in neural language models, the focus of linguistic steganography has shifted from edit-based approaches to generation-based ones. While the latter\u2019s payload capacity is impressive, generating genuine-looking texts remains challenging. In this paper, we revisit edit-based linguistic steganography, with the idea that a masked language model offers an off-the-shelf solution. The proposed method eliminates painstaking rule construction and has a high payload capacity for an edit-based model. It is also shown to be more secure against automatic detection than a generation-based method while offering better control of the security/payload capacity trade-off.",
        "author": "Honai Ueoka; Yugo Murawaki; Sadao Kurohashi",
        "authorids": "/h/honai-ueoka/; /y/yugo-murawaki/; /s/sadao-kurohashi/",
        "bibtex": "@inproceedings{ueoka-etal-2021-frustratingly,\n    title = \"Frustratingly Easy Edit-based Linguistic Steganography with a Masked Language Model\",\n    author = \"Ueoka, Honai  and\n      Murawaki, Yugo  and\n      Kurohashi, Sadao\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.433/\",\n    doi = \"10.18653/v1/2021.naacl-main.433\",\n    pages = \"5486--5492\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.433.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.433/",
        "pdf_size": 2178540,
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=182361575722169594&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Graduate School of Informatics, Kyoto University; Graduate School of Informatics, Kyoto University; Graduate School of Informatics, Kyoto University",
        "aff_domain": "icn.cce.i.kyoto-u.ac.jp;i.kyoto-u.ac.jp;i.kyoto-u.ac.jp",
        "email": "icn.cce.i.kyoto-u.ac.jp;i.kyoto-u.ac.jp;i.kyoto-u.ac.jp",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Kyoto University",
        "aff_unique_dep": "Graduate School of Informatics",
        "aff_unique_url": "https://www.kyoto-u.ac.jp",
        "aff_unique_abbr": "Kyoto U",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Kyoto",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2021.naacl-main.118",
        "title": "GEMNET: Effective Gated Gazetteer Representations for Recognizing Complex Entities in Low-context Input",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Named Entity Recognition (NER) remains difficult in real-world settings; current challenges include short texts (low context), emerging entities, and complex entities (e.g. movie names). Gazetteer features can help, but results have been mixed due to challenges with adding extra features, and a lack of realistic evaluation data. It has been shown that including gazetteer features can cause models to overuse or underuse them, leading to poor generalization. We propose GEMNET, a novel approach for gazetteer knowledge integration, including (1) a flexible Contextual Gazetteer Representation (CGR) encoder that can be fused with any word-level model; and (2) a Mixture-of- Experts gating network that overcomes the feature overuse issue by learning to conditionally combine the context and gazetteer features, instead of assigning them fixed weights. To comprehensively evaluate our approaches, we create 3 large NER datasets (24M tokens) reflecting current challenges. In an uncased setting, our methods show large gains (up to +49% F1) in recognizing difficult entities compared to existing baselines. On standard benchmarks, we achieve a new uncased SOTA on CoNLL03 and WNUT17.",
        "author": "Tao Meng; Anjie Fang; Oleg Rokhlenko; Shervin Malmasi",
        "authorids": "/t/tao-meng/; /a/anjie-fang/; /o/oleg-rokhlenko/; /s/shervin-malmasi/",
        "bibtex": "@inproceedings{meng-etal-2021-gemnet,\n    title = \"{GEMNET}: Effective Gated Gazetteer Representations for Recognizing Complex Entities in Low-context Input\",\n    author = \"Meng, Tao  and\n      Fang, Anjie  and\n      Rokhlenko, Oleg  and\n      Malmasi, Shervin\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.118/\",\n    doi = \"10.18653/v1/2021.naacl-main.118\",\n    pages = \"1499--1512\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.118.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.118/",
        "pdf_size": 751339,
        "gs_citation": 72,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11841233122489292529&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 3,
        "aff": "UCLA; Amazon.com, Inc.; Amazon.com, Inc.; Amazon.com, Inc.",
        "aff_domain": "cs.ucla.edu;amazon.com;amazon.com;amazon.com",
        "email": "cs.ucla.edu;amazon.com;amazon.com;amazon.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "University of California, Los Angeles;Amazon.com, Inc.",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ucla.edu;https://www.amazon.com",
        "aff_unique_abbr": "UCLA;Amazon",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.439",
        "title": "GPT Perdetry Test: Generating new meanings for new words",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Human innovation in language, such as inventing new words, is a challenge for pretrained language models. We assess the ability of one large model, GPT-3, to process new words and decide on their meaning. We create a set of nonce words and prompt GPT-3 to generate their dictionary definitions. We find GPT-3 produces plausible definitions that align with human judgments. Moreover, GPT-3\u2019s definitions are sometimes preferred to those invented by humans, signaling its intriguing ability not just to adapt, but to add to the evolving vocabulary of the English language.",
        "author": "Nikolay Malkin; Sameera Lanka; Pranav Goel; Sudha Rao; Nebojsa Jojic",
        "authorids": "/n/nikolay-malkin/; /s/sameera-lanka/; /p/pranav-goel/; /s/sudha-rao/; /n/nebojsa-jojic/",
        "bibtex": "@inproceedings{malkin-etal-2021-gpt,\n    title = \"{GPT} Perdetry Test: Generating new meanings for new words\",\n    author = \"Malkin, Nikolay  and\n      Lanka, Sameera  and\n      Goel, Pranav  and\n      Rao, Sudha  and\n      Jojic, Nebojsa\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.439/\",\n    doi = \"10.18653/v1/2021.naacl-main.439\",\n    pages = \"5542--5553\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.439.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.439/",
        "pdf_size": 257926,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3713936760717948761&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Yale University; Microsoft + Microsoft Research; University of Maryland; Microsoft Research; Microsoft Research",
        "aff_domain": "yale.edu;microsoft.com;umd.edu;microsoft.com;microsoft.com",
        "email": "yale.edu;microsoft.com;umd.edu;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1+1;2;1;1",
        "aff_unique_norm": "Yale University;Microsoft Corporation;University of Maryland",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.yale.edu;https://www.microsoft.com;https://www/umd.edu",
        "aff_unique_abbr": "Yale;Microsoft;UMD",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.384",
        "title": "GSum: A General Framework for Guided Neural Abstractive Summarization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Neural abstractive summarization models are flexible and can produce coherent summaries, but they are sometimes unfaithful and can be difficult to control. While previous studies attempt to provide different types of guidance to control the output and increase faithfulness, it is not clear how these strategies compare and contrast to each other. In this paper, we propose a general and extensible guided summarization framework (GSum) that can effectively take different kinds of external guidance as input, and we perform experiments across several different varieties. Experiments demonstrate that this model is effective, achieving state-of-the-art performance according to ROUGE on 4 popular summarization datasets when using highlighted sentences as guidance. In addition, we show that our guided model can generate more faithful summaries and demonstrate how different types of guidance generate qualitatively different summaries, lending a degree of controllability to the learned models.",
        "author": "Zi-Yi Dou; Pengfei Liu; Hiroaki Hayashi; Zhengbao Jiang; Graham Neubig",
        "authorids": "/z/zi-yi-dou/; /p/pengfei-liu/; /h/hiroaki-hayashi/; /z/zhengbao-jiang/; /g/graham-neubig/",
        "bibtex": "@inproceedings{dou-etal-2021-gsum,\n    title = \"{GS}um: A General Framework for Guided Neural Abstractive Summarization\",\n    author = \"Dou, Zi-Yi  and\n      Liu, Pengfei  and\n      Hayashi, Hiroaki  and\n      Jiang, Zhengbao  and\n      Neubig, Graham\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.384/\",\n    doi = \"10.18653/v1/2021.naacl-main.384\",\n    pages = \"4830--4842\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.384.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.384/",
        "pdf_size": 943519,
        "gs_citation": 284,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6897546920715313408&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Language Technologies Institute, Carnegie Mellon University; Language Technologies Institute, Carnegie Mellon University; Language Technologies Institute, Carnegie Mellon University; Language Technologies Institute, Carnegie Mellon University; Language Technologies Institute, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "github": "https://github.com/neulab/guided_summarization",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Language Technologies Institute",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.223",
        "title": "Game-theoretic Vocabulary Selection via the Shapley Value and Banzhaf Index",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The input vocabulary and the representations learned are crucial to the performance of neural NLP models. Using the full vocabulary results in less explainable and more memory intensive models, with the embedding layer often constituting the majority of model parameters. It is thus common to use a smaller vocabulary to lower memory requirements and construct more interpertable models. We propose a vocabulary selection method that views words as members of a team trying to maximize the model\u2019s performance. We apply power indices from cooperative game theory, including the Shapley value and Banzhaf index, that measure the relative importance of individual team members in accomplishing a joint task. We approximately compute these indices to identify the most influential words. Our empirical evaluation examines multiple NLP tasks, including sentence and document classification, question answering and textual entailment. We compare to baselines that select words based on frequency, TF-IDF and regression coefficients under L1 regularization, and show that this game-theoretic vocabulary selection outperforms all baseline on a range of different tasks and datasets.",
        "author": "Roma Patel; Marta Garnelo; Ian Gemp; Chris Dyer; Yoram Bachrach",
        "authorids": "/r/roma-patel/; /m/marta-garnelo/; /i/ian-gemp/; /c/chris-dyer/; /y/yoram-bachrach/",
        "bibtex": "@inproceedings{patel-etal-2021-game,\n    title = \"Game-theoretic Vocabulary Selection via the Shapley Value and Banzhaf Index\",\n    author = \"Patel, Roma  and\n      Garnelo, Marta  and\n      Gemp, Ian  and\n      Dyer, Chris  and\n      Bachrach, Yoram\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.223/\",\n    doi = \"10.18653/v1/2021.naacl-main.223\",\n    pages = \"2789--2798\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.223.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.223/",
        "pdf_size": 772614,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15870916901803473810&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Brown University + DeepMind; DeepMind; DeepMind; DeepMind; DeepMind",
        "aff_domain": "brown.edu; ; ; ;google.com",
        "email": "brown.edu; ; ; ;google.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;1;1;1;1",
        "aff_unique_norm": "Brown University;DeepMind",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.brown.edu;https://deepmind.com",
        "aff_unique_abbr": "Brown;DeepMind",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;1;1;1;1",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "id": "2021.naacl-main.76",
        "title": "Generalization in Instruction Following Systems",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Understanding and executing natural language instructions in a grounded domain is one of the hallmarks of artificial intelligence. In this paper, we focus on instruction understanding in the blocks world domain and investigate the language understanding abilities of two top-performing systems for the task. We aim to understand if the test performance of these models indicates an understanding of the spatial domain and of the natural language instructions relative to it, or whether they merely over-fit spurious signals in the dataset. We formulate a set of expectations one might have from an instruction following model and concretely characterize the different dimensions of robustness such a model should possess. Despite decent test performance, we find that state-of-the-art models fall short of these expectations and are extremely brittle. We then propose a learning strategy that involves data augmentation and show through extensive experiments that the proposed learning strategy yields models that are competitive on the original test set while satisfying our expectations much better.",
        "author": "Soham Dan; Michael Zhou; Dan Roth",
        "authorids": "/s/soham-dan/; /m/michael-zhou/; /d/dan-roth/",
        "bibtex": "@inproceedings{dan-etal-2021-generalization,\n    title = \"Generalization in Instruction Following Systems\",\n    author = \"Dan, Soham  and\n      Zhou, Michael  and\n      Roth, Dan\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.76/\",\n    doi = \"10.18653/v1/2021.naacl-main.76\",\n    pages = \"976--981\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.76.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.76/",
        "pdf_size": 1999494,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6780180721050431877&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "University of Pennsylvania; University of Pennsylvania; University of Pennsylvania",
        "aff_domain": "seas.upenn.edu;seas.upenn.edu;seas.upenn.edu",
        "email": "seas.upenn.edu;seas.upenn.edu;seas.upenn.edu",
        "github": "",
        "project": "http://cogcomp.org/page/publication_view/936",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Pennsylvania",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.upenn.edu",
        "aff_unique_abbr": "UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.160",
        "title": "Generating An Optimal Interview Question Plan Using A Knowledge Graph And Integer Linear Programming",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Given the diversity of the candidates and complexity of job requirements, and since interviewing is an inherently subjective process, it is an important task to ensure consistent, uniform, efficient and objective interviews that result in high quality recruitment. We propose an interview assistant system to automatically, and in an objective manner, select an optimal set of technical questions (from question banks) personalized for a candidate. This set can help a human interviewer to plan for an upcoming interview of that candidate. We formalize the problem of selecting a set of questions as an integer linear programming problem and use standard solvers to get a solution. We use knowledge graph as background knowledge in this formulation, and derive our objective functions and constraints from it. We use candidate\u2019s resume to personalize the selection of questions. We propose an intrinsic evaluation to compare a set of suggested questions with actually asked questions. We also use expert interviewers to comparatively evaluate our approach with a set of reasonable baselines.",
        "author": "Soham Datta; Prabir Mallick; Sangameshwar Patil; Indrajit Bhattacharya; Girish Palshikar",
        "authorids": "/s/soham-datta/; /p/prabir-mallick/; /s/sangameshwar-patil/; /i/indrajit-bhattacharya/; /g/girish-palshikar/",
        "bibtex": "@inproceedings{datta-etal-2021-generating,\n    title = \"Generating An Optimal Interview Question Plan Using A Knowledge Graph And Integer Linear Programming\",\n    author = \"Datta, Soham  and\n      Mallick, Prabir  and\n      Patil, Sangameshwar  and\n      Bhattacharya, Indrajit  and\n      Palshikar, Girish\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.160/\",\n    doi = \"10.18653/v1/2021.naacl-main.160\",\n    pages = \"1996--2005\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.160.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.160/",
        "pdf_size": 815459,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17905894397085807353&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "TCS Research; TCS Research; TCS Research; TCS Research; TCS Research",
        "aff_domain": "tcs.com;tcs.com;tcs.com;tcs.com;tcs.com",
        "email": "tcs.com;tcs.com;tcs.com;tcs.com;tcs.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Tata Consultancy Services",
        "aff_unique_dep": "Research",
        "aff_unique_url": "https://www.tcs.com",
        "aff_unique_abbr": "TCS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2021.naacl-main.120",
        "title": "Generating Negative Samples by Manipulating Golden Responses for Unsupervised Learning of a Response Evaluation Model",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Evaluating the quality of responses generated by open-domain conversation systems is a challenging task. This is partly because there can be multiple appropriate responses to a given dialogue history. Reference-based metrics that rely on comparisons to a set of known correct responses often fail to account for this variety, and consequently correlate poorly with human judgment. To address this problem, researchers have investigated the possibility of assessing response quality without using a set of known correct responses. RUBER demonstrated that an automatic response evaluation model could be made using unsupervised learning for the next-utterance prediction (NUP) task. For the unsupervised learning of such model, we propose a method of manipulating a golden response to create a new negative response that is designed to be inappropriate within the context while maintaining high similarity with the original golden response. We find, from our experiments on English datasets, that using the negative samples generated by our method alongside random negative samples can increase the model\u2019s correlation with human evaluations. The process of generating such negative samples is automated and does not rely on human annotation.",
        "author": "ChaeHun Park; Eugene Jang; Wonsuk Yang; Jong Park",
        "authorids": "/c/chaehun-park/; /e/eugene-jang/; /w/wonsuk-yang/; /j/jong-c-park/",
        "bibtex": "@inproceedings{park-etal-2021-generating,\n    title = \"Generating Negative Samples by Manipulating Golden Responses for Unsupervised Learning of a Response Evaluation Model\",\n    author = \"Park, ChaeHun  and\n      Jang, Eugene  and\n      Yang, Wonsuk  and\n      Park, Jong\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.120/\",\n    doi = \"10.18653/v1/2021.naacl-main.120\",\n    pages = \"1525--1534\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.120.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.120/",
        "pdf_size": 1246430,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2707141328147719980&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "School of Computing, Korea Advanced Institute of Science and Technology; School of Computing, Korea Advanced Institute of Science and Technology; School of Computing, Korea Advanced Institute of Science and Technology; School of Computing, Korea Advanced Institute of Science and Technology",
        "aff_domain": "nlp.kaist.ac.kr;nlp.kaist.ac.kr;nlp.kaist.ac.kr;nlp.kaist.ac.kr",
        "email": "nlp.kaist.ac.kr;nlp.kaist.ac.kr;nlp.kaist.ac.kr;nlp.kaist.ac.kr",
        "github": "https://github.com/nlpcl-lab/dialog-eval-hard-negative",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Korea Advanced Institute of Science and Technology",
        "aff_unique_dep": "School of Computing",
        "aff_unique_url": "https://www.kaist.ac.kr",
        "aff_unique_abbr": "KAIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2021.naacl-main.457",
        "title": "Generative Imagination Elevates Machine Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "There are common semantics shared across text and images. Given a sentence in a source language, whether depicting the visual scene helps translation into a target language? Existing multimodal neural machine translation methods (MNMT) require triplets of bilingual sentence - image for training and tuples of source sentence - image for inference. In this paper, we propose ImagiT, a novel machine translation method via visual imagination. ImagiT first learns to generate visual representation from the source sentence, and then utilizes both source sentence and the \u201cimagined representation\u201d to produce a target translation. Unlike previous methods, it only needs the source sentence at the inference time. Experiments demonstrate that ImagiT benefits from visual imagination and significantly outperforms the text-only neural machine translation baselines. Further analysis reveals that the imagination process in ImagiT helps fill in missing information when performing the degradation strategy.",
        "author": "Quanyu Long; Mingxuan Wang; Lei Li",
        "authorids": "/q/quanyu-long/; /m/mingxuan-wang/; /l/lei-li/",
        "bibtex": "@inproceedings{long-etal-2021-generative,\n    title = \"Generative Imagination Elevates Machine Translation\",\n    author = \"Long, Quanyu  and\n      Wang, Mingxuan  and\n      Li, Lei\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.457/\",\n    doi = \"10.18653/v1/2021.naacl-main.457\",\n    pages = \"5738--5748\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.457.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.457/",
        "pdf_size": 583781,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14549638596774166157&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Nanyang Technological University, Singapore; ByteDance AI Lab, China; ByteDance AI Lab, China",
        "aff_domain": "e.ntu.edu.sg;bytedance.com;bytedance.com",
        "email": "e.ntu.edu.sg;bytedance.com;bytedance.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Nanyang Technological University;ByteDance",
        "aff_unique_dep": ";AI Lab",
        "aff_unique_url": "https://www.ntu.edu.sg;https://www.bytedance.com",
        "aff_unique_abbr": "NTU;ByteDance",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "Singapore;China"
    },
    {
        "id": "2021.naacl-main.52",
        "title": "Get Your Vitamin C! Robust Fact Verification with Contrastive Evidence",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Typical fact verification models use retrieved written evidence to verify claims. Evidence sources, however, often change over time as more information is gathered and revised. In order to adapt, models must be sensitive to subtle differences in supporting evidence. We present VitaminC, a benchmark infused with challenging cases that require fact verification models to discern and adjust to slight factual changes. We collect over 100,000 Wikipedia revisions that modify an underlying fact, and leverage these revisions, together with additional synthetically constructed ones, to create a total of over 400,000 claim-evidence pairs. Unlike previous resources, the examples in VitaminC are contrastive, i.e., they contain evidence pairs that are nearly identical in language and content, with the exception that one supports a given claim while the other does not. We show that training using this design increases robustness\u2014improving accuracy by 10% on adversarial fact verification and 6% on adversarial natural language inference (NLI). Moreover, the structure of VitaminC leads us to define additional tasks for fact-checking resources: tagging relevant words in the evidence for verifying the claim, identifying factual revisions, and providing automatic edits via factually consistent text generation.",
        "author": "Tal Schuster; Adam Fisch; Regina Barzilay",
        "authorids": "/t/tal-schuster/; /a/adam-fisch/; /r/regina-barzilay/",
        "bibtex": "@inproceedings{schuster-etal-2021-get,\n    title = \"Get Your Vitamin {C}! Robust Fact Verification with Contrastive Evidence\",\n    author = \"Schuster, Tal  and\n      Fisch, Adam  and\n      Barzilay, Regina\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.52/\",\n    doi = \"10.18653/v1/2021.naacl-main.52\",\n    pages = \"624--643\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.52.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.52/",
        "pdf_size": 1178454,
        "gs_citation": 242,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5584339928412076004&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "https://github.com/TalSchuster/VitaminC",
        "project": "",
        "author_num": 3
    },
    {
        "id": "2021.naacl-industry.26",
        "title": "Goodwill Hunting: Analyzing and Repurposing Off-the-Shelf Named Entity Linking Systems",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Named entity linking (NEL) or mapping \u201cstrings\u201d to \u201cthings\u201d in a knowledge base is a fundamental preprocessing step in systems that require knowledge of entities such as information extraction and question answering. In this work, we lay out and investigate two challenges faced by individuals or organizations building NEL systems. Can they directly use an off-the-shelf system? If not, how easily can such a system be repurposed for their use case? First, we conduct a study of off-the-shelf commercial and academic NEL systems. We find that most systems struggle to link rare entities, with commercial solutions lagging their academic counterparts by 10%+. Second, for a use case where the NEL model is used in a sports question-answering (QA) system, we investigate how to close the loop in our analysis by repurposing the best off-the-shelf model (Bootleg) to correct sport-related errors. We show how tailoring a simple technique for patching models using weak labeling can provide a 25% absolute improvement in accuracy of sport-related errors.",
        "author": "Karan Goel; Laurel Orr; Nazneen Fatema Rajani; Jesse Vig; Christopher R\u00e9",
        "authorids": "/k/karan-goel/; /l/laurel-orr/; /n/nazneen-fatema-rajani/; /j/jesse-vig/; /c/christopher-re/",
        "bibtex": "@inproceedings{goel-etal-2021-goodwill,\n    title = \"Goodwill Hunting: Analyzing and Repurposing Off-the-Shelf Named Entity Linking Systems\",\n    author = \"Goel, Karan  and\n      Orr, Laurel  and\n      Rajani, Nazneen Fatema  and\n      Vig, Jesse  and\n      R{\\'e}, Christopher\",\n    editor = \"Kim, Young-bum  and\n      Li, Yunyao  and\n      Rambow, Owen\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-industry.26/\",\n    doi = \"10.18653/v1/2021.naacl-industry.26\",\n    pages = \"205--213\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-industry.26.pdf",
        "site": "https://aclanthology.org/2021.naacl-industry.26/",
        "pdf_size": 1159785,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3535108207926013962&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Stanford University; Stanford University; Salesforce Research; Salesforce Research; Stanford University",
        "aff_domain": "cs.stanford.edu; ; ; ; ",
        "email": "cs.stanford.edu; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;1;0",
        "aff_unique_norm": "Stanford University;Salesforce",
        "aff_unique_dep": ";Salesforce Research",
        "aff_unique_url": "https://www.stanford.edu;https://research.salesforce.com",
        "aff_unique_abbr": "Stanford;Salesforce",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.273",
        "title": "Graph Convolutional Networks for Event Causality Identification with Rich Document-level Structures",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We study the problem of Event Causality Identification (ECI) to detect causal relation between event mention pairs in text. Although deep learning models have recently shown state-of-the-art performance for ECI, they are limited to the intra-sentence setting where event mention pairs are presented in the same sentences. This work addresses this issue by developing a novel deep learning model for document-level ECI (DECI) to accept inter-sentence event mention pairs. As such, we propose a graph-based model that constructs interaction graphs to capture relevant connections between important objects for DECI in input documents. Such interaction graphs are then consumed by graph convolutional networks to learn document context-augmented representations for causality prediction between events. Various information sources are introduced to enrich the interaction graphs for DECI, featuring discourse, syntax, and semantic information. Our extensive experiments show that the proposed model achieves state-of-the-art performance on two benchmark datasets.",
        "author": "Minh Tran Phu; Thien Huu Nguyen",
        "authorids": "/m/minh-tran-phu/; /t/thien-huu-nguyen/",
        "bibtex": "@inproceedings{tran-phu-nguyen-2021-graph,\n    title = \"Graph Convolutional Networks for Event Causality Identification with Rich Document-level Structures\",\n    author = \"Tran Phu, Minh  and\n      Nguyen, Thien Huu\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.273/\",\n    doi = \"10.18653/v1/2021.naacl-main.273\",\n    pages = \"3480--3490\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.273.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.273/",
        "pdf_size": 409916,
        "gs_citation": 77,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=345306753908000375&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "VinAI Research, Hanoi, Vietnam; Department of Computer and Information Science, University of Oregon, Eugene, Oregon, USA",
        "aff_domain": "vinai.io;cs.uoregon.edu",
        "email": "vinai.io;cs.uoregon.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "VinAI Research;University of Oregon",
        "aff_unique_dep": ";Department of Computer and Information Science",
        "aff_unique_url": "https://www.vin.ai;https://www.uoregon.edu",
        "aff_unique_abbr": "VinAI;UO",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Hanoi;Eugene",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Vietnam;United States"
    },
    {
        "id": "2021.naacl-main.229",
        "title": "Graph Ensemble Learning over Multiple Dependency Trees for Aspect-level Sentiment Classification",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent work on aspect-level sentiment classification has demonstrated the efficacy of incorporating syntactic structures such as dependency trees with graph neural networks (GNN), but these approaches are usually vulnerable to parsing errors. To better leverage syntactic information in the face of unavoidable errors, we propose a simple yet effective graph ensemble technique, GraphMerge, to make use of the predictions from different parsers. Instead of assigning one set of model parameters to each dependency tree, we first combine the dependency relations from different parses before applying GNNs over the resulting graph. This allows GNN models to be robust to parse errors at no additional computational cost, and helps avoid overparameterization and overfitting from GNN layer stacking by introducing more connectivity into the ensemble graph. Our experiments on the SemEval 2014 Task 4 and ACL 14 Twitter datasets show that our GraphMerge model not only outperforms models with single dependency tree, but also beats other ensemble models without adding model parameters.",
        "author": "Xiaochen Hou; Peng Qi; Guangtao Wang; Rex Ying; Jing Huang; Xiaodong He; Bowen Zhou",
        "authorids": "/x/xiaochen-hou/; /p/peng-qi/; /g/guangtao-wang/; /r/rex-ying/; /j/jing-huang/; /x/xiaodong-he/; /b/bowen-zhou/",
        "bibtex": "@inproceedings{hou-etal-2021-graph,\n    title = \"Graph Ensemble Learning over Multiple Dependency Trees for Aspect-level Sentiment Classification\",\n    author = \"Hou, Xiaochen  and\n      Qi, Peng  and\n      Wang, Guangtao  and\n      Ying, Rex  and\n      Huang, Jing  and\n      He, Xiaodong  and\n      Zhou, Bowen\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.229/\",\n    doi = \"10.18653/v1/2021.naacl-main.229\",\n    pages = \"2884--2894\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.229.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.229/",
        "pdf_size": 467228,
        "gs_citation": 65,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14958871691946191573&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "JD AI Research, Mountain View, CA; JD AI Research, Mountain View, CA; JD AI Research, Mountain View, CA; Department of Computer Science, Stanford University, Stanford, CA; JD AI Research, Mountain View, CA; JD AI Research, Mountain View, CA; JD AI Research, Mountain View, CA",
        "aff_domain": "gmail.com; ; ; ; ; ; ",
        "email": "gmail.com; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;1;0;0;0",
        "aff_unique_norm": "JD AI Research;Stanford University",
        "aff_unique_dep": ";Department of Computer Science",
        "aff_unique_url": ";https://www.stanford.edu",
        "aff_unique_abbr": ";Stanford",
        "aff_campus_unique_index": "0;0;0;1;0;0;0",
        "aff_campus_unique": "Mountain View;Stanford",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-industry.19",
        "title": "Graph-based Multilingual Product Retrieval in E-Commerce Search",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Nowadays, with many e-commerce platforms conducting global business, e-commerce search systems are required to handle product retrieval under multilingual scenarios. Moreover, comparing with maintaining per-country specific e-commerce search systems, having an universal system across countries can further reduce the operational and computational costs, and facilitate business expansion to new countries. In this paper, we introduce an universal end-to-end multilingual retrieval system, and discuss our learnings and technical details when training and deploying the system to serve billion-scale product retrieval for e-commerce search. In particular, we propose a multilingual graph attention based retrieval network by leveraging recent advances in transformer-based multilingual language models and graph neural network architectures to capture the interactions between search queries and items in e-commerce search. Offline experiments on five countries data show that our algorithm outperforms the state-of-the-art baselines by 35% recall and 25% mAP on average. Moreover, the proposed model shows significant increase of conversion/revenue in online A/B experiments and has been deployed in production for multiple countries.",
        "author": "Hanqing Lu; Youna Hu; Tong Zhao; Tony Wu; Yiwei Song; Bing Yin",
        "authorids": "/h/hanqing-lu/; /y/youna-hu/; /t/tong-zhao/; /t/tony-wu/; /y/yiwei-song/; /b/bing-yin/",
        "bibtex": "@inproceedings{lu-etal-2021-graph,\n    title = \"Graph-based Multilingual Product Retrieval in {E}-Commerce Search\",\n    author = \"Lu, Hanqing  and\n      Hu, Youna  and\n      Zhao, Tong  and\n      Wu, Tony  and\n      Song, Yiwei  and\n      Yin, Bing\",\n    editor = \"Kim, Young-bum  and\n      Li, Yunyao  and\n      Rambow, Owen\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-industry.19/\",\n    doi = \"10.18653/v1/2021.naacl-industry.19\",\n    pages = \"146--153\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-industry.19.pdf",
        "site": "https://aclanthology.org/2021.naacl-industry.19/",
        "pdf_size": 560119,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7446649047818427162&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Amazon Search; Amazon Search; Amazon Personalization; Amazon Search; Amazon Search; Amazon Search",
        "aff_domain": "amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com",
        "email": "amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Amazon",
        "aff_unique_dep": "Amazon Search",
        "aff_unique_url": "https://www.amazon.com",
        "aff_unique_abbr": "Amazon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.321",
        "title": "Grey-box Adversarial Attack And Defence For Sentiment Classification",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We introduce a grey-box adversarial attack and defence framework for sentiment classification. We address the issues of differentiability, label preservation and input reconstruction for adversarial attack and defence in one unified framework. Our results show that once trained, the attacking model is capable of generating high-quality adversarial examples substantially faster (one order of magnitude less in time) than state-of-the-art attacking methods. These examples also preserve the original sentiment according to human evaluation. Additionally, our framework produces an improved classifier that is robust in defending against multiple adversarial attacking methods. Code is available at: https://github.com/ibm-aur-nlp/adv-def-text-dist.",
        "author": "Ying Xu; Xu Zhong; Antonio Jimeno Yepes; Jey Han Lau",
        "authorids": "/y/ying-xu/; /x/xu-zhong/; /a/antonio-jimeno-yepes/; /j/jey-han-lau/",
        "bibtex": "@inproceedings{xu-etal-2021-grey,\n    title = \"Grey-box Adversarial Attack And Defence For Sentiment Classification\",\n    author = \"Xu, Ying  and\n      Zhong, Xu  and\n      Jimeno Yepes, Antonio  and\n      Lau, Jey Han\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.321/\",\n    doi = \"10.18653/v1/2021.naacl-main.321\",\n    pages = \"4078--4087\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.321.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.321/",
        "pdf_size": 928875,
        "gs_citation": 75,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=995013642659658451&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4
    },
    {
        "id": "2021.naacl-main.80",
        "title": "Grounding Open-Domain Instructions to Automate Web Support Tasks",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Grounding natural language instructions on the web to perform previously unseen tasks enables accessibility and automation. We introduce a task and dataset to train AI agents from open-domain, step-by-step instructions originally written for people. We build RUSS (Rapid Universal Support Service) to tackle this problem. RUSS consists of two models: First, a BERT-LSTM with pointers parses instructions to WebLang, a domain-specific language we design for grounding natural language on the web. Then, a grounding model retrieves the unique IDs of any webpage elements requested in the WebLang. RUSS may interact with the user through a dialogue (e.g. ask for an address) or execute a web operation (e.g. click a button) inside the web runtime. To augment training, we synthesize natural language instructions mapped to WebLang. Our dataset consists of 80 different customer service problems from help websites, with a total of 741 step-by-step instructions and their corresponding actions. RUSS achieves 76.7% end-to-end accuracy predicting agent actions from single instructions. It outperforms state-of-the-art models that directly map instructions to actions without WebLang. Our user study shows that RUSS is preferred by actual users over web navigation.",
        "author": "Nancy Xu; Sam Masling; Michael Du; Giovanni Campagna; Larry Heck; James Landay; Monica Lam",
        "authorids": "/n/nancy-xu/; /s/sam-masling/; /m/michael-du/; /g/giovanni-campagna/; /l/larry-heck/; /j/james-landay/; /m/monica-lam/",
        "bibtex": "@inproceedings{xu-etal-2021-grounding,\n    title = \"Grounding Open-Domain Instructions to Automate Web Support Tasks\",\n    author = \"Xu, Nancy  and\n      Masling, Sam  and\n      Du, Michael  and\n      Campagna, Giovanni  and\n      Heck, Larry  and\n      Landay, James  and\n      Lam, Monica\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.80/\",\n    doi = \"10.18653/v1/2021.naacl-main.80\",\n    pages = \"1022--1032\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.80.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.80/",
        "pdf_size": 1922047,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=597790795552742382&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Computer Science Dept., Stanford University; Computer Science Dept., Stanford University; Computer Science Dept., Stanford University; Computer Science Dept., Stanford University; Viv Labs + Samsung Research; Computer Science Dept., Stanford University; Computer Science Dept., Stanford University",
        "aff_domain": "stanford.edu;stanford.edu;stanford.edu;stanford.edu;ieee.org;stanford.edu;stanford.edu",
        "email": "stanford.edu;stanford.edu;stanford.edu;stanford.edu;ieee.org;stanford.edu;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;1+2;0;0",
        "aff_unique_norm": "Stanford University;Viv Labs;Samsung Research",
        "aff_unique_dep": "Computer Science Dept.;;",
        "aff_unique_url": "https://www.stanford.edu;https://www.vivlabs.com;https://research.samsung.com",
        "aff_unique_abbr": "Stanford;;Samsung",
        "aff_campus_unique_index": "0;0;0;0;;0;0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;0;0;0;0+1;0;0",
        "aff_country_unique": "United States;South Korea"
    },
    {
        "id": "2021.naacl-main.257",
        "title": "Grouping Words with Semantic Diversity",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Deep Learning-based NLP systems can be sensitive to unseen tokens and hard to learn with high-dimensional inputs, which critically hinder learning generalization. We introduce an approach by grouping input words based on their semantic diversity to simplify input language representation with low ambiguity. Since the semantically diverse words reside in different contexts, we are able to substitute words with their groups and still distinguish word meanings relying on their contexts. We design several algorithms that compute diverse groupings based on random sampling, geometric distances, and entropy maximization, and we prove formal guarantees for the entropy-based algorithms. Experimental results show that our methods generalize NLP models and demonstrate enhanced accuracy on POS tagging and LM tasks and significant improvements on medium-scale machine translation tasks, up to +6.5 BLEU points. Our source code is available at https://github.com/abdulrafae/dg.",
        "author": "Karine Chubarian; Abdul Rafae Khan; Anastasios Sidiropoulos; Jia Xu",
        "authorids": "/k/karine-chubarian/; /a/abdul-rafae-khan/; /a/anastasios-sidiropoulos/; /j/jia-xu/",
        "bibtex": "@inproceedings{chubarian-etal-2021-grouping,\n    title = \"Grouping Words with Semantic Diversity\",\n    author = \"Chubarian, Karine  and\n      Khan, Abdul Rafae  and\n      Sidiropoulos, Anastasios  and\n      Xu, Jia\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.257/\",\n    doi = \"10.18653/v1/2021.naacl-main.257\",\n    pages = \"3217--3228\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.257.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.257/",
        "pdf_size": 310271,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5695342275798151917&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Computer Science and Technology, University of Illinois at Chicago; Department of Computer Science, Stevens Institute of Technology; Department of Computer Science and Technology, University of Illinois at Chicago; Department of Computer Science, Stevens Institute of Technology",
        "aff_domain": "uic.edu;stevens.edu;uic.edu;stevens.edu",
        "email": "uic.edu;stevens.edu;uic.edu;stevens.edu",
        "github": "https://github.com/abdulrafae/dg",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "University of Illinois at Chicago;Stevens Institute of Technology",
        "aff_unique_dep": "Department of Computer Science and Technology;Department of Computer Science",
        "aff_unique_url": "https://www.uic.edu;https://www.stevens.edu",
        "aff_unique_abbr": "UIC;SIT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Chicago;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.191",
        "title": "HONEST: Measuring Hurtful Sentence Completion in Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Language models have revolutionized the field of NLP. However, language models capture and proliferate hurtful stereotypes, especially in text generation. Our results show that 4.3% of the time, language models complete a sentence with a hurtful word. These cases are not random, but follow language and gender-specific patterns. We propose a score to measure hurtful sentence completions in language models (HONEST). It uses a systematic template- and lexicon-based bias evaluation methodology for six languages. Our findings suggest that these models replicate and amplify deep-seated societal stereotypes about gender roles. Sentence completions refer to sexual promiscuity when the target is female in 9% of the time, and in 4% to homosexuality when the target is male. The results raise questions about the use of these models in production settings.",
        "author": "Debora Nozza; Federico Bianchi; Dirk Hovy",
        "authorids": "/d/debora-nozza/; /f/federico-bianchi/; /d/dirk-hovy/",
        "bibtex": "@inproceedings{nozza-etal-2021-honest,\n    title = \"{HONEST}: Measuring Hurtful Sentence Completion in Language Models\",\n    author = \"Nozza, Debora  and\n      Bianchi, Federico  and\n      Hovy, Dirk\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.191/\",\n    doi = \"10.18653/v1/2021.naacl-main.191\",\n    pages = \"2398--2406\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.191.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.191/",
        "pdf_size": 296666,
        "gs_citation": 179,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14402362494022008394&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Bocconi University; Bocconi University; Bocconi University",
        "aff_domain": "unibocconi.it;unibocconi.it;unibocconi.it",
        "email": "unibocconi.it;unibocconi.it;unibocconi.it",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Bocconi University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.bocconi.edu",
        "aff_unique_abbr": "Bocconi",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Italy"
    },
    {
        "id": "2021.naacl-main.260",
        "title": "HTCInfoMax: A Global Model for Hierarchical Text Classification via Information Maximization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The current state-of-the-art model HiAGM for hierarchical text classification has two limitations. First, it correlates each text sample with all labels in the dataset which contains irrelevant information. Second, it does not consider any statistical constraint on the label representations learned by the structure encoder, while constraints for representation learning are proved to be helpful in previous work. In this paper, we propose HTCInfoMax to address these issues by introducing information maximization which includes two modules: text-label mutual information maximization and label prior matching. The first module can model the interaction between each text sample and its ground truth labels explicitly which filters out irrelevant information. The second one encourages the structure encoder to learn better representations with desired characteristics for all labels which can better handle label imbalance in hierarchical text classification. Experimental results on two benchmark datasets demonstrate the effectiveness of the proposed HTCInfoMax.",
        "author": "Zhongfen Deng; Hao Peng; Dongxiao He; Jianxin Li; Philip Yu",
        "authorids": "/z/zhongfen-deng/; /h/hao-peng/; /d/dongxiao-he/; /j/jianxin-li/; /p/philip-s-yu/",
        "bibtex": "@inproceedings{deng-etal-2021-htcinfomax,\n    title = \"{HTCI}nfo{M}ax: A Global Model for Hierarchical Text Classification via Information Maximization\",\n    author = \"Deng, Zhongfen  and\n      Peng, Hao  and\n      He, Dongxiao  and\n      Li, Jianxin  and\n      Yu, Philip\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.260/\",\n    doi = \"10.18653/v1/2021.naacl-main.260\",\n    pages = \"3259--3265\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.260.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.260/",
        "pdf_size": 378037,
        "gs_citation": 82,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13332336870496686421&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, University of Illinois at Chicago, Chicago, USA; BDBC, Beihang University, Beijing, China + School of Cyber Science and Technology, Beihang University, Beijing, China; School of Computer Science and Technology, Tianjin University, Tianjin, China; BDBC, Beihang University, Beijing, China; Department of Computer Science, University of Illinois at Chicago, Chicago, USA",
        "aff_domain": "uic.edu;act.buaa.edu.cn;tju.edu.cn;act.buaa.edu.cn;uic.edu",
        "email": "uic.edu;act.buaa.edu.cn;tju.edu.cn;act.buaa.edu.cn;uic.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1+1;2;1;0",
        "aff_unique_norm": "University of Illinois at Chicago;Beihang University;Tianjin University",
        "aff_unique_dep": "Department of Computer Science;BDBC;School of Computer Science and Technology",
        "aff_unique_url": "https://www.uic.edu;http://www.buaa.edu.cn;http://www.tju.edu.cn",
        "aff_unique_abbr": "UIC;Beihang;Tianjin University",
        "aff_campus_unique_index": "0;1+1;2;1;0",
        "aff_campus_unique": "Chicago;Beijing;Tianjin",
        "aff_country_unique_index": "0;1+1;1;1;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2021.naacl-main.89",
        "title": "Harnessing Multilinguality in Unsupervised Machine Translation for Rare Languages",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Unsupervised translation has reached impressive performance on resource-rich language pairs such as English-French and English-German. However, early studies have shown that in more realistic settings involving low-resource, rare languages, unsupervised translation performs poorly, achieving less than 3.0 BLEU. In this work, we show that multilinguality is critical to making unsupervised systems practical for low-resource settings. In particular, we present a single model for 5 low-resource languages (Gujarati, Kazakh, Nepali, Sinhala, and Turkish) to and from English directions, which leverages monolingual and auxiliary parallel data from other high-resource language pairs via a three-stage training scheme. We outperform all current state-of-the-art unsupervised baselines for these languages, achieving gains of up to 14.4 BLEU. Additionally, we outperform strong supervised baselines for various language pairs as well as match the performance of the current state-of-the-art supervised model for Nepali-English. We conduct a series of ablation studies to establish the robustness of our model under different degrees of data quality, as well as to analyze the factors which led to the superior performance of the proposed approach over traditional unsupervised models.",
        "author": "Xavier Garcia; Aditya Siddhant; Orhan Firat; Ankur Parikh",
        "authorids": "/x/xavier-garcia/; /a/aditya-siddhant/; /o/orhan-firat/; /a/ankur-parikh/",
        "bibtex": "@inproceedings{garcia-etal-2021-harnessing,\n    title = \"Harnessing Multilinguality in Unsupervised Machine Translation for Rare Languages\",\n    author = \"Garcia, Xavier  and\n      Siddhant, Aditya  and\n      Firat, Orhan  and\n      Parikh, Ankur\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.89/\",\n    doi = \"10.18653/v1/2021.naacl-main.89\",\n    pages = \"1126--1137\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.89.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.89/",
        "pdf_size": 334695,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7341317319770727801&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4
    },
    {
        "id": "2021.naacl-main.164",
        "title": "Heterogeneous Graph Neural Networks for Concept Prerequisite Relation Learning in Educational Data",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Prerequisite relations among concepts are crucial for educational applications, such as curriculum planning and intelligent tutoring. In this paper, we propose a novel concept prerequisite relation learning approach, named CPRL, which combines both concept representation learned from a heterogeneous graph and concept pairwise features. Furthermore, we extend CPRL under weakly supervised settings to make our method more practical, including learning prerequisite relations from learning object dependencies and generating training data with data programming. Our experiments on four datasets show that the proposed approach achieves the state-of-the-art results comparing with existing methods.",
        "author": "Chenghao Jia; Yongliang Shen; Yechun Tang; Lu Sun; Weiming Lu",
        "authorids": "/c/chenghao-jia/; /y/yongliang-shen/; /y/yechun-tang/; /l/lu-sun/; /w/weiming-lu/",
        "bibtex": "@inproceedings{jia-etal-2021-heterogeneous,\n    title = \"Heterogeneous Graph Neural Networks for Concept Prerequisite Relation Learning in Educational Data\",\n    author = \"Jia, Chenghao  and\n      Shen, Yongliang  and\n      Tang, Yechun  and\n      Sun, Lu  and\n      Lu, Weiming\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.164/\",\n    doi = \"10.18653/v1/2021.naacl-main.164\",\n    pages = \"2036--2047\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.164.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.164/",
        "pdf_size": 1241828,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10774782331841295871&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "College of Computer Science and Technology, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China",
        "aff_domain": "zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn",
        "email": "zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Zhejiang University",
        "aff_unique_dep": "College of Computer Science and Technology",
        "aff_unique_url": "http://www.zju.edu.cn",
        "aff_unique_abbr": "ZJU",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Hangzhou",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2021.naacl-main.449",
        "title": "Hierarchical Transformer for Task Oriented Dialog Systems",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Generative models for dialog systems have gained much interest because of the recent success of RNN and Transformer based models in tasks like question answering and summarization. Although the task of dialog response generation is generally seen as a sequence to sequence (Seq2Seq) problem, researchers in the past have found it challenging to train dialog systems using the standard Seq2Seq models. Therefore, to help the model learn meaningful utterance and conversation level features, Sordoni et al. (2015b), Serban et al. (2016) proposed Hierarchical RNN architecture, which was later adopted by several other RNN based dialog systems. With the transformer-based models dominating the seq2seq problems lately, the natural question to ask is the applicability of the notion of hierarchy in transformer-based dialog systems. In this paper, we propose a generalized framework for Hierarchical Transformer Encoders and show how a standard transformer can be morphed into any hierarchical encoder, including HRED and HIBERT like models, by using specially designed attention masks and positional encodings. We demonstrate that Hierarchical Encoding helps achieve better natural language understanding of the contexts in transformer-based models for task-oriented dialog systems through a wide range of experiments.",
        "author": "Bishal Santra; Potnuru Anusha; Pawan Goyal",
        "authorids": "/b/bishal-santra/; /p/potnuru-anusha/; /p/pawan-goyal/",
        "bibtex": "@inproceedings{santra-etal-2021-hierarchical,\n    title = \"Hierarchical Transformer for Task Oriented Dialog Systems\",\n    author = \"Santra, Bishal  and\n      Anusha, Potnuru  and\n      Goyal, Pawan\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.449/\",\n    doi = \"10.18653/v1/2021.naacl-main.449\",\n    pages = \"5649--5658\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.449.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.449/",
        "pdf_size": 667786,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12517503373189041742&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Computer Science and Engineering Dept., Indian Institute of Technology Kharagpur; Computer Science and Engineering Dept., Indian Institute of Technology Kharagpur; Computer Science and Engineering Dept., Indian Institute of Technology Kharagpur",
        "aff_domain": "gmail.com;gmail.com;cse.iitkgp.ac.in",
        "email": "gmail.com;gmail.com;cse.iitkgp.ac.in",
        "github": "https://github.com/bsantraigi/HIER; https://github.com/bsantraigi/hier-transformer-pytorch",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Indian Institute of Technology Kharagpur",
        "aff_unique_dep": "Computer Science and Engineering Dept.",
        "aff_unique_url": "https://www.iitkgp.ac.in",
        "aff_unique_abbr": "IIT Kharagpur",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Kharagpur",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2021.naacl-main.187",
        "title": "Highly Efficient Knowledge Graph Embedding Learning with Orthogonal Procrustes Analysis",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Knowledge Graph Embeddings (KGEs) have been intensively explored in recent years due to their promise for a wide range of applications. However, existing studies focus on improving the final model performance without acknowledging the computational cost of the proposed approaches, in terms of execution time and environmental impact. This paper proposes a simple yet effective KGE framework which can reduce the training time and carbon footprint by orders of magnitudes compared with state-of-the-art approaches, while producing competitive performance. We highlight three technical innovations: full batch learning via relational matrices, closed-form Orthogonal Procrustes Analysis for KGEs, and non-negative-sampling training. In addition, as the first KGE method whose entity embeddings also store full relation information, our trained models encode rich semantics and are highly interpretable. Comprehensive experiments and ablation studies involving 13 strong baselines and two standard datasets verify the effectiveness and efficiency of our algorithm.",
        "author": "Xutan Peng; Guanyi Chen; Chenghua Lin; Mark Stevenson",
        "authorids": "/x/xutan-peng/; /g/guanyi-chen/; /c/chenghua-lin/; /m/mark-stevenson/",
        "bibtex": "@inproceedings{peng-etal-2021-highly,\n    title = \"Highly Efficient Knowledge Graph Embedding Learning with {O}rthogonal {P}rocrustes {A}nalysis\",\n    author = \"Peng, Xutan  and\n      Chen, Guanyi  and\n      Lin, Chenghua  and\n      Stevenson, Mark\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.187/\",\n    doi = \"10.18653/v1/2021.naacl-main.187\",\n    pages = \"2364--2375\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.187.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.187/",
        "pdf_size": 2750932,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10329773261392225563&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, The University of Shef\ufb01eld; Department of Information and Computing Sciences, Utrecht University; Department of Computer Science, The University of Shef\ufb01eld; Department of Computer Science, The University of Shef\ufb01eld",
        "aff_domain": "shef.ac.uk;uu.nl;shef.ac.uk;shef.ac.uk",
        "email": "shef.ac.uk;uu.nl;shef.ac.uk;shef.ac.uk",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "The University of Sheffield;Utrecht University",
        "aff_unique_dep": "Department of Computer Science;Department of Information and Computing Sciences",
        "aff_unique_url": "https://www.sheffield.ac.uk;https://www.uu.nl",
        "aff_unique_abbr": "Sheffield;UU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "United Kingdom;Netherlands"
    },
    {
        "id": "2021.naacl-main.350",
        "title": "How (Non-)Optimal is the Lexicon?",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The mapping of lexical meanings to wordforms is a major feature of natural languages. While usage pressures might assign short words to frequent meanings (Zipf\u2019s law of abbreviation), the need for a productive and open-ended vocabulary, local constraints on sequences of symbols, and various other factors all shape the lexicons of the world\u2019s languages. Despite their importance in shaping lexical structure, the relative contributions of these factors have not been fully quantified. Taking a coding-theoretic view of the lexicon and making use of a novel generative statistical model, we define upper bounds for the compressibility of the lexicon under various constraints. Examining corpora from 7 typologically diverse languages, we use those upper bounds to quantify the lexicon\u2019s optimality and to explore the relative costs of major constraints on natural codes. We find that (compositional) morphology and graphotactics can sufficiently account for most of the complexity of natural codes\u2014as measured by code length.",
        "author": "Tiago Pimentel; Irene Nikkarinen; Kyle Mahowald; Ryan Cotterell; Dami\u00e1n Blasi",
        "authorids": "/t/tiago-pimentel/; /i/irene-nikkarinen/; /k/kyle-mahowald/; /r/ryan-cotterell/; /d/damian-blasi/",
        "bibtex": "@inproceedings{pimentel-etal-2021-non,\n    title = \"How (Non-)Optimal is the Lexicon?\",\n    author = \"Pimentel, Tiago  and\n      Nikkarinen, Irene  and\n      Mahowald, Kyle  and\n      Cotterell, Ryan  and\n      Blasi, Dami{\\'a}n\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.350/\",\n    doi = \"10.18653/v1/2021.naacl-main.350\",\n    pages = \"4426--4438\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.350.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.350/",
        "pdf_size": 829625,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5989479426370130370&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "University of Cambridge; University of Cambridge; University of California, Santa Barbara; ETH Z\u00fcrich; Harvard University + MPI for Evolutionary Anthropology + HSE University",
        "aff_domain": "cam.ac.uk;gmail.com;ucsb.edu;inf.ethz.ch;fas.harvard.edu",
        "email": "cam.ac.uk;gmail.com;ucsb.edu;inf.ethz.ch;fas.harvard.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;2;3+4+5",
        "aff_unique_norm": "University of Cambridge;University of California, Santa Barbara;ETH Z\u00fcrich;Harvard University;Max Planck Institute for Evolutionary Anthropology;Higher School of Economics",
        "aff_unique_dep": ";;;;Evolutionary Anthropology;",
        "aff_unique_url": "https://www.cam.ac.uk;https://www.ucsb.edu;https://www.ethz.ch;https://www.harvard.edu;https://www.eva.mpg.de;https://hse.ru",
        "aff_unique_abbr": "Cambridge;UCSB;ETHZ;Harvard;MPI-EVA;HSE",
        "aff_campus_unique_index": "0;0;1;",
        "aff_campus_unique": "Cambridge;Santa Barbara;",
        "aff_country_unique_index": "0;0;1;2;1+3+4",
        "aff_country_unique": "United Kingdom;United States;Switzerland;Germany;Russia"
    },
    {
        "id": "2021.naacl-main.121",
        "title": "How Robust are Fact Checking Systems on Colloquial Claims?",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Knowledge is now starting to power neural dialogue agents. At the same time, the risk of misinformation and disinformation from dialogue agents also rises. Verifying the veracity of information from formal sources are widely studied in computational fact checking. In this work, we ask: How robust are fact checking systems on claims in colloquial style? We aim to open up new discussions in the intersection of fact verification and dialogue safety. In order to investigate how fact checking systems behave on colloquial claims, we transfer the styles of claims from FEVER (Thorne et al., 2018) into colloquialism. We find that existing fact checking systems that perform well on claims in formal style significantly degenerate on colloquial claims with the same semantics. Especially, we show that document retrieval is the weakest spot in the system even vulnerable to filler words, such as \u201cyeah\u201d and \u201cyou know\u201d. The document recall of WikiAPI retriever (Hanselowski et al., 2018) which is 90.0% on FEVER, drops to 72.2% on the colloquial claims. We compare the characteristics of colloquial claims to those of claims in formal style, and demonstrate the challenging issues in them.",
        "author": "Byeongchang Kim; Hyunwoo Kim; Seokhee Hong; Gunhee Kim",
        "authorids": "/b/byeongchang-kim/; /h/hyunwoo-kim/; /s/seokhee-hong/; /g/gunhee-kim/",
        "bibtex": "@inproceedings{kim-etal-2021-robust,\n    title = \"How Robust are Fact Checking Systems on Colloquial Claims?\",\n    author = \"Kim, Byeongchang  and\n      Kim, Hyunwoo  and\n      Hong, Seokhee  and\n      Kim, Gunhee\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.121/\",\n    doi = \"10.18653/v1/2021.naacl-main.121\",\n    pages = \"1535--1548\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.121.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.121/",
        "pdf_size": 508638,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2538213792227305524&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Seoul National University; Seoul National University; Seoul National University; Seoul National University",
        "aff_domain": "vl.snu.ac.kr;vl.snu.ac.kr;vl.snu.ac.kr;snu.ac.kr",
        "email": "vl.snu.ac.kr;vl.snu.ac.kr;vl.snu.ac.kr;snu.ac.kr",
        "github": "",
        "project": "https://vl.snu.ac.kr/projects/colloquial-claims",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Seoul National University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.snu.ac.kr",
        "aff_unique_abbr": "SNU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2021.naacl-main.322",
        "title": "How low is too low? A monolingual take on lemmatisation in Indian languages",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Lemmatization aims to reduce the sparse data problem by relating the inflected forms of a word to its dictionary form. Most prior work on ML based lemmatization has focused on high resource languages, where data sets (word forms) are readily available. For languages which have no linguistic work available, especially on morphology or in languages where the computational realization of linguistic rules is complex and cumbersome, machine learning based lemmatizers are the way togo. In this paper, we devote our attention to lemmatisation for low resource, morphologically rich scheduled Indian languages using neural methods. Here, low resource means only a small number of word forms are available. We perform tests to analyse the variance in monolingual models\u2019 performance on varying the corpus size and contextual morphological tag data for training. We show that monolingual approaches with data augmentation can give competitive accuracy even in the low resource setting, which augurs well for NLP in low resource setting.",
        "author": "Kumar Saunack; Kumar Saurav; Pushpak Bhattacharyya",
        "authorids": "/k/kumar-saunack/; /k/kumar-saurav/; /p/pushpak-bhattacharyya/",
        "bibtex": "@inproceedings{saunack-etal-2021-low,\n    title = \"How low is too low? A monolingual take on lemmatisation in {I}ndian languages\",\n    author = \"Saunack, Kumar  and\n      Saurav, Kumar  and\n      Bhattacharyya, Pushpak\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.322/\",\n    doi = \"10.18653/v1/2021.naacl-main.322\",\n    pages = \"4088--4094\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.322.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.322/",
        "pdf_size": 186678,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3976558759585706920&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "IIT Bombay; IIT Bombay; IIT Bombay",
        "aff_domain": "cse.iitb.ac.in;cse.iitb.ac.in;cse.iitb.ac.in",
        "email": "cse.iitb.ac.in;cse.iitb.ac.in;cse.iitb.ac.in",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Indian Institute of Technology Bombay",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iitb.ac.in",
        "aff_unique_abbr": "IITB",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Mumbai",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2021.naacl-main.208",
        "title": "How many data points is a prompt worth?",
        "track": "main",
        "status": "Long",
        "award": true,
        "abstract": "When fine-tuning pretrained models for classification, researchers either use a generic model head or a task-specific prompt for prediction. Proponents of prompting have argued that prompts provide a method for injecting task-specific guidance, which is beneficial in low-data regimes. We aim to quantify this benefit through rigorous testing of prompts in a fair setting: comparing prompted and head-based fine-tuning in equal conditions across many tasks and data sizes. By controlling for many sources of advantage, we find that prompting does indeed provide a benefit, and that this benefit can be quantified per task. Results show that prompting is often worth 100s of data points on average across classification tasks.",
        "author": "Teven Le Scao; Alexander Rush",
        "authorids": "/t/teven-le-scao/; /a/alexander-m-rush/",
        "bibtex": "@inproceedings{le-scao-rush-2021-many,\n    title = \"How many data points is a prompt worth?\",\n    author = \"Le Scao, Teven  and\n      Rush, Alexander\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.208/\",\n    doi = \"10.18653/v1/2021.naacl-main.208\",\n    pages = \"2627--2636\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.208.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.208/",
        "pdf_size": 1173150,
        "gs_citation": 320,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8360782659116324744&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Hugging Face; Hugging Face",
        "aff_domain": "huggingface.co;huggingface.co",
        "email": "huggingface.co;huggingface.co",
        "github": "https://github.com/TevenLeScao/pet",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Hugging Face",
        "aff_unique_dep": "",
        "aff_unique_url": "https://huggingface.co",
        "aff_unique_abbr": "Hugging Face",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.64",
        "title": "How to Motivate Your Dragon: Teaching Goal-Driven Agents to Speak and Act in Fantasy Worlds",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We seek to create agents that both act and communicate with other agents in pursuit of a goal. Towards this end, we extend LIGHT (Urbanek et al. 2019)\u2014a large-scale crowd-sourced fantasy text-game\u2014with a dataset of quests. These contain natural language motivations paired with in-game goals and human demonstrations; completing a quest might require dialogue or actions (or both). We introduce a reinforcement learning system that (1) incorporates large-scale language modeling-based and commonsense reasoning-based pre-training to imbue the agent with relevant priors; and (2) leverages a factorized action space of action commands and dialogue, balancing between the two. We conduct zero-shot evaluations using held-out human expert demonstrations, showing that our agents are able to act consistently and talk naturally with respect to their motivations.",
        "author": "Prithviraj Ammanabrolu; Jack Urbanek; Margaret Li; Arthur Szlam; Tim Rockt\u00e4schel; Jason Weston",
        "authorids": "/p/prithviraj-ammanabrolu/; /j/jack-urbanek/; /m/margaret-li/; /a/arthur-szlam/; /t/tim-rocktaschel/; /j/jason-weston/",
        "bibtex": "@inproceedings{ammanabrolu-etal-2021-motivate,\n    title = \"How to Motivate Your Dragon: Teaching Goal-Driven Agents to Speak and Act in Fantasy Worlds\",\n    author = {Ammanabrolu, Prithviraj  and\n      Urbanek, Jack  and\n      Li, Margaret  and\n      Szlam, Arthur  and\n      Rockt{\\\"a}schel, Tim  and\n      Weston, Jason},\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.64/\",\n    doi = \"10.18653/v1/2021.naacl-main.64\",\n    pages = \"807--833\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.64.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.64/",
        "pdf_size": 4966999,
        "gs_citation": 59,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16576913832120520895&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Georgia Institute of Technology; Facebook AI Research; Facebook AI Research; Facebook AI Research; Facebook AI Research; Facebook AI Research",
        "aff_domain": "gatech.edu; ; ; ; ; ",
        "email": "gatech.edu; ; ; ; ; ",
        "github": "",
        "project": "https://parl.ai/projects/light/language",
        "author_num": 6,
        "aff_unique_index": "0;1;1;1;1;1",
        "aff_unique_norm": "Georgia Institute of Technology;Facebook",
        "aff_unique_dep": ";Facebook AI Research",
        "aff_unique_url": "https://www.gatech.edu;https://research.facebook.com",
        "aff_unique_abbr": "Georgia Tech;FAIR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.61",
        "title": "Human-like informative conversations: Better acknowledgements using conditional mutual information",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "This work aims to build a dialogue agent that can weave new factual content into conversations as naturally as humans. We draw insights from linguistic principles of conversational analysis and annotate human-human conversations from the Switchboard Dialog Act Corpus to examine humans strategies for acknowledgement, transition, detail selection and presentation. When current chatbots (explicitly provided with new factual content) introduce facts into a conversation, their generated responses do not acknowledge the prior turns. This is because models trained with two contexts - new factual content and conversational history - generate responses that are non-specific w.r.t. one of the contexts, typically the conversational history. We show that specificity w.r.t. conversational history is better captured by pointwise conditional mutual information (pcmi_h) than by the established use of pointwise mutual information (pmi). Our proposed method, Fused-PCMI, trades off pmi for pcmi_h and is preferred by humans for overall quality over the Max-PMI baseline 60% of the time. Human evaluators also judge responses with higher pcmi_h better at acknowledgement 74% of the time. The results demonstrate that systems mimicking human conversational traits (in this case acknowledgement) improve overall quality and more broadly illustrate the utility of linguistic principles in improving dialogue agents.",
        "author": "Ashwin Paranjape; Christopher Manning",
        "authorids": "/a/ashwin-paranjape/; /c/christopher-d-manning/",
        "bibtex": "@inproceedings{paranjape-manning-2021-human,\n    title = \"Human-like informative conversations: Better acknowledgements using conditional mutual information\",\n    author = \"Paranjape, Ashwin  and\n      Manning, Christopher\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.61/\",\n    doi = \"10.18653/v1/2021.naacl-main.61\",\n    pages = \"768--781\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.61.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.61/",
        "pdf_size": 2465458,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=655025943526998165&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Stanford University; Stanford University",
        "aff_domain": "cs.stanford.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.393",
        "title": "Hurdles to Progress in Long-form Question Answering",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The task of long-form question answering (LFQA) involves retrieving documents relevant to a given question and using them to generate a paragraph-length answer. While many models have recently been proposed for LFQA, we show in this paper that the task formulation raises fundamental challenges regarding evaluation and dataset creation that currently preclude meaningful modeling progress. To demonstrate these challenges, we first design a new system that relies on sparse attention and contrastive retriever learning to achieve state-of-the-art performance on the ELI5 LFQA dataset. While our system tops the public leaderboard, a detailed analysis reveals several troubling trends: (1) our system\u2019s generated answers are not actually grounded in the documents that it retrieves; (2) ELI5 contains significant train / validation overlap, as at least 81% of ELI5 validation questions occur in paraphrased form in the training set; (3) ROUGE-L is not an informative metric of generated answer quality and can be easily gamed; and (4) human evaluations used for other text generation tasks are unreliable for LFQA. We offer suggestions to mitigate each of these issues, which we hope will lead to more rigorous LFQA research and meaningful progress in the future.",
        "author": "Kalpesh Krishna; Aurko Roy; Mohit Iyyer",
        "authorids": "/k/kalpesh-krishna/; /a/aurko-roy/; /m/mohit-iyyer/",
        "bibtex": "@inproceedings{krishna-etal-2021-hurdles,\n    title = \"Hurdles to Progress in Long-form Question Answering\",\n    author = \"Krishna, Kalpesh  and\n      Roy, Aurko  and\n      Iyyer, Mohit\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.393/\",\n    doi = \"10.18653/v1/2021.naacl-main.393\",\n    pages = \"4940--4957\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.393.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.393/",
        "pdf_size": 639453,
        "gs_citation": 199,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6003986962692654492&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "University of Massachusetts Amherst\u2660; Google Research\u2666; University of Massachusetts Amherst\u2660",
        "aff_domain": "cs.umass.edu;google.com;cs.umass.edu",
        "email": "cs.umass.edu;google.com;cs.umass.edu",
        "github": "https://github.com/martiansideofthemoon/hurdles-longform-qa",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Massachusetts Amherst;Google",
        "aff_unique_dep": ";Google Research",
        "aff_unique_url": "https://www.umass.edu;https://research.google",
        "aff_unique_abbr": "UMass Amherst;Google Research",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Amherst;Mountain View",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.212",
        "title": "Hyperparameter-free Continuous Learning for Domain Classification in Natural Language Understanding",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Domain classification is the fundamental task in natural language understanding (NLU), which often requires fast accommodation to new emerging domains. This constraint makes it impossible to retrain all previous domains, even if they are accessible to the new model. Most existing continual learning approaches suffer from low accuracy and performance fluctuation, especially when the distributions of old and new data are significantly different. In fact, the key real-world problem is not the absence of old data, but the inefficiency to retrain the model with the whole old dataset. Is it potential to utilize some old data to yield high accuracy and maintain stable performance, while at the same time, without introducing extra hyperparameters? In this paper, we proposed a hyperparameter-free continual learning model for text data that can stably produce high performance under various environments. Specifically, we utilize Fisher information to select exemplars that can \u201crecord\u201d key information of the original model. Also, a novel scheme called dynamical weight consolidation is proposed to enable hyperparameter-free learning during the retrain process. Extensive experiments demonstrate baselines provide fluctuated performance which makes them useless in practice. On the contrary, our proposed model significantly and consistently outperforms the best state-of-the-art method by up to 20% in average accuracy, and each of its component contributes effectively to overall performance.",
        "author": "Ting Hua; Yilin Shen; Changsheng Zhao; Yen-Chang Hsu; Hongxia Jin",
        "authorids": "/t/ting-hua/; /y/yilin-shen/; /c/changsheng-zhao/; /y/yen-chang-hsu/; /h/hongxia-jin/",
        "bibtex": "@inproceedings{hua-etal-2021-hyperparameter,\n    title = \"Hyperparameter-free Continuous Learning for Domain Classification in Natural Language Understanding\",\n    author = \"Hua, Ting  and\n      Shen, Yilin  and\n      Zhao, Changsheng  and\n      Hsu, Yen-Chang  and\n      Jin, Hongxia\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.212/\",\n    doi = \"10.18653/v1/2021.naacl-main.212\",\n    pages = \"2669--2678\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.212.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.212/",
        "pdf_size": 1082346,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4901973017238908379&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Samsung Research America; Samsung Research America; Samsung Research America; Samsung Research America; Samsung Research America",
        "aff_domain": "samsung.com;samsung.com;samsung.com;samsung.com;samsung.com",
        "email": "samsung.com;samsung.com;samsung.com;samsung.com;samsung.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Samsung Research America",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.samsung.com/us/careers/research/",
        "aff_unique_abbr": "SRA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.55",
        "title": "Identifying Helpful Sentences in Product Reviews",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In recent years online shopping has gained momentum and became an important venue for customers wishing to save time and simplify their shopping process. A key advantage of shopping online is the ability to read what other customers are saying about products of interest. In this work, we aim to maintain this advantage in situations where extreme brevity is needed, for example, when shopping by voice. We suggest a novel task of extracting a single representative helpful sentence from a set of reviews for a given product. The selected sentence should meet two conditions: first, it should be helpful for a purchase decision and second, the opinion it expresses should be supported by multiple reviewers. This task is closely related to the task of Multi Document Summarization in the product reviews domain but differs in its objective and its level of conciseness. We collect a dataset in English of sentence helpfulness scores via crowd-sourcing and demonstrate its reliability despite the inherent subjectivity involved. Next, we describe a complete model that extracts representative helpful sentences with positive and negative sentiment towards the product and demonstrate that it outperforms several baselines.",
        "author": "Iftah Gamzu; Hila Gonen; Gilad Kutiel; Ran Levy; Eugene Agichtein",
        "authorids": "/i/iftah-gamzu/; /h/hila-gonen/; /g/gilad-kutiel/; /r/ran-levy/; /e/eugene-agichtein/",
        "bibtex": "@inproceedings{gamzu-etal-2021-identifying,\n    title = \"Identifying Helpful Sentences in Product Reviews\",\n    author = \"Gamzu, Iftah  and\n      Gonen, Hila  and\n      Kutiel, Gilad  and\n      Levy, Ran  and\n      Agichtein, Eugene\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.55/\",\n    doi = \"10.18653/v1/2021.naacl-main.55\",\n    pages = \"678--691\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.55.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.55/",
        "pdf_size": 879764,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2974309403971050025&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Amazon, Tel-Aviv, Israel; Amazon, Tel-Aviv, Israel; Amazon, Tel-Aviv, Israel; Amazon, Tel-Aviv, Israel; Amazon, Seattle, WA, USA + Emory University, GA, USA",
        "aff_domain": "amazon.com;amazon.com;amazon.com;amazon.com;amazon.com",
        "email": "amazon.com;amazon.com;amazon.com;amazon.com;amazon.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0+1",
        "aff_unique_norm": "Amazon;Emory University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.amazon.com;https://www.emory.edu",
        "aff_unique_abbr": "Amazon;Emory",
        "aff_campus_unique_index": "0;0;0;0;1+2",
        "aff_campus_unique": "Tel-Aviv;Seattle;Georgia",
        "aff_country_unique_index": "0;0;0;0;1+1",
        "aff_country_unique": "Israel;United States"
    },
    {
        "id": "2021.naacl-main.347",
        "title": "Identifying Medical Self-Disclosure in Online Communities",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Self-disclosure in online health conversations may offer a host of benefits, including earlier detection and treatment of medical issues that may have otherwise gone unaddressed. However, research analyzing medical self-disclosure in online communities is limited. We address this shortcoming by introducing a new dataset of health-related posts collected from online social platforms, categorized into three groups (No Self-Disclosure, Possible Self-Disclosure, and Clear Self-Disclosure) with high inter-annotator agreement (_k_=0.88). We make this data available to the research community. We also release a predictive model trained on this dataset that achieves an accuracy of 81.02%, establishing a strong performance benchmark for this task.",
        "author": "Mina Valizadeh; Pardis Ranjbar-Noiey; Cornelia Caragea; Natalie Parde",
        "authorids": "/m/mina-valizadeh/; /p/pardis-ranjbar-noiey/; /c/cornelia-caragea/; /n/natalie-parde/",
        "bibtex": "@inproceedings{valizadeh-etal-2021-identifying,\n    title = \"Identifying Medical Self-Disclosure in Online Communities\",\n    author = \"Valizadeh, Mina  and\n      Ranjbar-Noiey, Pardis  and\n      Caragea, Cornelia  and\n      Parde, Natalie\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.347/\",\n    doi = \"10.18653/v1/2021.naacl-main.347\",\n    pages = \"4398--4408\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.347.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.347/",
        "pdf_size": 362241,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9701793514330591256&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science, University of Illinois at Chicago; Department of Computer Science, University of Illinois at Chicago; Department of Computer Science, University of Illinois at Chicago; Department of Computer Science, University of Illinois at Chicago",
        "aff_domain": "uic.edu;uic.edu;uic.edu;uic.edu",
        "email": "uic.edu;uic.edu;uic.edu;uic.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Illinois at Chicago",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.uic.edu",
        "aff_unique_abbr": "UIC",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Chicago",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-industry.2",
        "title": "Identifying and Resolving Annotation Changes for Natural Language Understanding",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Annotation conflict resolution is crucial towards building machine learning models with acceptable performance. Past work on annotation conflict resolution had assumed that data is collected at once, with a fixed set of annotators and fixed annotation guidelines. Moreover, previous work dealt with atomic labeling tasks. In this paper, we address annotation conflict resolution for Natural Language Understanding (NLU), a structured prediction task, in a real-world setting of commercial voice-controlled personal assistants, where (1) regular data collections are needed to support new and existing functionalities, (2) annotation guidelines evolve over time, and (3) the pool of annotators change across data collections. We devise an approach combining information-theoretic measures and a supervised neural model to resolve conflicts in data annotation. We evaluate our approach both intrinsically and extrinsically on a real-world dataset with 3.5M utterances of a commercial dialog system in German. Our approach leads to dramatic improvements over a majority baseline especially in contentious cases. On the NLU task, our approach achieves 2.75% error reduction over a no-resolution baseline.",
        "author": "Jose Garrido Ramas; Giorgio Pessot; Abdalghani Abujabal; Martin Rajman",
        "authorids": "/j/jose-garrido-ramas/; /g/giorgio-pessot/; /a/abdalghani-abujabal/; /m/martin-rajman/",
        "bibtex": "@inproceedings{garrido-ramas-etal-2021-identifying,\n    title = \"Identifying and Resolving Annotation Changes for Natural Language Understanding\",\n    author = \"Garrido Ramas, Jose  and\n      Pessot, Giorgio  and\n      Abujabal, Abdalghani  and\n      Rajman, Martin\",\n    editor = \"Kim, Young-bum  and\n      Li, Yunyao  and\n      Rambow, Owen\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-industry.2/\",\n    doi = \"10.18653/v1/2021.naacl-industry.2\",\n    pages = \"10--18\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-industry.2.pdf",
        "site": "https://aclanthology.org/2021.naacl-industry.2/",
        "pdf_size": 608171,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1255370646199659556&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Amazon Alexa AI, Germany; Amazon Alexa AI, Germany; Amazon Alexa AI, Germany; EPFL, Lausanne, Switzerland",
        "aff_domain": "amazon.de;amazon.de;amazon.de;epfl.ch",
        "email": "amazon.de;amazon.de;amazon.de;epfl.ch",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Amazon Alexa AI;\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne",
        "aff_unique_dep": "AI;",
        "aff_unique_url": "https://www.amazon.de/alexia;https://www.epfl.ch",
        "aff_unique_abbr": "Amazon Alexa AI;EPFL",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Lausanne",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "Germany;Switzerland"
    },
    {
        "id": "2021.naacl-main.390",
        "title": "Identifying inherent disagreement in natural language inference",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Natural language inference (NLI) is the task of determining whether a piece of text is entailed, contradicted by or unrelated to another piece of text. In this paper, we investigate how to tease systematic inferences (i.e., items for which people agree on the NLI label) apart from disagreement items (i.e., items which lead to different annotations), which most prior work has overlooked. To distinguish systematic inferences from disagreement items, we propose Artificial Annotators (AAs) to simulate the uncertainty in the annotation process by capturing the modes in annotations. Results on the CommitmentBank, a corpus of naturally occurring discourses in English, confirm that our approach performs statistically significantly better than all baselines. We further show that AAs learn linguistic patterns and context-dependent reasoning.",
        "author": "Xinliang Frederick Zhang; Marie-Catherine de Marneffe",
        "authorids": "/x/xinliang-frederick-zhang/; /m/marie-catherine-de-marneffe/",
        "bibtex": "@inproceedings{zhang-de-marneffe-2021-identifying,\n    title = \"Identifying inherent disagreement in natural language inference\",\n    author = \"Zhang, Xinliang Frederick  and\n      de Marneffe, Marie-Catherine\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.390/\",\n    doi = \"10.18653/v1/2021.naacl-main.390\",\n    pages = \"4908--4915\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.390.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.390/",
        "pdf_size": 333931,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7798776547696479296&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science and Engineering, The Ohio State University; Department of Linguistics, The Ohio State University",
        "aff_domain": "osu.edu;osu.edu",
        "email": "osu.edu;osu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "The Ohio State University",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.osu.edu",
        "aff_unique_abbr": "OSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.363",
        "title": "If You Want to Go Far Go Together: Unsupervised Joint Candidate Evidence Retrieval for Multi-hop Question Answering",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Multi-hop reasoning requires aggregation and inference from multiple facts. To retrieve such facts, we propose a simple approach that retrieves and reranks set of evidence facts jointly. Our approach first generates unsupervised clusters of sentences as candidate evidence by accounting links between sentences and coverage with the given query. Then, a RoBERTa-based reranker is trained to bring the most representative evidence cluster to the top. We specifically emphasize on the importance of retrieving evidence jointly by showing several comparative analyses to other methods that retrieve and rerank evidence sentences individually. First, we introduce several attention- and embedding-based analyses, which indicate that jointly retrieving and reranking approaches can learn compositional knowledge required for multi-hop reasoning. Second, our experiments show that jointly retrieving candidate evidence leads to substantially higher evidence retrieval performance when fed to the same supervised reranker. In particular, our joint retrieval and then reranking approach achieves new state-of-the-art evidence retrieval performance on two multi-hop question answering (QA) datasets: 30.5 Recall@2 on QASC, and 67.6% F1 on MultiRC. When the evidence text from our joint retrieval approach is fed to a RoBERTa-based answer selection classifier, we achieve new state-of-the-art QA performance on MultiRC and second best result on QASC.",
        "author": "Vikas Yadav; Steven Bethard; Mihai Surdeanu",
        "authorids": "/v/vikas-yadav/; /s/steven-bethard/; /m/mihai-surdeanu/",
        "bibtex": "@inproceedings{yadav-etal-2021-want,\n    title = \"If You Want to Go Far Go Together: Unsupervised Joint Candidate Evidence Retrieval for Multi-hop Question Answering\",\n    author = \"Yadav, Vikas  and\n      Bethard, Steven  and\n      Surdeanu, Mihai\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.363/\",\n    doi = \"10.18653/v1/2021.naacl-main.363\",\n    pages = \"4571--4581\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.363.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.363/",
        "pdf_size": 10485304,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2633781738896224681&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "University of Arizona, Tucson, AZ, USA; University of Arizona, Tucson, AZ, USA; University of Arizona, Tucson, AZ, USA",
        "aff_domain": "email.arizona.edu;email.arizona.edu;email.arizona.edu",
        "email": "email.arizona.edu;email.arizona.edu;email.arizona.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Arizona",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.arizona.edu",
        "aff_unique_abbr": "UA",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Tucson",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.238",
        "title": "Imperfect also Deserves Reward: Multi-Level and Sequential Reward Modeling for Better Dialog Management",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "For task-oriented dialog systems, training a Reinforcement Learning (RL) based Dialog Management module suffers from low sample efficiency and slow convergence speed due to the sparse rewards in RL. To solve this problem, many strategies have been proposed to give proper rewards when training RL, but their rewards lack interpretability and cannot accurately estimate the distribution of state-action pairs in real dialogs. In this paper, we propose a multi-level reward modeling approach that factorizes a reward into a three-level hierarchy: domain, act, and slot. Based on inverse adversarial reinforcement learning, our designed reward model can provide more accurate and explainable reward signals for state-action pairs. Extensive evaluations show that our approach can be applied to a wide range of reinforcement learning-based dialog systems and significantly improves both the performance and the speed of convergence.",
        "author": "Zhengxu Hou; Bang Liu; Ruihui Zhao; Zijing Ou; Yafei Liu; Xi Chen; Yefeng Zheng",
        "authorids": "/z/zhengxu-hou/; /b/bang-liu/; /r/ruihui-zhao/; /z/zijing-ou/; /y/yafei-liu/; /x/xi-chen/; /y/yefeng-zheng/",
        "bibtex": "https://aclanthology.org/2021.naacl-main.238.bib",
        "pdf": "https://aclanthology.org/2021.naacl-main.238.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.238/",
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9759484588065602393&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7
    },
    {
        "id": "2021.naacl-main.48",
        "title": "Implicitly Abusive Language \u2013 What does it actually look like and why are we not getting there?",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Abusive language detection is an emerging field in natural language processing which has received a large amount of attention recently. Still the success of automatic detection is limited. Particularly, the detection of implicitly abusive language, i.e. abusive language that is not conveyed by abusive words (e.g. dumbass or scum), is not working well. In this position paper, we explain why existing datasets make learning implicit abuse difficult and what needs to be changed in the design of such datasets. Arguing for a divide-and-conquer strategy, we present a list of subtypes of implicitly abusive language and formulate research tasks and questions for future research.",
        "author": "Michael Wiegand; Josef Ruppenhofer; Elisabeth Eder",
        "authorids": "/m/michael-wiegand/; /j/josef-ruppenhofer/; /e/elisabeth-eder/",
        "bibtex": "@inproceedings{wiegand-etal-2021-implicitly-abusive,\n    title = \"Implicitly Abusive Language {--} What does it actually look like and why are we not getting there?\",\n    author = \"Wiegand, Michael  and\n      Ruppenhofer, Josef  and\n      Eder, Elisabeth\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.48/\",\n    doi = \"10.18653/v1/2021.naacl-main.48\",\n    pages = \"576--587\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.48.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.48/",
        "pdf_size": 325655,
        "gs_citation": 79,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14794908603598476530&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Digital Age Research Center (D!ARC), Alpen-Adria-Universit\u00e4t Klagenfurt; Leibniz Institute for German Language; Institut f\u00fcr Germanistik, Alpen-Adria-Universit\u00e4t Klagenfurt",
        "aff_domain": "aau.at;ids-mannheim.de;aau.at",
        "email": "aau.at;ids-mannheim.de;aau.at",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Alpen-Adria-Universit\u00e4t Klagenfurt;Leibniz Institute for German Language",
        "aff_unique_dep": "Digital Age Research Center (D!ARC);",
        "aff_unique_url": "https://www.aau.at;https://www.leibniz-institut.de",
        "aff_unique_abbr": "AAU;IFS",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Klagenfurt",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Austria;Germany"
    },
    {
        "id": "2021.naacl-main.82",
        "title": "Improving Cross-Modal Alignment in Vision Language Navigation via Syntactic Information",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Vision language navigation is the task that requires an agent to navigate through a 3D environment based on natural language instructions. One key challenge in this task is to ground instructions with the current visual information that the agent perceives. Most of the existing work employs soft attention over individual words to locate the instruction required for the next action. However, different words have different functions in a sentence (e.g., modifiers convey attributes, verbs convey actions). Syntax information like dependencies and phrase structures can aid the agent to locate important parts of the instruction. Hence, in this paper, we propose a navigation agent that utilizes syntax information derived from a dependency tree to enhance alignment between the instruction and the current visual scenes. Empirically, our agent outperforms the baseline model that does not use syntax information on the Room-to-Room dataset, especially in the unseen environment. Besides, our agent achieves the new state-of-the-art on Room-Across-Room dataset, which contains instructions in 3 languages (English, Hindi, and Telugu). We also show that our agent is better at aligning instructions with the current visual information via qualitative visualizations.",
        "author": "Jialu Li; Hao Tan; Mohit Bansal",
        "authorids": "/j/jialu-li/; /h/hao-tan/; /m/mohit-bansal/",
        "bibtex": "@inproceedings{li-etal-2021-improving,\n    title = \"Improving Cross-Modal Alignment in Vision Language Navigation via Syntactic Information\",\n    author = \"Li, Jialu  and\n      Tan, Hao  and\n      Bansal, Mohit\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.82/\",\n    doi = \"10.18653/v1/2021.naacl-main.82\",\n    pages = \"1041--1050\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.82.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.82/",
        "pdf_size": 1277226,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7489422275095957882&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "UNC Chapel Hill; UNC Chapel Hill; UNC Chapel Hill",
        "aff_domain": "cs.unc.edu;cs.unc.edu;cs.unc.edu",
        "email": "cs.unc.edu;cs.unc.edu;cs.unc.edu",
        "github": "https://github.com/jialuli-luka/SyntaxVLN",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of North Carolina at Chapel Hill",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.unc.edu",
        "aff_unique_abbr": "UNC",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Chapel Hill",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-demos.10",
        "title": "Improving Evidence Retrieval for Automated Explainable Fact-Checking",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "Automated fact-checking on a large-scale is a challenging task that has not been studied systematically until recently. Large noisy document collections like the web or news articles make the task more difficult. We describe a three-stage automated fact-checking system, named Quin+, using evidence retrieval and selection methods. We demonstrate that using dense passage representations leads to much higher evidence recall in a noisy setting. We also propose two sentence selection approaches, an embedding-based selection using a dense retrieval model, and a sequence labeling approach for context-aware selection. Quin+ is able to verify open-domain claims using results from web search engines.",
        "author": "Chris Samarinas; Wynne Hsu; Mong Li Lee",
        "authorids": "/c/chris-samarinas/; /w/wynne-hsu/; /m/mong-li-lee/",
        "bibtex": "@inproceedings{samarinas-etal-2021-improving,\n    title = \"Improving Evidence Retrieval for Automated Explainable Fact-Checking\",\n    author = \"Samarinas, Chris  and\n      Hsu, Wynne  and\n      Lee, Mong Li\",\n    editor = \"Sil, Avi  and\n      Lin, Xi Victoria\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-demos.10/\",\n    doi = \"10.18653/v1/2021.naacl-demos.10\",\n    pages = \"84--91\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-demos.10.pdf",
        "site": "https://aclanthology.org/2021.naacl-demos.10/",
        "pdf_size": 452652,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5373200948314638613&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3
    },
    {
        "id": "2021.naacl-main.416",
        "title": "Improving Factual Completeness and Consistency of Image-to-Text Radiology Report Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Neural image-to-text radiology report generation systems offer the potential to improve radiology reporting by reducing the repetitive process of report drafting and identifying possible medical errors. However, existing report generation systems, despite achieving high performances on natural language generation metrics such as CIDEr or BLEU, still suffer from incomplete and inconsistent generations. Here we introduce two new simple rewards to encourage the generation of factually complete and consistent radiology reports: one that encourages the system to generate radiology domain entities consistent with the reference, and one that uses natural language inference to encourage these entities to be described in inferentially consistent ways. We combine these with the novel use of an existing semantic equivalence metric (BERTScore). We further propose a report generation system that optimizes these rewards via reinforcement learning. On two open radiology report datasets, our system substantially improved the F1 score of a clinical information extraction performance by +22.1 (Delta +63.9%). We further show via a human evaluation and a qualitative analysis that our system leads to generations that are more factually complete and consistent compared to the baselines.",
        "author": "Yasuhide Miura; Yuhao Zhang; Emily Tsai; Curtis Langlotz; Dan Jurafsky",
        "authorids": "/y/yasuhide-miura/; /y/yuhao-zhang/; /e/emily-tsai/; /c/curtis-langlotz/; /d/dan-jurafsky/",
        "bibtex": "@inproceedings{miura-etal-2021-improving,\n    title = \"Improving Factual Completeness and Consistency of Image-to-Text Radiology Report Generation\",\n    author = \"Miura, Yasuhide  and\n      Zhang, Yuhao  and\n      Tsai, Emily  and\n      Langlotz, Curtis  and\n      Jurafsky, Dan\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.416/\",\n    doi = \"10.18653/v1/2021.naacl-main.416\",\n    pages = \"5288--5304\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.416.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.416/",
        "pdf_size": 1222324,
        "gs_citation": 195,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9652362911261475334&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Stanford University; Stanford University; Stanford University; Stanford University; Stanford University",
        "aff_domain": "stanford.edu;stanford.edu;stanford.edu;stanford.edu;stanford.edu",
        "email": "stanford.edu;stanford.edu;stanford.edu;stanford.edu;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.475",
        "title": "Improving Faithfulness in Abstractive Summarization with Contrast Candidate Generation and Selection",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Despite significant progress in neural abstractive summarization, recent studies have shown that the current models are prone to generating summaries that are unfaithful to the original context. To address the issue, we study contrast candidate generation and selection as a model-agnostic post-processing technique to correct the extrinsic hallucinations (i.e. information not present in the source text) in unfaithful summaries. We learn a discriminative correction model by generating alternative candidate summaries where named entities and quantities in the generated summary are replaced with ones with compatible semantic types from the source document. This model is then used to select the best candidate as the final output summary. Our experiments and analysis across a number of neural summarization systems show that our proposed method is effective in identifying and correcting extrinsic hallucinations. We analyze the typical hallucination phenomenon by different types of neural summarization systems, in hope to provide insights for future work on the direction.",
        "author": "Sihao Chen; Fan Zhang; Kazoo Sone; Dan Roth",
        "authorids": "/s/sihao-chen/; /f/fan-zhang/; /k/kazoo-sone/; /d/dan-roth/",
        "bibtex": "@inproceedings{chen-etal-2021-improving,\n    title = \"Improving Faithfulness in Abstractive Summarization with Contrast Candidate Generation and Selection\",\n    author = \"Chen, Sihao  and\n      Zhang, Fan  and\n      Sone, Kazoo  and\n      Roth, Dan\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.475/\",\n    doi = \"10.18653/v1/2021.naacl-main.475\",\n    pages = \"5935--5941\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.475.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.475/",
        "pdf_size": 279993,
        "gs_citation": 118,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6493464886301633601&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "University of Pennsylvania; Google; Google; University of Pennsylvania",
        "aff_domain": "cis.upenn.edu;google.com;google.com;cis.upenn.edu",
        "email": "cis.upenn.edu;google.com;google.com;cis.upenn.edu",
        "github": "",
        "project": "http://cogcomp.org/page/publication_view/938",
        "author_num": 4,
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "University of Pennsylvania;Google",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.upenn.edu;https://www.google.com",
        "aff_unique_abbr": "UPenn;Google",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.194",
        "title": "Improving Generation and Evaluation of Visual Stories via Semantic Consistency",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Story visualization is an underexplored task that falls at the intersection of many important research directions in both computer vision and natural language processing. In this task, given a series of natural language captions which compose a story, an agent must generate a sequence of images that correspond to the captions. Prior work has introduced recurrent generative models which outperform text-to-image synthesis models on this task. However, there is room for improvement of generated images in terms of visual quality, coherence and relevance. We present a number of improvements to prior modeling approaches, including (1) the addition of a dual learning framework that utilizes video captioning to reinforce the semantic alignment between the story and generated images, (2) a copy-transform mechanism for sequentially-consistent story visualization, and (3) MART-based transformers to model complex interactions between frames. We present ablation studies to demonstrate the effect of each of these techniques on the generative power of the model for both individual images as well as the entire narrative. Furthermore, due to the complexity and generative nature of the task, standard evaluation metrics do not accurately reflect performance. Therefore, we also provide an exploration of evaluation metrics for the model, focused on aspects of the generated frames such as the presence/quality of generated characters, the relevance to captions, and the diversity of the generated images. We also present correlation experiments of our proposed automated metrics with human evaluations.",
        "author": "Adyasha Maharana; Darryl Hannan; Mohit Bansal",
        "authorids": "/a/adyasha-maharana/; /d/darryl-hannan/; /m/mohit-bansal/",
        "bibtex": "@inproceedings{maharana-etal-2021-improving,\n    title = \"Improving Generation and Evaluation of Visual Stories via Semantic Consistency\",\n    author = \"Maharana, Adyasha  and\n      Hannan, Darryl  and\n      Bansal, Mohit\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.194/\",\n    doi = \"10.18653/v1/2021.naacl-main.194\",\n    pages = \"2427--2442\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.194.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.194/",
        "pdf_size": 13957341,
        "gs_citation": 59,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16731966601478598296&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Computer Science, University of North Carolina at Chapel Hill; Department of Computer Science, University of North Carolina at Chapel Hill; Department of Computer Science, University of North Carolina at Chapel Hill",
        "aff_domain": "cs.unc.edu;cs.unc.edu;cs.unc.edu",
        "email": "cs.unc.edu;cs.unc.edu;cs.unc.edu",
        "github": "https://github.com/adymaharana/StoryViz",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of North Carolina at Chapel Hill",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.unc.edu",
        "aff_unique_abbr": "UNC Chapel Hill",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Chapel Hill",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.127",
        "title": "Improving Neural RST Parsing Model with Silver Agreement Subtrees",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Most of the previous Rhetorical Structure Theory (RST) parsing methods are based on supervised learning such as neural networks, that require an annotated corpus of sufficient size and quality. However, the RST Discourse Treebank (RST-DT), the benchmark corpus for RST parsing in English, is small due to the costly annotation of RST trees. The lack of large annotated training data causes poor performance especially in relation labeling. Therefore, we propose a method for improving neural RST parsing models by exploiting silver data, i.e., automatically annotated data. We create large-scale silver data from an unlabeled corpus by using a state-of-the-art RST parser. To obtain high-quality silver data, we extract agreement subtrees from RST trees for documents built using the RST parsers. We then pre-train a neural RST parser with the obtained silver data and fine-tune it on the RST-DT. Experimental results show that our method achieved the best micro-F1 scores for Nuclearity and Relation at 75.0 and 63.2, respectively. Furthermore, we obtained a remarkable gain in the Relation score, 3.0 points, against the previous state-of-the-art parser.",
        "author": "Naoki Kobayashi; Tsutomu Hirao; Hidetaka Kamigaito; Manabu Okumura; Masaaki Nagata",
        "authorids": "/n/naoki-kobayashi/; /t/tsutomu-hirao/; /h/hidetaka-kamigaito/; /m/manabu-okumura/; /m/masaaki-nagata/",
        "bibtex": "@inproceedings{kobayashi-etal-2021-improving,\n    title = \"Improving Neural {RST} Parsing Model with Silver Agreement Subtrees\",\n    author = \"Kobayashi, Naoki  and\n      Hirao, Tsutomu  and\n      Kamigaito, Hidetaka  and\n      Okumura, Manabu  and\n      Nagata, Masaaki\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.127/\",\n    doi = \"10.18653/v1/2021.naacl-main.127\",\n    pages = \"1600--1612\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.127.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.127/",
        "pdf_size": 531861,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2993069157855837692&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Institute of Innovative Research, Tokyo Institute of Technology; NTT Communication Science Laboratories, NTT Corporation; Institute of Innovative Research, Tokyo Institute of Technology; Institute of Innovative Research, Tokyo Institute of Technology; NTT Communication Science Laboratories, NTT Corporation",
        "aff_domain": "lr.pi.titech.ac.jp;hco.ntt.co.jp;lr.pi.titech.ac.jp;pi.titech.ac.jp;hco.ntt.co.jp",
        "email": "lr.pi.titech.ac.jp;hco.ntt.co.jp;lr.pi.titech.ac.jp;pi.titech.ac.jp;hco.ntt.co.jp",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;0;0;1",
        "aff_unique_norm": "Tokyo Institute of Technology;NTT Corporation",
        "aff_unique_dep": "Institute of Innovative Research;Communication Science Laboratories",
        "aff_unique_url": "https://www.titech.ac.jp;https://www.ntt.co.jp",
        "aff_unique_abbr": "Titech;NTT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2021.naacl-main.83",
        "title": "Improving Pretrained Models for Zero-shot Multi-label Text Classification through Reinforced Label Hierarchy Reasoning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Exploiting label hierarchies has become a promising approach to tackling the zero-shot multi-label text classification (ZS-MTC) problem. Conventional methods aim to learn a matching model between text and labels, using a graph encoder to incorporate label hierarchies to obtain effective label representations (Rios and Kavuluru, 2018). More recently, pretrained models like BERT (Devlin et al., 2018) have been used to convert classification tasks into a textual entailment task (Yin et al., 2019). This approach is naturally suitable for the ZS-MTC task. However, pretrained models are underexplored in the existing work because they do not generate individual vector representations for text or labels, making it unintuitive to combine them with conventional graph encoding methods. In this paper, we explore to improve pretrained models with label hierarchies on the ZS-MTC task. We propose a Reinforced Label Hierarchy Reasoning (RLHR) approach to encourage interdependence among labels in the hierarchies during training. Meanwhile, to overcome the weakness of flat predictions, we design a rollback algorithm that can remove logical errors from predictions during inference. Experimental results on three real-life datasets show that our approach achieves better performance and outperforms previous non-pretrained methods on the ZS-MTC task.",
        "author": "Hui Liu; Danqing Zhang; Bing Yin; Xiaodan Zhu",
        "authorids": "/h/hui-liu/; /d/danqing-zhang/; /b/bing-yin/; /x/xiaodan-zhu/",
        "bibtex": "@inproceedings{liu-etal-2021-improving,\n    title = \"Improving Pretrained Models for Zero-shot Multi-label Text Classification through Reinforced Label Hierarchy Reasoning\",\n    author = \"Liu, Hui  and\n      Zhang, Danqing  and\n      Yin, Bing  and\n      Zhu, Xiaodan\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.83/\",\n    doi = \"10.18653/v1/2021.naacl-main.83\",\n    pages = \"1051--1062\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.83.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.83/",
        "pdf_size": 468951,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=497600916335045031&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Ingenuity Labs Research Institute & ECE, Queen\u2019s University, Canada; Amazon.com Inc, Palo Alto, CA, USA; Amazon.com Inc, Palo Alto, CA, USA; Ingenuity Labs Research Institute & ECE, Queen\u2019s University, Canada",
        "aff_domain": "queensu.ca;amazon.com;amazon.com;queensu.ca",
        "email": "queensu.ca;amazon.com;amazon.com;queensu.ca",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "Queen\u2019s University;Amazon.com Inc",
        "aff_unique_dep": "Ingenuity Labs Research Institute & ECE;",
        "aff_unique_url": "https://www.queensu.ca;https://www.amazon.com",
        "aff_unique_abbr": "Queen's U;Amazon",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Palo Alto",
        "aff_country_unique_index": "0;1;1;0",
        "aff_country_unique": "Canada;United States"
    },
    {
        "id": "2021.naacl-main.57",
        "title": "Improving Zero and Few-Shot Abstractive Summarization with Intermediate Fine-tuning and Data Augmentation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Models pretrained with self-supervised objectives on large text corpora achieve state-of-the-art performance on English text summarization tasks. However, these models are typically fine-tuned on hundreds of thousands of data points, an infeasible requirement when applying summarization to new, niche domains. In this work, we introduce a novel and generalizable method, called WikiTransfer, for fine-tuning pretrained models for summarization in an unsupervised, dataset-specific manner. WikiTransfer fine-tunes pretrained models on pseudo-summaries, produced from generic Wikipedia data, which contain characteristics of the target dataset, such as the length and level of abstraction of the desired summaries. WikiTransfer models achieve state-of-the-art, zero-shot abstractive summarization performance on the CNN-DailyMail dataset and demonstrate the effectiveness of our approach on three additional diverse datasets. These models are more robust to noisy data and also achieve better or comparable few-shot performance using 10 and 100 training examples when compared to few-shot transfer from other summarization datasets. To further boost performance, we employ data augmentation via round-trip translation as well as introduce a regularization term for improved few-shot transfer. To understand the role of dataset aspects in transfer performance and the quality of the resulting output summaries, we further study the effect of the components of our unsupervised fine-tuning data and analyze few-shot performance using both automatic and human evaluation.",
        "author": "Alexander Fabbri; Simeng Han; Haoyuan Li; Haoran Li; Marjan Ghazvininejad; Shafiq Joty; Dragomir Radev; Yashar Mehdad",
        "authorids": "/a/alexander-richard-fabbri/; /s/simeng-han/; /h/haoyuan-li/; /h/haoran-li/; /m/marjan-ghazvininejad/; /s/shafiq-joty/; /d/dragomir-radev/; /y/yashar-mehdad/",
        "bibtex": "@inproceedings{fabbri-etal-2021-improving,\n    title = \"Improving Zero and Few-Shot Abstractive Summarization with Intermediate Fine-tuning and Data Augmentation\",\n    author = \"Fabbri, Alexander  and\n      Han, Simeng  and\n      Li, Haoyuan  and\n      Li, Haoran  and\n      Ghazvininejad, Marjan  and\n      Joty, Shafiq  and\n      Radev, Dragomir  and\n      Mehdad, Yashar\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.57/\",\n    doi = \"10.18653/v1/2021.naacl-main.57\",\n    pages = \"704--717\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.57.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.57/",
        "pdf_size": 607344,
        "gs_citation": 117,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4345447600803400324&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Yale University; Nanyang Technological University; Renmin University of China; Facebook AI; Nanyang Technological University; Facebook AI; Yale University; Facebook AI",
        "aff_domain": "yale.edu;yale.edu;ntu.edu.sg;ruc.edu.cn;fb.com;fb.com;ntu.edu.sg;fb.com",
        "email": "yale.edu;yale.edu;ntu.edu.sg;ruc.edu.cn;fb.com;fb.com;ntu.edu.sg;fb.com",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;1;2;3;1;3;0;3",
        "aff_unique_norm": "Yale University;Nanyang Technological University;Renmin University of China;Facebook",
        "aff_unique_dep": ";;;Facebook AI",
        "aff_unique_url": "https://www.yale.edu;https://www.ntu.edu.sg;http://www.ruc.edu.cn;https://www.facebook.com",
        "aff_unique_abbr": "Yale;NTU;RUC;Facebook AI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;2;0;1;0;0;0",
        "aff_country_unique": "United States;Singapore;China"
    },
    {
        "id": "2021.naacl-main.465",
        "title": "Improving Zero-Shot Cross-lingual Transfer for Multilingual Question Answering over Knowledge Graph",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Multilingual question answering over knowledge graph (KGQA) aims to derive answers from a knowledge graph (KG) for questions in multiple languages. To be widely applicable, we focus on its zero-shot transfer setting. That is, we can only access training data in a high-resource language, while need to answer multilingual questions without any labeled data in target languages. A straightforward approach is resorting to pre-trained multilingual models (e.g., mBERT) for cross-lingual transfer, but there is a still significant gap of KGQA performance between source and target languages. In this paper, we exploit unsupervised bilingual lexicon induction (BLI) to map training questions in source language into those in target language as augmented training data, which circumvents language inconsistency between training and inference. Furthermore, we propose an adversarial learning strategy to alleviate syntax-disorder of the augmented data, making the model incline to both language- and syntax-independence. Consequently, our model narrows the gap in zero-shot cross-lingual transfer. Experiments on two multilingual KGQA datasets with 11 zero-resource languages verify its effectiveness.",
        "author": "Yucheng Zhou; Xiubo Geng; Tao Shen; Wenqiang Zhang; Daxin Jiang",
        "authorids": "/y/yucheng-zhou/; /x/xiubo-geng/; /t/tao-shen/; /w/wenqiang-zhang/; /d/daxin-jiang/",
        "bibtex": "@inproceedings{zhou-etal-2021-improving,\n    title = \"Improving Zero-Shot Cross-lingual Transfer for Multilingual Question Answering over Knowledge Graph\",\n    author = \"Zhou, Yucheng  and\n      Geng, Xiubo  and\n      Shen, Tao  and\n      Zhang, Wenqiang  and\n      Jiang, Daxin\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.465/\",\n    doi = \"10.18653/v1/2021.naacl-main.465\",\n    pages = \"5822--5834\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.465.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.465/",
        "pdf_size": 977017,
        "gs_citation": 90,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7072958587306147040&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Fudan University; Microsoft; Australian AI Institute, School of CS, FEIT, University of Technology Sydney; Fudan University; Microsoft",
        "aff_domain": "fudan.edu.cn;microsoft.com;student.uts.edu.au;fudan.edu.cn;microsoft.com",
        "email": "fudan.edu.cn;microsoft.com;student.uts.edu.au;fudan.edu.cn;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;2;0;1",
        "aff_unique_norm": "Fudan University;Microsoft Corporation;University of Technology Sydney",
        "aff_unique_dep": ";;School of Computer Science",
        "aff_unique_url": "https://www.fudan.edu.cn;https://www.microsoft.com;https://www.uts.edu.au",
        "aff_unique_abbr": "Fudan;Microsoft;UTS",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Sydney",
        "aff_country_unique_index": "0;1;2;0;1",
        "aff_country_unique": "China;United States;Australia"
    },
    {
        "id": "2021.naacl-main.16",
        "title": "Improving the Lexical Ability of Pretrained Language Models for Unsupervised Neural Machine Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Successful methods for unsupervised neural machine translation (UNMT) employ cross-lingual pretraining via self-supervision, often in the form of a masked language modeling or a sequence generation task, which requires the model to align the lexical- and high-level representations of the two languages. While cross-lingual pretraining works for similar languages with abundant corpora, it performs poorly in low-resource and distant languages. Previous research has shown that this is because the representations are not sufficiently aligned. In this paper, we enhance the bilingual masked language model pretraining with lexical-level information by using type-level cross-lingual subword embeddings. Empirical results demonstrate improved performance both on UNMT (up to 4.5 BLEU) and bilingual lexicon induction using our method compared to a UNMT baseline.",
        "author": "Alexandra Chronopoulou; Dario Stojanovski; Alexander Fraser",
        "authorids": "/a/alexandra-chronopoulou/; /d/dario-stojanovski/; /a/alexander-fraser/",
        "bibtex": "@inproceedings{chronopoulou-etal-2021-improving,\n    title = \"Improving the Lexical Ability of Pretrained Language Models for Unsupervised Neural Machine Translation\",\n    author = \"Chronopoulou, Alexandra  and\n      Stojanovski, Dario  and\n      Fraser, Alexander\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.16/\",\n    doi = \"10.18653/v1/2021.naacl-main.16\",\n    pages = \"173--180\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.16.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.16/",
        "pdf_size": 297239,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7381982316738863830&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Center for Information and Language Processing, LMU Munich, Germany; Center for Information and Language Processing, LMU Munich, Germany; Center for Information and Language Processing, LMU Munich, Germany",
        "aff_domain": "cis.lmu.de;cis.lmu.de;cis.lmu.de",
        "email": "cis.lmu.de;cis.lmu.de;cis.lmu.de",
        "github": "https://github.com/alexandra-chron/lexical_xlm_relm",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "LMU Munich",
        "aff_unique_dep": "Center for Information and Language Processing",
        "aff_unique_url": "https://www.lmu.de",
        "aff_unique_abbr": "LMU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Munich",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2021.naacl-main.224",
        "title": "Incorporating External Knowledge to Enhance Tabular Reasoning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Reasoning about tabular information presents unique challenges to modern NLP approaches which largely rely on pre-trained contextualized embeddings of text. In this paper, we study these challenges through the problem of tabular natural language inference. We propose easy and effective modifications to how information is presented to a model for this task. We show via systematic experiments that these strategies substantially improve tabular inference performance.",
        "author": "J. Neeraja; Vivek Gupta; Vivek Srikumar",
        "authorids": "/j/j-neeraja/; /v/vivek-gupta/; /v/vivek-srikumar/",
        "bibtex": "@inproceedings{neeraja-etal-2021-incorporating,\n    title = \"Incorporating External Knowledge to Enhance Tabular Reasoning\",\n    author = \"Neeraja, J.  and\n      Gupta, Vivek  and\n      Srikumar, Vivek\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.224/\",\n    doi = \"10.18653/v1/2021.naacl-main.224\",\n    pages = \"2799--2809\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.224.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.224/",
        "pdf_size": 336566,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3283691824010954884&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "IIT Guwahati; University of Utah; University of Utah",
        "aff_domain": "iitg.ac.in;cs.utah.edu;cs.utah.edu",
        "email": "iitg.ac.in;cs.utah.edu;cs.utah.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Indian Institute of Technology Guwahati;University of Utah",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.iitg.ac.in;https://www.utah.edu",
        "aff_unique_abbr": "IITG;Utah",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Guwahati;",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "India;United States"
    },
    {
        "id": "2021.naacl-main.125",
        "title": "Incorporating Syntax and Semantics in Coreference Resolution with Heterogeneous Graph Attention Network",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "External syntactic and semantic information has been largely ignored by existing neural coreference resolution models. In this paper, we present a heterogeneous graph-based model to incorporate syntactic and semantic structures of sentences. The proposed graph contains a syntactic sub-graph where tokens are connected based on a dependency tree, and a semantic sub-graph that contains arguments and predicates as nodes and semantic role labels as edges. By applying a graph attention network, we can obtain syntactically and semantically augmented word representation, which can be integrated using an attentive integration layer and gating mechanism. Experiments on the OntoNotes 5.0 benchmark show the effectiveness of our proposed model.",
        "author": "Fan Jiang; Trevor Cohn",
        "authorids": "/f/fan-jiang/; /t/trevor-cohn/",
        "bibtex": "@inproceedings{jiang-cohn-2021-incorporating,\n    title = \"Incorporating Syntax and Semantics in Coreference Resolution with Heterogeneous Graph Attention Network\",\n    author = \"Jiang, Fan  and\n      Cohn, Trevor\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.125/\",\n    doi = \"10.18653/v1/2021.naacl-main.125\",\n    pages = \"1584--1591\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.125.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.125/",
        "pdf_size": 506906,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17968243507593882488&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "School of Computing and Information Systems, The University of Melbourne, Victoria, Australia; School of Computing and Information Systems, The University of Melbourne, Victoria, Australia",
        "aff_domain": "student.unimelb.edu.au;unimelb.edu.au",
        "email": "student.unimelb.edu.au;unimelb.edu.au",
        "github": "https://github.com/Fantabulous-J/coref-HGAT",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "The University of Melbourne",
        "aff_unique_dep": "School of Computing and Information Systems",
        "aff_unique_url": "https://www.unimelb.edu.au",
        "aff_unique_abbr": "UniMelb",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Melbourne",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "2021.naacl-main.106",
        "title": "Incremental Few-shot Text Classification with Multi-round New Classes: Formulation, Dataset and System",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Text classification is usually studied by labeling natural language texts with relevant categories from a predefined set. In the real world, new classes might keep challenging the existing system with limited labeled data. The system should be intelligent enough to recognize upcoming new classes with a few examples. In this work, we define a new task in the NLP domain, incremental few-shot text classification, where the system incrementally handles multiple rounds of new classes. For each round, there is a batch of new classes with a few labeled examples per class. Two major challenges exist in this new task: (i) For the learning process, the system should incrementally learn new classes round by round without re-training on the examples of preceding classes; (ii) For the performance, the system should perform well on new classes without much loss on preceding classes. In addition to formulating the new task, we also release two benchmark datasets in the incremental few-shot setting: intent classification and relation classification. Moreover, we propose two entailment approaches, ENTAILMENT and HYBRID, which show promise for solving this novel problem.",
        "author": "Congying Xia; Wenpeng Yin; Yihao Feng; Philip Yu",
        "authorids": "/c/congying-xia/; /w/wenpeng-yin/; /y/yihao-feng/; /p/philip-s-yu/",
        "bibtex": "@inproceedings{xia-etal-2021-incremental,\n    title = \"Incremental Few-shot Text Classification with Multi-round New Classes: Formulation, Dataset and System\",\n    author = \"Xia, Congying  and\n      Yin, Wenpeng  and\n      Feng, Yihao  and\n      Yu, Philip\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.106/\",\n    doi = \"10.18653/v1/2021.naacl-main.106\",\n    pages = \"1351--1360\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.106.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.106/",
        "pdf_size": 371814,
        "gs_citation": 58,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16904534391737668674&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "University of Illinois at Chicago; Salesforce Research; University of Texas at Austin; University of Illinois at Chicago",
        "aff_domain": "uic.edu;salesforce.com;cs.utexas.edu;uic.edu",
        "email": "uic.edu;salesforce.com;cs.utexas.edu;uic.edu",
        "github": "https://github.com/congyingxia/IncrementalFSTC",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "University of Illinois at Chicago;Salesforce;University of Texas at Austin",
        "aff_unique_dep": ";Salesforce Research;",
        "aff_unique_url": "https://www.uic.edu;https://research.salesforce.com;https://www.utexas.edu",
        "aff_unique_abbr": "UIC;Salesforce;UT Austin",
        "aff_campus_unique_index": "0;2;0",
        "aff_campus_unique": "Chicago;;Austin",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.333",
        "title": "Inductive Topic Variational Graph Auto-Encoder for Text Classification",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Graph convolutional networks (GCNs) have been applied recently to text classification and produced an excellent performance. However, existing GCN-based methods do not assume an explicit latent semantic structure of documents, making learned representations less effective and difficult to interpret. They are also transductive in nature, thus cannot handle out-of-graph documents. To address these issues, we propose a novel model named inductive Topic Variational Graph Auto-Encoder (T-VGAE), which incorporates a topic model into variational graph-auto-encoder (VGAE) to capture the hidden semantic information between documents and words. T-VGAE inherits the interpretability of the topic model and the efficient information propagation mechanism of VGAE. It learns probabilistic representations of words and documents by jointly encoding and reconstructing the global word-level graph and bipartite graphs of documents, where each document is considered individually and decoupled from the global correlation graph so as to enable inductive learning. Our experiments on several benchmark datasets show that our method outperforms the existing competitive models on supervised and semi-supervised text classification, as well as unsupervised text representation learning. In addition, it has higher interpretability and is able to deal with unseen documents.",
        "author": "Qianqian Xie; Jimin Huang; Pan Du; Min Peng; Jian-Yun Nie",
        "authorids": "/q/qianqian-xie/; /j/jimin-huang/; /p/pan-du/; /m/min-peng/; /j/jian-yun-nie/",
        "bibtex": "https://aclanthology.org/2021.naacl-main.333.bib",
        "pdf": "https://aclanthology.org/2021.naacl-main.333.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.333/",
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13016671345535880470&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5
    },
    {
        "id": "2021.naacl-industry.39",
        "title": "Industry Scale Semi-Supervised Learning for Natural Language Understanding",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "This paper presents a production Semi-Supervised Learning (SSL) pipeline based on the student-teacher framework, which leverages millions of unlabeled examples to improve Natural Language Understanding (NLU) tasks. We investigate two questions related to the use of unlabeled data in production SSL context: 1) how to select samples from a huge unlabeled data pool that are beneficial for SSL training, and 2) how does the selected data affect the performance of different state-of-the-art SSL techniques. We compare four widely used SSL techniques, Pseudo-label (PL), Knowledge Distillation (KD), Virtual Adversarial Training (VAT) and Cross-View Training (CVT) in conjunction with two data selection methods including committee-based selection and submodular optimization based selection. We further examine the benefits and drawbacks of these techniques when applied to intent classification (IC) and named entity recognition (NER) tasks, and provide guidelines specifying when each of these methods might be beneficial to improve large scale NLU systems.",
        "author": "Luoxin Chen; Francisco Garcia; Varun Kumar; He Xie; Jianhua Lu",
        "authorids": "/l/luoxin-chen/; /f/francisco-garcia/; /v/varun-kumar/; /h/he-xie/; /j/jianhua-lu/",
        "bibtex": "@inproceedings{chen-etal-2021-industry,\n    title = \"Industry Scale Semi-Supervised Learning for Natural Language Understanding\",\n    author = \"Chen, Luoxin  and\n      Garcia, Francisco  and\n      Kumar, Varun  and\n      Xie, He  and\n      Lu, Jianhua\",\n    editor = \"Kim, Young-bum  and\n      Li, Yunyao  and\n      Rambow, Owen\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-industry.39/\",\n    doi = \"10.18653/v1/2021.naacl-industry.39\",\n    pages = \"311--318\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-industry.39.pdf",
        "site": "https://aclanthology.org/2021.naacl-industry.39/",
        "pdf_size": 316847,
        "gs_citation": 65,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7074233085825280462&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Alexa AI; Alexa AI; Alexa AI; Alexa AI; Alexa AI",
        "aff_domain": "amazon.com;amazon.com;amazon.com;amazon.com;amazon.com",
        "email": "amazon.com;amazon.com;amazon.com;amazon.com;amazon.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Alexa Internet",
        "aff_unique_dep": "Alexa AI",
        "aff_unique_url": "https://www.alexa.com",
        "aff_unique_abbr": "Alexa",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.476",
        "title": "Inference Time Style Control for Summarization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "How to generate summaries of different styles without requiring corpora in the target styles, or training separate models? We present two novel methods that can be deployed during summary decoding on any pre-trained Transformer-based summarization model. (1) Decoder state adjustment instantly modifies decoder final states with externally trained style scorers, to iteratively refine the output against a target style. (2) Word unit prediction constrains the word usage to impose strong lexical control during generation. In experiments of summarizing with simplicity control, automatic evaluation and human judges both find our models producing outputs in simpler languages while still informative. We also generate news headlines with various ideological leanings, which can be distinguished by humans with a reasonable probability.",
        "author": "Shuyang Cao; Lu Wang",
        "authorids": "/s/shuyang-cao/; /l/lu-wang/",
        "bibtex": "@inproceedings{cao-wang-2021-inference,\n    title = \"Inference Time Style Control for Summarization\",\n    author = \"Cao, Shuyang  and\n      Wang, Lu\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.476/\",\n    doi = \"10.18653/v1/2021.naacl-main.476\",\n    pages = \"5942--5953\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.476.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.476/",
        "pdf_size": 452113,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12378915049540065773&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Computer Science and Engineering, University of Michigan; Computer Science and Engineering, University of Michigan",
        "aff_domain": "umich.edu;umich.edu",
        "email": "umich.edu;umich.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "Computer Science and Engineering",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Ann Arbor",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.280",
        "title": "InfoXLM: An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In this work, we present an information-theoretic framework that formulates cross-lingual language model pre-training as maximizing mutual information between multilingual-multi-granularity texts. The unified view helps us to better understand the existing methods for learning cross-lingual representations. More importantly, inspired by the framework, we propose a new pre-training task based on contrastive learning. Specifically, we regard a bilingual sentence pair as two views of the same meaning and encourage their encoded representations to be more similar than the negative examples. By leveraging both monolingual and parallel corpora, we jointly train the pretext tasks to improve the cross-lingual transferability of pre-trained models. Experimental results on several benchmarks show that our approach achieves considerably better performance. The code and pre-trained models are available at https://aka.ms/infoxlm.",
        "author": "Zewen Chi; Li Dong; Furu Wei; Nan Yang; Saksham Singhal; Wenhui Wang; Xia Song; Xian-Ling Mao; Heyan Huang; Ming Zhou",
        "authorids": "/z/zewen-chi/; /l/li-dong/; /f/furu-wei/; /n/nan-yang/; /s/saksham-singhal/; /w/wenhui-wang/; /x/xia-song/; /x/xian-ling-mao/; /h/he-yan-huang/; /m/ming-zhou/",
        "bibtex": "@inproceedings{chi-etal-2021-infoxlm,\n    title = \"{I}nfo{XLM}: An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training\",\n    author = \"Chi, Zewen  and\n      Dong, Li  and\n      Wei, Furu  and\n      Yang, Nan  and\n      Singhal, Saksham  and\n      Wang, Wenhui  and\n      Song, Xia  and\n      Mao, Xian-Ling  and\n      Huang, Heyan  and\n      Zhou, Ming\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.280/\",\n    doi = \"10.18653/v1/2021.naacl-main.280\",\n    pages = \"3576--3588\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.280.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.280/",
        "pdf_size": 420344,
        "gs_citation": 371,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1926338421650792167&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Beijing Institute of Technology\u2020\u2021; Microsoft Corporation\u2020\u2021; Microsoft Corporation\u2020\u2021; Microsoft Corporation\u2020\u2021; Microsoft Corporation\u2020\u2021; Microsoft Corporation\u2020\u2021; Microsoft Corporation\u2020\u2021; Beijing Institute of Technology\u2020\u2021; Beijing Institute of Technology\u2020\u2021; Microsoft Corporation\u2020\u2021",
        "aff_domain": "bit.edu.cn;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;bit.edu.cn;bit.edu.cn;microsoft.com",
        "email": "bit.edu.cn;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;bit.edu.cn;bit.edu.cn;microsoft.com",
        "github": "",
        "project": "https://aka.ms/infoxlm",
        "author_num": 10,
        "aff_unique_index": "0;1;1;1;1;1;1;0;0;1",
        "aff_unique_norm": "Beijing Institute of Technology;Microsoft Corporation",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.bit.edu.cn/;https://www.microsoft.com",
        "aff_unique_abbr": "BIT;Microsoft",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;1;1;1;1;0;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2021.naacl-main.268",
        "title": "Integrating Lexical Information into Entity Neighbourhood Representations for Relation Prediction",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Relation prediction informed from a combination of text corpora and curated knowledge bases, combining knowledge graph completion with relation extraction, is a relatively little studied task. A system that can perform this task has the ability to extend an arbitrary set of relational database tables with information extracted from a document corpus. OpenKi[1] addresses this task through extraction of named entities and predicates via OpenIE tools then learning relation embeddings from the resulting entity-relation graph for relation prediction, outperforming previous approaches. We present an extension of OpenKi that incorporates embeddings of text-based representations of the entities and the relations. We demonstrate that this results in a substantial performance increase over a system without this information.",
        "author": "Ian Wood; Mark Johnson; Stephen Wan",
        "authorids": "/i/ian-wood/; /m/mark-johnson/; /s/stephen-wan/",
        "bibtex": "@inproceedings{wood-etal-2021-integrating,\n    title = \"Integrating Lexical Information into Entity Neighbourhood Representations for Relation Prediction\",\n    author = \"Wood, Ian  and\n      Johnson, Mark  and\n      Wan, Stephen\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.268/\",\n    doi = \"10.18653/v1/2021.naacl-main.268\",\n    pages = \"3429--3436\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.268.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.268/",
        "pdf_size": 326936,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7446695595965134358&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Macquarie University and CSIRO, Australia; CSIRO, Australia; Oracle Digital Assistant",
        "aff_domain": "mq.edu.au;data61.csiro.au;oracle.com",
        "email": "mq.edu.au;data61.csiro.au;oracle.com",
        "github": "https://github.com/drevicko/OpenKI",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Macquarie University;Commonwealth Scientific and Industrial Research Organisation;Oracle Corporation",
        "aff_unique_dep": ";;Digital Assistant",
        "aff_unique_url": "https://www.mq.edu.au;https://www.csiro.au;https://www.oracle.com",
        "aff_unique_abbr": "MQ;CSIRO;Oracle",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "Australia;United States"
    },
    {
        "id": "2021.naacl-industry.27",
        "title": "Intent Features for Rich Natural Language Understanding",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Complex natural language understanding modules in dialog systems have a richer understanding of user utterances, and thus are critical in providing a better user experience. However, these models are often created from scratch, for specific clients and use cases and require the annotation of large datasets. This encourages the sharing of annotated data across multiple clients. To facilitate this we introduce the idea of intent features: domain and topic agnostic properties of intents that can be learnt from the syntactic cues only, and hence can be shared. We introduce a new neural network architecture, the Global-Local model, that shows significant improvement over strong baselines for identifying these features in a deployed, multi-intent natural language understanding module, and more generally in a classification setting where a part of an utterance has to be classified utilizing the whole context.",
        "author": "Brian Lester; Sagnik Ray Choudhury; Rashmi Prasad; Srinivas Bangalore",
        "authorids": "/b/brian-lester/; /s/sagnik-ray-choudhury/; /r/rashmi-prasad/; /s/srinivas-bangalore/",
        "bibtex": "@inproceedings{lester-etal-2021-intent,\n    title = \"Intent Features for Rich Natural Language Understanding\",\n    author = \"Lester, Brian  and\n      Ray Choudhury, Sagnik  and\n      Prasad, Rashmi  and\n      Bangalore, Srinivas\",\n    editor = \"Kim, Young-bum  and\n      Li, Yunyao  and\n      Rambow, Owen\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-industry.27/\",\n    doi = \"10.18653/v1/2021.naacl-industry.27\",\n    pages = \"214--221\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-industry.27.pdf",
        "site": "https://aclanthology.org/2021.naacl-industry.27/",
        "pdf_size": 243426,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:ZyVQeY4X_N8J:scholar.google.com/&scioq=Intent+Features+for+Rich+Natural+Language+Understanding&hl=en&as_sdt=0,14",
        "gs_version_total": 7,
        "aff": "Interactions\u2660; University of Copenhagen\u2666 + Interactions\u2660; Interactions\u2660; Interactions\u2660",
        "aff_domain": "interactions.com;di.ku.dk;interactions.com;interactions.com",
        "email": "interactions.com;di.ku.dk;interactions.com;interactions.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "1",
        "aff_unique_norm": ";University of Copenhagen",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://www.ku.dk",
        "aff_unique_abbr": ";UCPH",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";Denmark"
    },
    {
        "id": "2021.naacl-demos.11",
        "title": "Interactive Plot Manipulation using Natural Language",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "We present an interactive Plotting Agent, a system that enables users to directly manipulate plots using natural language instructions within an interactive programming environment. The Plotting Agent maps language to plot updates. We formulate this problem as a slot-based task-oriented dialog problem, which we tackle with a sequence-to-sequence model. This plotting model while accurate in most cases, still makes errors, therefore, the system allows a feedback mode, wherein the user is presented with a top-k list of plots, among which the user can pick the desired one. From this kind of feedback, we can then, in principle, continuously learn and improve the system. Given that plotting is widely used across data-driven fields, we believe our demonstration will be of interest to both practitioners such as data scientists broadly defined, and researchers interested in natural language interfaces.",
        "author": "Yihan Wang; Yutong Shao; Ndapa Nakashole",
        "authorids": "/y/yihan-wang/; /y/yutong-shao/; /n/ndapandula-nakashole/",
        "bibtex": "@inproceedings{wang-etal-2021-interactive,\n    title = \"Interactive Plot Manipulation using Natural Language\",\n    author = \"Wang, Yihan  and\n      Shao, Yutong  and\n      Nakashole, Ndapa\",\n    editor = \"Sil, Avi  and\n      Lin, Xi Victoria\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-demos.11/\",\n    doi = \"10.18653/v1/2021.naacl-demos.11\",\n    pages = \"92--98\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-demos.11.pdf",
        "site": "https://aclanthology.org/2021.naacl-demos.11/",
        "pdf_size": 608401,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9147410299750146329&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Computer Science and Engineering, University of California, San Diego; Computer Science and Engineering, University of California, San Diego; Computer Science and Engineering, University of California, San Diego",
        "aff_domain": "ucsd.edu;eng.ucsd.edu;eng.ucsd.edu",
        "email": "ucsd.edu;eng.ucsd.edu;eng.ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "Computer Science and Engineering",
        "aff_unique_url": "https://www.ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.182",
        "title": "Introducing CAD: the Contextual Abuse Dataset",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Online abuse can inflict harm on users and communities, making online spaces unsafe and toxic. Progress in automatically detecting and classifying abusive content is often held back by the lack of high quality and detailed datasets. We introduce a new dataset of primarily English Reddit entries which addresses several limitations of prior work. It (1) contains six conceptually distinct primary categories as well as secondary categories, (2) has labels annotated in the context of the conversation thread, (3) contains rationales and (4) uses an expert-driven group-adjudication process for high quality annotations. We report several baseline models to benchmark the work of future researchers. The annotated dataset, annotation guidelines, models and code are freely available.",
        "author": "Bertie Vidgen; Dong Nguyen; Helen Margetts; Patricia Rossini; Rebekah Tromble",
        "authorids": "/b/bertie-vidgen/; /d/dong-nguyen/; /h/helen-margetts/; /p/patricia-rossini/; /r/rebekah-tromble/",
        "bibtex": "@inproceedings{vidgen-etal-2021-introducing,\n    title = \"Introducing {CAD}: the Contextual Abuse Dataset\",\n    author = \"Vidgen, Bertie  and\n      Nguyen, Dong  and\n      Margetts, Helen  and\n      Rossini, Patricia  and\n      Tromble, Rebekah\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.182/\",\n    doi = \"10.18653/v1/2021.naacl-main.182\",\n    pages = \"2289--2303\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.182.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.182/",
        "pdf_size": 318364,
        "gs_citation": 112,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15927984042726083075&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "The Alan Turing Institute; Utrecht University + The Alan Turing Institute; University of Oxford + The Alan Turing Institute; University of Liverpool; George Washington University + The Alan Turing Institute",
        "aff_domain": "turing.ac.uk; ; ; ; ",
        "email": "turing.ac.uk; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1+0;2+0;3;4+0",
        "aff_unique_norm": "The Alan Turing Institute;Utrecht University;University of Oxford;University of Liverpool;George Washington University",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://www.turing.ac.uk;https://www.uu.nl;https://www.ox.ac.uk;https://www.liverpool.ac.uk;https://www.gwu.edu",
        "aff_unique_abbr": "ATI;UU;Oxford;Liv Uni;GWU",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1+0;0+0;0;2+0",
        "aff_country_unique": "United Kingdom;Netherlands;United States"
    },
    {
        "id": "2021.naacl-main.328",
        "title": "Is Incoherence Surprising? Targeted Evaluation of Coherence Prediction from Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Coherent discourse is distinguished from a mere collection of utterances by the satisfaction of a diverse set of constraints, for example choice of expression, logical relation between denoted events, and implicit compatibility with world-knowledge. Do neural language models encode such constraints? We design an extendable set of test suites addressing different aspects of discourse and dialogue coherence. Unlike most previous coherence evaluation studies, we address specific linguistic devices beyond sentence order perturbations, which allow for a more fine-grained analysis of what constitutes coherence and what neural models trained on a language modelling objective are capable of encoding. Extending the targeted evaluation paradigm for neural language models (Marvin and Linzen, 2018) to phenomena beyond syntax, we show that this paradigm is equally suited to evaluate linguistic qualities that contribute to the notion of coherence.",
        "author": "Anne Beyer; Sharid Lo\u00e1iciga; David Schlangen",
        "authorids": "/a/anne-beyer/; /s/sharid-loaiciga/; /d/david-schlangen/",
        "bibtex": "@inproceedings{beyer-etal-2021-incoherence,\n    title = \"Is Incoherence Surprising? Targeted Evaluation of Coherence Prediction from Language Models\",\n    author = \"Beyer, Anne  and\n      Lo{\\'a}iciga, Sharid  and\n      Schlangen, David\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.328/\",\n    doi = \"10.18653/v1/2021.naacl-main.328\",\n    pages = \"4164--4173\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.328.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.328/",
        "pdf_size": 273878,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4485130959878587380&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Computational Linguistics, Department of Linguistics, University of Potsdam, Germany; Computational Linguistics, Department of Linguistics, University of Potsdam, Germany; Computational Linguistics, Department of Linguistics, University of Potsdam, Germany",
        "aff_domain": "uni-potsdam.de;uni-potsdam.de;uni-potsdam.de",
        "email": "uni-potsdam.de;uni-potsdam.de;uni-potsdam.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Potsdam",
        "aff_unique_dep": "Department of Linguistics",
        "aff_unique_url": "https://www.uni-potsdam.de",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2021.naacl-main.185",
        "title": "It\u2019s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners",
        "track": "main",
        "status": "Long",
        "award": true,
        "abstract": "When scaled to hundreds of billions of parameters, pretrained language models such as GPT-3 (Brown et al., 2020) achieve remarkable few-shot performance. However, enormous amounts of compute are required for training and applying such big models, resulting in a large carbon footprint and making it difficult for researchers and practitioners to use them. We show that performance similar to GPT-3 can be obtained with language models that are much \u201cgreener\u201d in that their parameter count is several orders of magnitude smaller. This is achieved by converting textual inputs into cloze questions that contain a task description, combined with gradient-based optimization; exploiting unlabeled data gives further improvements. We identify key factors required for successful natural language understanding with small language models.",
        "author": "Timo Schick; Hinrich Sch\u00fctze",
        "authorids": "/t/timo-schick/; /h/hinrich-schutze/",
        "bibtex": "@inproceedings{schick-schutze-2021-just,\n    title = \"It{'}s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners\",\n    author = {Schick, Timo  and\n      Sch{\\\"u}tze, Hinrich},\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.185/\",\n    doi = \"10.18653/v1/2021.naacl-main.185\",\n    pages = \"2339--2352\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.185.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.185/",
        "pdf_size": 475407,
        "gs_citation": 1036,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1018393599960294721&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Center for Information and Language Processing, LMU Munich, Germany + Sulzer GmbH, Munich, Germany; Center for Information and Language Processing, LMU Munich, Germany",
        "aff_domain": "sulzer.de; ",
        "email": "sulzer.de; ",
        "github": "https://github.com/timoschick/pet",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;0",
        "aff_unique_norm": "LMU Munich;Sulzer GmbH",
        "aff_unique_dep": "Center for Information and Language Processing;",
        "aff_unique_url": "https://www.lmu.de;https://www.sulzer.com",
        "aff_unique_abbr": "LMU;",
        "aff_campus_unique_index": "0+0;0",
        "aff_campus_unique": "Munich",
        "aff_country_unique_index": "0+0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2021.naacl-main.453",
        "title": "Jointly Extracting Explicit and Implicit Relational Triples with Reasoning Pattern Enhanced Binary Pointer Network",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Relational triple extraction is a crucial task for knowledge graph construction. Existing methods mainly focused on explicit relational triples that are directly expressed, but usually suffer from ignoring implicit triples that lack explicit expressions. This will lead to serious incompleteness of the constructed knowledge graphs. Fortunately, other triples in the sentence provide supplementary information for discovering entity pairs that may have implicit relations. Also, the relation types between the implicitly connected entity pairs can be identified with relational reasoning patterns in the real world. In this paper, we propose a unified framework to jointly extract explicit and implicit relational triples. To explore entity pairs that may be implicitly connected by relations, we propose a binary pointer network to extract overlapping relational triples relevant to each word sequentially and retain the information of previously extracted triples in an external memory. To infer the relation types of implicit relational triples, we propose to introduce real-world relational reasoning patterns in our model and capture these patterns with a relation network. We conduct experiments on several benchmark datasets, and the results prove the validity of our method.",
        "author": "Yubo Chen; Yunqi Zhang; Changran Hu; Yongfeng Huang",
        "authorids": "/y/yubo-chen/; /y/yunqi-zhang/; /c/changran-hu/; /y/yongfeng-huang/",
        "bibtex": "@inproceedings{chen-etal-2021-jointly,\n    title = \"Jointly Extracting Explicit and Implicit Relational Triples with Reasoning Pattern Enhanced Binary Pointer Network\",\n    author = \"Chen, Yubo  and\n      Zhang, Yunqi  and\n      Hu, Changran  and\n      Huang, Yongfeng\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.453/\",\n    doi = \"10.18653/v1/2021.naacl-main.453\",\n    pages = \"5694--5703\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.453.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.453/",
        "pdf_size": 793069,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17833777672275934199&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Department of Electronic Engineering & BNRist, Tsinghua University, Beijing, China; Department of Electronic Engineering & BNRist, Tsinghua University, Beijing, China; University of California, Berkeley, USA; Department of Electronic Engineering & BNRist, Tsinghua University, Beijing, China",
        "aff_domain": "gmail.com;outlook.com;berkeley.edu;tsinghua.edu.cn",
        "email": "gmail.com;outlook.com;berkeley.edu;tsinghua.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Tsinghua University;University of California, Berkeley",
        "aff_unique_dep": "Department of Electronic Engineering;",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.berkeley.edu",
        "aff_unique_abbr": "THU;UC Berkeley",
        "aff_campus_unique_index": "0;0;1;0",
        "aff_campus_unique": "Beijing;Berkeley",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2021.naacl-main.200",
        "title": "KILT: a Benchmark for Knowledge Intensive Language Tasks",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Challenging problems such as open-domain question answering, fact checking, slot filling and entity linking require access to large, external knowledge sources. While some models do well on individual tasks, developing general models is difficult as each task might require computationally expensive indexing of custom knowledge sources, in addition to dedicated infrastructure. To catalyze research on models that condition on specific information in large textual resources, we present a benchmark for knowledge-intensive language tasks (KILT). All tasks in KILT are grounded in the same snapshot of Wikipedia, reducing engineering turnaround through the re-use of components, as well as accelerating research into task-agnostic memory architectures. We test both task-specific and general baselines, evaluating downstream performance in addition to the ability of the models to provide provenance. We find that a shared dense vector index coupled with a seq2seq model is a strong baseline, outperforming more tailor-made approaches for fact checking, open-domain question answering and dialogue, and yielding competitive results on entity linking and slot filling, by generating disambiguated text. KILT data and code are available at https://github.com/facebookresearch/KILT.",
        "author": "Fabio Petroni; Aleksandra Piktus; Angela Fan; Patrick Lewis; Majid Yazdani; Nicola De Cao; James Thorne; Yacine Jernite; Vladimir Karpukhin; Jean Maillard; Vassilis Plachouras; Tim Rockt\u00e4schel; Sebastian Riedel",
        "authorids": "/f/fabio-petroni/; /a/aleksandra-piktus/; /a/angela-fan/; /p/patrick-lewis/; /m/majid-yazdani/; /n/nicola-de-cao/; /j/james-thorne/; /y/yacine-jernite/; /v/vladimir-karpukhin/; /j/jean-maillard/; /v/vassilis-plachouras/; /t/tim-rocktaschel/; /s/sebastian-riedel/",
        "bibtex": "@inproceedings{petroni-etal-2021-kilt,\n    title = \"{KILT}: a Benchmark for Knowledge Intensive Language Tasks\",\n    author = {Petroni, Fabio  and\n      Piktus, Aleksandra  and\n      Fan, Angela  and\n      Lewis, Patrick  and\n      Yazdani, Majid  and\n      De Cao, Nicola  and\n      Thorne, James  and\n      Jernite, Yacine  and\n      Karpukhin, Vladimir  and\n      Maillard, Jean  and\n      Plachouras, Vassilis  and\n      Rockt{\\\"a}schel, Tim  and\n      Riedel, Sebastian},\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.200/\",\n    doi = \"10.18653/v1/2021.naacl-main.200\",\n    pages = \"2523--2544\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.200.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.200/",
        "pdf_size": 850307,
        "gs_citation": 590,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10900569842268174596&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Facebook AI Research; Facebook AI Research; Facebook AI Research+LORIA; Facebook AI Research+University College London; Facebook AI Research; University of Amsterdam; University of Cambridge; HuggingFace; Facebook AI Research; Facebook AI Research; Facebook AI Research; Facebook AI Research+University College London; Facebook AI Research+University College London",
        "aff_domain": ";;;;;;;;;;;;",
        "email": ";;;;;;;;;;;;",
        "github": "https://github.com/facebookresearch/KILT",
        "project": "https://huggingface.co/datasets?search=kilt",
        "author_num": 13,
        "aff_unique_index": "0;0;0+1;0+2;0;3;4;5;0;0;0;0+2;0+2",
        "aff_unique_norm": "Facebook;LORIA;University College London;University of Amsterdam;University of Cambridge;HuggingFace",
        "aff_unique_dep": "Facebook AI Research;;;;;",
        "aff_unique_url": "https://research.facebook.com;https://www.loria.fr;https://www.ucl.ac.uk;https://www.uva.nl;https://www.cam.ac.uk;https://huggingface.co",
        "aff_unique_abbr": "FAIR;;UCL;UvA;Cambridge;HuggingFace",
        "aff_campus_unique_index": ";;1;;",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0;0;0+1;0+2;0;3;2;0;0;0;0;0+2;0+2",
        "aff_country_unique": "United States;France;United Kingdom;Netherlands"
    },
    {
        "id": "2021.naacl-main.170",
        "title": "KPQA: A Metric for Generative Question Answering Using Keyphrase Weights",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In the automatic evaluation of generative question answering (GenQA) systems, it is difficult to assess the correctness of generated answers due to the free-form of the answer. Especially, widely used n-gram similarity metrics often fail to discriminate the incorrect answers since they equally consider all of the tokens. To alleviate this problem, we propose KPQA metric, a new metric for evaluating the correctness of GenQA. Specifically, our new metric assigns different weights to each token via keyphrase prediction, thereby judging whether a generated answer sentence captures the key meaning of the reference answer. To evaluate our metric, we create high-quality human judgments of correctness on two GenQA datasets. Using our human-evaluation datasets, we show that our proposed metric has a significantly higher correlation with human judgments than existing metrics in various datasets. Code for KPQA-metric will be available at https://github.com/hwanheelee1993/KPQA.",
        "author": "Hwanhee Lee; Seunghyun Yoon; Franck Dernoncourt; Doo Soon Kim; Trung Bui; Joongbo Shin; Kyomin Jung",
        "authorids": "/h/hwanhee-lee/; /s/seunghyun-yoon/; /f/franck-dernoncourt/; /d/doo-soon-kim/; /t/trung-bui/; /j/joongbo-shin/; /k/kyomin-jung/",
        "bibtex": "@inproceedings{lee-etal-2021-kpqa,\n    title = \"{KPQA}: A Metric for Generative Question Answering Using Keyphrase Weights\",\n    author = \"Lee, Hwanhee  and\n      Yoon, Seunghyun  and\n      Dernoncourt, Franck  and\n      Kim, Doo Soon  and\n      Bui, Trung  and\n      Shin, Joongbo  and\n      Jung, Kyomin\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.170/\",\n    doi = \"10.18653/v1/2021.naacl-main.170\",\n    pages = \"2105--2115\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.170.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.170/",
        "pdf_size": 514070,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12417856948577889557&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Dept. of Electrical and Computer Engineering, Seoul National University, Seoul, Korea; Adobe Research, San Jose, CA, USA; Adobe Research, San Jose, CA, USA; Roku Inc., San Jose, CA, USA; Adobe Research, San Jose, CA, USA; Dept. of Electrical and Computer Engineering, Seoul National University, Seoul, Korea; Dept. of Electrical and Computer Engineering, Seoul National University, Seoul, Korea",
        "aff_domain": "snu.ac.kr;adobe.com;adobe.com;roku.com;adobe.com;snu.ac.kr;snu.ac.kr",
        "email": "snu.ac.kr;adobe.com;adobe.com;roku.com;adobe.com;snu.ac.kr;snu.ac.kr",
        "github": "https://github.com/hwanheelee1993/KPQA",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;1;2;1;0;0",
        "aff_unique_norm": "Seoul National University;Adobe Research;Roku Inc.",
        "aff_unique_dep": "Dept. of Electrical and Computer Engineering;;",
        "aff_unique_url": "https://www.snu.ac.kr;https://research.adobe.com;https://www.roku.com",
        "aff_unique_abbr": "SNU;Adobe;Roku",
        "aff_campus_unique_index": "0;1;1;1;1;0;0",
        "aff_campus_unique": "Seoul;San Jose",
        "aff_country_unique_index": "0;1;1;1;1;0;0",
        "aff_country_unique": "South Korea;United States"
    },
    {
        "id": "2021.naacl-main.376",
        "title": "Knowledge Enhanced Masked Language Model for Stance Detection",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Detecting stance on Twitter is especially challenging because of the short length of each tweet, the continuous coinage of new terminology and hashtags, and the deviation of sentence structure from standard prose. Fine-tuned language models using large-scale in-domain data have been shown to be the new state-of-the-art for many NLP tasks, including stance detection. In this paper, we propose a novel BERT-based fine-tuning method that enhances the masked language model for stance detection. Instead of random token masking, we propose using a weighted log-odds-ratio to identify words with high stance distinguishability and then model an attention mechanism that focuses on these words. We show that our proposed approach outperforms the state of the art for stance detection on Twitter data about the 2020 US Presidential election.",
        "author": "Kornraphop Kawintiranon; Lisa Singh",
        "authorids": "/k/kornraphop-kawintiranon/; /l/lisa-singh/",
        "bibtex": "@inproceedings{kawintiranon-singh-2021-knowledge,\n    title = \"Knowledge Enhanced Masked Language Model for Stance Detection\",\n    author = \"Kawintiranon, Kornraphop  and\n      Singh, Lisa\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.376/\",\n    doi = \"10.18653/v1/2021.naacl-main.376\",\n    pages = \"4725--4735\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.376.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.376/",
        "pdf_size": 463024,
        "gs_citation": 111,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10041029413638418272&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Computer Science, Georgetown University; Department of Computer Science, Georgetown University",
        "aff_domain": "georgetown.edu;georgetown.edu",
        "email": "georgetown.edu;georgetown.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Georgetown University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.georgetown.edu",
        "aff_unique_abbr": "GU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.278",
        "title": "Knowledge Graph Based Synthetic Corpus Generation for Knowledge-Enhanced Language Model Pre-training",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Prior work on Data-To-Text Generation, the task of converting knowledge graph (KG) triples into natural text, focused on domain-specific benchmark datasets. In this paper, however, we verbalize the entire English Wikidata KG, and discuss the unique challenges associated with a broad, open-domain, large-scale verbalization. We further show that verbalizing a comprehensive, encyclopedic KG like Wikidata can be used to integrate structured KGs and natural language corpora. In contrast to the many architectures that have been developed to integrate these two sources, our approach converts the KG into natural text, allowing it to be seamlessly integrated into existing language models. It carries the further advantages of improved factual accuracy and reduced toxicity in the resulting language model. We evaluate this approach by augmenting the retrieval corpus in a retrieval language model and showing significant improvements on the knowledge intensive tasks of open domain QA and the LAMA knowledge probe.",
        "author": "Oshin Agarwal; Heming Ge; Siamak Shakeri; Rami Al-Rfou",
        "authorids": "/o/oshin-agarwal/; /h/heming-ge/; /s/siamak-shakeri/; /r/rami-al-rfou/",
        "bibtex": "@inproceedings{agarwal-etal-2021-knowledge,\n    title = \"Knowledge Graph Based Synthetic Corpus Generation for Knowledge-Enhanced Language Model Pre-training\",\n    author = \"Agarwal, Oshin  and\n      Ge, Heming  and\n      Shakeri, Siamak  and\n      Al-Rfou, Rami\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.278/\",\n    doi = \"10.18653/v1/2021.naacl-main.278\",\n    pages = \"3554--3565\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.278.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.278/",
        "pdf_size": 486650,
        "gs_citation": 247,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5567443498560675335&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "University of Pennsylvania; Google Research; Google Research; Google Research",
        "aff_domain": "seas.upenn.edu;google.com;google.com;google.com",
        "email": "seas.upenn.edu;google.com;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "University of Pennsylvania;Google",
        "aff_unique_dep": ";Google Research",
        "aff_unique_url": "https://www.upenn.edu;https://research.google",
        "aff_unique_abbr": "UPenn;Google Research",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.261",
        "title": "Knowledge Guided Metric Learning for Few-Shot Text Classification",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Humans can distinguish new categories very efficiently with few examples, largely due to the fact that human beings can leverage knowledge obtained from relevant tasks. However, deep learning based text classification model tends to struggle to achieve satisfactory performance when labeled data are scarce. Inspired by human intelligence, we propose to introduce external knowledge into few-shot learning to imitate human knowledge. A novel parameter generator network is investigated to this end, which is able to use the external knowledge to generate different metrics for different tasks. Armed with this network, similar tasks can use similar metrics while different tasks use different metrics. Through experiments, we demonstrate that our method outperforms the SoTA few-shot text classification models.",
        "author": "Dianbo Sui; Yubo Chen; Binjie Mao; Delai Qiu; Kang Liu; Jun Zhao",
        "authorids": "/d/dianbo-sui/; /y/yubo-chen/; /b/binjie-mao/; /d/delai-qiu/; /k/kang-liu/; /j/jun-zhao/",
        "bibtex": "@inproceedings{sui-etal-2021-knowledge,\n    title = \"Knowledge Guided Metric Learning for Few-Shot Text Classification\",\n    author = \"Sui, Dianbo  and\n      Chen, Yubo  and\n      Mao, Binjie  and\n      Qiu, Delai  and\n      Liu, Kang  and\n      Zhao, Jun\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.261/\",\n    doi = \"10.18653/v1/2021.naacl-main.261\",\n    pages = \"3266--3271\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.261.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.261/",
        "pdf_size": 411215,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15762215177190647482&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "National Laboratory of Pattern Recognition, Institute of Automation, CAS + School of Artificial Intelligence, University of Chinese Academy of Sciences; National Laboratory of Pattern Recognition, Institute of Automation, CAS + School of Artificial Intelligence, University of Chinese Academy of Sciences; National Laboratory of Pattern Recognition, Institute of Automation, CAS + School of Artificial Intelligence, University of Chinese Academy of Sciences; Beijing Unisound Information Technology Co., Ltd.; National Laboratory of Pattern Recognition, Institute of Automation, CAS + School of Artificial Intelligence, University of Chinese Academy of Sciences; National Laboratory of Pattern Recognition, Institute of Automation, CAS + School of Artificial Intelligence, University of Chinese Academy of Sciences",
        "aff_domain": "nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;unisound.com;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "email": "nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;unisound.com;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;0+1;0+1;2;0+1;0+1",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences;Beijing Unisound Information Technology Co., Ltd.",
        "aff_unique_dep": "Institute of Automation;School of Artificial Intelligence;",
        "aff_unique_url": "http://www.ia.cas.cn;http://www.ucas.ac.cn;",
        "aff_unique_abbr": "CAS;UCAS;",
        "aff_campus_unique_index": ";;;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2021.naacl-main.1",
        "title": "Knowledge Router: Learning Disentangled Representations for Knowledge Graphs",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The design of expressive representations of entities and relations in a knowledge graph is an important endeavor. While many of the existing approaches have primarily focused on learning from relational patterns and structural information, the intrinsic complexity of KG entities has been more or less overlooked. More concretely, we hypothesize KG entities may be more complex than we think, i.e., an entity may wear many hats and relational triplets may form due to more than a single reason. To this end, this paper proposes to learn disentangled representations of KG entities - a new method that disentangles the inner latent properties of KG entities. Our disentangled process operates at the graph level and a neighborhood mechanism is leveraged to disentangle the hidden properties of each entity. This disentangled representation learning approach is model agnostic and compatible with canonical KG embedding approaches. We conduct extensive experiments on several benchmark datasets, equipping a variety of models (DistMult, SimplE, and QuatE) with our proposed disentangling mechanism. Experimental results demonstrate that our proposed approach substantially improves performance on key metrics.",
        "author": "Shuai Zhang; Xi Rao; Yi Tay; Ce Zhang",
        "authorids": "/s/shuai-zhang/; /x/xi-rao/; /y/yi-tay/; /c/ce-zhang/",
        "bibtex": "@inproceedings{zhang-etal-2021-knowledge,\n    title = \"Knowledge Router: Learning Disentangled Representations for Knowledge Graphs\",\n    author = \"Zhang, Shuai  and\n      Rao, Xi  and\n      Tay, Yi  and\n      Zhang, Ce\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.1/\",\n    doi = \"10.18653/v1/2021.naacl-main.1\",\n    pages = \"1--10\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.1.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.1/",
        "pdf_size": 732968,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=900432737852956913&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4
    },
    {
        "id": "2021.naacl-main.266",
        "title": "Knowledge-Driven Slot Constraints for Goal-Oriented Dialogue Systems",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In goal-oriented dialogue systems, users provide information through slot values to achieve specific goals. Practically, some combinations of slot values can be invalid according to external knowledge. For example, a combination of \u201ccheese pizza\u201d (a menu item) and \u201coreo cookies\u201d (a topping) from an input utterance \u201cCan I order a cheese pizza with oreo cookies on top?\u201d exemplifies such invalid combinations according to the menu of a restaurant business. Traditional dialogue systems allow execution of validation rules as a post-processing step after slots have been filled which can lead to error accumulation. In this paper, we formalize knowledge-driven slot constraints and present a new task of constraint violation detection accompanied with benchmarking data. Then, we propose methods to integrate the external knowledge into the system and model constraint violation detection as an end-to-end classification task and compare it to the traditional rule-based pipeline approach. Experiments on two domains of the MultiDoGO dataset reveal challenges of constraint violation detection and sets the stage for future work and improvements.",
        "author": "Piyawat Lertvittayakumjorn; Daniele Bonadiman; Saab Mansour",
        "authorids": "/p/piyawat-lertvittayakumjorn/; /d/daniele-bonadiman/; /s/saab-mansour/",
        "bibtex": "@inproceedings{lertvittayakumjorn-etal-2021-knowledge,\n    title = \"Knowledge-Driven Slot Constraints for Goal-Oriented Dialogue Systems\",\n    author = \"Lertvittayakumjorn, Piyawat  and\n      Bonadiman, Daniele  and\n      Mansour, Saab\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.266/\",\n    doi = \"10.18653/v1/2021.naacl-main.266\",\n    pages = \"3407--3419\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.266.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.266/",
        "pdf_size": 382046,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7724651075422922978&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Imperial College London; Amazon AI; Amazon AI",
        "aff_domain": "ic.ac.uk;amazon.com;amazon.com",
        "email": "ic.ac.uk;amazon.com;amazon.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Imperial College London;Amazon",
        "aff_unique_dep": ";Amazon AI",
        "aff_unique_url": "https://www.imperial.ac.uk;https://www.amazon.com",
        "aff_unique_abbr": "ICL;Amazon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "2021.naacl-industry.34",
        "title": "LATEX-Numeric: Language Agnostic Text Attribute Extraction for Numeric Attributes",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "In this paper, we present LATEX-Numeric - a high-precision fully-automated scalable framework for extracting E-commerce numeric attributes from unstructured product text like product description. Most of the past work on attribute extraction is not scalable as they rely on manually curated training data, either with or without use of active learning. We rely on distant supervision for training data generation, removing dependency on manual labels. One issue with distant supervision is that it leads to incomplete training annotation due to missing attribute values while matching. We propose a multi-task learning architecture to deal with missing labels in the training data, leading to F1 improvement of 9.2% for numeric attributes over state-of-the-art single-task architecture. While multi-task architecture benefits both numeric and non-numeric attributes, we present automated techniques to further improve the numeric attributes extraction models. Numeric attributes require a list of units (or aliases) for better matching with distant supervision. We propose an automated algorithm for alias creation using unstructured text and attribute values, leading to a 20.2% F1 improvement. Extensive experiments on real world datasets for 20 numeric attributes across 5 product categories and 3 English marketplaces show that LATEX-numeric achieves a high F1-score, without any manual intervention, making it suitable for practical applications. Finally we show that the improvements are language-agnostic and LATEX-Numeric achieves 13.9% F1 improvement for 3 non-English languages.",
        "author": "Kartik Mehta; Ioana Oprea; Nikhil Rasiwasia",
        "authorids": "/k/kartik-mehta/; /i/ioana-oprea/; /n/nikhil-rasiwasia/",
        "bibtex": "@inproceedings{mehta-etal-2021-latex,\n    title = \"{LATEX}-Numeric: Language Agnostic Text Attribute Extraction for Numeric Attributes\",\n    author = \"Mehta, Kartik  and\n      Oprea, Ioana  and\n      Rasiwasia, Nikhil\",\n    editor = \"Kim, Young-bum  and\n      Li, Yunyao  and\n      Rambow, Owen\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-industry.34/\",\n    doi = \"10.18653/v1/2021.naacl-industry.34\",\n    pages = \"272--279\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-industry.34.pdf",
        "site": "https://aclanthology.org/2021.naacl-industry.34/",
        "pdf_size": 492662,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9825327145114337508&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "India Machine Learning, Amazon; Retail Business Services, Amazon; India Machine Learning, Amazon",
        "aff_domain": "amazon.com;amazon.com;amazon.com",
        "email": "amazon.com;amazon.com;amazon.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Amazon",
        "aff_unique_dep": "India Machine Learning",
        "aff_unique_url": "https://www.amazon.in",
        "aff_unique_abbr": "Amazon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "India;United States"
    },
    {
        "id": "2021.naacl-industry.37",
        "title": "Label-Guided Learning for Item Categorization in e-Commerce",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Item categorization is an important application of text classification in e-commerce due to its impact on the online shopping experience of users. One class of text classification techniques that has gained attention recently is using the semantic information of the labels to guide the classification task. We have conducted a systematic investigation of the potential benefits of these methods on a real data set from a major e-commerce company in Japan. Furthermore, using a hyperbolic space to embed product labels that are organized in a hierarchical structure led to better performance compared to using a conventional Euclidean space embedding. These findings demonstrate how label-guided learning can improve item categorization systems in the e-commerce domain.",
        "author": "Lei Chen; Hirokazu Miyake",
        "authorids": "/l/lei-chen/; /h/hirokazu-miyake/",
        "bibtex": "@inproceedings{chen-miyake-2021-label,\n    title = \"Label-Guided Learning for Item Categorization in e-Commerce\",\n    author = \"Chen, Lei  and\n      Miyake, Hirokazu\",\n    editor = \"Kim, Young-bum  and\n      Li, Yunyao  and\n      Rambow, Owen\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-industry.37/\",\n    doi = \"10.18653/v1/2021.naacl-industry.37\",\n    pages = \"296--303\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-industry.37.pdf",
        "site": "https://aclanthology.org/2021.naacl-industry.37/",
        "pdf_size": 1072809,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18299947452226712770&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 2,
        "aff": "Rakuten Institute of Technology, Boston, MA; Rakuten Institute of Technology, Boston, MA",
        "aff_domain": "rakuten.com;rakuten.com",
        "email": "rakuten.com;rakuten.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Rakuten Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://rit.rakuten.com",
        "aff_unique_abbr": "RIT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Boston",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-industry.18",
        "title": "Language Scaling for Universal Suggested Replies Model",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "We consider the problem of scaling automated suggested replies for a commercial email application to multiple languages. Faced with increased compute requirements and low language resources for language expansion, we build a single universal model for improving the quality and reducing run-time costs of our production system. However, restricted data movement across regional centers prevents joint training across languages. To this end, we propose a multi-lingual multi-task continual learning framework, with auxiliary tasks and language adapters to train universal language representation across regions. The experimental results show positive cross-lingual transfer across languages while reducing catastrophic forgetting across regions. Our online results on real user traffic show significant CTR and Char-saved gain as well as 65% training cost reduction compared with per-language models. As a consequence, we have scaled the feature in multiple languages including low-resource markets.",
        "author": "Qianlan Ying; Payal Bajaj; Budhaditya Deb; Yu Yang; Wei Wang; Bojia Lin; Milad Shokouhi; Xia Song; Yang Yang; Daxin Jiang",
        "authorids": "/q/qianlan-ying/; /p/payal-bajaj/; /b/budhaditya-deb/; /y/yu-yang/; /w/wei-wang/; /b/bojia-lin/; /m/milad-shokouhi/; /x/xia-song/; /y/yang-yang/; /d/daxin-jiang/",
        "bibtex": "@inproceedings{ying-etal-2021-language,\n    title = \"Language Scaling for Universal Suggested Replies Model\",\n    author = \"Ying, Qianlan  and\n      Bajaj, Payal  and\n      Deb, Budhaditya  and\n      Yang, Yu  and\n      Wang, Wei  and\n      Lin, Bojia  and\n      Shokouhi, Milad  and\n      Song, Xia  and\n      Yang, Yang  and\n      Jiang, Daxin\",\n    editor = \"Kim, Young-bum  and\n      Li, Yunyao  and\n      Rambow, Owen\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-industry.18/\",\n    doi = \"10.18653/v1/2021.naacl-industry.18\",\n    pages = \"138--145\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-industry.18.pdf",
        "site": "https://aclanthology.org/2021.naacl-industry.18/",
        "pdf_size": 446050,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11788292382704711163&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Microsoft, Beijing, China; Microsoft, Bellevue, Washington, USA; Microsoft, Bellevue, Washington, USA; Microsoft, Beijing, China; Qualtrics, Seattle, Washington, USA; Microsoft, Beijing, China; Microsoft, Bellevue, Washington, USA; Microsoft, Bellevue, Washington, USA; Microsoft, Beijing, China; Microsoft, Beijing, China",
        "aff_domain": "microsoft.com;microsoft.com;microsoft.com;microsoft.com;gmail.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "email": "microsoft.com;microsoft.com;microsoft.com;microsoft.com;gmail.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0;1;1;0;2;0;1;1;0;0",
        "aff_unique_norm": "Microsoft;Microsoft Corporation;Qualtrics",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.microsoft.com;https://www.microsoft.com;https://www.qualtrics.com",
        "aff_unique_abbr": "MSFT;Microsoft;",
        "aff_campus_unique_index": "0;1;1;0;2;0;1;1;0;0",
        "aff_campus_unique": "Beijing;Bellevue;Seattle",
        "aff_country_unique_index": "0;1;1;0;1;0;1;1;0;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2021.naacl-main.348",
        "title": "Language in a (Search) Box: Grounding Language Learning in Real-World Human-Machine Interaction",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We investigate grounded language learning through real-world data, by modelling a teacher-learner dynamics through the natural interactions occurring between users and search engines; in particular, we explore the emergence of semantic generalization from unsupervised dense representations outside of synthetic environments. A grounding domain, a denotation function and a composition function are learned from user data only. We show how the resulting semantics for noun phrases exhibits compositional properties while being fully learnable without any explicit labelling. We benchmark our grounded semantics on compositionality and zero-shot inference tasks, and we show that it provides better results and better generalizations than SOTA non-grounded models, such as word2vec and BERT.",
        "author": "Federico Bianchi; Ciro Greco; Jacopo Tagliabue",
        "authorids": "/f/federico-bianchi/; /c/ciro-greco/; /j/jacopo-tagliabue/",
        "bibtex": "@inproceedings{bianchi-etal-2021-language,\n    title = \"Language in a (Search) Box: Grounding Language Learning in Real-World Human-Machine Interaction\",\n    author = \"Bianchi, Federico  and\n      Greco, Ciro  and\n      Tagliabue, Jacopo\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.348/\",\n    doi = \"10.18653/v1/2021.naacl-main.348\",\n    pages = \"4409--4415\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.348.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.348/",
        "pdf_size": 653126,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17146604714801932860&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Bocconi University; Coveo Labs; Coveo Labs",
        "aff_domain": "unibocconi.it;coveo.com;coveo.com",
        "email": "unibocconi.it;coveo.com;coveo.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Bocconi University;Coveo",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.bocconi.edu;https://www.coveo.com",
        "aff_unique_abbr": "Bocconi;Coveo",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "Italy;Canada"
    },
    {
        "id": "2021.naacl-main.115",
        "title": "Larger-Context Tagging: When and Why Does It Work?",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The development of neural networks and pretraining techniques has spawned many sentence-level tagging systems that achieved superior performance on typical benchmarks. However, a relatively less discussed topic is what if more context information is introduced into current top-scoring tagging systems. Although several existing works have attempted to shift tagging systems from sentence-level to document-level, there is still no consensus conclusion about when and why it works, which limits the applicability of the larger-context approach in tagging tasks. In this paper, instead of pursuing a state-of-the-art tagging system by architectural exploration, we focus on investigating when and why the larger-context training, as a general strategy, can work. To this end, we conduct a thorough comparative study on four proposed aggregators for context information collecting and present an attribute-aided evaluation method to interpret the improvement brought by larger-context training. Experimentally, we set up a testbed based on four tagging tasks and thirteen datasets. Hopefully, our preliminary observations can deepen the understanding of larger-context training and enlighten more follow-up works on the use of contextual information.",
        "author": "Jinlan Fu; Liangjing Feng; Qi Zhang; Xuanjing Huang; Pengfei Liu",
        "authorids": "/j/jinlan-fu/; /l/liangjing-feng/; /q/qi-zhang/; /x/xuan-jing-huang/; /p/pengfei-liu/",
        "bibtex": "@inproceedings{fu-etal-2021-larger,\n    title = \"Larger-Context Tagging: When and Why Does It Work?\",\n    author = \"Fu, Jinlan  and\n      Feng, Liangjing  and\n      Zhang, Qi  and\n      Huang, Xuanjing  and\n      Liu, Pengfei\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.115/\",\n    doi = \"10.18653/v1/2021.naacl-main.115\",\n    pages = \"1463--1475\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.115.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.115/",
        "pdf_size": 1489835,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12549271526725034352&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 8,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5
    },
    {
        "id": "2021.naacl-main.425",
        "title": "Latent-Optimized Adversarial Neural Transfer for Sarcasm Detection",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The existence of multiple datasets for sarcasm detection prompts us to apply transfer learning to exploit their commonality. The adversarial neural transfer (ANT) framework utilizes multiple loss terms that encourage the source-domain and the target-domain feature distributions to be similar while optimizing for domain-specific performance. However, these objectives may be in conflict, which can lead to optimization difficulties and sometimes diminished transfer. We propose a generalized latent optimization strategy that allows different losses to accommodate each other and improves training dynamics. The proposed method outperforms transfer learning and meta-learning baselines. In particular, we achieve 10.02% absolute performance gain over the previous state of the art on the iSarcasm dataset.",
        "author": "Xu Guo; Boyang Li; Han Yu; Chunyan Miao",
        "authorids": "/x/xu-guo/; /b/boyang-li/; /h/han-yu/; /c/chunyan-miao/",
        "bibtex": "@inproceedings{guo-etal-2021-latent,\n    title = \"Latent-Optimized Adversarial Neural Transfer for Sarcasm Detection\",\n    author = \"Guo, Xu  and\n      Li, Boyang  and\n      Yu, Han  and\n      Miao, Chunyan\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.425/\",\n    doi = \"10.18653/v1/2021.naacl-main.425\",\n    pages = \"5394--5407\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.425.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.425/",
        "pdf_size": 1033057,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12912416234622938893&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore",
        "aff_domain": "ntu.edu.sg;ntu.edu.sg;ntu.edu.sg;ntu.edu.sg",
        "email": "ntu.edu.sg;ntu.edu.sg;ntu.edu.sg;ntu.edu.sg",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Nanyang Technological University",
        "aff_unique_dep": "School of Computer Science and Engineering",
        "aff_unique_url": "https://www.ntu.edu.sg",
        "aff_unique_abbr": "NTU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Singapore",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "2021.naacl-main.137",
        "title": "Lattice-BERT: Leveraging Multi-Granularity Representations in Chinese Pre-trained Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Chinese pre-trained language models usually process text as a sequence of characters, while ignoring more coarse granularity, e.g., words. In this work, we propose a novel pre-training paradigm for Chinese \u2014 Lattice-BERT, which explicitly incorporates word representations along with characters, thus can model a sentence in a multi-granularity manner. Specifically, we construct a lattice graph from the characters and words in a sentence and feed all these text units into transformers. We design a lattice position attention mechanism to exploit the lattice structures in self-attention layers. We further propose a masked segment prediction task to push the model to learn from rich but redundant information inherent in lattices, while avoiding learning unexpected tricks. Experiments on 11 Chinese natural language understanding tasks show that our model can bring an average increase of 1.5% under the 12-layer setting, which achieves new state-of-the-art among base-size models on the CLUE benchmarks. Further analysis shows that Lattice-BERT can harness the lattice structures, and the improvement comes from the exploration of redundant information and multi-granularity representations. Our code will be available at https://github.com/alibaba/pretrained-language-models/LatticeBERT.",
        "author": "Yuxuan Lai; Yijia Liu; Yansong Feng; Songfang Huang; Dongyan Zhao",
        "authorids": "/y/yuxuan-lai/; /y/yijia-liu/; /y/yansong-feng/; /s/songfang-huang/; /d/dongyan-zhao/",
        "bibtex": "@inproceedings{lai-etal-2021-lattice,\n    title = \"Lattice-{BERT}: Leveraging Multi-Granularity Representations in {C}hinese Pre-trained Language Models\",\n    author = \"Lai, Yuxuan  and\n      Liu, Yijia  and\n      Feng, Yansong  and\n      Huang, Songfang  and\n      Zhao, Dongyan\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.137/\",\n    doi = \"10.18653/v1/2021.naacl-main.137\",\n    pages = \"1716--1731\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.137.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.137/",
        "pdf_size": 676682,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7134452536967093236&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Wangxuan Institute of Computer Technology, Peking University, China+The MOE Key Laboratory of Computational Linguistics, Peking University, China; Alibaba Group; Wangxuan Institute of Computer Technology, Peking University, China+The MOE Key Laboratory of Computational Linguistics, Peking University, China; Alibaba Group; Wangxuan Institute of Computer Technology, Peking University, China+The MOE Key Laboratory of Computational Linguistics, Peking University, China",
        "aff_domain": "pku.edu.cn;alibaba-inc.com;pku.edu.cn;alibaba-inc.com;pku.edu.cn",
        "email": "pku.edu.cn;alibaba-inc.com;pku.edu.cn;alibaba-inc.com;pku.edu.cn",
        "github": "https://github.com/alibaba/pretrained-language-models/LatticeBERT",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+0;1;0+0;1;0+0",
        "aff_unique_norm": "Peking University;Alibaba Group",
        "aff_unique_dep": "Wangxuan Institute of Computer Technology;",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.alibaba.com",
        "aff_unique_abbr": "PKU;Alibaba",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0+0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2021.naacl-main.410",
        "title": "Learning How to Ask: Querying LMs with Mixtures of Soft Prompts",
        "track": "main",
        "status": "Long",
        "award": true,
        "abstract": "Natural-language prompts have recently been used to coax pretrained language models into performing other AI tasks, using a fill-in-the-blank paradigm (Petroni et al., 2019) or a few-shot extrapolation paradigm (Brown et al., 2020). For example, language models retain factual knowledge from their training corpora that can be extracted by asking them to \u201cfill in the blank\u201d in a sentential prompt. However, where does this prompt come from? We explore the idea of learning prompts by gradient descent\u2014either fine-tuning prompts taken from previous work, or starting from random initialization. Our prompts consist of \u201csoft words,\u201d i.e., continuous vectors that are not necessarily word type embeddings from the language model. Furthermore, for each task, we optimize a mixture of prompts, learning which prompts are most effective and how to ensemble them. Across multiple English LMs and tasks, our approach hugely outperforms previous methods, showing that the implicit factual knowledge in language models was previously underestimated. Moreover, this knowledge is cheap to elicit: random initialization is nearly as good as informed initialization.",
        "author": "Guanghui Qin; Jason Eisner",
        "authorids": "/g/guanghui-qin/; /j/jason-eisner/",
        "bibtex": "@inproceedings{qin-eisner-2021-learning,\n    title = \"Learning How to Ask: Querying {LM}s with Mixtures of Soft Prompts\",\n    author = \"Qin, Guanghui  and\n      Eisner, Jason\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.410/\",\n    doi = \"10.18653/v1/2021.naacl-main.410\",\n    pages = \"5203--5212\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.410.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.410/",
        "pdf_size": 417590,
        "gs_citation": 590,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9453262034048414184&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Computer Science, Johns Hopkins University; Department of Computer Science, Johns Hopkins University",
        "aff_domain": "jhu.edu;cs.jhu.edu",
        "email": "jhu.edu;cs.jhu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Johns Hopkins University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.jhu.edu",
        "aff_unique_abbr": "JHU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.377",
        "title": "Learning Paralinguistic Features from Audiobooks through Style Voice Conversion",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Paralinguistics, the non-lexical components of speech, play a crucial role in human-human interaction. Models designed to recognize paralinguistic information, particularly speech emotion and style, are difficult to train because of the limited labeled datasets available. In this work, we present a new framework that enables a neural network to learn to extract paralinguistic attributes from speech using data that are not annotated for emotion. We assess the utility of the learned embeddings on the downstream tasks of emotion recognition and speaking style detection, demonstrating significant improvements over surface acoustic features as well as over embeddings extracted from other unsupervised approaches. Our work enables future systems to leverage the learned embedding extractor as a separate component capable of highlighting the paralinguistic components of speech.",
        "author": "Zakaria Aldeneh; Matthew Perez; Emily Mower Provost",
        "authorids": "/z/zakaria-aldeneh/; /m/matthew-perez/; /e/emily-mower-provost/",
        "bibtex": "@inproceedings{aldeneh-etal-2021-learning,\n    title = \"Learning Paralinguistic Features from Audiobooks through Style Voice Conversion\",\n    author = \"Aldeneh, Zakaria  and\n      Perez, Matthew  and\n      Mower Provost, Emily\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.377/\",\n    doi = \"10.18653/v1/2021.naacl-main.377\",\n    pages = \"4736--4745\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.377.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.377/",
        "pdf_size": 1310285,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12176236045571931063&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "University of Michigan, Ann Arbor; University of Michigan, Ann Arbor; University of Michigan, Ann Arbor",
        "aff_domain": "umich.edu;umich.edu;umich.edu",
        "email": "umich.edu;umich.edu;umich.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Ann Arbor",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.234",
        "title": "Learning Syntax from Naturally-Occurring Bracketings",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Naturally-occurring bracketings, such as answer fragments to natural language questions and hyperlinks on webpages, can reflect human syntactic intuition regarding phrasal boundaries. Their availability and approximate correspondence to syntax make them appealing as distant information sources to incorporate into unsupervised constituency parsing. But they are noisy and incomplete; to address this challenge, we develop a partial-brackets-aware structured ramp loss in learning. Experiments demonstrate that our distantly-supervised models trained on naturally-occurring bracketing data are more accurate in inducing syntactic structures than competing unsupervised systems. On the English WSJ corpus, our models achieve an unlabeled F1 score of 68.9 for constituency parsing.",
        "author": "Tianze Shi; Ozan \u0130rsoy; Igor Malioutov; Lillian Lee",
        "authorids": "/t/tianze-shi/; /o/ozan-irsoy/; /i/igor-malioutov/; /l/lillian-lee/",
        "bibtex": "@inproceedings{shi-etal-2021-learning,\n    title = \"Learning Syntax from Naturally-Occurring Bracketings\",\n    author = \"Shi, Tianze  and\n      {\\.I}rsoy, Ozan  and\n      Malioutov, Igor  and\n      Lee, Lillian\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.234/\",\n    doi = \"10.18653/v1/2021.naacl-main.234\",\n    pages = \"2941--2949\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.234.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.234/",
        "pdf_size": 397543,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17666096277220304224&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Cornell University; Bloomberg L.P.; Bloomberg L.P.; Cornell University",
        "aff_domain": "cs.cornell.edu;bloomberg.net;bloomberg.net;cs.cornell.edu",
        "email": "cs.cornell.edu;bloomberg.net;bloomberg.net;cs.cornell.edu",
        "github": "https://github.com/tzshi/nob-naacl21",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "Cornell University;Bloomberg",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cornell.edu;https://www.bloomberg.com",
        "aff_unique_abbr": "Cornell;Bloomberg",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.219",
        "title": "Learning from Executions for Semantic Parsing",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Semantic parsing aims at translating natural language (NL) utterances onto machine-interpretable programs, which can be executed against a real-world environment. The expensive annotation of utterance-program pairs has long been acknowledged as a major bottleneck for the deployment of contemporary neural models to real-life applications. In this work, we focus on the task of semi-supervised learning where a limited amount of annotated data is available together with many unlabeled NL utterances. Based on the observation that programs which correspond to NL utterances should always be executable, we propose to encourage a parser to generate executable programs for unlabeled utterances. Due to the large search space of executable programs, conventional methods that use beam-search for approximation, such as self-training and top-k marginal likelihood training, do not perform as well. Instead, we propose a set of new training objectives that are derived by approaching the problem of learning from executions from the posterior regularization perspective. Our new objectives outperform conventional methods on Overnight and GeoQuery, bridging the gap between semi-supervised and supervised learning.",
        "author": "Bailin Wang; Mirella Lapata; Ivan Titov",
        "authorids": "/b/bailin-wang/; /m/mirella-lapata/; /i/ivan-titov/",
        "bibtex": "@inproceedings{wang-etal-2021-learning-executions,\n    title = \"Learning from Executions for Semantic Parsing\",\n    author = \"Wang, Bailin  and\n      Lapata, Mirella  and\n      Titov, Ivan\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.219/\",\n    doi = \"10.18653/v1/2021.naacl-main.219\",\n    pages = \"2747--2759\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.219.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.219/",
        "pdf_size": 732972,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11958395465273607196&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Institute for Language, Cognition and Computation, School of Informatics, University of Edinburgh; Institute for Language, Cognition and Computation, School of Informatics, University of Edinburgh; Institute for Language, Cognition and Computation, School of Informatics, University of Edinburgh",
        "aff_domain": "ed.ac.uk;inf.ed.ac.uk;inf.ed.ac.uk",
        "email": "ed.ac.uk;inf.ed.ac.uk;inf.ed.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Edinburgh",
        "aff_unique_dep": "School of Informatics",
        "aff_unique_url": "https://www.ed.ac.uk",
        "aff_unique_abbr": "Edinburgh",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Edinburgh",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2021.naacl-main.217",
        "title": "Learning to Decompose and Organize Complex Tasks",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "People rely on digital task management tools, such as email or to-do apps, to manage their tasks. Some of these tasks are large and complex, leading to action paralysis and feelings of being overwhelmed on the part of the user. The micro-productivity literature has shown that such tasks could benefit from being decomposed and organized, in order to reduce user cognitive load. Thus in this paper, we propose a novel end-to-end pipeline that consumes a complex task and induces a dependency graph from unstructured text to represent sub-tasks and their relationships. Our solution first finds nodes for sub-tasks from multiple \u2018how-to\u2019 articles on the web by injecting a neural text generator with three key desiderata \u2013 relevance, abstraction, and consensus. Then we resolve and infer edges between these subtask nodes by learning task dependency relations. We collect a new dataset of complex tasks with their sub-task graph to develop and evaluate our solutions. Both components of our graph induction solution are evaluated in experiments, demonstrating that our models outperform a state-of-the-art text generator significantly. Our generalizable and scalable end-to-end solution has important implications for boosting user productivity and assisting with digital task management.",
        "author": "Yi Zhang; Sujay Kumar Jauhar; Julia Kiseleva; Ryen White; Dan Roth",
        "authorids": "/y/yi-zhang/; /s/sujay-kumar-jauhar/; /j/julia-kiseleva/; /r/ryen-white/; /d/dan-roth/",
        "bibtex": "@inproceedings{zhang-etal-2021-learning,\n    title = \"Learning to Decompose and Organize Complex Tasks\",\n    author = \"Zhang, Yi  and\n      Jauhar, Sujay Kumar  and\n      Kiseleva, Julia  and\n      White, Ryen  and\n      Roth, Dan\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.217/\",\n    doi = \"10.18653/v1/2021.naacl-main.217\",\n    pages = \"2726--2735\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.217.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.217/",
        "pdf_size": 424834,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2827274988319216115&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "University of Pennsylvania; Microsoft Research; Microsoft Research; Microsoft Research; University of Pennsylvania",
        "aff_domain": "cis.upenn.edu;microsoft.com;microsoft.com;microsoft.com;cis.upenn.edu",
        "email": "cis.upenn.edu;microsoft.com;microsoft.com;microsoft.com;cis.upenn.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;1;1;0",
        "aff_unique_norm": "University of Pennsylvania;Microsoft Corporation",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.upenn.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "UPenn;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.304",
        "title": "Learning to Learn to be Right for the Right Reasons",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Improving model generalization on held-out data is one of the core objectives in common- sense reasoning. Recent work has shown that models trained on the dataset with superficial cues tend to perform well on the easy test set with superficial cues but perform poorly on the hard test set without superficial cues. Previous approaches have resorted to manual methods of encouraging models not to overfit to superficial cues. While some of the methods have improved performance on hard instances, they also lead to degraded performance on easy in- stances. Here, we propose to explicitly learn a model that does well on both the easy test set with superficial cues and the hard test set without superficial cues. Using a meta-learning objective, we learn such a model that improves performance on both the easy test set and the hard test set. By evaluating our models on Choice of Plausible Alternatives (COPA) and Commonsense Explanation, we show that our proposed method leads to improved performance on both the easy test set and the hard test set upon which we observe up to 16.5 percentage points improvement over the baseline.",
        "author": "Pride Kavumba; Benjamin Heinzerling; Ana Brassard; Kentaro Inui",
        "authorids": "/p/pride-kavumba/; /b/benjamin-heinzerling/; /a/ana-brassard/; /k/kentaro-inui/",
        "bibtex": "@inproceedings{kavumba-etal-2021-learning,\n    title = \"Learning to Learn to be Right for the Right Reasons\",\n    author = \"Kavumba, Pride  and\n      Heinzerling, Benjamin  and\n      Brassard, Ana  and\n      Inui, Kentaro\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.304/\",\n    doi = \"10.18653/v1/2021.naacl-main.304\",\n    pages = \"3890--3898\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.304.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.304/",
        "pdf_size": 435925,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14361153951205847457&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Tohoku University+RIKEN AIP; RIKEN AIP+Tohoku University; RIKEN AIP+Tohoku University; Tohoku University+RIKEN AIP",
        "aff_domain": "ecei.tohoku.ac.jp;riken.jp;riken.jp;ecei.tohoku.ac.jp",
        "email": "ecei.tohoku.ac.jp;riken.jp;riken.jp;ecei.tohoku.ac.jp",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;1+0;1+0;0+1",
        "aff_unique_norm": "Tohoku University;RIKEN",
        "aff_unique_dep": ";Advanced Institute for Computational Science",
        "aff_unique_url": "https://www.tohoku.ac.jp;https://www.aip.riken.jp",
        "aff_unique_abbr": "Tohoku U;RIKEN AIP",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2021.naacl-main.134",
        "title": "Learning to Organize a Bag of Words into Sentences with Neural Networks: An Empirical Study",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Sequential information, a.k.a., orders, is assumed to be essential for processing a sequence with recurrent neural network or convolutional neural network based encoders. However, is it possible to encode natural languages without orders? Given a bag of words from a disordered sentence, humans may still be able to understand what those words mean by reordering or reconstructing them. Inspired by such an intuition, in this paper, we perform a study to investigate how \u201corder\u201d information takes effects in natural language learning. By running comprehensive comparisons, we quantitatively compare the ability of several representative neural models to organize sentences from a bag of words under three typical scenarios, and summarize some empirical findings and challenges, which can shed light on future research on this line of work.",
        "author": "Chongyang Tao; Shen Gao; Juntao Li; Yansong Feng; Dongyan Zhao; Rui Yan",
        "authorids": "/c/chongyang-tao/; /s/shen-gao/; /j/juntao-li/; /y/yansong-feng/; /d/dongyan-zhao/; /r/rui-yan/",
        "bibtex": "@inproceedings{tao-etal-2021-learning,\n    title = \"Learning to Organize a Bag of Words into Sentences with Neural Networks: An Empirical Study\",\n    author = \"Tao, Chongyang  and\n      Gao, Shen  and\n      Li, Juntao  and\n      Feng, Yansong  and\n      Zhao, Dongyan  and\n      Yan, Rui\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.134/\",\n    doi = \"10.18653/v1/2021.naacl-main.134\",\n    pages = \"1682--1691\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.134.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.134/",
        "pdf_size": 481849,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8265096969756554009&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Wangxuan Institute of Computer Technology, Peking University; Wangxuan Institute of Computer Technology, Peking University; Wangxuan Institute of Computer Technology, Peking University; Wangxuan Institute of Computer Technology, Peking University; Wangxuan Institute of Computer Technology, Peking University + Gaoling School of Arti\ufb01cial Intelligence, Renmin University of China + Beijing Academy of Arti\ufb01cial Intelligence; Wangxuan Institute of Computer Technology, Peking University + Gaoling School of Arti\ufb01cial Intelligence, Renmin University of China + Beijing Academy of Arti\ufb01cial Intelligence",
        "aff_domain": "pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn;ruc.edu.cn",
        "email": "pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn;ruc.edu.cn",
        "github": "",
        "project": "https://quizlet.com/143171956/arrange-words-and-form-meaningful-sentences-\ufb02ash-cards/",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0+1+2;0+1+2",
        "aff_unique_norm": "Peking University;Renmin University of China;Beijing Academy of Artificial Intelligence",
        "aff_unique_dep": "Wangxuan Institute of Computer Technology;Gaoling School of Arti\ufb01cial Intelligence;",
        "aff_unique_url": "http://www.pku.edu.cn;http://www.ruc.edu.cn;https://www.baaic.cn",
        "aff_unique_abbr": "PKU;RUC;BAAI",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0;0;0;0;0+0+0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2021.naacl-main.184",
        "title": "Learning to Recognize Dialect Features",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Building NLP systems that serve everyone requires accounting for dialect differences. But dialects are not monolithic entities: rather, distinctions between and within dialects are captured by the presence, absence, and frequency of dozens of dialect features in speech and text, such as the deletion of the copula in \u201cHe \u2205 running\u201d. In this paper, we introduce the task of dialect feature detection, and present two multitask learning approaches, both based on pretrained transformers. For most dialects, large-scale annotated corpora for these features are unavailable, making it difficult to train recognizers. We train our models on a small number of minimal pairs, building on how linguists typically define dialect features. Evaluation on a test set of 22 dialect features of Indian English demonstrates that these models learn to recognize many features with high accuracy, and that a few minimal pairs can be as effective for training as thousands of labeled examples. We also demonstrate the downstream applicability of dialect feature detection both as a measure of dialect density and as a dialect classifier.",
        "author": "Dorottya Demszky; Devyani Sharma; Jonathan Clark; Vinodkumar Prabhakaran; Jacob Eisenstein",
        "authorids": "/d/dorottya-demszky/; /d/devyani-sharma/; /j/jonathan-h-clark/; /v/vinodkumar-prabhakaran/; /j/jacob-eisenstein/",
        "bibtex": "@inproceedings{demszky-etal-2021-learning,\n    title = \"Learning to Recognize Dialect Features\",\n    author = \"Demszky, Dorottya  and\n      Sharma, Devyani  and\n      Clark, Jonathan  and\n      Prabhakaran, Vinodkumar  and\n      Eisenstein, Jacob\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.184/\",\n    doi = \"10.18653/v1/2021.naacl-main.184\",\n    pages = \"2315--2338\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.184.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.184/",
        "pdf_size": 1362750,
        "gs_citation": 50,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3674570071911754290&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 10,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5
    },
    {
        "id": "2021.naacl-main.220",
        "title": "Learning to Synthesize Data for Semantic Parsing",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Synthesizing data for semantic parsing has gained increasing attention recently. However, most methods require handcrafted (high-precision) rules in their generative process, hindering the exploration of diverse unseen data. In this work, we propose a generative model which features a (non-neural) PCFG that models the composition of programs (e.g., SQL), and a BART-based translation model that maps a program to an utterance. Due to the simplicity of PCFG and pre-trained BART, our generative model can be efficiently learned from existing data at hand. Moreover, explicitly modeling compositions using PCFG leads to better exploration of unseen programs, thus generate more diverse data. We evaluate our method in both in-domain and out-of-domain settings of text-to-SQL parsing on the standard benchmarks of GeoQuery and Spider, respectively. Our empirical results show that the synthesized data generated from our model can substantially help a semantic parser achieve better compositional and domain generalization.",
        "author": "Bailin Wang; Wenpeng Yin; Xi Victoria Lin; Caiming Xiong",
        "authorids": "/b/bailin-wang/; /w/wenpeng-yin/; /x/xi-victoria-lin/; /c/caiming-xiong/",
        "bibtex": "@inproceedings{wang-etal-2021-learning-synthesize,\n    title = \"Learning to Synthesize Data for Semantic Parsing\",\n    author = \"Wang, Bailin  and\n      Yin, Wenpeng  and\n      Lin, Xi Victoria  and\n      Xiong, Caiming\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.220/\",\n    doi = \"10.18653/v1/2021.naacl-main.220\",\n    pages = \"2760--2766\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.220.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.220/",
        "pdf_size": 344158,
        "gs_citation": 57,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12672122667776747641&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "University of Edinburgh; Salesforce Research; Facebook AI; Salesforce Research",
        "aff_domain": "ed.ac.uk;salesforce.com;fb.com;salesforce.com",
        "email": "ed.ac.uk;salesforce.com;fb.com;salesforce.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;2;1",
        "aff_unique_norm": "University of Edinburgh;Salesforce;Facebook",
        "aff_unique_dep": ";Salesforce Research;Facebook AI",
        "aff_unique_url": "https://www.ed.ac.uk;https://research.salesforce.com;https://www.facebook.com",
        "aff_unique_abbr": "Edinburgh;Salesforce;Facebook AI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;1",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "2021.naacl-main.358",
        "title": "Leveraging Deep Representations of Radiology Reports in Survival Analysis for Predicting Heart Failure Patient Mortality",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Utilizing clinical texts in survival analysis is difficult because they are largely unstructured. Current automatic extraction models fail to capture textual information comprehensively since their labels are limited in scope. Furthermore, they typically require a large amount of data and high-quality expert annotations for training. In this work, we present a novel method of using BERT-based hidden layer representations of clinical texts as covariates for proportional hazards models to predict patient survival outcomes. We show that hidden layers yield notably more accurate predictions than predefined features, outperforming the previous baseline model by 5.7% on average across C-index and time-dependent AUC. We make our work publicly available at https://github.com/bionlplab/heart_failure_mortality.",
        "author": "Hyun Gi Lee; Evan Sholle; Ashley Beecy; Subhi Al\u2019Aref; Yifan Peng",
        "authorids": "/h/hyun-gi-lee/; /e/evan-sholle/; /a/ashley-beecy/; /s/subhi-alaref/; /y/yifan-peng-cmu/",
        "bibtex": "@inproceedings{lee-etal-2021-leveraging,\n    title = \"Leveraging Deep Representations of Radiology Reports in Survival Analysis for Predicting Heart Failure Patient Mortality\",\n    author = \"Lee, Hyun Gi  and\n      Sholle, Evan  and\n      Beecy, Ashley  and\n      Al{'}Aref, Subhi  and\n      Peng, Yifan\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.358/\",\n    doi = \"10.18653/v1/2021.naacl-main.358\",\n    pages = \"4533--4538\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.358.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.358/",
        "pdf_size": 450843,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12833475942913719984&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Population Health Sciences, Weill Cornell Medicine; Information Technologies and Services, Weill Cornell Medicine; Department of Medicine, Weill Cornell Medicine; Department of Internal Medicine, UAMS College of Medicine; Department of Population Health Sciences, Weill Cornell Medicine",
        "aff_domain": "; ; ; ; ",
        "email": "; ; ; ; ",
        "github": "https://github.com/bionlplab/heart_failure_mortality",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "Weill Cornell Medicine;UAMS College of Medicine",
        "aff_unique_dep": "Department of Population Health Sciences;Department of Internal Medicine",
        "aff_unique_url": "https://weill.cornell.edu;https://www.uams.edu",
        "aff_unique_abbr": ";UAMS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.448",
        "title": "Leveraging Slot Descriptions for Zero-Shot Cross-Domain Dialogue StateTracking",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Zero-shot cross-domain dialogue state tracking (DST) enables us to handle unseen domains without the expense of collecting in-domain data. In this paper, we propose a slot descriptions enhanced generative approach for zero-shot cross-domain DST. Specifically, our model first encodes a dialogue context and a slot with a pre-trained self-attentive encoder, and generates slot value in auto-regressive manner. In addition, we incorporate Slot Type Informed Descriptions that capture the shared information of different slots to facilitates the cross-domain knowledge transfer. Experimental results on MultiWOZ shows that our model significantly improve existing state-of-the-art results in zero-shot cross-domain setting.",
        "author": "Zhaojiang Lin; Bing Liu; Seungwhan Moon; Paul Crook; Zhenpeng Zhou; Zhiguang Wang; Zhou Yu; Andrea Madotto; Eunjoon Cho; Rajen Subba",
        "authorids": "/z/zhaojiang-lin/; /b/bing-liu/; /s/seungwhan-moon/; /p/paul-a-crook/; /z/zhenpeng-zhou/; /z/zhiguang-wang/; /z/zhou-yu/; /a/andrea-madotto/; /e/eunjoon-cho/; /r/rajen-subba/",
        "bibtex": "@inproceedings{lin-etal-2021-leveraging,\n    title = \"Leveraging Slot Descriptions for Zero-Shot Cross-Domain Dialogue {S}tate{T}racking\",\n    author = \"Lin, Zhaojiang  and\n      Liu, Bing  and\n      Moon, Seungwhan  and\n      Crook, Paul  and\n      Zhou, Zhenpeng  and\n      Wang, Zhiguang  and\n      Yu, Zhou  and\n      Madotto, Andrea  and\n      Cho, Eunjoon  and\n      Subba, Rajen\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.448/\",\n    doi = \"10.18653/v1/2021.naacl-main.448\",\n    pages = \"5640--5648\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.448.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.448/",
        "pdf_size": 463348,
        "gs_citation": 100,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9584311572460371415&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "The Hong Kong University of Science and Technology; Facebook; Facebook; Facebook; Facebook; Facebook; Columbia University; The Hong Kong University of Science and Technology; Facebook; Facebook",
        "aff_domain": "ust.hk;fb.com; ; ; ; ; ;ust.hk; ; ",
        "email": "ust.hk;fb.com; ; ; ; ; ;ust.hk; ; ",
        "github": "",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0;1;1;1;1;1;2;0;1;1",
        "aff_unique_norm": "Hong Kong University of Science and Technology;Facebook, Inc.;Columbia University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.ust.hk;https://www.facebook.com;https://www.columbia.edu",
        "aff_unique_abbr": "HKUST;FB;Columbia",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0;1;1;1;1;1;1;0;1;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2021.naacl-main.183",
        "title": "Lifelong Learning of Hate Speech Classification on Social Media",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Existing work on automated hate speech classification assumes that the dataset is fixed and the classes are pre-defined. However, the amount of data in social media increases every day, and the hot topics changes rapidly, requiring the classifiers to be able to continuously adapt to new data without forgetting the previously learned knowledge. This ability, referred to as lifelong learning, is crucial for the real-word application of hate speech classifiers in social media. In this work, we propose lifelong learning of hate speech classification on social media. To alleviate catastrophic forgetting, we propose to use Variational Representation Learning (VRL) along with a memory module based on LB-SOINN (Load-Balancing Self-Organizing Incremental Neural Network). Experimentally, we show that combining variational representation learning and the LB-SOINN memory module achieves better performance than the commonly-used lifelong learning techniques.",
        "author": "Jing Qian; Hong Wang; Mai ElSherief; Xifeng Yan",
        "authorids": "/j/jing-qian/; /h/hong-wang/; /m/mai-elsherief/; /x/xifeng-yan/",
        "bibtex": "@inproceedings{qian-etal-2021-lifelong,\n    title = \"Lifelong Learning of Hate Speech Classification on Social Media\",\n    author = \"Qian, Jing  and\n      Wang, Hong  and\n      ElSherief, Mai  and\n      Yan, Xifeng\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.183/\",\n    doi = \"10.18653/v1/2021.naacl-main.183\",\n    pages = \"2304--2314\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.183.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.183/",
        "pdf_size": 664153,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8558898198224921295&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "University of California, Santa Barbara\u2020; University of California, Santa Barbara\u2020; Georgia Institute of Technology\u2217; University of California, Santa Barbara\u2020",
        "aff_domain": "cs.ucsb.edu;cs.ucsb.edu;gmail.com;cs.ucsb.edu",
        "email": "cs.ucsb.edu;cs.ucsb.edu;gmail.com;cs.ucsb.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "University of California, Santa Barbara;Georgia Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ucsb.edu;https://www.gatech.edu",
        "aff_unique_abbr": "UCSB;Georgia Tech",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Santa Barbara;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-industry.15",
        "title": "LightSeq: A High Performance Inference Library for Transformers",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Transformer and its variants have achieved great success in natural language processing. Since Transformer models are huge in size, serving these models is a challenge for real industrial applications. In this paper, we propose , a highly efficient inference library for models in the Transformer family. includes a series of GPU optimization techniques to both streamline the computation of Transformer layers and reduce memory footprint. supports models trained using PyTorch and Tensorflow. Experimental results on standard machine translation benchmarks show that achieves up to 14x speedup compared with TensorFlow and 1.4x speedup compared with , a concurrent CUDA implementation. The code will be released publicly after the review.",
        "author": "Xiaohui Wang; Ying Xiong; Yang Wei; Mingxuan Wang; Lei Li",
        "authorids": "/x/xiaohui-wang/; /y/ying-xiong/; /y/yang-wei/; /m/mingxuan-wang/; /l/lei-li/",
        "bibtex": "@inproceedings{wang-etal-2021-lightseq,\n    title = \"{L}ight{S}eq: A High Performance Inference Library for Transformers\",\n    author = \"Wang, Xiaohui  and\n      Xiong, Ying  and\n      Wei, Yang  and\n      Wang, Mingxuan  and\n      Li, Lei\",\n    editor = \"Kim, Young-bum  and\n      Li, Yunyao  and\n      Rambow, Owen\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-industry.15/\",\n    doi = \"10.18653/v1/2021.naacl-industry.15\",\n    pages = \"113--120\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-industry.15.pdf",
        "site": "https://aclanthology.org/2021.naacl-industry.15/",
        "pdf_size": 503700,
        "gs_citation": 77,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9902069686086287520&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "ByteDance AI Lab; ByteDance AI Lab; ByteDance AI Lab; ByteDance AI Lab; ByteDance AI Lab",
        "aff_domain": "bytedance.com;bytedance.com;bytedance.com;bytedance.com;bytedance.com",
        "email": "bytedance.com;bytedance.com;bytedance.com;bytedance.com;bytedance.com",
        "github": "https://github.com/bytedance/lightseq",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "ByteDance",
        "aff_unique_dep": "AI Lab",
        "aff_unique_url": "https://www.bytedance.com",
        "aff_unique_abbr": "ByteDance",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2021.naacl-main.77",
        "title": "LightningDOT: Pre-training Visual-Semantic Embeddings for Real-Time Image-Text Retrieval",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Multimodal pre-training has propelled great advancement in vision-and-language research. These large-scale pre-trained models, although successful, fatefully suffer from slow inference speed due to enormous computational cost mainly from cross-modal attention in Transformer architecture. When applied to real-life applications, such latency and computation demand severely deter the practical use of pre-trained models. In this paper, we study Image-text retrieval (ITR), the most mature scenario of V+L application, which has been widely studied even prior to the emergence of recent pre-trained models. We propose a simple yet highly effective approach, LightningDOT that accelerates the inference time of ITR by thousands of times, without sacrificing accuracy. LightningDOT removes the time-consuming cross-modal attention by extracting pre-cached feature indexes offline, and employing instant dot-product matching online, which significantly speeds up retrieval process. In fact, our LightningDOT achieves superior performance across mainstream ITR benchmarks such as Flickr30k and COCO datasets, outperforming existing pre-trained models that consume 1000 times magnitude of computational hours using the same features.",
        "author": "Siqi Sun; Yen-Chun Chen; Linjie Li; Shuohang Wang; Yuwei Fang; Jingjing Liu",
        "authorids": "/s/siqi-sun/; /y/yen-chun-chen/; /l/linjie-li/; /s/shuohang-wang/; /y/yuwei-fang/; /j/jingjing-liu/",
        "bibtex": "@inproceedings{sun-etal-2021-lightningdot,\n    title = \"{L}ightning{DOT}: Pre-training Visual-Semantic Embeddings for Real-Time Image-Text Retrieval\",\n    author = \"Sun, Siqi  and\n      Chen, Yen-Chun  and\n      Li, Linjie  and\n      Wang, Shuohang  and\n      Fang, Yuwei  and\n      Liu, Jingjing\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.77/\",\n    doi = \"10.18653/v1/2021.naacl-main.77\",\n    pages = \"982--997\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.77.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.77/",
        "pdf_size": 4483580,
        "gs_citation": 97,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15756762716755306074&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Microsoft Corporation; Microsoft Corporation; Microsoft Corporation; Microsoft Corporation; Microsoft Corporation; Microsoft Corporation",
        "aff_domain": "microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "email": "microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "github": "https://github.com/intersun/LightningDOT",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Microsoft Corporation",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.microsoft.com",
        "aff_unique_abbr": "Microsoft",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.405",
        "title": "Limitations of Autoregressive Models and Their Alternatives",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Standard autoregressive language models perform only polynomial-time computation to compute the probability of the next symbol. While this is attractive, it means they cannot model distributions whose next-symbol probability is hard to compute. Indeed, they cannot even model them well enough to solve associated easy decision problems for which an engineer might want to consult a language model. These limitations apply no matter how much computation and data are used to train the model, unless the model is given access to oracle parameters that grow superpolynomially in sequence length. Thus, simply training larger autoregressive language models is not a panacea for NLP. Alternatives include energy-based models (which give up efficient sampling) and latent-variable autoregressive models (which give up efficient scoring of a given string). Both are powerful enough to escape the above limitations.",
        "author": "Chu-Cheng Lin; Aaron Jaech; Xin Li; Matthew R. Gormley; Jason Eisner",
        "authorids": "/c/chu-cheng-lin/; /a/aaron-jaech/; /x/xin-li/; /m/matthew-r-gormley/; /j/jason-eisner/",
        "bibtex": "@inproceedings{lin-etal-2021-limitations,\n    title = \"Limitations of Autoregressive Models and Their Alternatives\",\n    author = \"Lin, Chu-Cheng  and\n      Jaech, Aaron  and\n      Li, Xin  and\n      Gormley, Matthew R.  and\n      Eisner, Jason\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.405/\",\n    doi = \"10.18653/v1/2021.naacl-main.405\",\n    pages = \"5147--5173\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.405.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.405/",
        "pdf_size": 697067,
        "gs_citation": 76,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5089222399889643698&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "department of computer science, johns hopkins university; facebook ai; department of computer science, johns hopkins university; machine learning department, carnegie mellon university; department of computer science, johns hopkins university",
        "aff_domain": "cs.jhu.edu;fb.com;cs.jhu.edu;cs.cmu.edu;cs.jhu.edu",
        "email": "cs.jhu.edu;fb.com;cs.jhu.edu;cs.cmu.edu;cs.jhu.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;0;2;0",
        "aff_unique_norm": "Johns Hopkins University;Facebook;Carnegie Mellon University",
        "aff_unique_dep": "Department of Computer Science;Facebook AI;Machine Learning Department",
        "aff_unique_url": "https://www.jhu.edu;https://ai.facebook.com;https://www.cmu.edu",
        "aff_unique_abbr": "JHU;FAI;CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.352",
        "title": "Linguistic Complexity Loss in Text-Based Therapy",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The complexity loss paradox, which posits that individuals suffering from disease exhibit surprisingly predictable behavioral dynamics, has been observed in a variety of both human and animal physiological systems. The recent advent of online text-based therapy presents a new opportunity to analyze the complexity loss paradox in a novel operationalization: linguistic complexity loss in text-based therapy conversations. In this paper, we analyze linguistic complexity correlates of mental health in the online therapy messages sent between therapists and 7,170 clients who provided 30,437 corresponding survey responses on their anxiety. We found that when clients reported more anxiety, they showed reduced lexical diversity as estimated by the moving average type-token ratio. Therapists, on the other hand, used language of higher reading difficulty, syntactic complexity, and age of acquisition when clients were more anxious. Finally, we found that clients, and to an even greater extent, therapists, exhibited consistent levels of many linguistic complexity measures. These results demonstrate how linguistic analysis of text-based communication can be leveraged as a marker for anxiety, an exciting prospect in a time of both increased online communication and increased mental health issues.",
        "author": "Jason Wei; Kelly Finn; Emma Templeton; Thalia Wheatley; Soroush Vosoughi",
        "authorids": "/j/jason-wei/; /k/kelly-finn/; /e/emma-templeton/; /t/thalia-wheatley/; /s/soroush-vosoughi/",
        "bibtex": "@inproceedings{wei-etal-2021-linguistic,\n    title = \"Linguistic Complexity Loss in Text-Based Therapy\",\n    author = \"Wei, Jason  and\n      Finn, Kelly  and\n      Templeton, Emma  and\n      Wheatley, Thalia  and\n      Vosoughi, Soroush\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.352/\",\n    doi = \"10.18653/v1/2021.naacl-main.352\",\n    pages = \"4450--4459\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.352.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.352/",
        "pdf_size": 593452,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7866530884231117134&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Dartmouth College, Hanover, NH + Department of Computer Science + Neukom Institute + Department of Psychological and Brain Sciences; Dartmouth College, Hanover, NH + Department of Computer Science + Neukom Institute + Department of Psychological and Brain Sciences; Dartmouth College, Hanover, NH + Department of Computer Science + Neukom Institute + Department of Psychological and Brain Sciences; Dartmouth College, Hanover, NH + Department of Computer Science + Neukom Institute + Department of Psychological and Brain Sciences; Dartmouth College, Hanover, NH + Department of Computer Science + Neukom Institute + Department of Psychological and Brain Sciences",
        "aff_domain": "dartmouth.edu;dartmouth.edu;dartmouth.edu;dartmouth.edu;dartmouth.edu",
        "email": "dartmouth.edu;dartmouth.edu;dartmouth.edu;dartmouth.edu;dartmouth.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1+2+3;0+1+2+3;0+1+2+3;0+1+2+3;0+1+2+3",
        "aff_unique_norm": "Dartmouth College;Unknown Institution;Neukom Institute;University of California, Santa Barbara",
        "aff_unique_dep": ";Department of Computer Science;;Department of Psychological and Brain Sciences",
        "aff_unique_url": "https://dartmouth.edu;;;https://www.psych.ucsb.edu",
        "aff_unique_abbr": "Dartmouth;;;UCSB",
        "aff_campus_unique_index": "0+2;0+2;0+2;0+2;0+2",
        "aff_campus_unique": "Hanover;;Santa Barbara",
        "aff_country_unique_index": "0+0+0;0+0+0;0+0+0;0+0+0;0+0+0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "2021.naacl-main.65",
        "title": "Linking Entities to Unseen Knowledge Bases with Arbitrary Schemas",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In entity linking, mentions of named entities in raw text are disambiguated against a knowledge base (KB). This work focuses on linking to unseen KBs that do not have training data and whose schema is unknown during training. Our approach relies on methods to flexibly convert entities with several attribute-value pairs from arbitrary KBs into flat strings, which we use in conjunction with state-of-the-art models for zero-shot linking. We further improve the generalization of our model using two regularization schemes based on shuffling of entity attributes and handling of unseen attributes. Experiments on English datasets where models are trained on the CoNLL dataset, and tested on the TAC-KBP 2010 dataset show that our models are 12% (absolute) more accurate than baseline models that simply flatten entities from the target KB. Unlike prior work, our approach also allows for seamlessly combining multiple training datasets. We test this ability by adding both a completely different dataset (Wikia), as well as increasing amount of training data from the TAC-KBP 2010 training set. Our models are more accurate across the board compared to baselines.",
        "author": "Yogarshi Vyas; Miguel Ballesteros",
        "authorids": "/y/yogarshi-vyas/; /m/miguel-ballesteros/",
        "bibtex": "@inproceedings{vyas-ballesteros-2021-linking,\n    title = \"Linking Entities to Unseen Knowledge Bases with Arbitrary Schemas\",\n    author = \"Vyas, Yogarshi  and\n      Ballesteros, Miguel\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.65/\",\n    doi = \"10.18653/v1/2021.naacl-main.65\",\n    pages = \"834--844\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.65.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.65/",
        "pdf_size": 345580,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15957177815419055770&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Amazon AI; Amazon AI",
        "aff_domain": "amazon.com;amazon.com",
        "email": "amazon.com;amazon.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Amazon",
        "aff_unique_dep": "Amazon AI",
        "aff_unique_url": "https://www.amazon.com",
        "aff_unique_abbr": "Amazon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.104",
        "title": "Looking Beyond Sentence-Level Natural Language Inference for Question Answering and Text Summarization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Natural Language Inference (NLI) has garnered significant attention in recent years; however, the promise of applying NLI breakthroughs to other downstream NLP tasks has remained unfulfilled. In this work, we use the multiple-choice reading comprehension (MCRC) and checking factual correctness of textual summarization (CFCS) tasks to investigate potential reasons for this. Our findings show that: (1) the relatively shorter length of premises in traditional NLI datasets is the primary challenge prohibiting usage in downstream applications (which do better with longer contexts); (2) this challenge can be addressed by automatically converting resource-rich reading comprehension datasets into longer-premise NLI datasets; and (3) models trained on the converted, longer-premise datasets outperform those trained using short-premise traditional NLI datasets on downstream tasks primarily due to the difference in premise lengths.",
        "author": "Anshuman Mishra; Dhruvesh Patel; Aparna Vijayakumar; Xiang Lorraine Li; Pavan Kapanipathi; Kartik Talamadupula",
        "authorids": "/a/anshuman-mishra/; /d/dhruvesh-patel/; /a/aparna-vijayakumar/; /x/xiang-lorraine-li/; /p/pavan-kapanipathi/; /k/kartik-talamadupula/",
        "bibtex": "@inproceedings{mishra-etal-2021-looking,\n    title = \"Looking Beyond Sentence-Level Natural Language Inference for Question Answering and Text Summarization\",\n    author = \"Mishra, Anshuman  and\n      Patel, Dhruvesh  and\n      Vijayakumar, Aparna  and\n      Li, Xiang Lorraine  and\n      Kapanipathi, Pavan  and\n      Talamadupula, Kartik\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.104/\",\n    doi = \"10.18653/v1/2021.naacl-main.104\",\n    pages = \"1322--1336\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.104.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.104/",
        "pdf_size": 437080,
        "gs_citation": 58,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5596166339974302855&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "College of Information and Computer Sciences, University of Massachusetts Amherst; College of Information and Computer Sciences, University of Massachusetts Amherst; College of Information and Computer Sciences, University of Massachusetts Amherst; College of Information and Computer Sciences, University of Massachusetts Amherst; IBM Research; IBM Research",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;1;1",
        "aff_unique_norm": "University of Massachusetts Amherst;IBM",
        "aff_unique_dep": "College of Information and Computer Sciences;IBM Research",
        "aff_unique_url": "https://www.umass.edu;https://www.ibm.com/research",
        "aff_unique_abbr": "UMass Amherst;IBM",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Amherst;",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.74",
        "title": "Low-Complexity Probing via Finding Subnetworks",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The dominant approach in probing neural networks for linguistic properties is to train a new shallow multi-layer perceptron (MLP) on top of the model\u2019s internal representations. This approach can detect properties encoded in the model, but at the cost of adding new parameters that may learn the task directly. We instead propose a subtractive pruning-based probe, where we find an existing subnetwork that performs the linguistic task of interest. Compared to an MLP, the subnetwork probe achieves both higher accuracy on pre-trained models and lower accuracy on random models, so it is both better at finding properties of interest and worse at learning on its own. Next, by varying the complexity of each probe, we show that subnetwork probing Pareto-dominates MLP probing in that it achieves higher accuracy given any budget of probe complexity. Finally, we analyze the resulting subnetworks across various tasks to locate where each task is encoded, and we find that lower-level tasks are captured in lower layers, reproducing similar findings in past work.",
        "author": "Steven Cao; Victor Sanh; Alexander Rush",
        "authorids": "/s/steven-cao/; /v/victor-sanh/; /a/alexander-m-rush/",
        "bibtex": "@inproceedings{cao-etal-2021-low,\n    title = \"Low-Complexity Probing via Finding Subnetworks\",\n    author = \"Cao, Steven  and\n      Sanh, Victor  and\n      Rush, Alexander\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.74/\",\n    doi = \"10.18653/v1/2021.naacl-main.74\",\n    pages = \"960--966\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.74.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.74/",
        "pdf_size": 829495,
        "gs_citation": 56,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3296787855080733709&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "University of California, Berkeley + Hugging Face; Hugging Face; Hugging Face",
        "aff_domain": "berkeley.edu;huggingface.co;huggingface.co",
        "email": "berkeley.edu;huggingface.co;huggingface.co",
        "github": "https://github.com/stevenxcao/subnetwork-probing",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;1;1",
        "aff_unique_norm": "University of California, Berkeley;Hugging Face",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.berkeley.edu;https://huggingface.co",
        "aff_unique_abbr": "UC Berkeley;Hugging Face",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.336",
        "title": "MERMAID: Metaphor Generation with Symbolism and Discriminative Decoding",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Generating metaphors is a challenging task as it requires a proper understanding of abstract concepts, making connections between unrelated concepts, and deviating from the literal meaning. In this paper, we aim to generate a metaphoric sentence given a literal expression by replacing relevant verbs. Based on a theoretically-grounded connection between metaphors and symbols, we propose a method to automatically construct a parallel corpus by transforming a large number of metaphorical sentences from the Gutenberg Poetry corpus (CITATION) to their literal counterpart using recent advances in masked language modeling coupled with commonsense inference. For the generation task, we incorporate a metaphor discriminator to guide the decoding of a sequence to sequence model fine-tuned on our parallel data to generate high-quality metaphors. Human evaluation on an independent test set of literal statements shows that our best model generates metaphors better than three well-crafted baselines 66% of the time on average. A task-based evaluation shows that human-written poems enhanced with metaphors proposed by our model are preferred 68% of the time compared to poems without metaphors.",
        "author": "Tuhin Chakrabarty; Xurui Zhang; Smaranda Muresan; Nanyun Peng",
        "authorids": "/t/tuhin-chakrabarty/; /x/xurui-zhang/; /s/smaranda-muresan/; /n/nanyun-peng/",
        "bibtex": "@inproceedings{chakrabarty-etal-2021-mermaid,\n    title = \"{MERMAID}: Metaphor Generation with Symbolism and Discriminative Decoding\",\n    author = \"Chakrabarty, Tuhin  and\n      Zhang, Xurui  and\n      Muresan, Smaranda  and\n      Peng, Nanyun\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.336/\",\n    doi = \"10.18653/v1/2021.naacl-main.336\",\n    pages = \"4250--4261\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.336.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.336/",
        "pdf_size": 612007,
        "gs_citation": 77,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2623428158865435821&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science, Columbia University; Tsinghua University + Computer Science Department, University of California, Los Angeles; Department of Computer Science, Columbia University + Data Science Institute, Columbia University; Computer Science Department, University of California, Los Angeles",
        "aff_domain": "cs.columbia.edu;gmail.com;cs.columbia.edu;cs.ucla.edu",
        "email": "cs.columbia.edu;gmail.com;cs.columbia.edu;cs.ucla.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1+2;0+0;2",
        "aff_unique_norm": "Columbia University;Tsinghua University;University of California, Los Angeles",
        "aff_unique_dep": "Department of Computer Science;;Computer Science Department",
        "aff_unique_url": "https://www.columbia.edu;https://www.tsinghua.edu.cn;https://www.ucla.edu",
        "aff_unique_abbr": "Columbia;THU;UCLA",
        "aff_campus_unique_index": "1;;1",
        "aff_campus_unique": ";Los Angeles",
        "aff_country_unique_index": "0;1+0;0+0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2021.naacl-main.418",
        "title": "MIMOQA: Multimodal Input Multimodal Output Question Answering",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Multimodal research has picked up significantly in the space of question answering with the task being extended to visual question answering, charts question answering as well as multimodal input question answering. However, all these explorations produce a unimodal textual output as the answer. In this paper, we propose a novel task - MIMOQA - Multimodal Input Multimodal Output Question Answering in which the output is also multimodal. Through human experiments, we empirically show that such multimodal outputs provide better cognitive understanding of the answers. We also propose a novel multimodal question-answering framework, MExBERT, that incorporates a joint textual and visual attention towards producing such a multimodal output. Our method relies on a novel multimodal dataset curated for this problem from publicly available unimodal datasets. We show the superior performance of MExBERT against strong baselines on both the automatic as well as human metrics.",
        "author": "Hrituraj Singh; Anshul Nasery; Denil Mehta; Aishwarya Agarwal; Jatin Lamba; Balaji Vasan Srinivasan",
        "authorids": "/h/hrituraj-singh/; /a/anshul-nasery/; /d/denil-mehta/; /a/aishwarya-agarwal/; /j/jatin-lamba/; /b/balaji-vasan-srinivasan/",
        "bibtex": "@inproceedings{singh-etal-2021-mimoqa,\n    title = \"{MIMOQA}: Multimodal Input Multimodal Output Question Answering\",\n    author = \"Singh, Hrituraj  and\n      Nasery, Anshul  and\n      Mehta, Denil  and\n      Agarwal, Aishwarya  and\n      Lamba, Jatin  and\n      Srinivasan, Balaji Vasan\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.418/\",\n    doi = \"10.18653/v1/2021.naacl-main.418\",\n    pages = \"5317--5332\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.418.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.418/",
        "pdf_size": 2057387,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1780168209685823067&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Triomics, Noida, India; Indian Institute of Technology Bombay, India; Indian Institute of Technology Bombay, India; Indian Institute of Technology Bombay, India; Indian Institute of Technology Bombay, India; Adobe Research, Bangalore, India",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;1;1;1;2",
        "aff_unique_norm": "Triomics;Indian Institute of Technology Bombay;Adobe Research",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";https://www.iitb.ac.in;https://research.adobe.com",
        "aff_unique_abbr": ";IIT Bombay;Adobe",
        "aff_campus_unique_index": "0;1;1;1;1;2",
        "aff_campus_unique": "Noida;Bombay;Bangalore",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2021.naacl-main.473",
        "title": "MM-AVS: A Full-Scale Dataset for Multi-modal Summarization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Multimodal summarization becomes increasingly significant as it is the basis for question answering, Web search, and many other downstream tasks. However, its learning materials have been lacking a holistic organization by integrating resources from various modalities, thereby lagging behind the research progress of this field. In this study, we release a full-scale multimodal dataset comprehensively gathering documents, summaries, images, captions, videos, audios, transcripts, and titles in English from CNN and Daily Mail. To our best knowledge, this is the first collection that spans all modalities and nearly comprises all types of materials available in this community. In addition, we devise a baseline model based on the novel dataset, which employs a newly proposed Jump-Attention mechanism based on transcripts. The experimental results validate the important assistance role of the external information for multimodal summarization.",
        "author": "Xiyan Fu; Jun Wang; Zhenglu Yang",
        "authorids": "/x/xiyan-fu/; /j/jun-wang/; /z/zhenglu-yang/",
        "bibtex": "@inproceedings{fu-etal-2021-mm,\n    title = \"{MM}-{AVS}: A Full-Scale Dataset for Multi-modal Summarization\",\n    author = \"Fu, Xiyan  and\n      Wang, Jun  and\n      Yang, Zhenglu\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.473/\",\n    doi = \"10.18653/v1/2021.naacl-main.473\",\n    pages = \"5922--5926\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.473.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.473/",
        "pdf_size": 473844,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13251549588069958762&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Nankai University, China; Ludong University, China; Nankai University, China",
        "aff_domain": "mail.nankai.edu.cn;mail.nankai.edu.cn;nankai.edu.cn",
        "email": "mail.nankai.edu.cn;mail.nankai.edu.cn;nankai.edu.cn",
        "github": "https://github.com/xiyan524/MM-AVS",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Nankai University;Ludong University",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.nankai.edu.cn;http://www.ldu.edu.cn",
        "aff_unique_abbr": "NKU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2021.naacl-main.79",
        "title": "MTAG: Modal-Temporal Attention Graph for Unaligned Human Multimodal Language Sequences",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Human communication is multimodal in nature; it is through multiple modalities such as language, voice, and facial expressions, that opinions and emotions are expressed. Data in this domain exhibits complex multi-relational and temporal interactions. Learning from this data is a fundamentally challenging research problem. In this paper, we propose Modal-Temporal Attention Graph (MTAG). MTAG is an interpretable graph-based neural model that provides a suitable framework for analyzing multimodal sequential data. We first introduce a procedure to convert unaligned multimodal sequence data into a graph with heterogeneous nodes and edges that captures the rich interactions across modalities and through time. Then, a novel graph fusion operation, called MTAG fusion, along with a dynamic pruning and read-out technique, is designed to efficiently process this modal-temporal graph and capture various interactions. By learning to focus only on the important interactions within the graph, MTAG achieves state-of-the-art performance on multimodal sentiment analysis and emotion recognition benchmarks, while utilizing significantly fewer model parameters.",
        "author": "Jianing Yang; Yongxin Wang; Ruitao Yi; Yuying Zhu; Azaan Rehman; Amir Zadeh; Soujanya Poria; Louis-Philippe Morency",
        "authorids": "/j/jianing-yang/; /y/yongxin-wang/; /r/ruitao-yi/; /y/yuying-zhu/; /a/azaan-rehman/; /a/amir-zadeh/; /s/soujanya-poria/; /l/louis-philippe-morency/",
        "bibtex": "@inproceedings{yang-etal-2021-mtag,\n    title = \"{MTAG}: Modal-Temporal Attention Graph for Unaligned Human Multimodal Language Sequences\",\n    author = \"Yang, Jianing  and\n      Wang, Yongxin  and\n      Yi, Ruitao  and\n      Zhu, Yuying  and\n      Rehman, Azaan  and\n      Zadeh, Amir  and\n      Poria, Soujanya  and\n      Morency, Louis-Philippe\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.79/\",\n    doi = \"10.18653/v1/2021.naacl-main.79\",\n    pages = \"1009--1021\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.79.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.79/",
        "pdf_size": 10109069,
        "gs_citation": 99,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10239562729889070905&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; Singapore University of Technology and Design; Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;sutd.edu.sg;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;sutd.edu.sg;cs.cmu.edu",
        "github": "https://github.com/jedyang97/MTAG",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;0;0;0;0;1;0",
        "aff_unique_norm": "Carnegie Mellon University;Singapore University of Technology and Design",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cmu.edu;https://www.sutd.edu.sg",
        "aff_unique_abbr": "CMU;SUTD",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;1;0",
        "aff_country_unique": "United States;Singapore"
    },
    {
        "id": "2021.naacl-demos.17",
        "title": "MUDES: Multilingual Detection of Offensive Spans",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "The interest in offensive content identification in social media has grown substantially in recent years. Previous work has dealt mostly with post level annotations. However, identifying offensive spans is useful in many ways. To help coping with this important challenge, we present MUDES, a multilingual system to detect offensive spans in texts. MUDES features pre-trained models, a Python API for developers, and a user-friendly web-based interface. A detailed description of MUDES\u2019 components is presented in this paper.",
        "author": "Tharindu Ranasinghe; Marcos Zampieri",
        "authorids": "/t/tharindu-ranasinghe/; /m/marcos-zampieri/",
        "bibtex": "@inproceedings{ranasinghe-zampieri-2021-mudes,\n    title = \"{MUDES}: Multilingual Detection of Offensive Spans\",\n    author = \"Ranasinghe, Tharindu  and\n      Zampieri, Marcos\",\n    editor = \"Sil, Avi  and\n      Lin, Xi Victoria\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-demos.17/\",\n    doi = \"10.18653/v1/2021.naacl-demos.17\",\n    pages = \"144--152\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-demos.17.pdf",
        "site": "https://aclanthology.org/2021.naacl-demos.17/",
        "pdf_size": 735184,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3855478467180679049&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 10,
        "aff": "University of Wolverhampton; Rochester Institute of Technology",
        "aff_domain": "wlv.ac.uk;rit.edu",
        "email": "wlv.ac.uk;rit.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Wolverhampton;Rochester Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.wolverhampton.ac.uk;https://www.rit.edu",
        "aff_unique_abbr": "Wolverhampton;RIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "2021.naacl-main.216",
        "title": "MUSER: MUltimodal Stress detection using Emotion Recognition as an Auxiliary Task",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The capability to automatically detect human stress can benefit artificial intelligent agents involved in affective computing and human-computer interaction. Stress and emotion are both human affective states, and stress has proven to have important implications on the regulation and expression of emotion. Although a series of methods have been established for multimodal stress detection, limited steps have been taken to explore the underlying inter-dependence between stress and emotion. In this work, we investigate the value of emotion recognition as an auxiliary task to improve stress detection. We propose MUSER \u2013 a transformer-based model architecture and a novel multi-task learning algorithm with speed-based dynamic sampling strategy. Evaluation on the Multimodal Stressed Emotion (MuSE) dataset shows that our model is effective for stress detection with both internal and external auxiliary tasks, and achieves state-of-the-art results.",
        "author": "Yiqun Yao; Michalis Papakostas; Mihai Burzo; Mohamed Abouelenien; Rada Mihalcea",
        "authorids": "/y/yiqun-yao/; /m/michalis-papakostas/; /m/mihai-burzo/; /m/mohamed-abouelenien/; /r/rada-mihalcea/",
        "bibtex": "@inproceedings{yao-etal-2021-muser,\n    title = \"{MUSER}: {MU}ltimodal Stress detection using Emotion Recognition as an Auxiliary Task\",\n    author = \"Yao, Yiqun  and\n      Papakostas, Michalis  and\n      Burzo, Mihai  and\n      Abouelenien, Mohamed  and\n      Mihalcea, Rada\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.216/\",\n    doi = \"10.18653/v1/2021.naacl-main.216\",\n    pages = \"2714--2725\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.216.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.216/",
        "pdf_size": 419090,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6425890528328220819&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Computer Science and Engineering, University of Michigan; Computer Science and Engineering, University of Michigan; Mechanical Engineering, University of Michigan; Computer and Information Science, University of Michigan; Computer Science and Engineering, University of Michigan",
        "aff_domain": "umich.edu;umich.edu;umich.edu;umich.edu;umich.edu",
        "email": "umich.edu;umich.edu;umich.edu;umich.edu;umich.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "Computer Science and Engineering",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Ann Arbor;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.462",
        "title": "Machine Translated Text Detection Through Text Similarity with Round-Trip Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Translated texts have been used for malicious purposes, i.e., plagiarism or fake reviews. Existing detectors have been built around a specific translator (e.g., Google) but fail to detect a translated text from a strange translator. If we use the same translator, the translated text is similar to its round-trip translation, which is when text is translated into another language and translated back into the original language. However, a round-trip translated text is significantly different from the original text or a translated text using a strange translator. Hence, we propose a detector using text similarity with round-trip translation (TSRT). TSRT achieves 86.9% accuracy in detecting a translated text from a strange translator. It outperforms existing detectors (77.9%) and human recognition (53.3%).",
        "author": "Hoang-Quoc Nguyen-Son; Tran Thao; Seira Hidano; Ishita Gupta; Shinsaku Kiyomoto",
        "authorids": "/h/hoang-quoc-nguyen-son/; /t/tran-thao/; /s/seira-hidano/; /i/ishita-gupta/; /s/shinsaku-kiyomoto/",
        "bibtex": "@inproceedings{nguyen-son-etal-2021-machine,\n    title = \"Machine Translated Text Detection Through Text Similarity with Round-Trip Translation\",\n    author = \"Nguyen-Son, Hoang-Quoc  and\n      Thao, Tran  and\n      Hidano, Seira  and\n      Gupta, Ishita  and\n      Kiyomoto, Shinsaku\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.462/\",\n    doi = \"10.18653/v1/2021.naacl-main.462\",\n    pages = \"5792--5797\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.462.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.462/",
        "pdf_size": 271792,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1656654424145878459&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "KDDI Research, Inc., Japan; The University of Tokyo, Japan; KDDI Research, Inc., Japan; Indian Institute of Technology, India; KDDI Research, Inc., Japan",
        "aff_domain": "kddi-research.jp;kddi-research.jp;kddi-research.jp;yamagula.ic.i.u-tokyo.ac.jp;ee.iitd.ac.in",
        "email": "kddi-research.jp;kddi-research.jp;kddi-research.jp;yamagula.ic.i.u-tokyo.ac.jp;ee.iitd.ac.in",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;0;2;0",
        "aff_unique_norm": "KDDI Research, Inc.;The University of Tokyo;Indian Institute of Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.kddi-research.com;https://www.u-tokyo.ac.jp;https://iit.edu",
        "aff_unique_abbr": "KDDI;UTokyo;IIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1;0",
        "aff_country_unique": "Japan;India"
    },
    {
        "id": "2021.naacl-demos.2",
        "title": "Machine-Assisted Script Curation",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "We describe Machine-Aided Script Curator (MASC), a system for human-machine collaborative script authoring. Scripts produced with MASC include (1) English descriptions of sub-events that comprise a larger, complex event; (2) event types for each of those events; (3) a record of entities expected to participate in multiple sub-events; and (4) temporal sequencing between the sub-events. MASC automates portions of the script creation process with suggestions for event types, links to Wikidata, and sub-events that may have been forgotten. We illustrate how these automations are useful to the script writer with a few case-study scripts.",
        "author": "Manuel Ciosici; Joseph Cummings; Mitchell DeHaven; Alex Hedges; Yash Kankanampati; Dong-Ho Lee; Ralph Weischedel; Marjorie Freedman",
        "authorids": "/m/manuel-r-ciosici/; /j/joseph-cummings/; /m/mitchell-dehaven/; /a/alex-hedges/; /y/yash-kankanampati/; /d/dong-ho-lee/; /r/ralph-weischedel/; /m/marjorie-freedman/",
        "bibtex": "@inproceedings{ciosici-etal-2021-machine,\n    title = \"Machine-Assisted Script Curation\",\n    author = \"Ciosici, Manuel  and\n      Cummings, Joseph  and\n      DeHaven, Mitchell  and\n      Hedges, Alex  and\n      Kankanampati, Yash  and\n      Lee, Dong-Ho  and\n      Weischedel, Ralph  and\n      Freedman, Marjorie\",\n    editor = \"Sil, Avi  and\n      Lin, Xi Victoria\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-demos.2/\",\n    doi = \"10.18653/v1/2021.naacl-demos.2\",\n    pages = \"8--17\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-demos.2.pdf",
        "site": "https://aclanthology.org/2021.naacl-demos.2/",
        "pdf_size": 805880,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5752565562151739251&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": ";;;;;;;",
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "github": "https://github.com/isi-vista/MASC",
        "project": "https://youtu.be/slvZWAYkRmA",
        "author_num": 8
    },
    {
        "id": "2021.naacl-main.90",
        "title": "Macro-Average: Rare Types Are Important Too",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "While traditional corpus-level evaluation metrics for machine translation (MT) correlate well with fluency, they struggle to reflect adequacy. Model-based MT metrics trained on segment-level human judgments have emerged as an attractive replacement due to strong correlation results. These models, however, require potentially expensive re-training for new domains and languages. Furthermore, their decisions are inherently non-transparent and appear to reflect unwelcome biases. We explore the simple type-based classifier metric, MacroF1, and study its applicability to MT evaluation. We find that MacroF1 is competitive on direct assessment, and outperforms others in indicating downstream cross-lingual information retrieval task performance. Further, we show that MacroF1 can be used to effectively compare supervised and unsupervised neural machine translation, and reveal significant qualitative differences in the methods\u2019 outputs.",
        "author": "Thamme Gowda; Weiqiu You; Constantine Lignos; Jonathan May",
        "authorids": "/t/thamme-gowda/; /w/weiqiu-you/; /c/constantine-lignos/; /j/jonathan-may/",
        "bibtex": "@inproceedings{gowda-etal-2021-macro,\n    title = \"Macro-Average: Rare Types Are Important Too\",\n    author = \"Gowda, Thamme  and\n      You, Weiqiu  and\n      Lignos, Constantine  and\n      May, Jonathan\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.90/\",\n    doi = \"10.18653/v1/2021.naacl-main.90\",\n    pages = \"1138--1157\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.90.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.90/",
        "pdf_size": 580865,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15169897901999194882&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Information Sciences Institute, University of Southern California; Dept of Computer and Information Science, University of Pennsylvania; Michtom School of Computer Science, Brandeis University; Information Sciences Institute, University of Southern California",
        "aff_domain": "isi.edu;seas.upenn.edu;brandeis.edu;isi.edu",
        "email": "isi.edu;seas.upenn.edu;brandeis.edu;isi.edu",
        "github": "https://github.com/thammegowda/007-mt-eval-macro",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "University of Southern California;University of Pennsylvania;Brandeis University",
        "aff_unique_dep": "Information Sciences Institute;Dept of Computer and Information Science;Michtom School of Computer Science",
        "aff_unique_url": "https://www.usc.edu;https://www.upenn.edu;https://www.brandeis.edu",
        "aff_unique_abbr": "USC;UPenn;Brandeis",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.135",
        "title": "Mask Attention Networks: Rethinking and Strengthen Transformer",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Transformer is an attention-based neural network, which consists of two sublayers, namely, Self-Attention Network (SAN) and Feed-Forward Network (FFN). Existing research explores to enhance the two sublayers separately to improve the capability of Transformer for text representation. In this paper, we present a novel understanding of SAN and FFN as Mask Attention Networks (MANs) and show that they are two special cases of MANs with static mask matrices. However, their static mask matrices limit the capability for localness modeling in text representation learning. We therefore introduce a new layer named dynamic mask attention network (DMAN) with a learnable mask matrix which is able to model localness adaptively. To incorporate advantages of DMAN, SAN, and FFN, we propose a sequential layered structure to combine the three types of layers. Extensive experiments on various tasks, including neural machine translation and text summarization demonstrate that our model outperforms the original Transformer.",
        "author": "Zhihao Fan; Yeyun Gong; Dayiheng Liu; Zhongyu Wei; Siyuan Wang; Jian Jiao; Nan Duan; Ruofei Zhang; Xuanjing Huang",
        "authorids": "/z/zhihao-fan/; /y/yeyun-gong/; /d/dayiheng-liu/; /z/zhongyu-wei/; /s/siyuan-wang/; /j/jian-jiao/; /n/nan-duan/; /r/ruofei-zhang/; /x/xuan-jing-huang/",
        "bibtex": "@inproceedings{fan-etal-2021-mask,\n    title = \"Mask Attention Networks: Rethinking and Strengthen Transformer\",\n    author = \"Fan, Zhihao  and\n      Gong, Yeyun  and\n      Liu, Dayiheng  and\n      Wei, Zhongyu  and\n      Wang, Siyuan  and\n      Jiao, Jian  and\n      Duan, Nan  and\n      Zhang, Ruofei  and\n      Huang, Xuanjing\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.135/\",\n    doi = \"10.18653/v1/2021.naacl-main.135\",\n    pages = \"1692--1701\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.135.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.135/",
        "pdf_size": 372782,
        "gs_citation": 83,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2227950449568019360&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "School of Data Science, Fudan University, China+Research Institute of Intelligent and Complex Systems, Fudan University, China; Microsoft Research Asia; DAMO Academy; Microsoft; School of Computer Science, Fudan University, China+Research Institute of Intelligent and Complex Systems, Fudan University, China; Microsoft; Microsoft; Microsoft; School of Computer Science, Fudan University, China",
        "aff_domain": "fudan.edu.cn;microsoft.com;alibaba-inc.com;fudan.edu.cn;fudan.edu.cn;microsoft.com;microsoft.com;microsoft.com;fudan.edu.cn",
        "email": "fudan.edu.cn;microsoft.com;alibaba-inc.com;fudan.edu.cn;fudan.edu.cn;microsoft.com;microsoft.com;microsoft.com;fudan.edu.cn",
        "github": "",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0+0;1;2;3;0+0;3;3;3;0",
        "aff_unique_norm": "Fudan University;Microsoft Research;DAMO Academy;Microsoft Corporation",
        "aff_unique_dep": "School of Data Science;Research;;",
        "aff_unique_url": "https://www.fudan.edu.cn;https://www.microsoft.com/en-us/research/group/asia;https://www.damo.ac.cn;https://www.microsoft.com",
        "aff_unique_abbr": "Fudan;MSR Asia;DAMO;Microsoft",
        "aff_campus_unique_index": ";1;",
        "aff_campus_unique": ";Asia",
        "aff_country_unique_index": "0+0;0;0;1;0+0;1;1;1;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2021.naacl-main.163",
        "title": "Masked Conditional Random Fields for Sequence Labeling",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Conditional Random Field (CRF) based neural models are among the most performant methods for solving sequence labeling problems. Despite its great success, CRF has the shortcoming of occasionally generating illegal sequences of tags, e.g. sequences containing an \u201cI-\u201d tag immediately after an \u201cO\u201d tag, which is forbidden by the underlying BIO tagging scheme. In this work, we propose Masked Conditional Random Field (MCRF), an easy to implement variant of CRF that impose restrictions on candidate paths during both training and decoding phases. We show that the proposed method thoroughly resolves this issue and brings significant improvement over existing CRF-based models with near zero additional cost.",
        "author": "Tianwen Wei; Jianwei Qi; Shenghuan He; Songtao Sun",
        "authorids": "/t/tianwen-wei/; /j/jianwei-qi/; /s/shenghuan-he/; /s/songtao-sun/",
        "bibtex": "@inproceedings{wei-etal-2021-masked,\n    title = \"Masked Conditional Random Fields for Sequence Labeling\",\n    author = \"Wei, Tianwen  and\n      Qi, Jianwei  and\n      He, Shenghuan  and\n      Sun, Songtao\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.163/\",\n    doi = \"10.18653/v1/2021.naacl-main.163\",\n    pages = \"2024--2035\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.163.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.163/",
        "pdf_size": 471413,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17703923273395570351&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4
    },
    {
        "id": "2021.naacl-main.78",
        "title": "Measuring Social Biases in Grounded Vision and Language Embeddings",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We generalize the notion of measuring social biases in word embeddings to visually grounded word embeddings. Biases are present in grounded embeddings, and indeed seem to be equally or more significant than for ungrounded embeddings. This is despite the fact that vision and language can suffer from different biases, which one might hope could attenuate the biases in both. Multiple ways exist to generalize metrics measuring bias in word embeddings to this new setting. We introduce the space of generalizations (Grounded-WEAT and Grounded-SEAT) and demonstrate that three generalizations answer different yet important questions about how biases, language, and vision interact. These metrics are used on a new dataset, the first for grounded bias, created by augmenting standard linguistic bias benchmarks with 10,228 images from COCO, Conceptual Captions, and Google Images. Dataset construction is challenging because vision datasets are themselves very biased. The presence of these biases in systems will begin to have real-world consequences as they are deployed, making carefully measuring bias and then mitigating it critical to building a fair society.",
        "author": "Candace Ross; Boris Katz; Andrei Barbu",
        "authorids": "/c/candace-ross/; /b/boris-katz/; /a/andrei-barbu/",
        "bibtex": "@inproceedings{ross-etal-2021-measuring,\n    title = \"Measuring Social Biases in Grounded Vision and Language Embeddings\",\n    author = \"Ross, Candace  and\n      Katz, Boris  and\n      Barbu, Andrei\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.78/\",\n    doi = \"10.18653/v1/2021.naacl-main.78\",\n    pages = \"998--1008\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.78.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.78/",
        "pdf_size": 807574,
        "gs_citation": 76,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14736860238303667970&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "CSAIL, Massachusetts Institute of Technology; CSAIL, Massachusetts Institute of Technology; CSAIL, Massachusetts Institute of Technology",
        "aff_domain": "mit.edu;mit.edu;mit.edu",
        "email": "mit.edu;mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory",
        "aff_unique_url": "https://www.csail.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.450",
        "title": "Measuring the \u2018I don\u2019t know\u2019 Problem through the Lens of Gricean Quantity",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We consider the intrinsic evaluation of neural generative dialog models through the lens of Grice\u2019s Maxims of Conversation (1975). Based on the maxim of Quantity (be informative), we propose Relative Utterance Quantity (RUQ) to diagnose the \u2018I don\u2019t know\u2019 problem, in which a dialog system produces generic responses. The linguistically motivated RUQ diagnostic compares the model score of a generic response to that of the reference response. We find that for reasonable baseline models, \u2018I don\u2019t know\u2019 is preferred over the reference the majority of the time, but this can be reduced to less than 5% with hyperparameter tuning. RUQ allows for the direct analysis of the \u2018I don\u2019t know\u2019 problem, which has been addressed but not analyzed by prior work.",
        "author": "Huda Khayrallah; Jo\u00e3o Sedoc",
        "authorids": "/h/huda-khayrallah/; /j/joao-sedoc/",
        "bibtex": "@inproceedings{khayrallah-sedoc-2021-measuring,\n    title = \"Measuring the {\\textquoteleft}{I} don{'}t know' Problem through the Lens of {G}ricean Quantity\",\n    author = \"Khayrallah, Huda  and\n      Sedoc, Jo{\\~a}o\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.450/\",\n    doi = \"10.18653/v1/2021.naacl-main.450\",\n    pages = \"5659--5670\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.450.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.450/",
        "pdf_size": 760287,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4657648168361899909&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Johns Hopkins University; New York University",
        "aff_domain": "jhu.edu;stern.nyu.edu",
        "email": "jhu.edu;stern.nyu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Johns Hopkins University;New York University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.jhu.edu;https://www.nyu.edu",
        "aff_unique_abbr": "JHU;NYU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.474",
        "title": "MediaSum: A Large-scale Media Interview Dataset for Dialogue Summarization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "This paper introduces MediaSum, a large-scale media interview dataset consisting of 463.6K transcripts with abstractive summaries. To create this dataset, we collect interview transcripts from NPR and CNN and employ the overview and topic descriptions as summaries. Compared with existing public corpora for dialogue summarization, our dataset is an order of magnitude larger and contains complex multi-party conversations from multiple domains. We conduct statistical analysis to demonstrate the unique positional bias exhibited in the transcripts of televised and radioed interviews. We also show that MediaSum can be used in transfer learning to improve a model\u2019s performance on other dialogue summarization tasks.",
        "author": "Chenguang Zhu; Yang Liu; Jie Mei; Michael Zeng",
        "authorids": "/c/chenguang-zhu/; /y/yang-liu-edinburgh/; /j/jie-mei/; /m/michael-zeng/",
        "bibtex": "@inproceedings{zhu-etal-2021-mediasum,\n    title = \"{M}edia{S}um: A Large-scale Media Interview Dataset for Dialogue Summarization\",\n    author = \"Zhu, Chenguang  and\n      Liu, Yang  and\n      Mei, Jie  and\n      Zeng, Michael\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.474/\",\n    doi = \"10.18653/v1/2021.naacl-main.474\",\n    pages = \"5927--5934\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.474.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.474/",
        "pdf_size": 280378,
        "gs_citation": 153,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12198142330954434832&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Microsoft Cognitive Services Research Group; Microsoft Cognitive Services Research Group; Microsoft Cognitive Services Research Group; Microsoft Cognitive Services Research Group",
        "aff_domain": "microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "email": "microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "github": "https://github.com/zcgzcgzcg1/MediaSum",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Microsoft",
        "aff_unique_dep": "Cognitive Services Research Group",
        "aff_unique_url": "https://www.microsoft.com",
        "aff_unique_abbr": "Microsoft",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.8",
        "title": "Mediators in Determining what Processing BERT Performs First",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Probing neural models for the ability to perform downstream tasks using their activation patterns is often used to localize what parts of the network specialize in performing what tasks. However, little work addressed potential mediating factors in such comparisons. As a test-case mediating factor, we consider the prediction\u2019s context length, namely the length of the span whose processing is minimally required to perform the prediction. We show that not controlling for context length may lead to contradictory conclusions as to the localization patterns of the network, depending on the distribution of the probing dataset. Indeed, when probing BERT with seven tasks, we find that it is possible to get 196 different rankings between them when manipulating the distribution of context lengths in the probing dataset. We conclude by presenting best practices for conducting such comparisons in the future.",
        "author": "Aviv Slobodkin; Leshem Choshen; Omri Abend",
        "authorids": "/a/aviv-slobodkin/; /l/leshem-choshen/; /o/omri-abend/",
        "bibtex": "@inproceedings{slobodkin-etal-2021-mediators,\n    title = \"Mediators in Determining what Processing {BERT} Performs First\",\n    author = \"Slobodkin, Aviv  and\n      Choshen, Leshem  and\n      Abend, Omri\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.8/\",\n    doi = \"10.18653/v1/2021.naacl-main.8\",\n    pages = \"86--93\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.8.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.8/",
        "pdf_size": 957175,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5977965789270312871&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "School of Computer Science and Engineering, The Hebrew University of Jerusalem; School of Computer Science and Engineering, The Hebrew University of Jerusalem; School of Computer Science and Engineering, The Hebrew University of Jerusalem",
        "aff_domain": "mail.huji.ac.il;mail.huji.ac.il;mail.huji.ac.il",
        "email": "mail.huji.ac.il;mail.huji.ac.il;mail.huji.ac.il",
        "github": "https://github.com/lovodkin93/BERT-context-distance",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "The Hebrew University of Jerusalem",
        "aff_unique_dep": "School of Computer Science and Engineering",
        "aff_unique_url": "https://www.huji.ac.il",
        "aff_unique_abbr": "HUJI",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Jerusalem",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "2021.naacl-main.141",
        "title": "MelBERT: Metaphor Detection via Contextualized Late Interaction using Metaphorical Identification Theories",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Automated metaphor detection is a challenging task to identify the metaphorical expression of words in a sentence. To tackle this problem, we adopt pre-trained contextualized models, e.g., BERT and RoBERTa. To this end, we propose a novel metaphor detection model, namely metaphor-aware late interaction over BERT (MelBERT). Our model not only leverages contextualized word representation but also benefits from linguistic metaphor identification theories to detect whether the target word is metaphorical. Our empirical results demonstrate that MelBERT outperforms several strong baselines on four benchmark datasets, i.e., VUA-18, VUA-20, MOH-X, and TroFi.",
        "author": "Minjin Choi; Sunkyung Lee; Eunseong Choi; Heesoo Park; Junhyuk Lee; Dongwon Lee; Jongwuk Lee",
        "authorids": "/m/minjin-choi/; /s/sunkyung-lee/; /e/eunseong-choi/; /h/heesoo-park/; /j/junhyuk-lee/; /d/dongwon-lee/; /j/jongwuk-lee/",
        "bibtex": "@inproceedings{choi-etal-2021-melbert,\n    title = \"{M}el{BERT}: Metaphor Detection via Contextualized Late Interaction using Metaphorical Identification Theories\",\n    author = \"Choi, Minjin  and\n      Lee, Sunkyung  and\n      Choi, Eunseong  and\n      Park, Heesoo  and\n      Lee, Junhyuk  and\n      Lee, Dongwon  and\n      Lee, Jongwuk\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.141/\",\n    doi = \"10.18653/v1/2021.naacl-main.141\",\n    pages = \"1763--1773\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.141.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.141/",
        "pdf_size": 1141346,
        "gs_citation": 110,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6961422266915855370&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Sungkyunkwan University; Sungkyunkwan University; Sungkyunkwan University; Bering Lab; Sungkyunkwan University; The Pennsylvania State University; Sungkyunkwan University",
        "aff_domain": "skku.edu;skku.edu;skku.edu;beringlab.com;skku.edu;psu.edu;skku.edu",
        "email": "skku.edu;skku.edu;skku.edu;beringlab.com;skku.edu;psu.edu;skku.edu",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;1;0;2;0",
        "aff_unique_norm": "Sungkyunkwan University;Bering Lab;The Pennsylvania State University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.skku.edu;;https://www.psu.edu",
        "aff_unique_abbr": "SKKU;;PSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;2;0",
        "aff_country_unique": "South Korea;;United States"
    },
    {
        "id": "2021.naacl-main.33",
        "title": "Meta-Learning for Domain Generalization in Semantic Parsing",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The importance of building semantic parsers which can be applied to new domains and generate programs unseen at training has long been acknowledged, and datasets testing out-of-domain performance are becoming increasingly available. However, little or no attention has been devoted to learning algorithms or objectives which promote domain generalization, with virtually all existing approaches relying on standard supervised learning. In this work, we use a meta-learning framework which targets zero-shot domain generalization for semantic parsing. We apply a model-agnostic training algorithm that simulates zero-shot parsing by constructing virtual train and test sets from disjoint domains. The learning objective capitalizes on the intuition that gradient steps that improve source-domain performance should also improve target-domain performance, thus encouraging a parser to generalize to unseen target domains. Experimental results on the (English) Spider and Chinese Spider datasets show that the meta-learning objective significantly boosts the performance of a baseline parser.",
        "author": "Bailin Wang; Mirella Lapata; Ivan Titov",
        "authorids": "/b/bailin-wang/; /m/mirella-lapata/; /i/ivan-titov/",
        "bibtex": "@inproceedings{wang-etal-2021-meta,\n    title = \"Meta-Learning for Domain Generalization in Semantic Parsing\",\n    author = \"Wang, Bailin  and\n      Lapata, Mirella  and\n      Titov, Ivan\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.33/\",\n    doi = \"10.18653/v1/2021.naacl-main.33\",\n    pages = \"366--379\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.33.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.33/",
        "pdf_size": 488018,
        "gs_citation": 89,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12416541644927343625&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Institute for Language, Cognition and Computation, School of Informatics, University of Edinburgh; Institute for Language, Cognition and Computation, School of Informatics, University of Edinburgh; Institute for Language, Cognition and Computation, School of Informatics, University of Edinburgh",
        "aff_domain": "ed.ac.uk;inf.ed.ac.uk;inf.ed.ac.uk",
        "email": "ed.ac.uk;inf.ed.ac.uk;inf.ed.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Edinburgh",
        "aff_unique_dep": "School of Informatics",
        "aff_unique_url": "https://www.ed.ac.uk",
        "aff_unique_abbr": "Edinburgh",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Edinburgh",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2021.naacl-main.42",
        "title": "MetaXL: Meta Representation Transformation for Low-resource Cross-lingual Learning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The combination of multilingual pre-trained representations and cross-lingual transfer learning is one of the most effective methods for building functional NLP systems for low-resource languages. However, for extremely low-resource languages without large-scale monolingual corpora for pre-training or sufficient annotated data for fine-tuning, transfer learning remains an understudied and challenging task. Moreover, recent work shows that multilingual representations are surprisingly disjoint across languages, bringing additional challenges for transfer onto extremely low-resource languages. In this paper, we propose MetaXL, a meta-learning based framework that learns to transform representations judiciously from auxiliary languages to a target one and brings their representation spaces closer for effective transfer. Extensive experiments on real-world low-resource languages \u2013 without access to large-scale monolingual corpora or large amounts of labeled data \u2013 for tasks like cross-lingual sentiment analysis and named entity recognition show the effectiveness of our approach. Code for MetaXL is publicly available at github.com/microsoft/MetaXL.",
        "author": "Mengzhou Xia; Guoqing Zheng; Subhabrata Mukherjee; Milad Shokouhi; Graham Neubig; Ahmed Hassan Awadallah",
        "authorids": "/m/mengzhou-xia/; /g/guoqing-zheng/; /s/subhabrata-mukherjee/; /m/milad-shokouhi/; /g/graham-neubig/; /a/ahmed-hassan/",
        "bibtex": "@inproceedings{xia-etal-2021-metaxl,\n    title = \"{M}eta{XL}: Meta Representation Transformation for Low-resource Cross-lingual Learning\",\n    author = \"Xia, Mengzhou  and\n      Zheng, Guoqing  and\n      Mukherjee, Subhabrata  and\n      Shokouhi, Milad  and\n      Neubig, Graham  and\n      Awadallah, Ahmed Hassan\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.42/\",\n    doi = \"10.18653/v1/2021.naacl-main.42\",\n    pages = \"499--511\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.42.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.42/",
        "pdf_size": 467426,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17659755820345108645&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Princeton University; Microsoft Research; Microsoft Research; Microsoft Research; Carnegie Mellon University; Microsoft Research",
        "aff_domain": "princeton.edu;microsoft.com;microsoft.com;microsoft.com;cs.cmu.edu;microsoft.com",
        "email": "princeton.edu;microsoft.com;microsoft.com;microsoft.com;cs.cmu.edu;microsoft.com",
        "github": "github.com/microsoft/MetaXL",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;1;1;2;1",
        "aff_unique_norm": "Princeton University;Microsoft Corporation;Carnegie Mellon University",
        "aff_unique_dep": ";Microsoft Research;",
        "aff_unique_url": "https://www.princeton.edu;https://www.microsoft.com/en-us/research;https://www.cmu.edu",
        "aff_unique_abbr": "Princeton;MSR;CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.161",
        "title": "Model Extraction and Adversarial Transferability, Your BERT is Vulnerable!",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Natural language processing (NLP) tasks, ranging from text classification to text generation, have been revolutionised by the pretrained language models, such as BERT. This allows corporations to easily build powerful APIs by encapsulating fine-tuned BERT models for downstream tasks. However, when a fine-tuned BERT model is deployed as a service, it may suffer from different attacks launched by the malicious users. In this work, we first present how an adversary can steal a BERT-based API service (the victim/target model) on multiple benchmark datasets with limited prior knowledge and queries. We further show that the extracted model can lead to highly transferable adversarial attacks against the victim model. Our studies indicate that the potential vulnerabilities of BERT-based API services still hold, even when there is an architectural mismatch between the victim model and the attack model. Finally, we investigate two defence strategies to protect the victim model, and find that unless the performance of the victim model is sacrificed, both model extraction and adversarial transferability can effectively compromise the target models.",
        "author": "Xuanli He; Lingjuan Lyu; Qiongkai Xu; Lichao Sun",
        "authorids": "/x/xuanli-he/; /l/lingjuan-lyu/; /q/qiongkai-xu/; /l/lichao-sun/",
        "bibtex": "@inproceedings{he-etal-2021-model,\n    title = \"Model Extraction and Adversarial Transferability, Your {BERT} is Vulnerable!\",\n    author = \"He, Xuanli  and\n      Lyu, Lingjuan  and\n      Xu, Qiongkai  and\n      Sun, Lichao\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.161/\",\n    doi = \"10.18653/v1/2021.naacl-main.161\",\n    pages = \"2006--2012\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.161.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.161/",
        "pdf_size": 508012,
        "gs_citation": 112,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15256382539381773624&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Monash University; Ant Group; The Australian National University and Data61 CSIRO; Lehigh University",
        "aff_domain": "monash.edu;antgroup.com;anu.edu.au;lehigh.edu",
        "email": "monash.edu;antgroup.com;anu.edu.au;lehigh.edu",
        "github": "https://github.com/xlhex/extract_and_transfer",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "Monash University;Ant Group;The Australian National University;Lehigh University",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.monash.edu;https://www.antgroup.com;https://www.anu.edu.au;https://www.lehigh.edu",
        "aff_unique_abbr": "Monash;Ant Group;ANU;Lehigh",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;2",
        "aff_country_unique": "Australia;China;United States"
    },
    {
        "id": "2021.naacl-main.318",
        "title": "Modeling Diagnostic Label Correlation for Automatic ICD Coding",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Given the clinical notes written in electronic health records (EHRs), it is challenging to predict the diagnostic codes which is formulated as a multi-label classification task. The large set of labels, the hierarchical dependency, and the imbalanced data make this prediction task extremely hard. Most existing work built a binary prediction for each label independently, ignoring the dependencies between labels. To address this problem, we propose a two-stage framework to improve automatic ICD coding by capturing the label correlation. Specifically, we train a label set distribution estimator to rescore the probability of each label set candidate generated by a base predictor. This paper is the first attempt at learning the label set distribution as a reranking module for ICD coding. In the experiments, our proposed framework is able to improve upon best-performing predictors for medical code prediction on the benchmark MIMIC datasets.",
        "author": "Shang-Chi Tsai; Chao-Wei Huang; Yun-Nung Chen",
        "authorids": "/s/shang-chi-tsai/; /c/chao-wei-huang/; /y/yun-nung-chen/",
        "bibtex": "@inproceedings{tsai-etal-2021-modeling,\n    title = \"Modeling Diagnostic Label Correlation for Automatic {ICD} Coding\",\n    author = \"Tsai, Shang-Chi  and\n      Huang, Chao-Wei  and\n      Chen, Yun-Nung\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.318/\",\n    doi = \"10.18653/v1/2021.naacl-main.318\",\n    pages = \"4043--4052\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.318.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.318/",
        "pdf_size": 524993,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8296351321637436178&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "National Taiwan University, Taipei, Taiwan; National Taiwan University, Taipei, Taiwan; National Taiwan University, Taipei, Taiwan",
        "aff_domain": "csie.ntu.edu.tw;csie.ntu.edu.tw;ieee.org",
        "email": "csie.ntu.edu.tw;csie.ntu.edu.tw;ieee.org",
        "github": "https://github.com/MiuLab/ICD-Correlation",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "National Taiwan University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ntu.edu.tw",
        "aff_unique_abbr": "NTU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Taiwan",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2021.naacl-main.138",
        "title": "Modeling Event Plausibility with Consistent Conceptual Abstraction",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Understanding natural language requires common sense, one aspect of which is the ability to discern the plausibility of events. While distributional models\u2014most recently pre-trained, Transformer language models\u2014have demonstrated improvements in modeling event plausibility, their performance still falls short of humans\u2019. In this work, we show that Transformer-based plausibility models are markedly inconsistent across the conceptual classes of a lexical hierarchy, inferring that \u201ca person breathing\u201d is plausible while \u201ca dentist breathing\u201d is not, for example. We find this inconsistency persists even when models are softly injected with lexical knowledge, and we present a simple post-hoc method of forcing model consistency that improves correlation with human plausibility judgements.",
        "author": "Ian Porada; Kaheer Suleman; Adam Trischler; Jackie Chi Kit Cheung",
        "authorids": "/i/ian-porada/; /k/kaheer-suleman/; /a/adam-trischler/; /j/jackie-chi-kit-cheung/",
        "bibtex": "@inproceedings{porada-etal-2021-modeling,\n    title = \"Modeling Event Plausibility with Consistent Conceptual Abstraction\",\n    author = \"Porada, Ian  and\n      Suleman, Kaheer  and\n      Trischler, Adam  and\n      Cheung, Jackie Chi Kit\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.138/\",\n    doi = \"10.18653/v1/2021.naacl-main.138\",\n    pages = \"1732--1743\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.138.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.138/",
        "pdf_size": 600617,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10465093234219443382&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Mila, McGill University; Microsoft Research Montr\u00e9al; Microsoft Research Montr\u00e9al; Mila, McGill University",
        "aff_domain": "mail.mcgill.ca;microsoft.com;microsoft.com;cs.mcgill.ca",
        "email": "mail.mcgill.ca;microsoft.com;microsoft.com;cs.mcgill.ca",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "McGill University;Microsoft Research",
        "aff_unique_dep": "Mila;Microsoft Research",
        "aff_unique_url": "https://www.mcgill.ca;https://www.microsoft.com/en-us/research/group/microsoft-research-montreal",
        "aff_unique_abbr": "McGill;MSR Montreal",
        "aff_campus_unique_index": "0;1;1;0",
        "aff_campus_unique": "Montreal;Montr\u00e9al",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2021.naacl-main.179",
        "title": "Modeling Framing in Immigration Discourse on Social Media",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The framing of political issues can influence policy and public opinion. Even though the public plays a key role in creating and spreading frames, little is known about how ordinary people on social media frame political issues. By creating a new dataset of immigration-related tweets labeled for multiple framing typologies from political communication theory, we develop supervised models to detect frames. We demonstrate how users\u2019 ideology and region impact framing choices, and how a message\u2019s framing influences audience responses. We find that the more commonly-used issue-generic frames obscure important ideological and regional patterns that are only revealed by immigration-specific frames. Furthermore, frames oriented towards human interests, culture, and politics are associated with higher user engagement. This large-scale analysis of a complex social and linguistic phenomenon contributes to both NLP and social science research.",
        "author": "Julia Mendelsohn; Ceren Budak; David Jurgens",
        "authorids": "/j/julia-mendelsohn/; /c/ceren-budak/; /d/david-jurgens/",
        "bibtex": "@inproceedings{mendelsohn-etal-2021-modeling,\n    title = \"Modeling Framing in Immigration Discourse on Social Media\",\n    author = \"Mendelsohn, Julia  and\n      Budak, Ceren  and\n      Jurgens, David\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.179/\",\n    doi = \"10.18653/v1/2021.naacl-main.179\",\n    pages = \"2219--2263\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.179.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.179/",
        "pdf_size": 527301,
        "gs_citation": 103,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11320754109926979903&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "University of Michigan; University of Michigan; University of Michigan",
        "aff_domain": "umich.edu;umich.edu;umich.edu",
        "email": "umich.edu;umich.edu;umich.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.391",
        "title": "Modeling Human Mental States with an Entity-based Narrative Graph",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Understanding narrative text requires capturing characters\u2019 motivations, goals, and mental states. This paper proposes an Entity-based Narrative Graph (ENG) to model the internal- states of characters in a story. We explicitly model entities, their interactions and the context in which they appear, and learn rich representations for them. We experiment with different task-adaptive pre-training objectives, in-domain training, and symbolic inference to capture dependencies between different decisions in the output space. We evaluate our model on two narrative understanding tasks: predicting character mental states, and desire fulfillment, and conduct a qualitative analysis.",
        "author": "I-Ta Lee; Maria Leonor Pacheco; Dan Goldwasser",
        "authorids": "/i/i-ta-lee/; /m/maria-leonor-pacheco/; /d/dan-goldwasser/",
        "bibtex": "@inproceedings{lee-etal-2021-modeling,\n    title = \"Modeling Human Mental States with an Entity-based Narrative Graph\",\n    author = \"Lee, I-Ta  and\n      Pacheco, Maria Leonor  and\n      Goldwasser, Dan\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.391/\",\n    doi = \"10.18653/v1/2021.naacl-main.391\",\n    pages = \"4916--4926\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.391.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.391/",
        "pdf_size": 643527,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2482996462451216613&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3
    },
    {
        "id": "2021.naacl-main.180",
        "title": "Modeling the Severity of Complaints in Social Media",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The speech act of complaining is used by humans to communicate a negative mismatch between reality and expectations as a reaction to an unfavorable situation. Linguistic theory of pragmatics categorizes complaints into various severity levels based on the face-threat that the complainer is willing to undertake. This is particularly useful for understanding the intent of complainers and how humans develop suitable apology strategies. In this paper, we study the severity level of complaints for the first time in computational linguistics. To facilitate this, we enrich a publicly available data set of complaints with four severity categories and train different transformer-based networks combined with linguistic information achieving 55.7 macro F1. We also jointly model binary complaint classification and complaint severity in a multi-task setting achieving new state-of-the-art results on binary complaint detection reaching up to 88.2 macro F1. Finally, we present a qualitative analysis of the behavior of our models in predicting complaint severity levels.",
        "author": "Mali Jin; Nikolaos Aletras",
        "authorids": "/m/mali-jin/; /n/nikolaos-aletras/",
        "bibtex": "@inproceedings{jin-aletras-2021-modeling,\n    title = \"Modeling the Severity of Complaints in Social Media\",\n    author = \"Jin, Mali  and\n      Aletras, Nikolaos\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.180/\",\n    doi = \"10.18653/v1/2021.naacl-main.180\",\n    pages = \"2264--2274\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.180.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.180/",
        "pdf_size": 539991,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14662546932024807826&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science, University of Sheffield, UK; Department of Computer Science, University of Sheffield, UK",
        "aff_domain": "sheffield.ac.uk;sheffield.ac.uk",
        "email": "sheffield.ac.uk;sheffield.ac.uk",
        "github": "https://github.com/mali726/Complaint-Severity",
        "project": "https://archive.org/details/complaint_severity_data",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Sheffield",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.sheffield.ac.uk",
        "aff_unique_abbr": "Sheffield",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2021.naacl-main.81",
        "title": "Modular Networks for Compositional Instruction Following",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Standard architectures used in instruction following often struggle on novel compositions of subgoals (e.g. navigating to landmarks or picking up objects) observed during training. We propose a modular architecture for following natural language instructions that describe sequences of diverse subgoals. In our approach, subgoal modules each carry out natural language instructions for a specific subgoal type. A sequence of modules to execute is chosen by learning to segment the instructions and predicting a subgoal type for each segment. When compared to standard, non-modular sequence-to-sequence approaches on ALFRED, a challenging instruction following benchmark, we find that modularization improves generalization to novel subgoal compositions, as well as to environments unseen in training.",
        "author": "Rodolfo Corona; Daniel Fried; Coline Devin; Dan Klein; Trevor Darrell",
        "authorids": "/r/rodolfo-corona/; /d/daniel-fried/; /c/coline-devin/; /d/dan-klein/; /t/trevor-darrell/",
        "bibtex": "@inproceedings{corona-etal-2021-modular,\n    title = \"Modular Networks for Compositional Instruction Following\",\n    author = \"Corona, Rodolfo  and\n      Fried, Daniel  and\n      Devin, Coline  and\n      Klein, Dan  and\n      Darrell, Trevor\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.81/\",\n    doi = \"10.18653/v1/2021.naacl-main.81\",\n    pages = \"1033--1040\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.81.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.81/",
        "pdf_size": 2694280,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11786532699204818241&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "UC Berkeley; UC Berkeley; UC Berkeley; UC Berkeley; UC Berkeley",
        "aff_domain": "berkeley.edu;berkeley.edu;berkeley.edu;berkeley.edu;berkeley.edu",
        "email": "berkeley.edu;berkeley.edu;berkeley.edu;berkeley.edu;berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.39",
        "title": "Multi-Adversarial Learning for Cross-Lingual Word Embeddings",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Generative adversarial networks (GANs) have succeeded in inducing cross-lingual word embeddings - maps of matching words across languages - without supervision. Despite these successes, GANs\u2019 performance for the difficult case of distant languages is still not satisfactory. These limitations have been explained by GANs\u2019 incorrect assumption that source and target embedding spaces are related by a single linear mapping and are approximately isomorphic. We assume instead that, especially across distant languages, the mapping is only piece-wise linear, and propose a multi-adversarial learning method. This novel method induces the seed cross-lingual dictionary through multiple mappings, each induced to fit the mapping for one subspace. Our experiments on unsupervised bilingual lexicon induction and cross-lingual document classification show that this method improves performance over previous single-mapping methods, especially for distant languages.",
        "author": "Haozhou Wang; James Henderson; Paola Merlo",
        "authorids": "/h/haozhou-wang/; /j/james-henderson/; /p/paola-merlo/",
        "bibtex": "@inproceedings{wang-etal-2021-multi,\n    title = \"Multi-Adversarial Learning for Cross-Lingual Word Embeddings\",\n    author = \"Wang, Haozhou  and\n      Henderson, James  and\n      Merlo, Paola\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.39/\",\n    doi = \"10.18653/v1/2021.naacl-main.39\",\n    pages = \"463--472\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.39.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.39/",
        "pdf_size": 550226,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11767977224636285840&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "University of Geneva; Idiap Research Institute; University of Geneva",
        "aff_domain": "unige.ch;idiap.ch;unige.ch",
        "email": "unige.ch;idiap.ch;unige.ch",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Geneva;Idiap Research Institute",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.unige.ch;https://www.idiap.ch",
        "aff_unique_abbr": "UNIGE;Idiap",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "2021.naacl-main.454",
        "title": "Multi-Grained Knowledge Distillation for Named Entity Recognition",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Although pre-trained big models (e.g., BERT, ERNIE, XLNet, GPT3 etc.) have delivered top performance in Seq2seq modeling, their deployments in real-world applications are often hindered by the excessive computations and memory demand involved. For many applications, including named entity recognition (NER), matching the state-of-the-art result under budget has attracted considerable attention. Drawing power from the recent advance in knowledge distillation (KD), this work presents a novel distillation scheme to efficiently transfer the knowledge learned from big models to their more affordable counterpart. Our solution highlights the construction of surrogate labels through the k-best Viterbi algorithm to distill knowledge from the teacher model. To maximally assimilate knowledge into the student model, we propose a multi-grained distillation scheme, which integrates cross entropy involved in conditional random field (CRF) and fuzzy learning. To validate the effectiveness of our proposal, we conducted a comprehensive evaluation on five NER benchmarks, reporting cross-the-board performance gains relative to competing prior-arts. We further discuss ablation results to dissect our gains.",
        "author": "Xuan Zhou; Xiao Zhang; Chenyang Tao; Junya Chen; Bing Xu; Wei Wang; Jing Xiao",
        "authorids": "/x/xuan-zhou/; /x/xiao-zhang/; /c/chenyang-tao/; /j/junya-chen/; /b/bing-xu/; /w/wei-wang/; /j/jing-xiao/",
        "bibtex": "@inproceedings{zhou-etal-2021-multi,\n    title = \"Multi-Grained Knowledge Distillation for Named Entity Recognition\",\n    author = \"Zhou, Xuan  and\n      Zhang, Xiao  and\n      Tao, Chenyang  and\n      Chen, Junya  and\n      Xu, Bing  and\n      Wang, Wei  and\n      Xiao, Jing\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.454/\",\n    doi = \"10.18653/v1/2021.naacl-main.454\",\n    pages = \"5704--5716\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.454.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.454/",
        "pdf_size": 646144,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2186752339429897439&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Ping An Technology (Shenzhen) Co., Ltd; Ping An Technology (Shenzhen) Co., Ltd; Duke University; Duke University; Ping An Technology (Shenzhen) Co., Ltd; Ping An Technology (Shenzhen) Co., Ltd; Ping An Technology (Shenzhen) Co., Ltd",
        "aff_domain": "pingan.com.cn;pingan.com.cn;duke.edu;duke.edu;pingan.com.cn;pingan.com.cn;pingan.com.cn",
        "email": "pingan.com.cn;pingan.com.cn;duke.edu;duke.edu;pingan.com.cn;pingan.com.cn;pingan.com.cn",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;1;1;0;0;0",
        "aff_unique_norm": "Ping An Technology;Duke University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.pingan.com;https://www.duke.edu",
        "aff_unique_abbr": "Ping An Tech;Duke",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Shenzhen;",
        "aff_country_unique_index": "0;0;1;1;0;0;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2021.naacl-main.309",
        "title": "Multi-Hop Transformer for Document-Level Machine Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Document-level neural machine translation (NMT) has proven to be of profound value for its effectiveness on capturing contextual information. Nevertheless, existing approaches 1) simply introduce the representations of context sentences without explicitly characterizing the inter-sentence reasoning process; and 2) feed ground-truth target contexts as extra inputs at the training time, thus facing the problem of exposure bias. We approach these problems with an inspiration from human behavior \u2013 human translators ordinarily emerge a translation draft in their mind and progressively revise it according to the reasoning in discourse. To this end, we propose a novel Multi-Hop Transformer (MHT) which offers NMT abilities to explicitly model the human-like draft-editing and reasoning process. Specifically, our model serves the sentence-level translation as a draft and properly refines its representations by attending to multiple antecedent sentences iteratively. Experiments on four widely used document translation tasks demonstrate that our method can significantly improve document-level translation performance and can tackle discourse phenomena, such as coreference error and the problem of polysemy.",
        "author": "Long Zhang; Tong Zhang; Haibo Zhang; Baosong Yang; Wei Ye; Shikun Zhang",
        "authorids": "/l/long-zhang/; /t/tong-zhang/; /h/haibo-zhang/; /b/baosong-yang/; /w/wei-ye/; /s/shikun-zhang/",
        "bibtex": "@inproceedings{zhang-etal-2021-multi,\n    title = \"Multi-Hop Transformer for Document-Level Machine Translation\",\n    author = \"Zhang, Long  and\n      Zhang, Tong  and\n      Zhang, Haibo  and\n      Yang, Baosong  and\n      Ye, Wei  and\n      Zhang, Shikun\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.309/\",\n    doi = \"10.18653/v1/2021.naacl-main.309\",\n    pages = \"3953--3963\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.309.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.309/",
        "pdf_size": 597390,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1594674622298092386&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "National Engineering Research Center for Software Engineering, Peking University+School of Software and Microelectronics, Peking University; National Engineering Research Center for Software Engineering, Peking University+School of Software and Microelectronics, Peking University; Alibaba Group; Alibaba Group; National Engineering Research Center for Software Engineering, Peking University; National Engineering Research Center for Software Engineering, Peking University",
        "aff_domain": "pku.edu.cn;pku.edu.cn;alibaba-inc.com;alibaba-inc.com;pku.edu.cn;pku.edu.cn",
        "email": "pku.edu.cn;pku.edu.cn;alibaba-inc.com;alibaba-inc.com;pku.edu.cn;pku.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+0;0+0;1;1;0;0",
        "aff_unique_norm": "Peking University;Alibaba Group",
        "aff_unique_dep": "National Engineering Research Center for Software Engineering;",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.alibaba.com",
        "aff_unique_abbr": "PKU;Alibaba",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2021.naacl-main.368",
        "title": "Multi-Step Reasoning Over Unstructured Text with Beam Dense Retrieval",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Complex question answering often requires finding a reasoning chain that consists of multiple evidence pieces. Current approaches incorporate the strengths of structured knowledge and unstructured text, assuming text corpora is semi-structured. Building on dense retrieval methods, we propose a new multi-step retrieval approach (BeamDR) that iteratively forms an evidence chain through beam search in dense representations. When evaluated on multi-hop question answering, BeamDR is competitive to state-of-the-art systems, without using any semi-structured information. Through query composition in dense space, BeamDR captures the implicit relationships between evidence in the reasoning chain. The code is available at https://github.com/henryzhao5852/BeamDR.",
        "author": "Chen Zhao; Chenyan Xiong; Jordan Boyd-Graber; Hal Daum\u00e9 III",
        "authorids": "/c/chen-zhao/; /c/chenyan-xiong/; /j/jordan-boyd-graber/; /h/hal-daume-iii/",
        "bibtex": "@inproceedings{zhao-etal-2021-multi-step,\n    title = \"Multi-Step Reasoning Over Unstructured Text with Beam Dense Retrieval\",\n    author = \"Zhao, Chen  and\n      Xiong, Chenyan  and\n      Boyd-Graber, Jordan  and\n      Daum{\\'e} III, Hal\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.368/\",\n    doi = \"10.18653/v1/2021.naacl-main.368\",\n    pages = \"4635--4641\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.368.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.368/",
        "pdf_size": 398114,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1719064582291348587&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "University of Maryland; Microsoft Research; University of Maryland; Microsoft Research + University of Maryland",
        "aff_domain": "cs.umd.edu;microsoft.com;umiacs.umd.edu;hal3.name",
        "email": "cs.umd.edu;microsoft.com;umiacs.umd.edu;hal3.name",
        "github": "https://github.com/henryzhao5852/BeamDR",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;1+0",
        "aff_unique_norm": "University of Maryland;Microsoft Corporation",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www/umd.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "UMD;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.275",
        "title": "Multi-Style Transfer with Discriminative Feedback on Disjoint Corpus",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Style transfer has been widely explored in natural language generation with non-parallel corpus by directly or indirectly extracting a notion of style from source and target domain corpus. A common shortcoming of existing approaches is the prerequisite of joint annotations across all the stylistic dimensions under consideration. Availability of such dataset across a combination of styles limits the extension of these setups to multiple style dimensions. While cascading single-dimensional models across multiple styles is a possibility, it suffers from content loss, especially when the style dimensions are not completely independent of each other. In our work, we relax this requirement of jointly annotated data across multiple styles by using independently acquired data across different style dimensions without any additional annotations. We initialize an encoder-decoder setup with transformer-based language model pre-trained on a generic corpus and enhance its re-writing capability to multiple target style dimensions by employing multiple style-aware language models as discriminators. Through quantitative and qualitative evaluation, we show the ability of our model to control styles across multiple style dimensions while preserving content of the input text. We compare it against baselines involving cascaded state-of-the-art uni-dimensional style transfer models.",
        "author": "Navita Goyal; Balaji Vasan Srinivasan; Anandhavelu N; Abhilasha Sancheti",
        "authorids": "/n/navita-goyal/; /b/balaji-vasan-srinivasan/; /a/anandhavelu-n/; /a/abhilasha-sancheti/",
        "bibtex": "@inproceedings{goyal-etal-2021-multi,\n    title = \"Multi-Style Transfer with Discriminative Feedback on Disjoint Corpus\",\n    author = \"Goyal, Navita  and\n      Srinivasan, Balaji Vasan  and\n      N, Anandhavelu  and\n      Sancheti, Abhilasha\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.275/\",\n    doi = \"10.18653/v1/2021.naacl-main.275\",\n    pages = \"3500--3510\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.275.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.275/",
        "pdf_size": 982564,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13350995812542276028&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Adobe Research, India; Adobe Research, India; Adobe Research, India; Adobe Research, India",
        "aff_domain": "adobe.com;adobe.com;adobe.com;adobe.com",
        "email": "adobe.com;adobe.com;adobe.com;adobe.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Adobe Research",
        "aff_unique_dep": "",
        "aff_unique_url": "https://research.adobe.com",
        "aff_unique_abbr": "Adobe",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2021.naacl-main.313",
        "title": "Multi-Task Learning with Shared Encoder for Non-Autoregressive Machine Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Non-Autoregressive machine Translation (NAT) models have demonstrated significant inference speedup but suffer from inferior translation accuracy. The common practice to tackle the problem is transferring the Autoregressive machine Translation (AT) knowledge to NAT models, e.g., with knowledge distillation. In this work, we hypothesize and empirically verify that AT and NAT encoders capture different linguistic properties of source sentences. Therefore, we propose to adopt multi-task learning to transfer the AT knowledge to NAT models through encoder sharing. Specifically, we take the AT model as an auxiliary task to enhance NAT model performance. Experimental results on WMT14 En-De and WMT16 En-Ro datasets show that the proposed Multi-Task NAT achieves significant improvements over the baseline NAT models. Furthermore, the performance on large-scale WMT19 and WMT20 En-De datasets confirm the consistency of our proposed method. In addition, experimental results demonstrate that our Multi-Task NAT is complementary to knowledge distillation, the standard knowledge transfer method for NAT.",
        "author": "Yongchang Hao; Shilin He; Wenxiang Jiao; Zhaopeng Tu; Michael Lyu; Xing Wang",
        "authorids": "/y/yongchang-hao/; /s/shilin-he/; /w/wenxiang-jiao/; /z/zhaopeng-tu/; /m/michael-lyu/; /x/xing-wang/",
        "bibtex": "@inproceedings{hao-etal-2021-multi,\n    title = \"Multi-Task Learning with Shared Encoder for Non-Autoregressive Machine Translation\",\n    author = \"Hao, Yongchang  and\n      He, Shilin  and\n      Jiao, Wenxiang  and\n      Tu, Zhaopeng  and\n      Lyu, Michael  and\n      Wang, Xing\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.313/\",\n    doi = \"10.18653/v1/2021.naacl-main.313\",\n    pages = \"3989--3996\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.313.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.313/",
        "pdf_size": 320138,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8809165945746340205&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "School of Computer Science and Technology, Soochow University; Department of Computer Science and Engineering, The Chinese University of Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong; Tencent AI Lab; Department of Computer Science and Engineering, The Chinese University of Hong Kong; Tencent AI Lab",
        "aff_domain": "gmai.com;cse.cuhk.edu.hk;cse.cuhk.edu.hk;cse.cuhk.edu.hk;tencent.com;tencent.com",
        "email": "gmai.com;cse.cuhk.edu.hk;cse.cuhk.edu.hk;cse.cuhk.edu.hk;tencent.com;tencent.com",
        "github": "https://github.com/yongchanghao/multi-task-nattranslation",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;1;2;1;2",
        "aff_unique_norm": "Soochow University;The Chinese University of Hong Kong;Tencent",
        "aff_unique_dep": "School of Computer Science and Technology;Department of Computer Science and Engineering;Tencent AI Lab",
        "aff_unique_url": "https://eng.suda.edu.cn/;https://www.cuhk.edu.hk;https://ai.tencent.com",
        "aff_unique_abbr": "Soochow U;CUHK;Tencent AI Lab",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2021.naacl-main.332",
        "title": "Multi-source Neural Topic Modeling in Multi-view Embedding Spaces",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Though word embeddings and topics are complementary representations, several past works have only used pretrained word embeddings in (neural) topic modeling to address data sparsity in short-text or small collection of documents. This work presents a novel neural topic modeling framework using multi-view embed ding spaces: (1) pretrained topic-embeddings, and (2) pretrained word-embeddings (context-insensitive from Glove and context-sensitive from BERT models) jointly from one or many sources to improve topic quality and better deal with polysemy. In doing so, we first build respective pools of pretrained topic (i.e., TopicPool) and word embeddings (i.e., WordPool). We then identify one or more relevant source domain(s) and transfer knowledge to guide meaningful learning in the sparse target domain. Within neural topic modeling, we quantify the quality of topics and document representations via generalization (perplexity), interpretability (topic coherence) and information retrieval (IR) using short-text, long-text, small and large document collections from news and medical domains. Introducing the multi-source multi-view embedding spaces, we have shown state-of-the-art neural topic modeling using 6 source (high-resource) and 5 target (low-resource) corpora.",
        "author": "Pankaj Gupta; Yatin Chaudhary; Hinrich Sch\u00fctze",
        "authorids": "/p/pankaj-gupta/; /y/yatin-chaudhary/; /h/hinrich-schutze/",
        "bibtex": "@inproceedings{gupta-etal-2021-multi,\n    title = \"Multi-source Neural Topic Modeling in Multi-view Embedding Spaces\",\n    author = {Gupta, Pankaj  and\n      Chaudhary, Yatin  and\n      Sch{\\\"u}tze, Hinrich},\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.332/\",\n    doi = \"10.18653/v1/2021.naacl-main.332\",\n    pages = \"4205--4217\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.332.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.332/",
        "pdf_size": 653051,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10237453706067432535&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "DRIMCo GmbH Munich, Germany; DRIMCo GmbH Munich, Germany + CIS, University of Munich (LMU) Munich, Germany; CIS, University of Munich (LMU) Munich, Germany",
        "aff_domain": "drimco.net;drimco.net; ",
        "email": "drimco.net;drimco.net; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0+1;1",
        "aff_unique_norm": "DRIMCo GmbH;University of Munich (LMU)",
        "aff_unique_dep": ";Computer and Information Science",
        "aff_unique_url": ";https://www.lmu.de",
        "aff_unique_abbr": ";LMU",
        "aff_campus_unique_index": "0;0+0;0",
        "aff_campus_unique": "Munich",
        "aff_country_unique_index": "0;0+0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2021.naacl-main.227",
        "title": "Multi-task Learning of Negation and Speculation for Targeted Sentiment Classification",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The majority of work in targeted sentiment analysis has concentrated on finding better methods to improve the overall results. Within this paper we show that these models are not robust to linguistic phenomena, specifically negation and speculation. In this paper, we propose a multi-task learning method to incorporate information from syntactic and semantic auxiliary tasks, including negation and speculation scope detection, to create English-language models that are more robust to these phenomena. Further we create two challenge datasets to evaluate model performance on negated and speculative samples. We find that multi-task models and transfer learning via language modelling can improve performance on these challenge datasets, but the overall performances indicate that there is still much room for improvement. We release both the datasets and the source code at https://github.com/jerbarnes/multitask_negation_for_targeted_sentiment.",
        "author": "Andrew Moore; Jeremy Barnes",
        "authorids": "/a/andrew-moore/; /j/jeremy-barnes/",
        "bibtex": "@inproceedings{moore-barnes-2021-multi,\n    title = \"Multi-task Learning of Negation and Speculation for Targeted Sentiment Classification\",\n    author = \"Moore, Andrew  and\n      Barnes, Jeremy\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.227/\",\n    doi = \"10.18653/v1/2021.naacl-main.227\",\n    pages = \"2838--2869\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.227.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.227/",
        "pdf_size": 490413,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5952441195064457439&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "School of Computing and Communications, Lancaster University; University of Oslo, Department of Informatics",
        "aff_domain": "lancaster.ac.uk;ifi.uio.no",
        "email": "lancaster.ac.uk;ifi.uio.no",
        "github": "https://github.com/jerbarnes/multitask_negation_for_targeted_sentiment",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Lancaster University;University of Oslo",
        "aff_unique_dep": "School of Computing and Communications;Department of Informatics",
        "aff_unique_url": "https://www.lancaster.ac.uk;https://www.uio.no",
        "aff_unique_abbr": ";UiO",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United Kingdom;Norway"
    },
    {
        "id": "2021.naacl-main.40",
        "title": "Multi-view Subword Regularization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Multilingual pretrained representations generally rely on subword segmentation algorithms to create a shared multilingual vocabulary. However, standard heuristic algorithms often lead to sub-optimal segmentation, especially for languages with limited amounts of data. In this paper, we take two major steps towards alleviating this problem. First, we demonstrate empirically that applying existing subword regularization methods (Kudo, 2018; Provilkov et al., 2020) during fine-tuning of pre-trained multilingual representations improves the effectiveness of cross-lingual transfer. Second, to take full advantage of different possible input segmentations, we propose Multi-view Subword Regularization (MVR), a method that enforces the consistency of predictors between using inputs tokenized by the standard and probabilistic segmentations. Results on the XTREME multilingual benchmark (Hu et al., 2020) show that MVR brings consistent improvements of up to 2.5 points over using standard segmentation algorithms.",
        "author": "Xinyi Wang; Sebastian Ruder; Graham Neubig",
        "authorids": "/x/xinyi-wang/; /s/sebastian-ruder/; /g/graham-neubig/",
        "bibtex": "https://aclanthology.org/2021.naacl-main.40.bib",
        "pdf": "https://aclanthology.org/2021.naacl-main.40.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.40/",
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2689111106738312725&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3
    },
    {
        "id": "2021.naacl-main.344",
        "title": "MultiOpEd: A Corpus of Multi-Perspective News Editorials",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We propose MultiOpEd, an open-domain news editorial corpus that supports various tasks pertaining to the argumentation structure in news editorials, focusing on automatic perspective discovery. News editorial is a genre of persuasive text, where the argumentation structure is usually implicit. However, the arguments presented in an editorial typically center around a concise, focused thesis, which we refer to as their perspective. MultiOpEd aims at supporting the study of multiple tasks relevant to automatic perspective discovery, where a system is expected to produce a single-sentence thesis statement summarizing the arguments presented. We argue that identifying and abstracting such natural language perspectives from editorials is a crucial step toward studying the implicit argumentation structure in news editorials. We first discuss the challenges and define a few conceptual tasks towards our goal. To demonstrate the utility of MultiOpEd and the induced tasks, we study the problem of perspective summarization in a multi-task learning setting, as a case study. We show that, with the induced tasks as auxiliary tasks, we can improve the quality of the perspective summary generated. We hope that MultiOpEd will be a useful resource for future studies on argumentation in the news editorial domain.",
        "author": "Siyi Liu; Sihao Chen; Xander Uyttendaele; Dan Roth",
        "authorids": "/s/siyi-liu/; /s/sihao-chen/; /x/xander-uyttendaele/; /d/dan-roth/",
        "bibtex": "@inproceedings{liu-etal-2021-multioped,\n    title = \"{M}ulti{O}p{E}d: A Corpus of Multi-Perspective News Editorials\",\n    author = \"Liu, Siyi  and\n      Chen, Sihao  and\n      Uyttendaele, Xander  and\n      Roth, Dan\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.344/\",\n    doi = \"10.18653/v1/2021.naacl-main.344\",\n    pages = \"4345--4361\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.344.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.344/",
        "pdf_size": 4687659,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5025860453608285338&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "University of Pennsylvania; University of Pennsylvania; University of Pennsylvania; University of Pennsylvania",
        "aff_domain": "seas.upenn.edu;seas.upenn.edu;seas.upenn.edu;seas.upenn.edu",
        "email": "seas.upenn.edu;seas.upenn.edu;seas.upenn.edu;seas.upenn.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Pennsylvania",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.upenn.edu",
        "aff_unique_abbr": "UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-demos.9",
        "title": "Multifaceted Domain-Specific Document Embeddings",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "Current document embeddings require large training corpora but fail to learn high-quality representations when confronted with a small number of domain-specific documents and rare terms. Further, they transform each document into a single embedding vector, making it hard to capture different notions of document similarity or explain why two documents are considered similar. In this work, we propose our Faceted Domain Encoder, a novel approach to learn multifaceted embeddings for domain-specific documents. It is based on a Siamese neural network architecture and leverages knowledge graphs to further enhance the embeddings even if only a few training samples are available. The model identifies different types of domain knowledge and encodes them into separate dimensions of the embedding, thereby enabling multiple ways of finding and comparing related documents in the vector space. We evaluate our approach on two benchmark datasets and find that it achieves the same embedding quality as state-of-the-art models while requiring only a tiny fraction of their training data. An interactive demo, our source code, and the evaluation datasets are available online: https://hpi.de/naumann/s/multifaceted-embeddings and a screencast is available on YouTube: https://youtu.be/HHcsX2clEwg",
        "author": "Julian Risch; Philipp Hager; Ralf Krestel",
        "authorids": "/j/julian-risch/; /p/philipp-hager/; /r/ralf-krestel/",
        "bibtex": "@inproceedings{risch-etal-2021-multifaceted,\n    title = \"Multifaceted Domain-Specific Document Embeddings\",\n    author = \"Risch, Julian  and\n      Hager, Philipp  and\n      Krestel, Ralf\",\n    editor = \"Sil, Avi  and\n      Lin, Xi Victoria\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-demos.9/\",\n    doi = \"10.18653/v1/2021.naacl-demos.9\",\n    pages = \"78--83\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-demos.9.pdf",
        "site": "https://aclanthology.org/2021.naacl-demos.9/",
        "pdf_size": 512802,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1222307141068361784&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "https://hpi.de/naumann/s/multifaceted-embeddings",
        "author_num": 3
    },
    {
        "id": "2021.naacl-main.20",
        "title": "Multilingual BERT Post-Pretraining Alignment",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We propose a simple method to align multilingual contextual embeddings as a post-pretraining step for improved cross-lingual transferability of the pretrained language models. Using parallel data, our method aligns embeddings on the word level through the recently proposed Translation Language Modeling objective as well as on the sentence level via contrastive learning and random input shuffling. We also perform sentence-level code-switching with English when finetuning on downstream tasks. On XNLI, our best model (initialized from mBERT) improves over mBERT by 4.7% in the zero-shot setting and achieves comparable result to XLM for translate-train while using less than 18% of the same parallel data and 31% fewer model parameters. On MLQA, our model outperforms XLM-R_Base, which has 57% more parameters than ours.",
        "author": "Lin Pan; Chung-Wei Hang; Haode Qi; Abhishek Shah; Saloni Potdar; Mo Yu",
        "authorids": "/l/lin-pan/; /c/chung-wei-hang/; /h/haode-qi/; /a/abhishek-shah/; /s/saloni-potdar/; /m/mo-yu/",
        "bibtex": "@inproceedings{pan-etal-2021-multilingual,\n    title = \"Multilingual {BERT} Post-Pretraining Alignment\",\n    author = \"Pan, Lin  and\n      Hang, Chung-Wei  and\n      Qi, Haode  and\n      Shah, Abhishek  and\n      Potdar, Saloni  and\n      Yu, Mo\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.20/\",\n    doi = \"10.18653/v1/2021.naacl-main.20\",\n    pages = \"210--219\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.20.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.20/",
        "pdf_size": 517174,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17759110882801942685&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "IBM Watson; IBM Watson; IBM Watson; IBM Watson; IBM Watson; MIT-IBM Watson AI Lab",
        "aff_domain": "us.ibm.com;us.ibm.com;ibm.com;ibm.com;us.ibm.com;us.ibm.com",
        "email": "us.ibm.com;us.ibm.com;ibm.com;ibm.com;us.ibm.com;us.ibm.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;1",
        "aff_unique_norm": "IBM;Massachusetts Institute of Technology",
        "aff_unique_dep": "IBM Watson;IBM Watson AI Lab",
        "aff_unique_url": "https://www.ibm.com/watson;https://www.mitibmwatsonailab.org",
        "aff_unique_abbr": "IBM Watson;MIT-IBM AI Lab",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.10",
        "title": "Multilingual Language Models Predict Human Reading Behavior",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We analyze if large language models are able to predict patterns of human reading behavior. We compare the performance of language-specific and multilingual pretrained transformer models to predict reading time measures reflecting natural human sentence processing on Dutch, English, German, and Russian texts. This results in accurate models of human reading behavior, which indicates that transformer models implicitly encode relative importance in language in a way that is comparable to human processing mechanisms. We find that BERT and XLM models successfully predict a range of eye tracking features. In a series of experiments, we analyze the cross-domain and cross-language abilities of these models and show how they reflect human sentence processing.",
        "author": "Nora Hollenstein; Federico Pirovano; Ce Zhang; Lena J\u00e4ger; Lisa Beinborn",
        "authorids": "/n/nora-hollenstein/; /f/federico-pirovano/; /c/ce-zhang/; /l/lena-jager/; /l/lisa-beinborn/",
        "bibtex": "@inproceedings{hollenstein-etal-2021-multilingual,\n    title = \"Multilingual Language Models Predict Human Reading Behavior\",\n    author = {Hollenstein, Nora  and\n      Pirovano, Federico  and\n      Zhang, Ce  and\n      J{\\\"a}ger, Lena  and\n      Beinborn, Lisa},\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.10/\",\n    doi = \"10.18653/v1/2021.naacl-main.10\",\n    pages = \"106--123\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.10.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.10/",
        "pdf_size": 1010351,
        "gs_citation": 57,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7253810011200227568&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "ETH Zurich; ETH Zurich; ETH Zurich; University of Zurich + University of Potsdam; Vrije Universiteit Amsterdam",
        "aff_domain": "inf.ethz.ch;inf.ethz.ch;inf.ethz.ch;cl.uzh.ch;vu.nl",
        "email": "inf.ethz.ch;inf.ethz.ch;inf.ethz.ch;cl.uzh.ch;vu.nl",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;1+2;3",
        "aff_unique_norm": "ETH Zurich;University of Zurich;University of Potsdam;Vrije Universiteit Amsterdam",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.ethz.ch;https://www.uzh.ch;https://www.uni-potsdam.de;https://www.vu.nl",
        "aff_unique_abbr": "ETHZ;UZH;UP;VU Amsterdam",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0+1;2",
        "aff_country_unique": "Switzerland;Germany;Netherlands"
    },
    {
        "id": "2021.naacl-main.195",
        "title": "Multilingual Multimodal Pre-training for Zero-Shot Cross-Lingual Transfer of Vision-Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "This paper studies zero-shot cross-lingual transfer of vision-language models. Specifically, we focus on multilingual text-to-video search and propose a Transformer-based model that learns contextual multilingual multimodal embeddings. Under a zero-shot setting, we empirically demonstrate that performance degrades significantly when we query the multilingual text-video model with non-English sentences. To address this problem, we introduce a multilingual multimodal pre-training strategy, and collect a new multilingual instructional video dataset (Multi-HowTo100M) for pre-training. Experiments on VTT show that our method significantly improves video search in non-English languages without additional annotations. Furthermore, when multilingual annotations are available, our method outperforms recent baselines by a large margin in multilingual text-to-video search on VTT and VATEX; as well as in multilingual text-to-image search on Multi30K. Our model and Multi-HowTo100M is available at http://github.com/berniebear/Multi-HT100M.",
        "author": "Po-Yao Huang; Mandela Patrick; Junjie Hu; Graham Neubig; Florian Metze; Alexander Hauptmann",
        "authorids": "/p/po-yao-huang/; /m/mandela-patrick/; /j/junjie-hu/; /g/graham-neubig/; /f/florian-metze/; /a/alexander-g-hauptmann/",
        "bibtex": "@inproceedings{huang-etal-2021-multilingual,\n    title = \"Multilingual Multimodal Pre-training for Zero-Shot Cross-Lingual Transfer of Vision-Language Models\",\n    author = \"Huang, Po-Yao  and\n      Patrick, Mandela  and\n      Hu, Junjie  and\n      Neubig, Graham  and\n      Metze, Florian  and\n      Hauptmann, Alexander\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.195/\",\n    doi = \"10.18653/v1/2021.naacl-main.195\",\n    pages = \"2443--2459\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.195.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.195/",
        "pdf_size": 11309700,
        "gs_citation": 69,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1021172599071415733&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "School of Computer Science, Carnegie Mellon University + Facebook AI; Visual Geometry Group, University of Oxford + Facebook AI; School of Computer Science, Carnegie Mellon University; School of Computer Science, Carnegie Mellon University; Facebook AI; School of Computer Science, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;fb.com;cs.cmu.edu;cs.cmu.edu;fb.com;cs.cmu.edu",
        "email": "cs.cmu.edu;fb.com;cs.cmu.edu;cs.cmu.edu;fb.com;cs.cmu.edu",
        "github": "http://github.com/berniebear/Multi-HT100M",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;2+1;0;0;1;0",
        "aff_unique_norm": "Carnegie Mellon University;Facebook;University of Oxford",
        "aff_unique_dep": "School of Computer Science;Facebook AI;Visual Geometry Group",
        "aff_unique_url": "https://www.cmu.edu;https://www.facebook.com;https://www.ox.ac.uk",
        "aff_unique_abbr": "CMU;Facebook AI;Oxford",
        "aff_campus_unique_index": "0;2;0;0;0",
        "aff_campus_unique": "Pittsburgh;;Oxford",
        "aff_country_unique_index": "0+0;1+0;0;0;0;0",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "id": "2021.naacl-main.417",
        "title": "Multimodal End-to-End Sparse Model for Emotion Recognition",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Existing works in multimodal affective computing tasks, such as emotion recognition and personality recognition, generally adopt a two-phase pipeline by first extracting feature representations for each single modality with hand crafted algorithms, and then performing end-to-end learning with extracted features. However, the extracted features are fixed and cannot be further fine-tuned on different target tasks, and manually finding feature extracting algorithms does not generalize or scale well to different tasks, which can lead to sub-optimal performance. In this paper, we develop a fully end-to-end model that connects the two phases and optimizes them jointly. In addition, we restructure the current datasets to enable the fully end-to-end training. Furthermore, to reduce the computational overhead brought by the end-to-end model, we introduce a sparse cross-modal attention mechanism for the feature extraction. Experimental results show that our fully end-to-end model significantly surpasses the current state-of-the-art models based on the two-phase pipeline. Moreover, by adding the sparse cross-modal attention, our model can maintain the performance with around half less computation in the feature extraction part of the model.",
        "author": "Wenliang Dai; Samuel Cahyawijaya; Zihan Liu; Pascale Fung",
        "authorids": "/w/wenliang-dai/; /s/samuel-cahyawijaya/; /z/zihan-liu/; /p/pascale-fung/",
        "bibtex": "@inproceedings{dai-etal-2021-multimodal,\n    title = \"Multimodal End-to-End Sparse Model for Emotion Recognition\",\n    author = \"Dai, Wenliang  and\n      Cahyawijaya, Samuel  and\n      Liu, Zihan  and\n      Fung, Pascale\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.417/\",\n    doi = \"10.18653/v1/2021.naacl-main.417\",\n    pages = \"5305--5316\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.417.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.417/",
        "pdf_size": 3922896,
        "gs_citation": 87,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8166863295850758590&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Center for Arti\ufb01cial Intelligence Research (CAiRE) + Department of Electronic and Computer Engineering + The Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong; Center for Arti\ufb01cial Intelligence Research (CAiRE) + Department of Electronic and Computer Engineering + The Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong; Center for Arti\ufb01cial Intelligence Research (CAiRE) + Department of Electronic and Computer Engineering + The Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong; Center for Arti\ufb01cial Intelligence Research (CAiRE) + Department of Electronic and Computer Engineering + The Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong",
        "aff_domain": "connect.ust.hk;connect.ust.hk;connect.ust.hk;ece.ust.hk",
        "email": "connect.ust.hk;connect.ust.hk;connect.ust.hk;ece.ust.hk",
        "github": "https://github.com/wenliangdai/Multimodal-End2end-Sparse",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1+2;0+1+2;0+1+2;0+1+2",
        "aff_unique_norm": "Center for Arti\ufb01cial Intelligence Research;University of California, Santa Barbara;The Hong Kong University of Science and Technology",
        "aff_unique_dep": "Arti\ufb01cial Intelligence Research;Department of Electronic and Computer Engineering;",
        "aff_unique_url": ";https://www.ece.ucsb.edu;https://www.ust.hk",
        "aff_unique_abbr": "CAiRE;UCSB ECE;HKUST",
        "aff_campus_unique_index": "1;1;1;1",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "1+2;1+2;1+2;1+2",
        "aff_country_unique": ";United States;China"
    },
    {
        "id": "2021.naacl-main.387",
        "title": "Multitask Learning for Emotionally Analyzing Sexual Abuse Disclosures",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The #MeToo movement on social media platforms initiated discussions over several facets of sexual harassment in our society. Prior work by the NLP community for automated identification of the narratives related to sexual abuse disclosures barely explored this social phenomenon as an independent task. However, emotional attributes associated with textual conversations related to the #MeToo social movement are complexly intertwined with such narratives. We formulate the task of identifying narratives related to the sexual abuse disclosures in online posts as a joint modeling task that leverages their emotional attributes through multitask learning. Our results demonstrate that positive knowledge transfer via context-specific shared representations of a flexible cross-stitched parameter sharing model helps establish the inherent benefit of jointly modeling tasks related to sexual abuse disclosures with emotion classification from the text in homogeneous and heterogeneous settings. We show how for more domain-specific tasks related to sexual abuse disclosures such as sarcasm identification and dialogue act (refutation, justification, allegation) classification, homogeneous multitask learning is helpful, whereas for more general tasks such as stance and hate speech detection, heterogeneous multitask learning with emotion classification works better.",
        "author": "Ramit Sawhney; Puneet Mathur; Taru Jain; Akash Kumar Gautam; Rajiv Ratn Shah",
        "authorids": "/r/ramit-sawhney/; /p/puneet-mathur/; /t/taru-jain/; /a/akash-kumar-gautam/; /r/rajiv-shah/",
        "bibtex": "@inproceedings{sawhney-etal-2021-multitask,\n    title = \"Multitask Learning for Emotionally Analyzing Sexual Abuse Disclosures\",\n    author = \"Sawhney, Ramit  and\n      Mathur, Puneet  and\n      Jain, Taru  and\n      Gautam, Akash Kumar  and\n      Shah, Rajiv Ratn\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.387/\",\n    doi = \"10.18653/v1/2021.naacl-main.387\",\n    pages = \"4881--4892\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.387.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.387/",
        "pdf_size": 4378598,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=724509938682482463&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Department of Computer Engineering, IIIT-Delhi; University of Maryland, College Park; Department of Computer Engineering, IIIT-Delhi; Department of Computer Engineering, IIIT-Delhi; Department of Computer Engineering, IIIT-Delhi",
        "aff_domain": "iiitd.ac.in;cs.umd.edu; ;iiitd.ac.in;iiitd.ac.in",
        "email": "iiitd.ac.in;cs.umd.edu; ;iiitd.ac.in;iiitd.ac.in",
        "github": "https://github.com/midas-research/metoo-mtl-naacl",
        "project": "https://metoomvmt.org/",
        "author_num": 5,
        "aff_unique_index": "0;1;0;0;0",
        "aff_unique_norm": "IIIT-Delhi;University of Maryland",
        "aff_unique_dep": "Department of Computer Engineering;",
        "aff_unique_url": "https://www.iiitdelhi.ac.in;https://www/umd.edu",
        "aff_unique_abbr": "IIIT-D;UMD",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";College Park",
        "aff_country_unique_index": "0;1;0;0;0",
        "aff_country_unique": "India;United States"
    },
    {
        "id": "2021.naacl-main.421",
        "title": "Multitasking Inhibits Semantic Drift",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "When intelligent agents communicate to accomplish shared goals, how do these goals shape the agents\u2019 language? We study the dynamics of learning in latent language policies (LLPs), in which instructor agents generate natural-language subgoal descriptions and executor agents map these descriptions to low-level actions. LLPs can solve challenging long-horizon reinforcement learning problems and provide a rich model for studying task-oriented language use. But previous work has found that LLP training is prone to semantic drift (use of messages in ways inconsistent with their original natural language meanings). Here, we demonstrate theoretically and empirically that multitask training is an effective counter to this problem: we prove that multitask training eliminates semantic drift in a well-studied family of signaling games, and show that multitask training of neural LLPs in a complex strategy game reduces drift and while improving sample efficiency.",
        "author": "Athul Paul Jacob; Mike Lewis; Jacob Andreas",
        "authorids": "/a/athul-paul-jacob/; /m/mike-lewis/; /j/jacob-andreas/",
        "bibtex": "@inproceedings{jacob-etal-2021-multitasking,\n    title = \"Multitasking Inhibits Semantic Drift\",\n    author = \"Jacob, Athul Paul  and\n      Lewis, Mike  and\n      Andreas, Jacob\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.421/\",\n    doi = \"10.18653/v1/2021.naacl-main.421\",\n    pages = \"5351--5366\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.421.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.421/",
        "pdf_size": 2506310,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5947770251789076108&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "MIT CSAIL; Facebook AI Research; MIT CSAIL",
        "aff_domain": "mit.edu;fb.com;mit.edu",
        "email": "mit.edu;fb.com;mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Massachusetts Institute of Technology;Facebook",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory;Facebook AI Research",
        "aff_unique_url": "https://www.csail.mit.edu;https://research.facebook.com",
        "aff_unique_abbr": "MIT CSAIL;FAIR",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-demos.3",
        "title": "NAMER: A Node-Based Multitasking Framework for Multi-Hop Knowledge Base Question Answering",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "We present NAMER, an open-domain Chinese knowledge base question answering system based on a novel node-based framework that better grasps the structural mapping between questions and KB queries by aligning the nodes in a query with their corresponding mentions in question. Equipped with techniques including data augmentation and multitasking, we show that the proposed framework outperforms the previous SoTA on CCKS CKBQA dataset. Moreover, we develop a novel data annotation strategy that facilitates the node-to-mention alignment, a dataset (https://github.com/ridiculouz/CKBQA) with such strategy is also published to promote further research. An online demo of NAMER (http://kbqademo.gstore.cn) is provided to visualize our framework and supply extra information for users, a video illustration (https://youtu.be/yetnVye_hg4) of NAMER is also available.",
        "author": "Minhao Zhang; Ruoyu Zhang; Lei Zou; Yinnian Lin; Sen Hu",
        "authorids": "/m/minhao-zhang/; /r/ruoyu-zhang/; /l/lei-zou/; /y/yinnian-lin/; /s/sen-hu/",
        "bibtex": "@inproceedings{zhang-etal-2021-namer,\n    title = \"{NAMER}: A Node-Based Multitasking Framework for Multi-Hop Knowledge Base Question Answering\",\n    author = \"Zhang, Minhao  and\n      Zhang, Ruoyu  and\n      Zou, Lei  and\n      Lin, Yinnian  and\n      Hu, Sen\",\n    editor = \"Sil, Avi  and\n      Lin, Xi Victoria\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-demos.3/\",\n    doi = \"10.18653/v1/2021.naacl-demos.3\",\n    pages = \"18--25\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-demos.3.pdf",
        "site": "https://aclanthology.org/2021.naacl-demos.3/",
        "pdf_size": 582367,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5595914870981437832&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Peking University, China; National Engineering Laboratory for Big Data Analysis Technology and Application (PKU), China; Peking University, China; Peking University, China; Peking University, China",
        "aff_domain": "pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn",
        "email": "pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn",
        "github": "https://github.com/ridiculouz/CKBQA",
        "project": "http://kbqademo.gstore.cn",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Peking University",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.pku.edu.cn",
        "aff_unique_abbr": "Peking U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2021.naacl-main.444",
        "title": "NL-EDIT: Correcting Semantic Parse Errors through Natural Language Interaction",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We study semantic parsing in an interactive setting in which users correct errors with natural language feedback. We present NL-EDIT, a model for interpreting natural language feedback in the interaction context to generate a sequence of edits that can be applied to the initial parse to correct its errors. We show that NL-EDIT can boost the accuracy of existing text-to-SQL parsers by up to 20% with only one turn of correction. We analyze the limitations of the model and discuss directions for improvement and evaluation. The code and datasets used in this paper are publicly available at http://aka.ms/NLEdit.",
        "author": "Ahmed Elgohary; Christopher Meek; Matthew Richardson; Adam Fourney; Gonzalo Ramos; Ahmed Hassan Awadallah",
        "authorids": "/a/ahmed-elgohary/; /c/christopher-meek/; /m/matthew-richardson/; /a/adam-fourney/; /g/gonzalo-ramos/; /a/ahmed-hassan/",
        "bibtex": "@inproceedings{elgohary-etal-2021-nl,\n    title = \"{NL}-{EDIT}: Correcting Semantic Parse Errors through Natural Language Interaction\",\n    author = \"Elgohary, Ahmed  and\n      Meek, Christopher  and\n      Richardson, Matthew  and\n      Fourney, Adam  and\n      Ramos, Gonzalo  and\n      Awadallah, Ahmed Hassan\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.444/\",\n    doi = \"10.18653/v1/2021.naacl-main.444\",\n    pages = \"5599--5610\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.444.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.444/",
        "pdf_size": 998402,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16312812486413123124&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "University of Maryland+Microsoft Research; Microsoft Research; Microsoft Research; Microsoft Research; Microsoft Research; Microsoft Research",
        "aff_domain": "cs.umd.edu;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "email": "cs.umd.edu;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "github": "",
        "project": "http://aka.ms/NLEdit",
        "author_num": 6,
        "aff_unique_index": "0+1;1;1;1;1;1",
        "aff_unique_norm": "University of Maryland;Microsoft Corporation",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www/umd.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "UMD;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.251",
        "title": "Negative language transfer in learner English: A new dataset",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Automatic personalized corrective feedback can help language learners from different backgrounds better acquire a new language. This paper introduces a learner English dataset in which learner errors are accompanied by information about possible error sources. This dataset contains manually annotated error causes for learner writing errors. These causes tie learner mistakes to structures from their first languages, when the rules in English and in the first language diverge. This new dataset will enable second language acquisition researchers to computationally analyze a large quantity of learner errors that are related to language transfer from the learners\u2019 first language. The dataset can also be applied in personalizing grammatical error correction systems according to the learners\u2019 first language and in providing feedback that is informed by the cause of an error.",
        "author": "Leticia Farias Wanderley; Nicole Zhao; Carrie Demmans Epp",
        "authorids": "/l/leticia-farias-wanderley/; /n/nicole-zhao/; /c/carrie-demmans-epp/",
        "bibtex": "@inproceedings{farias-wanderley-etal-2021-negative,\n    title = \"Negative language transfer in learner {E}nglish: A new dataset\",\n    author = \"Farias Wanderley, Leticia  and\n      Zhao, Nicole  and\n      Demmans Epp, Carrie\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.251/\",\n    doi = \"10.18653/v1/2021.naacl-main.251\",\n    pages = \"3129--3142\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.251.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.251/",
        "pdf_size": 232987,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18011410749677679730&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 2,
        "aff": "EdTeKLA Research Group, Department of Computing Science, University of Alberta; EdTeKLA Research Group, Department of Computing Science, University of Alberta; EdTeKLA Research Group, Department of Computing Science, University of Alberta",
        "aff_domain": "ualberta.ca;ualberta.ca;ualberta.ca",
        "email": "ualberta.ca;ualberta.ca;ualberta.ca",
        "github": "https://github.com/EdTeKLA/LanguageTransfer",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Alberta",
        "aff_unique_dep": "Department of Computing Science",
        "aff_unique_url": "https://www.ualberta.ca",
        "aff_unique_abbr": "UAlberta",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2021.naacl-main.67",
        "title": "Neural Language Modeling for Contextualized Temporal Graph Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "This paper presents the first study on using large-scale pre-trained language models for automated generation of an event-level temporal graph for a document. Despite the huge success of neural pre-training methods in NLP tasks, its potential for temporal reasoning over event graphs has not been sufficiently explored. Part of the reason is the difficulty in obtaining large training corpora with human-annotated events and temporal links. We address this challenge by using existing IE/NLP tools to automatically generate a large quantity (89,000) of system-produced document-graph pairs, and propose a novel formulation of the contextualized graph generation problem as a sequence-to-sequence mapping task. These strategies enable us to leverage and fine-tune pre-trained language models on the system-induced training data for the graph generation task. Our experiments show that our approach is highly effective in generating structurally and semantically valid graphs. Further, evaluation on a challenging hand-labeled, out-of-domain corpus shows that our method outperforms the closest existing method by a large margin on several metrics. We also show a downstream application of our approach by adapting it to answer open-ended temporal questions in a reading comprehension setting.",
        "author": "Aman Madaan; Yiming Yang",
        "authorids": "/a/aman-madaan/; /y/yiming-yang/",
        "bibtex": "@inproceedings{madaan-yang-2021-neural,\n    title = \"Neural Language Modeling for Contextualized Temporal Graph Generation\",\n    author = \"Madaan, Aman  and\n      Yang, Yiming\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.67/\",\n    doi = \"10.18653/v1/2021.naacl-main.67\",\n    pages = \"864--881\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.67.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.67/",
        "pdf_size": 2997692,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14427912699665041220&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "https://github.com/madaan/temporal-graph-gensis",
        "project": "",
        "author_num": 2
    },
    {
        "id": "2021.naacl-main.17",
        "title": "Neural Machine Translation without Embeddings",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Many NLP models operate over sequences of subword tokens produced by hand-crafted tokenization rules and heuristic subword induction algorithms. A simple universal alternative is to represent every computerized text as a sequence of bytes via UTF-8, obviating the need for an embedding layer since there are fewer token types (256) than dimensions. Surprisingly, replacing the ubiquitous embedding layer with one-hot representations of each byte does not hurt performance; experiments on byte-to-byte machine translation from English to 10 different languages show a consistent improvement in BLEU, rivaling character-level and even standard subword-level models. A deeper investigation reveals that the combination of embeddingless models with decoder-input dropout amounts to token dropout, which benefits byte-to-byte models in particular.",
        "author": "Uri Shaham; Omer Levy",
        "authorids": "/u/uri-shaham/; /o/omer-levy/",
        "bibtex": "@inproceedings{shaham-levy-2021-neural,\n    title = \"Neural Machine Translation without Embeddings\",\n    author = \"Shaham, Uri  and\n      Levy, Omer\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.17/\",\n    doi = \"10.18653/v1/2021.naacl-main.17\",\n    pages = \"181--186\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.17.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.17/",
        "pdf_size": 294641,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=796229637770567597&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "The Blavatnik School of Computer Science, Tel Aviv University; The Blavatnik School of Computer Science, Tel Aviv University + Facebook AI Research",
        "aff_domain": ";",
        "email": ";",
        "github": "https://github.com/UriSha/EmbeddinglessNMT",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "Tel Aviv University;Facebook",
        "aff_unique_dep": "Blavatnik School of Computer Science;Facebook AI Research",
        "aff_unique_url": "https://www.tau.ac.il;https://research.facebook.com",
        "aff_unique_abbr": "TAU;FAIR",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Tel Aviv;",
        "aff_country_unique_index": "0;0+1",
        "aff_country_unique": "Israel;United States"
    },
    {
        "id": "2021.naacl-main.430",
        "title": "Neural Network Surgery: Injecting Data Patterns into Pre-trained Models with Minimal Instance-wise Side Effects",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Side effects during neural network tuning are typically measured by overall accuracy changes. However, we find that even with similar overall accuracy, existing tuning methods result in non-negligible instance-wise side effects. Motivated by neuroscientific evidence and theoretical results, we demonstrate that side effects can be controlled by the number of changed parameters and thus, we propose to conduct neural network surgery by only modifying a limited number of parameters. Neural network surgery can be realized using diverse techniques and we investigate three lines of methods. Experimental results on representative tuning problems validate the effectiveness of the surgery approach. The dynamic selecting method achieves the best overall performance that not only satisfies the tuning goal but also induces fewer instance-wise side effects by changing only 10-5 of the parameters.",
        "author": "Zhiyuan Zhang; Xuancheng Ren; Qi Su; Xu Sun; Bin He",
        "authorids": "/z/zhiyuan-zhang/; /x/xuancheng-ren/; /q/qi-su/; /x/xu-sun/; /b/bin-he/",
        "bibtex": "@inproceedings{zhang-etal-2021-neural,\n    title = \"Neural Network Surgery: Injecting Data Patterns into Pre-trained Models with Minimal Instance-wise Side Effects\",\n    author = \"Zhang, Zhiyuan  and\n      Ren, Xuancheng  and\n      Su, Qi  and\n      Sun, Xu  and\n      He, Bin\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.430/\",\n    doi = \"10.18653/v1/2021.naacl-main.430\",\n    pages = \"5453--5466\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.430.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.430/",
        "pdf_size": 540937,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15495926781892011494&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "MOE Key Laboratory of Computational Linguistic, School of EECS, Peking University; MOE Key Laboratory of Computational Linguistic, School of EECS, Peking University; MOE Key Laboratory of Computational Linguistic, School of EECS, Peking University; MOE Key Laboratory of Computational Linguistic, School of EECS, Peking University + Center for Data Science, Peking University; Huawei Noah\u2019s Ark Lab",
        "aff_domain": "pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn;huawei.com",
        "email": "pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn;huawei.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0+0;1",
        "aff_unique_norm": "Peking University;Huawei",
        "aff_unique_dep": "School of EECS;Noah\u2019s Ark Lab",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.huawei.com",
        "aff_unique_abbr": "PKU;Huawei",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0;0;0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2021.naacl-main.429",
        "title": "Neural Quality Estimation with Multiple Hypotheses for Grammatical Error Correction",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Grammatical Error Correction (GEC) aims to correct writing errors and help language learners improve their writing skills. However, existing GEC models tend to produce spurious corrections or fail to detect lots of errors. The quality estimation model is necessary to ensure learners get accurate GEC results and avoid misleading from poorly corrected sentences. Well-trained GEC models can generate several high-quality hypotheses through decoding, such as beam search, which provide valuable GEC evidence and can be used to evaluate GEC quality. However, existing models neglect the possible GEC evidence from different hypotheses. This paper presents the Neural Verification Network (VERNet) for GEC quality estimation with multiple hypotheses. VERNet establishes interactions among hypotheses with a reasoning graph and conducts two kinds of attention mechanisms to propagate GEC evidence to verify the quality of generated hypotheses. Our experiments on four GEC datasets show that VERNet achieves state-of-the-art grammatical error detection performance, achieves the best quality estimation results, and significantly improves GEC performance by reranking hypotheses. All data and source codes are available at https://github.com/thunlp/VERNet.",
        "author": "Zhenghao Liu; Xiaoyuan Yi; Maosong Sun; Liner Yang; Tat-Seng Chua",
        "authorids": "/z/zhenghao-liu/; /x/xiaoyuan-yi/; /m/maosong-sun/; /l/liner-yang/; /t/tat-seng-chua/",
        "bibtex": "@inproceedings{liu-etal-2021-neural,\n    title = \"Neural Quality Estimation with Multiple Hypotheses for Grammatical Error Correction\",\n    author = \"Liu, Zhenghao  and\n      Yi, Xiaoyuan  and\n      Sun, Maosong  and\n      Yang, Liner  and\n      Chua, Tat-Seng\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.429/\",\n    doi = \"10.18653/v1/2021.naacl-main.429\",\n    pages = \"5441--5452\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.429.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.429/",
        "pdf_size": 1398061,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15029474453086457875&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "https://github.com/thunlp/VERNet",
        "project": "",
        "author_num": 5
    },
    {
        "id": "2021.naacl-main.116",
        "title": "Neural Sequence Segmentation as Determining the Leftmost Segments",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Prior methods to text segmentation are mostly at token level. Despite the adequacy, this nature limits their full potential to capture the long-term dependencies among segments. In this work, we propose a novel framework that incrementally segments natural language sentences at segment level. For every step in segmentation, it recognizes the leftmost segment of the remaining sequence. Implementations involve LSTM-minus technique to construct the phrase representations and recurrent neural networks (RNN) to model the iterations of determining the leftmost segments. We have conducted extensive experiments on syntactic chunking and Chinese part-of-speech (POS) tagging across 3 datasets, demonstrating that our methods have significantly outperformed previous all baselines and achieved new state-of-the-art results. Moreover, qualitative analysis and the study on segmenting long-length sentences verify its effectiveness in modeling long-term dependencies.",
        "author": "Yangming Li; Lemao Liu; Kaisheng Yao",
        "authorids": "/y/yangming-li/; /l/lemao-liu/; /k/kaisheng-yao/",
        "bibtex": "@inproceedings{li-etal-2021-neural,\n    title = \"Neural Sequence Segmentation as Determining the Leftmost Segments\",\n    author = \"Li, Yangming  and\n      Liu, Lemao  and\n      Yao, Kaisheng\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.116/\",\n    doi = \"10.18653/v1/2021.naacl-main.116\",\n    pages = \"1476--1486\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.116.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.116/",
        "pdf_size": 377846,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6839752059257304607&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Tencent AI Lab; Tencent AI Lab; Ant Group",
        "aff_domain": "tencent.com;tencent.com;antgroup.com",
        "email": "tencent.com;tencent.com;antgroup.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Tencent;Ant Group",
        "aff_unique_dep": "Tencent AI Lab;",
        "aff_unique_url": "https://ai.tencent.com;https://www.antgroup.com",
        "aff_unique_abbr": "Tencent AI Lab;Ant Group",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2021.naacl-main.339",
        "title": "NeuroLogic Decoding: (Un)supervised Neural Text Generation with Predicate Logic Constraints",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Conditional text generation often requires lexical constraints, i.e., which words should or shouldn\u2019t be included in the output text. While the dominant recipe for conditional text generation has been large-scale pretrained language models that are finetuned on the task-specific training data, such models do not learn to follow the underlying constraints reliably, even when supervised with large amounts of task-specific examples. We propose NeuroLogic Decoding, a simple yet effective algorithm that enables neural language models \u2013 supervised or not \u2013 to generate fluent text while satisfying complex lexical constraints. Our approach is powerful yet efficient. It handles any set of lexical constraints that is expressible under predicate logic, while its asymptotic runtime is equivalent to conventional beam search. Empirical results on four benchmarks show that NeuroLogic Decoding outperforms previous approaches, including algorithms that handle a subset of our constraints. Moreover, we find that unsupervised models with NeuroLogic Decoding often outperform supervised models with conventional decoding, even when the latter is based on considerably larger networks. Our results suggest the limit of large-scale neural networks for fine-grained controllable generation and the promise of inference-time algorithms.",
        "author": "Ximing Lu; Peter West; Rowan Zellers; Ronan Le Bras; Chandra Bhagavatula; Yejin Choi",
        "authorids": "/x/ximing-lu/; /p/peter-west/; /r/rowan-zellers/; /r/ronan-le-bras/; /c/chandra-bhagavatula/; /y/yejin-choi/",
        "bibtex": "@inproceedings{lu-etal-2021-neurologic,\n    title = \"{N}euro{L}ogic Decoding: (Un)supervised Neural Text Generation with Predicate Logic Constraints\",\n    author = \"Lu, Ximing  and\n      West, Peter  and\n      Zellers, Rowan  and\n      Le Bras, Ronan  and\n      Bhagavatula, Chandra  and\n      Choi, Yejin\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.339/\",\n    doi = \"10.18653/v1/2021.naacl-main.339\",\n    pages = \"4288--4299\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.339.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.339/",
        "pdf_size": 1088015,
        "gs_citation": 165,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13668849515210613307&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Paul G. Allen School of Computer Science & Engineering, University of Washington+Allen Institute for Arti\ufb01cial Intelligence; Paul G. Allen School of Computer Science & Engineering, University of Washington+Allen Institute for Arti\ufb01cial Intelligence; Paul G. Allen School of Computer Science & Engineering, University of Washington+Allen Institute for Arti\ufb01cial Intelligence; Allen Institute for Arti\ufb01cial Intelligence; Allen Institute for Arti\ufb01cial Intelligence; Paul G. Allen School of Computer Science & Engineering, University of Washington+Allen Institute for Arti\ufb01cial Intelligence",
        "aff_domain": "cs.washington.edu;cs.washington.edu;cs.washington.edu;allenai.org;allenai.org;cs.washington.edu",
        "email": "cs.washington.edu;cs.washington.edu;cs.washington.edu;allenai.org;allenai.org;cs.washington.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;0+1;0+1;1;1;0+1",
        "aff_unique_norm": "University of Washington;Allen Institute for Artificial Intelligence",
        "aff_unique_dep": "Paul G. Allen School of Computer Science & Engineering;Artificial Intelligence",
        "aff_unique_url": "https://www.cs.washington.edu;https://allenai.org",
        "aff_unique_abbr": "UW;AI2",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Seattle;",
        "aff_country_unique_index": "0+0;0+0;0+0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.255",
        "title": "News Headline Grouping as a Challenging NLU Task",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent progress in Natural Language Understanding (NLU) has seen the latest models outperform human performance on many standard tasks. These impressive results have led the community to introspect on dataset limitations, and iterate on more nuanced challenges. In this paper, we introduce the task of HeadLine Grouping (HLG) and a corresponding dataset (HLGD) consisting of 20,056 pairs of news headlines, each labeled with a binary judgement as to whether the pair belongs within the same group. On HLGD, human annotators achieve high performance of around 0.9 F-1, while current state-of-the art Transformer models only reach 0.75 F-1, opening the path for further improvements. We further propose a novel unsupervised Headline Generator Swap model for the task of HeadLine Grouping that achieves within 3 F-1 of the best supervised model. Finally, we analyze high-performing models with consistency tests, and find that models are not consistent in their predictions, revealing modeling limits of current architectures.",
        "author": "Philippe Laban; Lucas Bandarkar; Marti A. Hearst",
        "authorids": "/p/philippe-laban/; /l/lucas-bandarkar/; /m/marti-a-hearst/",
        "bibtex": "@inproceedings{laban-etal-2021-news,\n    title = \"News Headline Grouping as a Challenging {NLU} Task\",\n    author = \"Laban, Philippe  and\n      Bandarkar, Lucas  and\n      Hearst, Marti A.\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.255/\",\n    doi = \"10.18653/v1/2021.naacl-main.255\",\n    pages = \"3186--3198\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.255.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.255/",
        "pdf_size": 631650,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12655406289564277846&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "UC Berkeley; UC Berkeley; UC Berkeley",
        "aff_domain": "berkeley.edu;berkeley.edu;berkeley.edu",
        "email": "berkeley.edu;berkeley.edu;berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-industry.25",
        "title": "Noise Robust Named Entity Understanding for Voice Assistants",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Named Entity Recognition (NER) and Entity Linking (EL) play an essential role in voice assistant interaction, but are challenging due to the special difficulties associated with spoken user queries. In this paper, we propose a novel architecture that jointly solves the NER and EL tasks by combining them in a joint reranking module. We show that our proposed framework improves NER accuracy by up to 3.13% and EL accuracy by up to 3.6% in F1 score. The features used also lead to better accuracies in other natural language understanding tasks, such as domain classification and semantic parsing.",
        "author": "Deepak Muralidharan; Joel Ruben Antony Moniz; Sida Gao; Xiao Yang; Justine Kao; Stephen Pulman; Atish Kothari; Ray Shen; Yinying Pan; Vivek Kaul; Mubarak Seyed Ibrahim; Gang Xiang; Nan Dun; Yidan Zhou; Andy O; Yuan Zhang; Pooja Chitkara; Xuan Wang; Alkesh Patel; Kushal Tayal; Roger Zheng; Peter Grasch; Jason D Williams; Lin Li",
        "authorids": "/d/deepak-muralidharan/; /j/joel-ruben-antony-moniz/; /s/sida-gao/; /x/xiao-yang/; /j/justine-kao/; /s/stephen-pulman/; /a/atish-kothari/; /r/ray-shen/; /y/yinying-pan/; /v/vivek-kaul/; /m/mubarak-seyed-ibrahim/; /g/gang-xiang/; /n/nan-dun/; /y/yidan-zhou/; /a/andy-o/; /y/yuan-zhang/; /p/pooja-chitkara/; /x/xuan-wang/; /a/alkesh-patel/; /k/kushal-tayal/; /r/roger-zheng/; /p/peter-grasch/; /j/jason-d-williams/; /l/lin-li/",
        "bibtex": "@inproceedings{muralidharan-etal-2021-noise,\n    title = \"Noise Robust Named Entity Understanding for Voice Assistants\",\n    author = \"Muralidharan, Deepak  and\n      Moniz, Joel Ruben Antony  and\n      Gao, Sida  and\n      Yang, Xiao  and\n      Kao, Justine  and\n      Pulman, Stephen  and\n      Kothari, Atish  and\n      Shen, Ray  and\n      Pan, Yinying  and\n      Kaul, Vivek  and\n      Seyed Ibrahim, Mubarak  and\n      Xiang, Gang  and\n      Dun, Nan  and\n      Zhou, Yidan  and\n      O, Andy  and\n      Zhang, Yuan  and\n      Chitkara, Pooja  and\n      Wang, Xuan  and\n      Patel, Alkesh  and\n      Tayal, Kushal  and\n      Zheng, Roger  and\n      Grasch, Peter  and\n      Williams, Jason D  and\n      Li, Lin\",\n    editor = \"Kim, Young-bum  and\n      Li, Yunyao  and\n      Rambow, Owen\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-industry.25/\",\n    doi = \"10.18653/v1/2021.naacl-industry.25\",\n    pages = \"196--204\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-industry.25.pdf",
        "site": "https://aclanthology.org/2021.naacl-industry.25/",
        "pdf_size": 283155,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10212243658928599834&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Apple; Apple; Apple; Apple+graceyx.scut@gmail.com; Apple; Apple; Apple; Apple; Apple; Apple; Apple; Apple; Apple; Apple; Apple; Apple; Apple; Apple; Apple; Apple; Apple; Apple; Apple; Apple+lli9@apple.com",
        "aff_domain": "apple.com;apple.com;apple.com;apple.com;apple.com;apple.com;apple.com;apple.com;apple.com;apple.com;apple.com;apple.com;apple.com;apple.com;apple.com;apple.com;apple.com;apple.com;apple.com;apple.com;apple.com;apple.com;apple.com;apple.com",
        "email": "apple.com;apple.com;apple.com;apple.com;apple.com;apple.com;apple.com;apple.com;apple.com;apple.com;apple.com;apple.com;apple.com;apple.com;apple.com;apple.com;apple.com;apple.com;apple.com;apple.com;apple.com;apple.com;apple.com;apple.com",
        "github": "",
        "project": "",
        "author_num": 24,
        "aff_unique_index": "0;0;0;0+1;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0+0",
        "aff_unique_norm": "Apple Inc.;South China University of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.apple.com;https://www.scut.edu.cn",
        "aff_unique_abbr": "Apple;SCUT",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0+1;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0+0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2021.naacl-main.258",
        "title": "Noise Stability Regularization for Improving BERT Fine-tuning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Fine-tuning pre-trained language models suchas BERT has become a common practice dom-inating leaderboards across various NLP tasks. Despite its recent success and wide adoption,this process is unstable when there are onlya small number of training samples available. The brittleness of this process is often reflectedby the sensitivity to random seeds. In this pa-per, we propose to tackle this problem basedon the noise stability property of deep nets,which is investigated in recent literature (Aroraet al., 2018; Sanyal et al., 2020). Specifically,we introduce a novel and effective regulariza-tion method to improve fine-tuning on NLPtasks, referred to asLayer-wiseNoiseStabilityRegularization (LNSR). We extend the theo-ries about adding noise to the input and provethat our method gives a stabler regularizationeffect. We provide supportive evidence by ex-perimentally confirming that well-performingmodels show a low sensitivity to noise andfine-tuning with LNSR exhibits clearly bet-ter generalizability and stability. Furthermore,our method also demonstrates advantages overother state-of-the-art algorithms including L2-SP (Li et al., 2018), Mixout (Lee et al., 2020)and SMART (Jiang et al., 20)",
        "author": "Hang Hua; Xingjian Li; Dejing Dou; Chengzhong Xu; Jiebo Luo",
        "authorids": "/h/hang-hua/; /x/xingjian-li/; /d/dejing-dou/; /c/chengzhong-xu/; /j/jiebo-luo/",
        "bibtex": "@inproceedings{hua-etal-2021-noise,\n    title = \"Noise Stability Regularization for Improving {BERT} Fine-tuning\",\n    author = \"Hua, Hang  and\n      Li, Xingjian  and\n      Dou, Dejing  and\n      Xu, Chengzhong  and\n      Luo, Jiebo\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.258/\",\n    doi = \"10.18653/v1/2021.naacl-main.258\",\n    pages = \"3229--3241\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.258.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.258/",
        "pdf_size": 1252833,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6850190329237453264&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "University of Rochester, Rochester, NY, USA; Big Data Lab, Baidu Research, Beijing, China + Department of Computer Science, University of Macau, Macau, China; Big Data Lab, Baidu Research, Beijing, China; Department of Computer Science, University of Macau, Macau, China; University of Rochester, Rochester, NY, USA",
        "aff_domain": "cs.rochester.edu;cs.rochester.edu;baidu.com;baidu.com;um.edu.mo",
        "email": "cs.rochester.edu;cs.rochester.edu;baidu.com;baidu.com;um.edu.mo",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1+2;1;2;0",
        "aff_unique_norm": "University of Rochester;Baidu Research;University of Macau",
        "aff_unique_dep": ";Big Data Lab;Department of Computer Science",
        "aff_unique_url": "https://www.rochester.edu;https://baidu.com;https://www.um.edu.mo",
        "aff_unique_abbr": "U of R;Baidu;UM",
        "aff_campus_unique_index": "0;1+2;1;2;0",
        "aff_campus_unique": "Rochester;Beijing;Macau",
        "aff_country_unique_index": "0;1+1;1;1;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2021.naacl-main.56",
        "title": "Noisy Self-Knowledge Distillation for Text Summarization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In this paper we apply self-knowledge distillation to text summarization which we argue can alleviate problems with maximum-likelihood training on single reference and noisy datasets. Instead of relying on one-hot annotation labels, our student summarization model is trained with guidance from a teacher which generates smoothed labels to help regularize training. Furthermore, to better model uncertainty during training, we introduce multiple noise signals for both teacher and student models. We demonstrate experimentally on three benchmarks that our framework boosts the performance of both pretrained and non-pretrained summarizers achieving state-of-the-art results.",
        "author": "Yang Liu; Sheng Shen; Mirella Lapata",
        "authorids": "/y/yang-liu-edinburgh/; /s/sheng-shen/; /m/mirella-lapata/",
        "bibtex": "@inproceedings{liu-etal-2021-noisy,\n    title = \"Noisy Self-Knowledge Distillation for Text Summarization\",\n    author = \"Liu, Yang  and\n      Shen, Sheng  and\n      Lapata, Mirella\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.56/\",\n    doi = \"10.18653/v1/2021.naacl-main.56\",\n    pages = \"692--703\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.56.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.56/",
        "pdf_size": 326924,
        "gs_citation": 58,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13025818133599841162&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 5,
        "aff": "Microsoft Cognitive Services Research; University of California, Berkeley; School of Informatics, University of Edinburgh",
        "aff_domain": "microsoft.com;berkeley.edu;inf.ed.ac.uk",
        "email": "microsoft.com;berkeley.edu;inf.ed.ac.uk",
        "github": "https://github.com/nlpyang/NoisySumm",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Microsoft;University of California, Berkeley;University of Edinburgh",
        "aff_unique_dep": "Cognitive Services Research;;School of Informatics",
        "aff_unique_url": "https://www.microsoft.com;https://www.berkeley.edu;https://www.ed.ac.uk",
        "aff_unique_abbr": "Microsoft;UC Berkeley;Edinburgh",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Berkeley;Edinburgh",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "id": "2021.naacl-main.269",
        "title": "Noisy-Labeled NER with Confidence Estimation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent studies in deep learning have shown significant progress in named entity recognition (NER). However, most existing works assume clean data annotation, while real-world scenarios typically involve a large amount of noises from a variety of sources (e.g., pseudo, weak, or distant annotations). This work studies NER under a noisy labeled setting with calibrated confidence estimation. Based on empirical observations of different training dynamics of noisy and clean labels, we propose strategies for estimating confidence scores based on local and global independence assumptions. We partially marginalize out labels of low confidence with a CRF model. We further propose a calibration method for confidence scores based on the structure of entity labels. We integrate our approach into a self-training framework for boosting performance. Experiments in general noisy settings with four languages and distantly labeled settings demonstrate the effectiveness of our method.",
        "author": "Kun Liu; Yao Fu; Chuanqi Tan; Mosha Chen; Ningyu Zhang; Songfang Huang; Sheng Gao",
        "authorids": "/k/kun-liu/; /y/yao-fu/; /c/chuanqi-tan/; /m/mosha-chen/; /n/ningyu-zhang/; /s/songfang-huang/; /s/sheng-gao/",
        "bibtex": "@inproceedings{liu-etal-2021-noisy-labeled,\n    title = \"Noisy-Labeled {NER} with Confidence Estimation\",\n    author = \"Liu, Kun  and\n      Fu, Yao  and\n      Tan, Chuanqi  and\n      Chen, Mosha  and\n      Zhang, Ningyu  and\n      Huang, Songfang  and\n      Gao, Sheng\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.269/\",\n    doi = \"10.18653/v1/2021.naacl-main.269\",\n    pages = \"3437--3445\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.269.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.269/",
        "pdf_size": 482835,
        "gs_citation": 67,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12244571107371610672&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Alibaba Group; University of Edinburgh; Zhejiang University; Guizhou Provincial Key Laboratory of Public Big Data, Guizhou University; Alibaba Group; Alibaba Group; Guizhou Provincial Key Laboratory of Public Big Data, Guizhou University",
        "aff_domain": "gmail.com;ed.ac.uk;alibaba-inc.com;alibaba-inc.com;zju.edu.cn;alibaba-inc.com;gmail.com",
        "email": "gmail.com;ed.ac.uk;alibaba-inc.com;alibaba-inc.com;zju.edu.cn;alibaba-inc.com;gmail.com",
        "github": "https://github.com/liukun95/Noisy-NER-Confidence-Estimation",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;2;3;0;0;3",
        "aff_unique_norm": "Alibaba Group;University of Edinburgh;Zhejiang University;Guizhou University",
        "aff_unique_dep": ";;;Guizhou Provincial Key Laboratory of Public Big Data",
        "aff_unique_url": "https://www.alibaba.com;https://www.ed.ac.uk;https://www.zju.edu.cn;https://www.gzu.edu.cn",
        "aff_unique_abbr": "Alibaba;Edinburgh;ZJU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0;0;0;0",
        "aff_country_unique": "China;United Kingdom"
    },
    {
        "id": "2021.naacl-main.236",
        "title": "Non-Autoregressive Semantic Parsing for Compositional Task-Oriented Dialog",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Semantic parsing using sequence-to-sequence models allows parsing of deeper representations compared to traditional word tagging based models. In spite of these advantages, widespread adoption of these models for real-time conversational use cases has been stymied by higher compute requirements and thus higher latency. In this work, we propose a non-autoregressive approach to predict semantic parse trees with an efficient seq2seq model architecture. By combining non-autoregressive prediction with convolutional neural networks, we achieve significant latency gains and parameter size reduction compared to traditional RNN models. Our novel architecture achieves up to an 81% reduction in latency on TOP dataset and retains competitive performance to non-pretrained models on three different semantic parsing datasets.",
        "author": "Arun Babu; Akshat Shrivastava; Armen Aghajanyan; Ahmed Aly; Angela Fan; Marjan Ghazvininejad",
        "authorids": "/a/arun-babu/; /a/akshat-shrivastava/; /a/armen-aghajanyan/; /a/ahmed-aly/; /a/angela-fan/; /m/marjan-ghazvininejad/",
        "bibtex": "@inproceedings{babu-etal-2021-non,\n    title = \"Non-Autoregressive Semantic Parsing for Compositional Task-Oriented Dialog\",\n    author = \"Babu, Arun  and\n      Shrivastava, Akshat  and\n      Aghajanyan, Armen  and\n      Aly, Ahmed  and\n      Fan, Angela  and\n      Ghazvininejad, Marjan\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.236/\",\n    doi = \"10.18653/v1/2021.naacl-main.236\",\n    pages = \"2969--2978\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.236.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.236/",
        "pdf_size": 400556,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1668679254549638999&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Facebook; Facebook; Facebook; Facebook; Facebook; Facebook",
        "aff_domain": "fb.com;fb.com;fb.com;fb.com;fb.com;fb.com",
        "email": "fb.com;fb.com;fb.com;fb.com;fb.com;fb.com",
        "github": "https://github.com/facebookresearch/pytext",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Facebook, Inc.",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.facebook.com",
        "aff_unique_abbr": "FB",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.458",
        "title": "Non-Autoregressive Translation by Learning Target Categorical Codes",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Non-autoregressive Transformer is a promising text generation model. However, current non-autoregressive models still fall behind their autoregressive counterparts in translation quality. We attribute this accuracy gap to the lack of dependency modeling among decoder inputs. In this paper, we propose CNAT, which learns implicitly categorical codes as latent variables into the non-autoregressive decoding. The interaction among these categorical codes remedies the missing dependencies and improves the model capacity. Experiment results show that our model achieves comparable or better performance in machine translation tasks than several strong baselines.",
        "author": "Yu Bao; Shujian Huang; Tong Xiao; Dongqi Wang; Xinyu Dai; Jiajun Chen",
        "authorids": "/y/yu-bao/; /s/shujian-huang/; /t/tong-xiao/; /d/dongqi-wang/; /x/xinyu-dai/; /j/jiajun-chen/",
        "bibtex": "@inproceedings{bao-etal-2021-non,\n    title = \"Non-Autoregressive Translation by Learning Target Categorical Codes\",\n    author = \"Bao, Yu  and\n      Huang, Shujian  and\n      Xiao, Tong  and\n      Wang, Dongqi  and\n      Dai, Xinyu  and\n      Chen, Jiajun\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.458/\",\n    doi = \"10.18653/v1/2021.naacl-main.458\",\n    pages = \"5749--5759\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.458.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.458/",
        "pdf_size": 1836891,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3449552520066080900&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "National Key Laboratory for Novel Software Technology, Nanjing University + Collaborative Innovation Center of Novel Software Technology and Industrialization; National Key Laboratory for Novel Software Technology, Nanjing University + Collaborative Innovation Center of Novel Software Technology and Industrialization; NiuTrans Co., Ltd., Shenyang, China; National Key Laboratory for Novel Software Technology, Nanjing University + Collaborative Innovation Center of Novel Software Technology and Industrialization; National Key Laboratory for Novel Software Technology, Nanjing University + Collaborative Innovation Center of Novel Software Technology and Industrialization; National Key Laboratory for Novel Software Technology, Nanjing University + Collaborative Innovation Center of Novel Software Technology and Industrialization",
        "aff_domain": "smail.nju.edu.cn;smail.nju.edu.cn;mail.neu.edu.cn;nju.edu.cn;nju.edu.cn;nju.edu.cn",
        "email": "smail.nju.edu.cn;smail.nju.edu.cn;mail.neu.edu.cn;nju.edu.cn;nju.edu.cn;nju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;0+1;2;0+1;0+1;0+1",
        "aff_unique_norm": "Nanjing University;Collaborative Innovation Center of Novel Software Technology and Industrialization;NiuTrans Co., Ltd.",
        "aff_unique_dep": "National Key Laboratory for Novel Software Technology;;",
        "aff_unique_url": "http://www.nju.edu.cn;;",
        "aff_unique_abbr": "Nanjing University;;",
        "aff_campus_unique_index": ";;;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China;"
    },
    {
        "id": "2021.naacl-main.142",
        "title": "Non-Parametric Few-Shot Learning for Word Sense Disambiguation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Word sense disambiguation (WSD) is a long-standing problem in natural language processing. One significant challenge in supervised all-words WSD is to classify among senses for a majority of words that lie in the long-tail distribution. For instance, 84% of the annotated words have less than 10 examples in the SemCor training data. This issue is more pronounced as the imbalance occurs in both word and sense distributions. In this work, we propose MetricWSD, a non-parametric few-shot learning approach to mitigate this data imbalance issue. By learning to compute distances among the senses of a given word through episodic training, MetricWSD transfers knowledge (a learned metric space) from high-frequency words to infrequent ones. MetricWSD constructs the training episodes tailored to word frequencies and explicitly addresses the problem of the skewed distribution, as opposed to mixing all the words trained with parametric models in previous work. Without resorting to any lexical resources, MetricWSD obtains strong performance against parametric alternatives, achieving a 75.1 F1 score on the unified WSD evaluation benchmark (Raganato et al., 2017b). Our analysis further validates that infrequent words and senses enjoy significant improvement.",
        "author": "Howard Chen; Mengzhou Xia; Danqi Chen",
        "authorids": "/h/howard-chen/; /m/mengzhou-xia/; /d/danqi-chen/",
        "bibtex": "@inproceedings{chen-etal-2021-non,\n    title = \"Non-Parametric Few-Shot Learning for Word Sense Disambiguation\",\n    author = \"Chen, Howard  and\n      Xia, Mengzhou  and\n      Chen, Danqi\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.142/\",\n    doi = \"10.18653/v1/2021.naacl-main.142\",\n    pages = \"1774--1781\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.142.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.142/",
        "pdf_size": 424952,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=597259921742860183&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Princeton University; Princeton University; Princeton University",
        "aff_domain": "cs.princeton.edu;cs.princeton.edu;cs.princeton.edu",
        "email": "cs.princeton.edu;cs.princeton.edu;cs.princeton.edu",
        "github": "https://github.com/princeton-nlp/metric-wsd",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Princeton University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.princeton.edu",
        "aff_unique_abbr": "Princeton",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.411",
        "title": "Nutri-bullets Hybrid: Consensual Multi-document Summarization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We present a method for generating comparative summaries that highlight similarities and contradictions in input documents. The key challenge in creating such summaries is the lack of large parallel training data required for training typical summarization systems. To this end, we introduce a hybrid generation approach inspired by traditional concept-to-text systems. To enable accurate comparison between different sources, the model first learns to extract pertinent relations from input documents. The content planning component uses deterministic operators to aggregate these relations after identifying a subset for inclusion into a summary. The surface realization component lexicalizes this information using a text-infilling language model. By separately modeling content selection and realization, we can effectively train them with limited annotations. We implemented and tested the model in the domain of nutrition and health \u2013 rife with inconsistencies. Compared to conventional methods, our framework leads to more faithful, relevant and aggregation-sensitive summarization \u2013 while being equally fluent.",
        "author": "Darsh Shah; Lili Yu; Tao Lei; Regina Barzilay",
        "authorids": "/d/darsh-shah/; /l/lili-yu/; /t/tao-lei/; /r/regina-barzilay/",
        "bibtex": "@inproceedings{shah-etal-2021-nutri,\n    title = \"Nutri-bullets Hybrid: Consensual Multi-document Summarization\",\n    author = \"Shah, Darsh  and\n      Yu, Lili  and\n      Lei, Tao  and\n      Barzilay, Regina\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.411/\",\n    doi = \"10.18653/v1/2021.naacl-main.411\",\n    pages = \"5213--5222\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.411.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.411/",
        "pdf_size": 1265110,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15577914156680070786&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Computer Science and Artificial Intelligence Lab, Massachusetts Institute of Technology; ASAPP, Inc.; ASAPP, Inc.; Computer Science and Artificial Intelligence Lab, Massachusetts Institute of Technology",
        "aff_domain": "csail.mit.edu;asapp.com;asapp.com;csail.mit.edu",
        "email": "csail.mit.edu;asapp.com;asapp.com;csail.mit.edu",
        "github": "https://github.com/darsh10/Nutribullets",
        "project": "https://www.healthline.com;https://foodforbreastcancer.com",
        "author_num": 4,
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "Massachusetts Institute of Technology;ASAPP",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Lab;Inc.",
        "aff_unique_url": "https://www.csail.mit.edu;https://www.asapp.com",
        "aff_unique_abbr": "MIT;ASAPP",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.419",
        "title": "OCID-Ref: A 3D Robotic Dataset With Embodied Language For Clutter Scene Grounding",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "To effectively apply robots in working environments and assist humans, it is essential to develop and evaluate how visual grounding (VG) can affect machine performance on occluded objects. However, current VG works are limited in working environments, such as offices and warehouses, where objects are usually occluded due to space utilization issues. In our work, we propose a novel OCID-Ref dataset featuring a referring expression segmentation task with referring expressions of occluded objects. OCID-Ref consists of 305,694 referring expressions from 2,300 scenes with providing RGB image and point cloud inputs. To resolve challenging occlusion issues, we argue that it\u2019s crucial to take advantage of both 2D and 3D signals to resolve challenging occlusion issues. Our experimental results demonstrate the effectiveness of aggregating 2D and 3D signals but referring to occluded objects still remains challenging for the modern visual grounding systems. OCID-Ref is publicly available at https://github.com/lluma/OCID-Ref",
        "author": "Ke-Jyun Wang; Yun-Hsuan Liu; Hung-Ting Su; Jen-Wei Wang; Yu-Siang Wang; Winston Hsu; Wen-Chin Chen",
        "authorids": "/k/ke-jyun-wang/; /y/yun-hsuan-liu/; /h/hung-ting-su/; /j/jen-wei-wang/; /y/yu-siang-wang/; /w/winston-hsu/; /w/wen-chin-chen/",
        "bibtex": "@inproceedings{wang-etal-2021-ocid,\n    title = \"{OCID}-Ref: A 3{D} Robotic Dataset With Embodied Language For Clutter Scene Grounding\",\n    author = \"Wang, Ke-Jyun  and\n      Liu, Yun-Hsuan  and\n      Su, Hung-Ting  and\n      Wang, Jen-Wei  and\n      Wang, Yu-Siang  and\n      Hsu, Winston  and\n      Chen, Wen-Chin\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.419/\",\n    doi = \"10.18653/v1/2021.naacl-main.419\",\n    pages = \"5333--5338\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.419.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.419/",
        "pdf_size": 12488528,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2849546578488843435&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7
    },
    {
        "id": "2021.naacl-main.256",
        "title": "Ol\u00e1, Bonjour, Salve! XFORMAL: A Benchmark for Multilingual Formality Style Transfer",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We take the first step towards multilingual style transfer by creating and releasing XFORMAL, a benchmark of multiple formal reformulations of informal text in Brazilian Portuguese, French, and Italian. Results on XFORMAL suggest that state-of-the-art style transfer approaches perform close to simple baselines, indicating that style transfer is even more challenging when moving multilingual.",
        "author": "Eleftheria Briakou; Di Lu; Ke Zhang; Joel Tetreault",
        "authorids": "/e/eleftheria-briakou/; /d/di-lu/; /k/ke-zhang/; /j/joel-tetreault/",
        "bibtex": "@inproceedings{briakou-etal-2021-ola,\n    title = \"Ol{\\'a}, Bonjour, Salve! {XFORMAL}: A Benchmark for Multilingual Formality Style Transfer\",\n    author = \"Briakou, Eleftheria  and\n      Lu, Di  and\n      Zhang, Ke  and\n      Tetreault, Joel\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.256/\",\n    doi = \"10.18653/v1/2021.naacl-main.256\",\n    pages = \"3199--3216\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.256.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.256/",
        "pdf_size": 2272882,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1898793871020228434&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "University of Maryland; Dataminr, Inc.; Dataminr, Inc.; Dataminr, Inc.",
        "aff_domain": "cs.umd.edu;dataminr.com;dataminr.com;dataminr.com",
        "email": "cs.umd.edu;dataminr.com;dataminr.com;dataminr.com",
        "github": "https://github.com/Elbria/xformal",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "University of Maryland;Dataminr",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www/umd.edu;https://www.dataminr.com",
        "aff_unique_abbr": "UMD;Dataminr",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.72",
        "title": "On Attention Redundancy: A Comprehensive Study",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Multi-layer multi-head self-attention mechanism is widely applied in modern neural language models. Attention redundancy has been observed among attention heads but has not been deeply studied in the literature. Using BERT-base model as an example, this paper provides a comprehensive study on attention redundancy which is helpful for model interpretation and model compression. We analyze the attention redundancy with Five-Ws and How. (What) We define and focus the study on redundancy matrices generated from pre-trained and fine-tuned BERT-base model for GLUE datasets. (How) We use both token-based and sentence-based distance functions to measure the redundancy. (Where) Clear and similar redundancy patterns (cluster structure) are observed among attention heads. (When) Redundancy patterns are similar in both pre-training and fine-tuning phases. (Who) We discover that redundancy patterns are task-agnostic. Similar redundancy patterns even exist for randomly generated token sequences. (\u201cWhy\u201d) We also evaluate influences of the pre-training dropout ratios on attention redundancy. Based on the phase-independent and task-agnostic attention redundancy patterns, we propose a simple zero-shot pruning method as a case study. Experiments on fine-tuning GLUE tasks verify its effectiveness. The comprehensive analyses on attention redundancy make model understanding and zero-shot model pruning promising.",
        "author": "Yuchen Bian; Jiaji Huang; Xingyu Cai; Jiahong Yuan; Kenneth Church",
        "authorids": "/y/yuchen-bian/; /j/jiaji-huang/; /x/xingyu-cai/; /j/jiahong-yuan/; /k/kenneth-church/",
        "bibtex": "@inproceedings{bian-etal-2021-attention,\n    title = \"On Attention Redundancy: A Comprehensive Study\",\n    author = \"Bian, Yuchen  and\n      Huang, Jiaji  and\n      Cai, Xingyu  and\n      Yuan, Jiahong  and\n      Church, Kenneth\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.72/\",\n    doi = \"10.18653/v1/2021.naacl-main.72\",\n    pages = \"930--945\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.72.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.72/",
        "pdf_size": 8860686,
        "gs_citation": 58,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7212377947600798520&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Baidu Research, Sunnyvale, CA, USA; Baidu Research, Sunnyvale, CA, USA; Baidu Research, Sunnyvale, CA, USA; Baidu Research, Sunnyvale, CA, USA; Baidu Research, Sunnyvale, CA, USA",
        "aff_domain": "baidu.com;baidu.com;baidu.com;baidu.com;baidu.com",
        "email": "baidu.com;baidu.com;baidu.com;baidu.com;baidu.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Baidu Research",
        "aff_unique_dep": "Research",
        "aff_unique_url": "https://research.baidu.com",
        "aff_unique_abbr": "Baidu Res.",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Sunnyvale",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.354",
        "title": "On Biasing Transformer Attention Towards Monotonicity",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Many sequence-to-sequence tasks in natural language processing are roughly monotonic in the alignment between source and target sequence, and previous work has facilitated or enforced learning of monotonic attention behavior via specialized attention functions or pretraining. In this work, we introduce a monotonicity loss function that is compatible with standard attention mechanisms and test it on several sequence-to-sequence tasks: grapheme-to-phoneme conversion, morphological inflection, transliteration, and dialect normalization. Experiments show that we can achieve largely monotonic behavior. Performance is mixed, with larger gains on top of RNN baselines. General monotonicity does not benefit transformer multihead attention, however, we see isolated improvements when only a subset of heads is biased towards monotonic behavior.",
        "author": "Annette Rios; Chantal Amrhein; No\u00ebmi Aepli; Rico Sennrich",
        "authorids": "/a/annette-rios-gonzales/; /c/chantal-amrhein/; /n/noemi-aepli/; /r/rico-sennrich/",
        "bibtex": "@inproceedings{rios-etal-2021-biasing,\n    title = \"On Biasing Transformer Attention Towards Monotonicity\",\n    author = {Rios, Annette  and\n      Amrhein, Chantal  and\n      Aepli, No{\\\"e}mi  and\n      Sennrich, Rico},\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.354/\",\n    doi = \"10.18653/v1/2021.naacl-main.354\",\n    pages = \"4474--4488\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.354.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.354/",
        "pdf_size": 649741,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8678489492211660243&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computational Linguistics, University of Zurich; Department of Computational Linguistics, University of Zurich; Department of Computational Linguistics, University of Zurich; Department of Computational Linguistics, University of Zurich + School of Informatics, University of Edinburgh",
        "aff_domain": "cl.uzh.ch;cl.uzh.ch;cl.uzh.ch;cl.uzh.ch",
        "email": "cl.uzh.ch;cl.uzh.ch;cl.uzh.ch;cl.uzh.ch",
        "github": "https://github.com/ZurichNLP/monotonicity_loss",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0+1",
        "aff_unique_norm": "University of Zurich;University of Edinburgh",
        "aff_unique_dep": "Department of Computational Linguistics;School of Informatics",
        "aff_unique_url": "https://www.unizh.ch;https://www.ed.ac.uk",
        "aff_unique_abbr": "UZH;Edinburgh",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Edinburgh",
        "aff_country_unique_index": "0;0;0;0+1",
        "aff_country_unique": "Switzerland;United Kingdom"
    },
    {
        "id": "2021.naacl-main.337",
        "title": "On Learning Text Style Transfer with Direct Rewards",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In most cases, the lack of parallel corpora makes it impossible to directly train supervised models for the text style transfer task. In this paper, we explore training algorithms that instead optimize reward functions that explicitly consider different aspects of the style-transferred outputs. In particular, we leverage semantic similarity metrics originally used for fine-tuning neural machine translation models to explicitly assess the preservation of content between system outputs and input texts. We also investigate the potential weaknesses of the existing automatic metrics and propose efficient strategies of using these metrics for training. The experimental results show that our model provides significant gains in both automatic and human evaluation over strong baselines, indicating the effectiveness of our proposed methods and training strategies.",
        "author": "Yixin Liu; Graham Neubig; John Wieting",
        "authorids": "/y/yixin-liu/; /g/graham-neubig/; /j/john-wieting/",
        "bibtex": "@inproceedings{liu-etal-2021-learning,\n    title = \"On Learning Text Style Transfer with Direct Rewards\",\n    author = \"Liu, Yixin  and\n      Neubig, Graham  and\n      Wieting, John\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.337/\",\n    doi = \"10.18653/v1/2021.naacl-main.337\",\n    pages = \"4262--4273\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.337.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.337/",
        "pdf_size": 386837,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13847357478789539794&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "github": "https://github.com/yixinL7/Direct-Style-Transfer",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.296",
        "title": "On Transferability of Bias Mitigation Effects in Language Model Fine-Tuning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Fine-tuned language models have been shown to exhibit biases against protected groups in a host of modeling tasks such as text classification and coreference resolution. Previous works focus on detecting these biases, reducing bias in data representations, and using auxiliary training objectives to mitigate bias during fine-tuning. Although these techniques achieve bias reduction for the task and domain at hand, the effects of bias mitigation may not directly transfer to new tasks, requiring additional data collection and customized annotation of sensitive attributes, and re-evaluation of appropriate fairness metrics. We explore the feasibility and benefits of upstream bias mitigation (UBM) for reducing bias on downstream tasks, by first applying bias mitigation to an upstream model through fine-tuning and subsequently using it for downstream fine-tuning. We find, in extensive experiments across hate speech detection, toxicity detection and coreference resolution tasks over various bias factors, that the effects of UBM are indeed transferable to new downstream tasks or domains via fine-tuning, creating less biased downstream models than directly fine-tuning on the downstream task or transferring from a vanilla upstream model. Though challenges remain, we show that UBM promises more efficient and accessible bias mitigation in LM fine-tuning.",
        "author": "Xisen Jin; Francesco Barbieri; Brendan Kennedy; Aida Mostafazadeh Davani; Leonardo Neves; Xiang Ren",
        "authorids": "/x/xisen-jin/; /f/francesco-barbieri/; /b/brendan-kennedy/; /a/aida-mostafazadeh-davani/; /l/leonardo-neves/; /x/xiang-ren/",
        "bibtex": "@inproceedings{jin-etal-2021-transferability,\n    title = \"On Transferability of Bias Mitigation Effects in Language Model Fine-Tuning\",\n    author = \"Jin, Xisen  and\n      Barbieri, Francesco  and\n      Kennedy, Brendan  and\n      Mostafazadeh Davani, Aida  and\n      Neves, Leonardo  and\n      Ren, Xiang\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.296/\",\n    doi = \"10.18653/v1/2021.naacl-main.296\",\n    pages = \"3770--3783\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.296.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.296/",
        "pdf_size": 872022,
        "gs_citation": 81,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14172792303596711619&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "University of Southern California+Snap Inc.; Snap Inc.; University of Southern California+Snap Inc.; University of Southern California+Snap Inc.; Snap Inc.; University of Southern California+Snap Inc.",
        "aff_domain": "usc.edu;snap.com;usc.edu;usc.edu;snap.com;usc.edu",
        "email": "usc.edu;snap.com;usc.edu;usc.edu;snap.com;usc.edu",
        "github": "https://github.com/INK-USC/Upstream-Bias-Mitigation",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;1;0+1;0+1;1;0+1",
        "aff_unique_norm": "University of Southern California;Snap Inc.",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.usc.edu;https://www.snapinc.com",
        "aff_unique_abbr": "USC;Snap",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0+0;0;0+0;0+0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.432",
        "title": "On Unifying Misinformation Detection",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In this paper, we introduce UnifiedM2, a general-purpose misinformation model that jointly models multiple domains of misinformation with a single, unified setup. The model is trained to handle four tasks: detecting news bias, clickbait, fake news, and verifying rumors. By grouping these tasks together, UnifiedM2 learns a richer representation of misinformation, which leads to state-of-the-art or comparable performance across all tasks. Furthermore, we demonstrate that UnifiedM2\u2019s learned representation is helpful for few-shot learning of unseen misinformation tasks/datasets and the model\u2019s generalizability to unseen events.",
        "author": "Nayeon Lee; Belinda Z. Li; Sinong Wang; Pascale Fung; Hao Ma; Wen-tau Yih; Madian Khabsa",
        "authorids": "/n/nayeon-lee/; /b/belinda-z-li/; /s/sinong-wang/; /p/pascale-fung/; /h/hao-ma/; /w/wen-tau-yih/; /m/madian-khabsa/",
        "bibtex": "@inproceedings{lee-etal-2021-unifying,\n    title = \"On Unifying Misinformation Detection\",\n    author = \"Lee, Nayeon  and\n      Li, Belinda Z.  and\n      Wang, Sinong  and\n      Fung, Pascale  and\n      Ma, Hao  and\n      Yih, Wen-tau  and\n      Khabsa, Madian\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.432/\",\n    doi = \"10.18653/v1/2021.naacl-main.432\",\n    pages = \"5479--5485\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.432.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.432/",
        "pdf_size": 362776,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10052021554141205631&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Hong Kong University of Science and Technology; MIT CSAIL + Facebook AI; Facebook AI; Hong Kong University of Science and Technology; Facebook AI; Facebook AI; Facebook AI",
        "aff_domain": "connect.ust.hk; ;fb.com; ; ; ;",
        "email": "connect.ust.hk; ;fb.com; ; ; ;",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1+2;2;0;2;2;2",
        "aff_unique_norm": "Hong Kong University of Science and Technology;Massachusetts Institute of Technology;Facebook",
        "aff_unique_dep": ";Computer Science and Artificial Intelligence Laboratory;Facebook AI",
        "aff_unique_url": "https://www.ust.hk;https://www.csail.mit.edu;https://www.facebook.com",
        "aff_unique_abbr": "HKUST;MIT CSAIL;Facebook AI",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Hong Kong SAR;Cambridge;",
        "aff_country_unique_index": "0;1+1;1;0;1;1;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2021.naacl-main.50",
        "title": "On learning and representing social meaning in NLP: a sociolinguistic perspective",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The field of NLP has made substantial progress in building meaning representations. However, an important aspect of linguistic meaning, social meaning, has been largely overlooked. We introduce the concept of social meaning to NLP and discuss how insights from sociolinguistics can inform work on representation learning in NLP. We also identify key challenges for this new line of research.",
        "author": "Dong Nguyen; Laura Rosseel; Jack Grieve",
        "authorids": "/d/dong-nguyen/; /l/laura-rosseel/; /j/jack-grieve/",
        "bibtex": "@inproceedings{nguyen-etal-2021-learning,\n    title = \"On learning and representing social meaning in {NLP}: a sociolinguistic perspective\",\n    author = \"Nguyen, Dong  and\n      Rosseel, Laura  and\n      Grieve, Jack\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.50/\",\n    doi = \"10.18653/v1/2021.naacl-main.50\",\n    pages = \"603--612\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.50.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.50/",
        "pdf_size": 272015,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7420097065214850412&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Utrecht University; Vrije Universiteit Brussel; University of Birmingham",
        "aff_domain": "uu.nl;vub.be;bham.ac.uk",
        "email": "uu.nl;vub.be;bham.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Utrecht University;Vrije Universiteit Brussel;University of Birmingham",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.uu.nl;https://www.vub.be;https://www.birmingham.ac.uk",
        "aff_unique_abbr": "UU;VUB;Birmingham",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Brussels",
        "aff_country_unique_index": "0;1;2",
        "aff_country_unique": "Netherlands;Belgium;United Kingdom"
    },
    {
        "id": "2021.naacl-main.213",
        "title": "On the Embeddings of Variables in Recurrent Neural Networks for Source Code",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Source code processing heavily relies on the methods widely used in natural language processing (NLP), but involves specifics that need to be taken into account to achieve higher quality. An example of this specificity is that the semantics of a variable is defined not only by its name but also by the contexts in which the variable occurs. In this work, we develop dynamic embeddings, a recurrent mechanism that adjusts the learned semantics of the variable when it obtains more information about the variable\u2019s role in the program. We show that using the proposed dynamic embeddings significantly improves the performance of the recurrent neural network, in code completion and bug fixing tasks.",
        "author": "Nadezhda Chirkova",
        "authorids": "/n/nadezhda-chirkova/",
        "bibtex": "@inproceedings{chirkova-2021-embeddings,\n    title = \"On the Embeddings of Variables in Recurrent Neural Networks for Source Code\",\n    author = \"Chirkova, Nadezhda\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.213/\",\n    doi = \"10.18653/v1/2021.naacl-main.213\",\n    pages = \"2679--2689\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.213.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.213/",
        "pdf_size": 2267461,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=780189661337362105&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 3,
        "aff": "HSE University*",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "aff_unique_index": "0",
        "aff_unique_norm": "Higher School of Economics",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.hse.ru",
        "aff_unique_abbr": "HSE",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Russia"
    },
    {
        "id": "2021.naacl-main.299",
        "title": "On the Impact of Random Seeds on the Fairness of Clinical Classifiers",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent work has shown that fine-tuning large networks is surprisingly sensitive to changes in random seed(s). We explore the implications of this phenomenon for model fairness across demographic groups in clinical prediction tasks over electronic health records (EHR) in MIMIC-III \u2014\u2014 the standard dataset in clinical NLP research. Apparent subgroup performance varies substantially for seeds that yield similar overall performance, although there is no evidence of a trade-off between overall and subgroup performance. However, we also find that the small sample sizes inherent to looking at intersections of minority groups and somewhat rare conditions limit our ability to accurately estimate disparities. Further, we find that jointly optimizing for high overall performance and low disparities does not yield statistically significant improvements. Our results suggest that fairness work using MIMIC-III should carefully account for variations in apparent differences that may arise from stochasticity and small sample sizes.",
        "author": "Silvio Amir; Jan-Willem van de Meent; Byron Wallace",
        "authorids": "/s/silvio-amir/; /j/jan-willem-van-de-meent/; /b/byron-c-wallace/",
        "bibtex": "@inproceedings{amir-etal-2021-impact,\n    title = \"On the Impact of Random Seeds on the Fairness of Clinical Classifiers\",\n    author = \"Amir, Silvio  and\n      van de Meent, Jan-Willem  and\n      Wallace, Byron\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.299/\",\n    doi = \"10.18653/v1/2021.naacl-main.299\",\n    pages = \"3808--3823\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.299.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.299/",
        "pdf_size": 4759841,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4341211208273010454&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Northeastern University; Northeastern University; Northeastern University",
        "aff_domain": "northeastern.edu;northeastern.edu;northeastern.edu",
        "email": "northeastern.edu;northeastern.edu;northeastern.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Northeastern University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.northeastern.edu",
        "aff_unique_abbr": "NEU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.404",
        "title": "On the Inductive Bias of Masked Language Modeling: From Statistical to Syntactic Dependencies",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We study how masking and predicting tokens in an unsupervised fashion can give rise to linguistic structures and downstream performance gains. Recent theories have suggested that pretrained language models acquire useful inductive biases through masks that implicitly act as cloze reductions for downstream tasks. While appealing, we show that the success of the random masking strategy used in practice cannot be explained by such cloze-like masks alone. We construct cloze-like masks using task-specific lexicons for three different classification datasets and show that the majority of pretrained performance gains come from generic masks that are not associated with the lexicon. To explain the empirical success of these generic masks, we demonstrate a correspondence between the Masked Language Model (MLM) objective and existing methods for learning statistical dependencies in graphical models. Using this, we derive a method for extracting these learned statistical dependencies in MLMs and show that these dependencies encode useful inductive biases in the form of syntactic structures. In an unsupervised parsing evaluation, simply forming a minimum spanning tree on the implied statistical dependence structure outperforms a classic method for unsupervised parsing (58.74 vs. 55.91 UUAS).",
        "author": "Tianyi Zhang; Tatsunori B. Hashimoto",
        "authorids": "/t/tianyi-zhang/; /t/tatsunori-b-hashimoto/",
        "bibtex": "@inproceedings{zhang-hashimoto-2021-inductive,\n    title = \"On the Inductive Bias of Masked Language Modeling: From Statistical to Syntactic Dependencies\",\n    author = \"Zhang, Tianyi  and\n      Hashimoto, Tatsunori B.\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.404/\",\n    doi = \"10.18653/v1/2021.naacl-main.404\",\n    pages = \"5131--5146\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.404.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.404/",
        "pdf_size": 884274,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2052344588730405444&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Computer Science Department, Stanford University; Computer Science Department, Stanford University",
        "aff_domain": "cs.stanford.edu;stanford.edu",
        "email": "cs.stanford.edu;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.101",
        "title": "On the Transferability of Minimal Prediction Preserving Inputs in Question Answering",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent work (Feng et al., 2018) establishes the presence of short, uninterpretable input fragments that yield high confidence and accuracy in neural models. We refer to these as Minimal Prediction Preserving Inputs (MPPIs). In the context of question answering, we investigate competing hypotheses for the existence of MPPIs, including poor posterior calibration of neural models, lack of pretraining, and \u201cdataset bias\u201d (where a model learns to attend to spurious, non-generalizable cues in the training data). We discover a perplexing invariance of MPPIs to random training seed, model architecture, pretraining, and training domain. MPPIs demonstrate remarkable transferability across domains achieving significantly higher performance than comparably short queries. Additionally, penalizing over-confidence on MPPIs fails to improve either generalization or adversarial robustness. These results suggest the interpretability of MPPIs is insufficient to characterize generalization capacity of these models. We hope this focused investigation encourages more systematic analysis of model behavior outside of the human interpretable distribution of examples.",
        "author": "Shayne Longpre; Yi Lu; Chris DuBois",
        "authorids": "/s/shayne-longpre/; /y/yi-lu/; /c/chris-dubois/",
        "bibtex": "@inproceedings{longpre-etal-2021-transferability,\n    title = \"On the Transferability of Minimal Prediction Preserving Inputs in Question Answering\",\n    author = \"Longpre, Shayne  and\n      Lu, Yi  and\n      DuBois, Chris\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.101/\",\n    doi = \"10.18653/v1/2021.naacl-main.101\",\n    pages = \"1288--1300\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.101.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.101/",
        "pdf_size": 660291,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10704256922120830899&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Apple Inc.; Apple Inc.; Apple Inc.",
        "aff_domain": "apple.com;apple.com;apple.com",
        "email": "apple.com;apple.com;apple.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Apple Inc.",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.apple.com",
        "aff_unique_abbr": "Apple",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.406",
        "title": "On the Transformer Growth for Progressive BERT Training",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "As the excessive pre-training cost arouses the need to improve efficiency, considerable efforts have been made to train BERT progressively\u2013start from an inferior but low-cost model and gradually increase the computational complexity. Our objective is to help advance the understanding of such Transformer growth and discover principles that guide progressive training. First, we find that similar to network architecture selection, Transformer growth also favors compound scaling. Specifically, while existing methods only conduct network growth in a single dimension, we observe that it is beneficial to use compound growth operators and balance multiple dimensions (e.g., depth, width, and input length of the model). Moreover, we explore alternative growth operators in each dimension via controlled comparison to give practical guidance for operator selection. In light of our analyses, the proposed method CompoundGrow speeds up BERT pre-training by 73.6% and 82.2% for the base and large models respectively while achieving comparable performances.",
        "author": "Xiaotao Gu; Liyuan Liu; Hongkun Yu; Jing Li; Chen Chen; Jiawei Han",
        "authorids": "/x/xiaotao-gu/; /l/liyuan-liu/; /h/hongkun-yu/; /j/jing-li/; /c/chen-chen/; /j/jiawei-han/",
        "bibtex": "@inproceedings{gu-etal-2021-transformer,\n    title = \"On the Transformer Growth for Progressive {BERT} Training\",\n    author = \"Gu, Xiaotao  and\n      Liu, Liyuan  and\n      Yu, Hongkun  and\n      Li, Jing  and\n      Chen, Chen  and\n      Han, Jiawei\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.406/\",\n    doi = \"10.18653/v1/2021.naacl-main.406\",\n    pages = \"5174--5180\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.406.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.406/",
        "pdf_size": 374685,
        "gs_citation": 65,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13150714856825196048&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; Google Research + University of Illinois at Urbana-Champaign; Google Research + University of Illinois at Urbana-Champaign; Google Research + University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign",
        "aff_domain": "illinois.edu;illinois.edu;google.com;google.com;google.com;illinois.edu",
        "email": "illinois.edu;illinois.edu;google.com;google.com;google.com;illinois.edu",
        "github": "https://github.com/google-research/google-research/tree/master/grow_bert",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;1+0;1+0;1+0;0",
        "aff_unique_norm": "University of Illinois at Urbana-Champaign;Google",
        "aff_unique_dep": ";Google Research",
        "aff_unique_url": "https://illinois.edu;https://research.google",
        "aff_unique_abbr": "UIUC;Google Research",
        "aff_campus_unique_index": "0;0;1+0;1+0;1+0;0",
        "aff_campus_unique": "Urbana-Champaign;Mountain View",
        "aff_country_unique_index": "0;0;0+0;0+0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.359",
        "title": "On the Use of Context for Predicting Citation Worthiness of Sentences in Scholarly Articles",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In this paper, we study the importance of context in predicting the citation worthiness of sentences in scholarly articles. We formulate this problem as a sequence labeling task solved using a hierarchical BiLSTM model. We contribute a new benchmark dataset containing over two million sentences and their corresponding labels. We preserve the sentence order in this dataset and perform document-level train/test splits, which importantly allows incorporating contextual information in the modeling process. We evaluate the proposed approach on three benchmark datasets. Our results quantify the benefits of using context and contextual embeddings for citation worthiness. Lastly, through error analysis, we provide insights into cases where context plays an essential role in predicting citation worthiness.",
        "author": "Rakesh Gosangi; Ravneet Arora; Mohsen Gheisarieha; Debanjan Mahata; Haimin Zhang",
        "authorids": "/r/rakesh-gosangi/; /r/ravneet-arora/; /m/mohsen-gheisarieha/; /d/debanjan-mahata/; /h/haimin-zhang/",
        "bibtex": "@inproceedings{gosangi-etal-2021-use,\n    title = \"On the Use of Context for Predicting Citation Worthiness of Sentences in Scholarly Articles\",\n    author = \"Gosangi, Rakesh  and\n      Arora, Ravneet  and\n      Gheisarieha, Mohsen  and\n      Mahata, Debanjan  and\n      Zhang, Haimin\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.359/\",\n    doi = \"10.18653/v1/2021.naacl-main.359\",\n    pages = \"4539--4545\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.359.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.359/",
        "pdf_size": 254252,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6845205744053831863&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Bloomberg, New York; Bloomberg, New York; Bloomberg, New York; Bloomberg, New York; Bloomberg, New York",
        "aff_domain": "bloomberg.net;bloomberg.net;bloomberg.net;bloomberg.net;gmail.com",
        "email": "bloomberg.net;bloomberg.net;bloomberg.net;bloomberg.net;gmail.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Bloomberg",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.bloomberg.com",
        "aff_unique_abbr": "Bloomberg",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "New York",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-industry.30",
        "title": "OodGAN: Generative Adversarial Network for Out-of-Domain Data Generation",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Detecting an Out-of-Domain (OOD) utterance is crucial for a robust dialog system. Most dialog systems are trained on a pool of annotated OOD data to achieve this goal. However, collecting the annotated OOD data for a given domain is an expensive process. To mitigate this issue, previous works have proposed generative adversarial networks (GAN) based models to generate OOD data for a given domain automatically. However, these proposed models do not work directly with the text. They work with the text\u2019s latent space instead, enforcing these models to include components responsible for encoding text into latent space and decoding it back, such as auto-encoder. These components increase the model complexity, making it difficult to train. We propose OodGAN, a sequential generative adversarial network (SeqGAN) based model for OOD data generation. Our proposed model works directly on the text and hence eliminates the need to include an auto-encoder. OOD data generated using OodGAN model outperforms state-of-the-art in OOD detection metrics for ROSTD (67% relative improvement in FPR 0.95) and OSQ datasets (28% relative improvement in FPR 0.95)",
        "author": "Petr Marek; Vishal Ishwar Naik; Anuj Goyal; Vincent Auvray",
        "authorids": "/p/petr-marek/; /v/vishal-ishwar-naik/; /a/anuj-goyal/; /v/vincent-auvray/",
        "bibtex": "@inproceedings{marek-etal-2021-oodgan,\n    title = \"{O}od{GAN}: Generative Adversarial Network for Out-of-Domain Data Generation\",\n    author = \"Marek, Petr  and\n      Naik, Vishal Ishwar  and\n      Goyal, Anuj  and\n      Auvray, Vincent\",\n    editor = \"Kim, Young-bum  and\n      Li, Yunyao  and\n      Rambow, Owen\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-industry.30/\",\n    doi = \"10.18653/v1/2021.naacl-industry.30\",\n    pages = \"238--245\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-industry.30.pdf",
        "site": "https://aclanthology.org/2021.naacl-industry.30/",
        "pdf_size": 1515311,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9257926688858981567&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 6,
        "aff": "Czech Technical University in Prague; Amazon Alexa AI; Amazon Alexa AI; Amazon Alexa AI",
        "aff_domain": "fel.cvut.cz;amazon.com;amazon.de;amazon.com",
        "email": "fel.cvut.cz;amazon.com;amazon.de;amazon.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "Czech Technical University;Amazon",
        "aff_unique_dep": ";Alexa AI",
        "aff_unique_url": "https://www.ctu.cz;https://www.amazon.com",
        "aff_unique_abbr": "CTU;Amazon",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Prague;",
        "aff_country_unique_index": "0;1;1;1",
        "aff_country_unique": "Czech Republic;United States"
    },
    {
        "id": "2021.naacl-main.43",
        "title": "Open Domain Question Answering over Tables via Dense Retrieval",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent advances in open-domain QA have led to strong models based on dense retrieval, but only focused on retrieving textual passages. In this work, we tackle open-domain QA over tables for the first time, and show that retrieval can be improved by a retriever designed to handle tabular context. We present an effective pre-training procedure for our retriever and improve retrieval quality with mined hard negatives. As relevant datasets are missing, we extract a subset of Natural Questions (Kwiatkowski et al., 2019) into a Table QA dataset. We find that our retriever improves retrieval results from 72.0 to 81.1 recall@10 and end-to-end QA results from 33.8 to 37.7 exact match, over a BERT based retriever.",
        "author": "Jonathan Herzig; Thomas M\u00fcller; Syrine Krichene; Julian Eisenschlos",
        "authorids": "/j/jonathan-herzig/; /t/thomas-mueller/; /s/syrine-krichene/; /j/julian-eisenschlos/",
        "bibtex": "@inproceedings{herzig-etal-2021-open,\n    title = \"Open Domain Question Answering over Tables via Dense Retrieval\",\n    author = {Herzig, Jonathan  and\n      M{\\\"u}ller, Thomas  and\n      Krichene, Syrine  and\n      Eisenschlos, Julian},\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.43/\",\n    doi = \"10.18653/v1/2021.naacl-main.43\",\n    pages = \"512--519\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.43.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.43/",
        "pdf_size": 352050,
        "gs_citation": 113,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8089372667306935171&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "School of Computer Science, Tel-Aviv University; Google Research; Google Research; Google Research",
        "aff_domain": "cs.tau.ac.il;google.com;google.com;google.com",
        "email": "cs.tau.ac.il;google.com;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "Tel-Aviv University;Google",
        "aff_unique_dep": "School of Computer Science;Google Research",
        "aff_unique_url": "https://www.tau.ac.il;https://research.google",
        "aff_unique_abbr": "TAU;Google Research",
        "aff_campus_unique_index": "0;1;1;1",
        "aff_campus_unique": "Tel-Aviv;Mountain View",
        "aff_country_unique_index": "0;1;1;1",
        "aff_country_unique": "Israel;United States"
    },
    {
        "id": "2021.naacl-main.452",
        "title": "Open Hierarchical Relation Extraction",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Open relation extraction (OpenRE) aims to extract novel relation types from open-domain corpora, which plays an important role in completing the relation schemes of knowledge bases (KBs). Most OpenRE methods cast different relation types in isolation without considering their hierarchical dependency. We argue that OpenRE is inherently in close connection with relation hierarchies. To establish the bidirectional connections between OpenRE and relation hierarchy, we propose the task of open hierarchical relation extraction and present a novel OHRE framework for the task. We propose a dynamic hierarchical triplet objective and hierarchical curriculum training paradigm, to effectively integrate hierarchy information into relation representations for better novel relation extraction. We also present a top-down hierarchy expansion algorithm to add the extracted relations into existing hierarchies with reasonable interpretability. Comprehensive experiments show that OHRE outperforms state-of-the-art models by a large margin on both relation clustering and hierarchy expansion.",
        "author": "Kai Zhang; Yuan Yao; Ruobing Xie; Xu Han; Zhiyuan Liu; Fen Lin; Leyu Lin; Maosong Sun",
        "authorids": "/k/kai-zhang/; /y/yuan-yao/; /r/ruobing-xie/; /x/xu-han/; /z/zhiyuan-liu/; /f/fen-lin/; /l/leyu-lin/; /m/maosong-sun/",
        "bibtex": "@inproceedings{zhang-etal-2021-open,\n    title = \"Open Hierarchical Relation Extraction\",\n    author = \"Zhang, Kai  and\n      Yao, Yuan  and\n      Xie, Ruobing  and\n      Han, Xu  and\n      Liu, Zhiyuan  and\n      Lin, Fen  and\n      Lin, Leyu  and\n      Sun, Maosong\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.452/\",\n    doi = \"10.18653/v1/2021.naacl-main.452\",\n    pages = \"5682--5693\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.452.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.452/",
        "pdf_size": 1535508,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6479753215618598549&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science and Technology, Institute for Artificial Intelligence, Tsinghua University, Beijing, China+Beijing National Research Center for Information Science and Technology, China; Department of Computer Science and Technology, Institute for Artificial Intelligence, Tsinghua University, Beijing, China+Beijing National Research Center for Information Science and Technology, China; WeChat Search Application Department, Tencent, China; Department of Computer Science and Technology, Institute for Artificial Intelligence, Tsinghua University, Beijing, China+Beijing National Research Center for Information Science and Technology, China; Department of Computer Science and Technology, Institute for Artificial Intelligence, Tsinghua University, Beijing, China+Beijing National Research Center for Information Science and Technology, China; WeChat Search Application Department, Tencent, China; WeChat Search Application Department, Tencent, China; Department of Computer Science and Technology, Institute for Artificial Intelligence, Tsinghua University, Beijing, China+Beijing National Research Center for Information Science and Technology, China",
        "aff_domain": "gmail.com;163.com; ; ;tsinghua.edu.cn; ; ; ",
        "email": "gmail.com;163.com; ; ;tsinghua.edu.cn; ; ; ",
        "github": "https://github.com/thunlp/OHRE",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0+1;0+1;2;0+1;0+1;2;2;0+1",
        "aff_unique_norm": "Tsinghua University;Beijing National Research Center for Information Science and Technology;Tencent",
        "aff_unique_dep": "Department of Computer Science and Technology, Institute for Artificial Intelligence;;WeChat Search Application Department",
        "aff_unique_url": "https://www.tsinghua.edu.cn;;https://www.tencent.com",
        "aff_unique_abbr": "Tsinghua;;Tencent",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0+0;0+0;0;0+0;0+0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2021.naacl-main.44",
        "title": "Open-Domain Question Answering Goes Conversational via Question Rewriting",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We introduce a new dataset for Question Rewriting in Conversational Context (QReCC), which contains 14K conversations with 80K question-answer pairs. The task in QReCC is to find answers to conversational questions within a collection of 10M web pages (split into 54M passages). Answers to questions in the same conversation may be distributed across several web pages. QReCC provides annotations that allow us to train and evaluate individual subtasks of question rewriting, passage retrieval and reading comprehension required for the end-to-end conversational question answering (QA) task. We report the effectiveness of a strong baseline approach that combines the state-of-the-art model for question rewriting, and competitive models for open-domain QA. Our results set the first baseline for the QReCC dataset with F1 of 19.10, compared to the human upper bound of 75.45, indicating the difficulty of the setup and a large room for improvement.",
        "author": "Raviteja Anantha; Svitlana Vakulenko; Zhucheng Tu; Shayne Longpre; Stephen Pulman; Srinivas Chappidi",
        "authorids": "/r/raviteja-anantha/; /s/svitlana-vakulenko/; /z/zhucheng-tu/; /s/shayne-longpre/; /s/stephen-pulman/; /s/srinivas-chappidi/",
        "bibtex": "@inproceedings{anantha-etal-2021-open,\n    title = \"Open-Domain Question Answering Goes Conversational via Question Rewriting\",\n    author = \"Anantha, Raviteja  and\n      Vakulenko, Svitlana  and\n      Tu, Zhucheng  and\n      Longpre, Shayne  and\n      Pulman, Stephen  and\n      Chappidi, Srinivas\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.44/\",\n    doi = \"10.18653/v1/2021.naacl-main.44\",\n    pages = \"520--534\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.44.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.44/",
        "pdf_size": 2885642,
        "gs_citation": 190,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6985253037363315802&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "https://github.com/apple/ml-qrecc",
        "project": "",
        "author_num": 6
    },
    {
        "id": "2021.naacl-industry.3",
        "title": "Optimizing NLU Reranking Using Entity Resolution Signals in Multi-domain Dialog Systems",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "In dialog systems, the Natural Language Understanding (NLU) component typically makes the interpretation decision (including domain, intent and slots) for an utterance before the mentioned entities are resolved. This may result in intent classification and slot tagging errors. In this work, we propose to leverage Entity Resolution (ER) features in NLU reranking and introduce a novel loss term based on ER signals to better learn model weights in the reranking framework. In addition, for a multi-domain dialog scenario, we propose a score distribution matching method to ensure scores generated by the NLU reranking models for different domains are properly calibrated. In offline experiments, we demonstrate our proposed approach significantly outperforms the baseline model on both single-domain and cross-domain evaluations.",
        "author": "Tong Wang; Jiangning Chen; Mohsen Malmir; Shuyan Dong; Xin He; Han Wang; Chengwei Su; Yue Liu; Yang Liu",
        "authorids": "/t/tong-wang/; /j/jiangning-chen/; /m/mohsen-malmir/; /s/shuyan-dong/; /x/xin-he/; /h/han-wang/; /c/chengwei-su/; /y/yue-liu/; /y/yang-liu-icsi/",
        "bibtex": "@inproceedings{wang-etal-2021-optimizing,\n    title = \"Optimizing {NLU} Reranking Using Entity Resolution Signals in Multi-domain Dialog Systems\",\n    author = \"Wang, Tong  and\n      Chen, Jiangning  and\n      Malmir, Mohsen  and\n      Dong, Shuyan  and\n      He, Xin  and\n      Wang, Han  and\n      Su, Chengwei  and\n      Liu, Yue  and\n      Liu, Yang\",\n    editor = \"Kim, Young-bum  and\n      Li, Yunyao  and\n      Rambow, Owen\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-industry.3/\",\n    doi = \"10.18653/v1/2021.naacl-industry.3\",\n    pages = \"19--25\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-industry.3.pdf",
        "site": "https://aclanthology.org/2021.naacl-industry.3/",
        "pdf_size": 261950,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18304409621330053484&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Amazon Alexa; Amazon Alexa; Amazon Alexa; Amazon Alexa; Amazon Alexa; Amazon Alexa; Amazon Alexa; Amazon Alexa; Amazon Alexa",
        "aff_domain": "amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com",
        "email": "amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com",
        "github": "",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Amazon",
        "aff_unique_dep": "Amazon Alexa",
        "aff_unique_url": "https://www.amazon.com/alexa",
        "aff_unique_abbr": "Amazon Alexa",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.233",
        "title": "Outside Computation with Superior Functions",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We show that a general algorithm for efficient computation of outside values under the minimum of superior functions framework proposed by Knuth (1977) would yield a sub-exponential time algorithm for SAT, violating the Strong Exponential Time Hypothesis (SETH).",
        "author": "Parker Riley; Daniel Gildea",
        "authorids": "/p/parker-riley/; /d/daniel-gildea/",
        "bibtex": "@inproceedings{riley-gildea-2021-outside,\n    title = \"Outside Computation with Superior Functions\",\n    author = \"Riley, Parker  and\n      Gildea, Daniel\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.233/\",\n    doi = \"10.18653/v1/2021.naacl-main.233\",\n    pages = \"2936--2940\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.233.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.233/",
        "pdf_size": 285760,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:LZyH_V7CWhUJ:scholar.google.com/&scioq=Outside+Computation+with+Superior+Functions&hl=en&as_sdt=0,5",
        "gs_version_total": 4,
        "aff": "Department of Computer Science, University of Rochester; Department of Computer Science, University of Rochester",
        "aff_domain": "; ",
        "email": "; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Rochester",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.rochester.edu",
        "aff_unique_abbr": "U of R",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.117",
        "title": "PCFGs Can Do Better: Inducing Probabilistic Context-Free Grammars with Many Symbols",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Probabilistic context-free grammars (PCFGs) with neural parameterization have been shown to be effective in unsupervised phrase-structure grammar induction. However, due to the cubic computational complexity of PCFG representation and parsing, previous approaches cannot scale up to a relatively large number of (nonterminal and preterminal) symbols. In this work, we present a new parameterization form of PCFGs based on tensor decomposition, which has at most quadratic computational complexity in the symbol number and therefore allows us to use a much larger number of symbols. We further use neural parameterization for the new form to improve unsupervised parsing performance. We evaluate our model across ten languages and empirically demonstrate the effectiveness of using more symbols.",
        "author": "Songlin Yang; Yanpeng Zhao; Kewei Tu",
        "authorids": "/s/songlin-yang/; /y/yanpeng-zhao/; /k/kewei-tu/",
        "bibtex": "@inproceedings{yang-etal-2021-pcfgs,\n    title = \"{PCFG}s Can Do Better: Inducing Probabilistic Context-Free Grammars with Many Symbols\",\n    author = \"Yang, Songlin  and\n      Zhao, Yanpeng  and\n      Tu, Kewei\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.117/\",\n    doi = \"10.18653/v1/2021.naacl-main.117\",\n    pages = \"1487--1498\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.117.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.117/",
        "pdf_size": 526078,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15446039236619564717&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "School of Information Science and Technology, ShanghaiTech University + Shanghai Engineering Research Center of Intelligent Vision and Imaging + Shanghai Institute of Microsystem and Information Technology, Chinese Academy of Sciences + University of Chinese Academy of Sciences; ILCC, University of Edinburgh; School of Information Science and Technology, ShanghaiTech University + Shanghai Engineering Research Center of Intelligent Vision and Imaging + Shanghai Institute of Microsystem and Information Technology, Chinese Academy of Sciences + University of Chinese Academy of Sciences",
        "aff_domain": "shanghaitech.edu.cn;gmail.com;shanghaitech.edu.cn",
        "email": "shanghaitech.edu.cn;gmail.com;shanghaitech.edu.cn",
        "github": "https://github.com/sustcsonglin/TN-PCFG",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1+2+3;4;0+1+2+3",
        "aff_unique_norm": "ShanghaiTech University;Shanghai Engineering Research Center of Intelligent Vision and Imaging;Shanghai Institute of Microsystem and Information Technology;University of Chinese Academy of Sciences;University of Edinburgh",
        "aff_unique_dep": "School of Information Science and Technology;;;;ILCC",
        "aff_unique_url": "https://www.shanghaitech.edu.cn;;http://www.sim.cas.cn;http://www.ucas.ac.cn;https://www.ed.ac.uk",
        "aff_unique_abbr": "ShanghaiTech;;SIM;UCAS;Edinburgh",
        "aff_campus_unique_index": "0+0;2;0+0",
        "aff_campus_unique": "Shanghai;;Edinburgh",
        "aff_country_unique_index": "0+0+0+0;1;0+0+0+0",
        "aff_country_unique": "China;United Kingdom"
    },
    {
        "id": "2021.naacl-main.22",
        "title": "Paragraph-level Rationale Extraction through Regularization: A case study on European Court of Human Rights Cases",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Interpretability or explainability is an emerging research field in NLP. From a user-centric point of view, the goal is to build models that provide proper justification for their decisions, similar to those of humans, by requiring the models to satisfy additional constraints. To this end, we introduce a new application on legal text where, contrary to mainstream literature targeting word-level rationales, we conceive rationales as selected paragraphs in multi-paragraph structured court cases. We also release a new dataset comprising European Court of Human Rights cases, including annotations for paragraph-level rationales. We use this dataset to study the effect of already proposed rationale constraints, i.e., sparsity, continuity, and comprehensiveness, formulated as regularizers. Our findings indicate that some of these constraints are not beneficial in paragraph-level rationale extraction, while others need re-formulation to better handle the multi-label nature of the task we consider. We also introduce a new constraint, singularity, which further improves the quality of rationales, even compared with noisy rationale supervision. Experimental results indicate that the newly introduced task is very challenging and there is a large scope for further research.",
        "author": "Ilias Chalkidis; Manos Fergadiotis; Dimitrios Tsarapatsanis; Nikolaos Aletras; Ion Androutsopoulos; Prodromos Malakasiotis",
        "authorids": "/i/ilias-chalkidis/; /m/manos-fergadiotis/; /d/dimitrios-tsarapatsanis/; /n/nikolaos-aletras/; /i/ion-androutsopoulos/; /p/prodromos-malakasiotis/",
        "bibtex": "@inproceedings{chalkidis-etal-2021-paragraph,\n    title = \"Paragraph-level Rationale Extraction through Regularization: A case study on {E}uropean Court of Human Rights Cases\",\n    author = \"Chalkidis, Ilias  and\n      Fergadiotis, Manos  and\n      Tsarapatsanis, Dimitrios  and\n      Aletras, Nikolaos  and\n      Androutsopoulos, Ion  and\n      Malakasiotis, Prodromos\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.22/\",\n    doi = \"10.18653/v1/2021.naacl-main.22\",\n    pages = \"226--241\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.22.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.22/",
        "pdf_size": 2575192,
        "gs_citation": 126,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15203238936373276659&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "EY AI Centre of Excellence in Document Intelligence, NCSR \u201cDemokritos\u201d + Department of Informatics, Athens University of Economics and Business; EY AI Centre of Excellence in Document Intelligence, NCSR \u201cDemokritos\u201d + Department of Informatics, Athens University of Economics and Business; Law School, University of York; Computer Science Department, University of Shef\ufb01eld; Department of Informatics, Athens University of Economics and Business + EY AI Centre of Excellence in Document Intelligence, NCSR \u201cDemokritos\u201d; Department of Informatics, Athens University of Economics and Business + EY AI Centre of Excellence in Document Intelligence, NCSR \u201cDemokritos\u201d",
        "aff_domain": "aueb.gr; ; ; ;di.uoa.gr; ",
        "email": "aueb.gr; ; ; ;di.uoa.gr; ",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;0+1;2;3;1+0;1+0",
        "aff_unique_norm": "NCSR \u201cDemokritos\u201d;Athens University of Economics and Business;University of York;University of Sheffield",
        "aff_unique_dep": "EY AI Centre of Excellence in Document Intelligence;Department of Informatics;Law School;Computer Science Department",
        "aff_unique_url": "https://www.demokritos.gr;https://www.aueb.gr;https://www.york.ac.uk;https://www.sheffield.ac.uk",
        "aff_unique_abbr": ";AUEB;York;Sheffield",
        "aff_campus_unique_index": "1;1;1;1",
        "aff_campus_unique": ";Athens",
        "aff_country_unique_index": "0+0;0+0;1;1;0+0;0+0",
        "aff_country_unique": "Greece;United Kingdom"
    },
    {
        "id": "2021.naacl-main.395",
        "title": "Paragraph-level Simplification of Medical Texts",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We consider the problem of learning to simplify medical texts. This is important because most reliable, up-to-date information in biomedicine is dense with jargon and thus practically inaccessible to the lay audience. Furthermore, manual simplification does not scale to the rapidly growing body of biomedical literature, motivating the need for automated approaches. Unfortunately, there are no large-scale resources available for this task. In this work we introduce a new corpus of parallel texts in English comprising technical and lay summaries of all published evidence pertaining to different clinical topics. We then propose a new metric based on likelihood scores from a masked language model pretrained on scientific texts. We show that this automated measure better differentiates between technical and lay summaries than existing heuristics. We introduce and evaluate baseline encoder-decoder Transformer models for simplification and propose a novel augmentation to these in which we explicitly penalize the decoder for producing \u201cjargon\u201d terms; we find that this yields improvements over baselines in terms of readability.",
        "author": "Ashwin Devaraj; Iain Marshall; Byron Wallace; Junyi Jessy Li",
        "authorids": "/a/ashwin-devaraj/; /i/iain-marshall/; /b/byron-c-wallace/; /j/junyi-jessy-li/",
        "bibtex": "@inproceedings{devaraj-etal-2021-paragraph,\n    title = \"Paragraph-level Simplification of Medical Texts\",\n    author = \"Devaraj, Ashwin  and\n      Marshall, Iain  and\n      Wallace, Byron  and\n      Li, Junyi Jessy\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.395/\",\n    doi = \"10.18653/v1/2021.naacl-main.395\",\n    pages = \"4972--4984\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.395.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.395/",
        "pdf_size": 469718,
        "gs_citation": 129,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8651523092045693392&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "The University of Texas at Austin; Northeastern University; King\u2019s College London; The University of Texas at Austin",
        "aff_domain": "utexas.edu;northeastern.edu;kcl.ac.uk;austin.utexas.edu",
        "email": "utexas.edu;northeastern.edu;kcl.ac.uk;austin.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "University of Texas at Austin;Northeastern University;King's College London",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.utexas.edu;https://www.northeastern.edu;https://www.kcl.ac.uk",
        "aff_unique_abbr": "UT Austin;NEU;KCL",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Austin;",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "id": "2021.naacl-main.157",
        "title": "Personalized Response Generation via Generative Split Memory Network",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Despite the impressive successes of generation and dialogue systems, how to endow a text generation system with particular personality traits to deliver more personalized responses remains under-investigated. In this work, we look at how to generate personalized responses for questions on Reddit by utilizing personalized user profiles and posting histories. Specifically, we release an open-domain single-turn dialog dataset made up of 1.5M conversation pairs together with 300k profiles of users and related comments. We then propose a memory network to generate personalized responses in dialogue that utilizes a novel mechanism of splitting memories: one for user profile meta attributes and the other for user-generated information like comment histories. Experimental results show the quantitative and qualitative improvements of our simple split memory network model over the state-of-the-art response generation baselines.",
        "author": "Yuwei Wu; Xuezhe Ma; Diyi Yang",
        "authorids": "/y/yuwei-wu/; /x/xuezhe-ma/; /d/diyi-yang/",
        "bibtex": "@inproceedings{wu-etal-2021-personalized,\n    title = \"Personalized Response Generation via Generative Split Memory Network\",\n    author = \"Wu, Yuwei  and\n      Ma, Xuezhe  and\n      Yang, Diyi\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.157/\",\n    doi = \"10.18653/v1/2021.naacl-main.157\",\n    pages = \"1956--1970\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.157.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.157/",
        "pdf_size": 645707,
        "gs_citation": 70,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12400630416302589907&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Shanghai Jiao Tong University; University of Southern California; Georgia Institute of Technology",
        "aff_domain": "sjtu.edu.cn;usc.edu;gatech.edu",
        "email": "sjtu.edu.cn;usc.edu;gatech.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Shanghai Jiao Tong University;University of Southern California;Georgia Institute of Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.sjtu.edu.cn;https://www.usc.edu;https://www.gatech.edu",
        "aff_unique_abbr": "SJTU;USC;Georgia Tech",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Los Angeles",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2021.naacl-demos.1",
        "title": "PhoNLP: A joint multi-task learning model for Vietnamese part-of-speech tagging, named entity recognition and dependency parsing",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "We present the first multi-task learning model \u2013 named PhoNLP \u2013 for joint Vietnamese part-of-speech (POS) tagging, named entity recognition (NER) and dependency parsing. Experiments on Vietnamese benchmark datasets show that PhoNLP produces state-of-the-art results, outperforming a single-task learning approach that fine-tunes the pre-trained Vietnamese language model PhoBERT (Nguyen and Nguyen, 2020) for each task independently. We publicly release PhoNLP as an open-source toolkit under the Apache License 2.0. Although we specify PhoNLP for Vietnamese, our PhoNLP training and evaluation command scripts in fact can directly work for other languages that have a pre-trained BERT-based language model and gold annotated corpora available for the three tasks of POS tagging, NER and dependency parsing. We hope that PhoNLP can serve as a strong baseline and useful toolkit for future NLP research and applications to not only Vietnamese but also the other languages. Our PhoNLP is available at https://github.com/VinAIResearch/PhoNLP",
        "author": "Linh The Nguyen; Dat Quoc Nguyen",
        "authorids": "/l/linh-the-nguyen/; /d/dat-quoc-nguyen/",
        "bibtex": "@inproceedings{nguyen-nguyen-2021-phonlp,\n    title = \"{P}ho{NLP}: A joint multi-task learning model for {V}ietnamese part-of-speech tagging, named entity recognition and dependency parsing\",\n    author = \"Nguyen, Linh The  and\n      Nguyen, Dat Quoc\",\n    editor = \"Sil, Avi  and\n      Lin, Xi Victoria\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-demos.1/\",\n    doi = \"10.18653/v1/2021.naacl-demos.1\",\n    pages = \"1--7\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-demos.1.pdf",
        "site": "https://aclanthology.org/2021.naacl-demos.1/",
        "pdf_size": 536918,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4407619621831571234&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "VinAI Research, Hanoi, Vietnam; VinAI Research, Hanoi, Vietnam",
        "aff_domain": "vinai.io;vinai.io",
        "email": "vinai.io;vinai.io",
        "github": "https://github.com/VinAIResearch/PhoNLP",
        "project": "https://vlsp.org.vn/",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "VinAI Research",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.vin.ai",
        "aff_unique_abbr": "VinAI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hanoi",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Vietnam"
    },
    {
        "id": "2021.naacl-main.343",
        "title": "Plot-guided Adversarial Example Construction for Evaluating Open-domain Story Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "With the recent advances of open-domain story generation, the lack of reliable automatic evaluation metrics becomes an increasingly imperative issue that hinders the fast development of story generation. According to conducted researches in this regard, learnable evaluation metrics have promised more accurate assessments by having higher correlations with human judgments. A critical bottleneck of obtaining a reliable learnable evaluation metric is the lack of high-quality training data for classifiers to efficiently distinguish plausible and implausible machine-generated stories. Previous works relied on heuristically manipulated plausible examples to mimic possible system drawbacks such as repetition, contradiction, or irrelevant content in the text level, which can be unnatural and oversimplify the characteristics of implausible machine-generated stories. We propose to tackle these issues by generating a more comprehensive set of implausible stories using plots, which are structured representations of controllable factors used to generate stories. Since these plots are compact and structured, it is easier to manipulate them to generate text with targeted undesirable properties, while at the same time maintain the grammatical correctness and naturalness of the generated sentences. To improve the quality of generated implausible stories, we further apply the adversarial filtering procedure presented by (CITATION) to select a more nuanced set of implausible texts. Experiments show that the evaluation metrics trained on our generated data result in more reliable automatic assessments that correlate remarkably better with human judgments compared to the baselines.",
        "author": "Sarik Ghazarian; Zixi Liu; Akash S M; Ralph Weischedel; Aram Galstyan; Nanyun Peng",
        "authorids": "/s/sarik-ghazarian/; /z/zixi-liu/; /a/akash-s-m/; /r/ralph-weischedel/; /a/aram-galstyan/; /n/nanyun-peng/",
        "bibtex": "@inproceedings{ghazarian-etal-2021-plot,\n    title = \"Plot-guided Adversarial Example Construction for Evaluating Open-domain Story Generation\",\n    author = \"Ghazarian, Sarik  and\n      Liu, Zixi  and\n      S M, Akash  and\n      Weischedel, Ralph  and\n      Galstyan, Aram  and\n      Peng, Nanyun\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.343/\",\n    doi = \"10.18653/v1/2021.naacl-main.343\",\n    pages = \"4334--4344\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.343.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.343/",
        "pdf_size": 331109,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2879305956546375691&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "University of Southern California / Information Sciences Institute; University of Southern California / Information Sciences Institute; Indian Institute of Technology Roorkee; University of Southern California / Information Sciences Institute; University of Southern California / Information Sciences Institute; Computer Science Department of University of California, Los Angeles",
        "aff_domain": "isi.edu;isi.edu;ce.iitr.ac.in;isi.edu;isi.edu;cs.ucla.edu",
        "email": "isi.edu;isi.edu;ce.iitr.ac.in;isi.edu;isi.edu;cs.ucla.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;1;0;0;2",
        "aff_unique_norm": "University of Southern California;Indian Institute of Technology Roorkee;University of California, Los Angeles",
        "aff_unique_dep": "Information Sciences Institute;;Computer Science Department",
        "aff_unique_url": "https://www.usc.edu;https://www.iitr.ac.in;https://www.ucla.edu",
        "aff_unique_abbr": "USC;IIT Roorkee;UCLA",
        "aff_campus_unique_index": "0;0;1;0;0;0",
        "aff_campus_unique": "Los Angeles;Roorkee",
        "aff_country_unique_index": "0;0;1;0;0;0",
        "aff_country_unique": "United States;India"
    },
    {
        "id": "2021.naacl-main.85",
        "title": "Posterior Differential Regularization with f-divergence for Improving Model Robustness",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We address the problem of enhancing model robustness through regularization. Specifically, we focus on methods that regularize the model posterior difference between clean and noisy inputs. Theoretically, we provide a connection of two recent methods, Jacobian Regularization and Virtual Adversarial Training, under this framework. Additionally, we generalize the posterior differential regularization to the family of f-divergences and characterize the overall framework in terms of the Jacobian matrix. Empirically, we compare those regularizations and standard BERT training on a diverse set of tasks to provide a comprehensive profile of their effect on model generalization. For both fully supervised and semi-supervised settings, we show that regularizing the posterior difference with f-divergence can result in well-improved model robustness. In particular, with a proper f-divergence, a BERT-base model can achieve comparable generalization as its BERT-large counterpart for in-domain, adversarial and domain shift scenarios, indicating the great potential of the proposed framework for enhancing NLP model robustness.",
        "author": "Hao Cheng; Xiaodong Liu; Lis Pereira; Yaoliang Yu; Jianfeng Gao",
        "authorids": "/h/hao-cheng/; /x/xiaodong-liu/; /l/lis-pereira/; /y/yaoliang-yu/; /j/jianfeng-gao/",
        "bibtex": "@inproceedings{cheng-etal-2021-posterior,\n    title = \"Posterior Differential Regularization with f-divergence for Improving Model Robustness\",\n    author = \"Cheng, Hao  and\n      Liu, Xiaodong  and\n      Pereira, Lis  and\n      Yu, Yaoliang  and\n      Gao, Jianfeng\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.85/\",\n    doi = \"10.18653/v1/2021.naacl-main.85\",\n    pages = \"1078--1089\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.85.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.85/",
        "pdf_size": 415043,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=611522121742856181&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Microsoft Research; Microsoft Research; Ochanomizu University; University of Waterloo & Vector Institute; Microsoft Research",
        "aff_domain": "microsoft.com;microsoft.com;ocha.ac.jp;uwaterloo.ca;microsoft.com",
        "email": "microsoft.com;microsoft.com;ocha.ac.jp;uwaterloo.ca;microsoft.com",
        "github": "https://github.com/hao-cheng/f-divergence",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;2;0",
        "aff_unique_norm": "Microsoft Corporation;Ochanomizu University;University of Waterloo",
        "aff_unique_dep": "Microsoft Research;;",
        "aff_unique_url": "https://www.microsoft.com/en-us/research;https://www.ochanomizu-u.ac.jp;https://uwaterloo.ca",
        "aff_unique_abbr": "MSR;Ochanomizu U;UW",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;2;0",
        "aff_country_unique": "United States;Japan;Canada"
    },
    {
        "id": "2021.naacl-industry.16",
        "title": "Practical Transformer-based Multilingual Text Classification",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Transformer-based methods are appealing for multilingual text classification, but common research benchmarks like XNLI (Conneau et al., 2018) do not reflect the data availability and task variety of industry applications. We present an empirical comparison of transformer-based text classification models in a variety of practical monolingual and multilingual pretraining and fine-tuning settings. We evaluate these methods on two distinct tasks in five different languages. Departing from prior work, our results show that multilingual language models can outperform monolingual ones in some downstream tasks and target languages. We additionally show that practical modifications such as task- and domain-adaptive pretraining and data augmentation can improve classification performance without the need for additional labeled data.",
        "author": "Cindy Wang; Michele Banko",
        "authorids": "/c/cindy-wang/; /m/michele-banko/",
        "bibtex": "@inproceedings{wang-banko-2021-practical,\n    title = \"Practical Transformer-based Multilingual Text Classification\",\n    author = \"Wang, Cindy  and\n      Banko, Michele\",\n    editor = \"Kim, Young-bum  and\n      Li, Yunyao  and\n      Rambow, Owen\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-industry.16/\",\n    doi = \"10.18653/v1/2021.naacl-industry.16\",\n    pages = \"121--129\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-industry.16.pdf",
        "site": "https://aclanthology.org/2021.naacl-industry.16/",
        "pdf_size": 365378,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10172358010158559647&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Sentropy Technologies; Sentropy Technologies",
        "aff_domain": "sentropy.io;sentropy.io",
        "email": "sentropy.io;sentropy.io",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Sentropy Technologies",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.436",
        "title": "Pre-training with Meta Learning for Chinese Word Segmentation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent researches show that pre-trained models (PTMs) are beneficial to Chinese Word Segmentation (CWS). However, PTMs used in previous works usually adopt language modeling as pre-training tasks, lacking task-specific prior segmentation knowledge and ignoring the discrepancy between pre-training tasks and downstream CWS tasks. In this paper, we propose a CWS-specific pre-trained model MetaSeg, which employs a unified architecture and incorporates meta learning algorithm into a multi-criteria pre-training task. Empirical results show that MetaSeg could utilize common prior segmentation knowledge from different existing criteria and alleviate the discrepancy between pre-trained models and downstream CWS tasks. Besides, MetaSeg can achieve new state-of-the-art performance on twelve widely-used CWS datasets and significantly improve model performance in low-resource settings.",
        "author": "Zhen Ke; Liang Shi; Songtao Sun; Erli Meng; Bin Wang; Xipeng Qiu",
        "authorids": "/z/zhen-ke/; /l/liang-shi/; /s/songtao-sun/; /e/erli-meng/; /b/bin-wang/; /x/xipeng-qiu/",
        "bibtex": "@inproceedings{ke-etal-2021-pre,\n    title = \"Pre-training with Meta Learning for {C}hinese Word Segmentation\",\n    author = \"Ke, Zhen  and\n      Shi, Liang  and\n      Sun, Songtao  and\n      Meng, Erli  and\n      Wang, Bin  and\n      Qiu, Xipeng\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.436/\",\n    doi = \"10.18653/v1/2021.naacl-main.436\",\n    pages = \"5514--5523\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.436.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.436/",
        "pdf_size": 695578,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18407794363774263862&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Xiaomi AI Lab, Xiaomi Inc., Beijing, China; Xiaomi AI Lab, Xiaomi Inc., Beijing, China; Xiaomi AI Lab, Xiaomi Inc., Beijing, China; Xiaomi AI Lab, Xiaomi Inc., Beijing, China; Xiaomi AI Lab, Xiaomi Inc., Beijing, China; Shanghai Key Laboratory of Intelligent Information Processing, Fudan University",
        "aff_domain": "xiaomi.com;xiaomi.com;xiaomi.com;xiaomi.com;xiaomi.com;fudan.edu.cn",
        "email": "xiaomi.com;xiaomi.com;xiaomi.com;xiaomi.com;xiaomi.com;fudan.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;1",
        "aff_unique_norm": "Xiaomi Inc.;Fudan University",
        "aff_unique_dep": "Xiaomi AI Lab;Shanghai Key Laboratory of Intelligent Information Processing",
        "aff_unique_url": "https://www.xiaomi.com;https://www.fudan.edu.cn",
        "aff_unique_abbr": "Xiaomi;Fudan",
        "aff_campus_unique_index": "0;0;0;0;0;1",
        "aff_campus_unique": "Beijing;Shanghai",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2021.naacl-main.326",
        "title": "Predicting Discourse Trees from Transformer-based Neural Summarizers",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Previous work indicates that discourse information benefits summarization. In this paper, we explore whether this synergy between discourse and summarization is bidirectional, by inferring document-level discourse trees from pre-trained neural summarizers. In particular, we generate unlabeled RST-style discourse trees from the self-attention matrices of the transformer model. Experiments across models and datasets reveal that the summarizer learns both, dependency- and constituency-style discourse information, which is typically encoded in a single head, covering long- and short-distance discourse dependencies. Overall, the experimental results suggest that the learned discourse information is general and transferable inter-domain.",
        "author": "Wen Xiao; Patrick Huber; Giuseppe Carenini",
        "authorids": "/w/wen-xiao/; /p/patrick-huber/; /g/giuseppe-carenini/",
        "bibtex": "@inproceedings{xiao-etal-2021-predicting,\n    title = \"Predicting Discourse Trees from Transformer-based Neural Summarizers\",\n    author = \"Xiao, Wen  and\n      Huber, Patrick  and\n      Carenini, Giuseppe\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.326/\",\n    doi = \"10.18653/v1/2021.naacl-main.326\",\n    pages = \"4139--4152\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.326.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.326/",
        "pdf_size": 726880,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1203971202020840567&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science, University of British Columbia; Department of Computer Science, University of British Columbia; Department of Computer Science, University of British Columbia",
        "aff_domain": "cs.ubc.ca;cs.ubc.ca;cs.ubc.ca",
        "email": "cs.ubc.ca;cs.ubc.ca;cs.ubc.ca",
        "github": "https://github.com/Wendy-Xiao/summ_guided_disco_parser",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of British Columbia",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ubc.ca",
        "aff_unique_abbr": "UBC",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Vancouver",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2021.naacl-main.51",
        "title": "Preregistering NLP research",
        "track": "main",
        "status": "Long",
        "award": true,
        "abstract": "Preregistration refers to the practice of specifying what you are going to do, and what you expect to find in your study, before carrying out the study. This practice is increasingly common in medicine and psychology, but is rarely discussed in NLP. This paper discusses preregistration in more detail, explores how NLP researchers could preregister their work, and presents several preregistration questions for different kinds of studies. Finally, we argue in favour of registered reports, which could provide firmer grounds for slow science in NLP research. The goal of this paper is to elicit a discussion in the NLP community, which we hope to synthesise into a general NLP preregistration form in future research.",
        "author": "Emiel van Miltenburg; Chris van der Lee; Emiel Krahmer",
        "authorids": "/e/emiel-van-miltenburg/; /c/chris-van-der-lee/; /e/emiel-krahmer/",
        "bibtex": "@inproceedings{van-miltenburg-etal-2021-preregistering,\n    title = \"Preregistering {NLP} research\",\n    author = \"van Miltenburg, Emiel  and\n      van der Lee, Chris  and\n      Krahmer, Emiel\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.51/\",\n    doi = \"10.18653/v1/2021.naacl-main.51\",\n    pages = \"613--623\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.51.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.51/",
        "pdf_size": 242205,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5083585403511114274&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Tilburg Center for Cognition and Communication (TiCC), Tilburg University, Tilburg, The Netherlands; Tilburg Center for Cognition and Communication (TiCC), Tilburg University, Tilburg, The Netherlands; Tilburg Center for Cognition and Communication (TiCC), Tilburg University, Tilburg, The Netherlands",
        "aff_domain": "tilburguniversity.edu;tilburguniversity.edu;tilburguniversity.edu",
        "email": "tilburguniversity.edu;tilburguniversity.edu;tilburguniversity.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Tilburg University",
        "aff_unique_dep": "Tilburg Center for Cognition and Communication (TiCC)",
        "aff_unique_url": "https://www.tilburguniversity.edu",
        "aff_unique_abbr": "Tilburg U",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Tilburg",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Netherlands"
    },
    {
        "id": "2021.naacl-industry.5",
        "title": "Pretrain-Finetune Based Training of Task-Oriented Dialogue Systems in a Real-World Setting",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "One main challenge in building task-oriented dialogue systems is the limited amount of supervised training data available. In this work, we present a method for training retrieval-based dialogue systems using a small amount of high-quality, annotated data and a larger, unlabeled dataset. We show that pretraining using unlabeled data can bring better model performance with a 31% boost in Recall@1 compared with no pretraining. The proposed finetuning technique based on a small amount of high-quality, annotated data resulted in 26% offline and 33% online performance improvement in Recall@1 over the pretrained model. The model is deployed in an agent-support application and evaluated on live customer service contacts, providing additional insights into the real-world implications compared with most other publications in the domain often using asynchronous transcripts (e.g. Reddit data). The high performance of 74% Recall@1 shown in the customer service example demonstrates the effectiveness of this pretrain-finetune approach in dealing with the limited supervised data challenge.",
        "author": "Manisha Srivastava; Yichao Lu; Riley Peschon; Chenyang Li",
        "authorids": "/m/manisha-srivastava/; /y/yichao-lu/; /r/riley-peschon/; /c/chenyang-li/",
        "bibtex": "@inproceedings{srivastava-etal-2021-pretrain,\n    title = \"Pretrain-Finetune Based Training of Task-Oriented Dialogue Systems in a Real-World Setting\",\n    author = \"Srivastava, Manisha  and\n      Lu, Yichao  and\n      Peschon, Riley  and\n      Li, Chenyang\",\n    editor = \"Kim, Young-bum  and\n      Li, Yunyao  and\n      Rambow, Owen\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-industry.5/\",\n    doi = \"10.18653/v1/2021.naacl-industry.5\",\n    pages = \"34--40\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-industry.5.pdf",
        "site": "https://aclanthology.org/2021.naacl-industry.5/",
        "pdf_size": 255924,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1120938022502645482&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Amazon Inc., Seattle, USA; Amazon Inc., Seattle, USA; Amazon Inc., Seattle, USA; Amazon Inc., Seattle, USA",
        "aff_domain": "amazon;amazon;amazon;amazon",
        "email": "amazon;amazon;amazon;amazon",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Amazon",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.amazon.com",
        "aff_unique_abbr": "Amazon",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Seattle",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.298",
        "title": "Privacy Regularization: Joint Privacy-Utility Optimization in LanguageModels",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Neural language models are known to have a high capacity for memorization of training samples. This may have serious privacy im- plications when training models on user content such as email correspondence. Differential privacy (DP), a popular choice to train models with privacy guarantees, comes with significant costs in terms of utility degradation and disparate impact on subgroups of users. In this work, we introduce two privacy-preserving regularization methods for training language models that enable joint optimization of utility and privacy through (1) the use of a discriminator and (2) the inclusion of a novel triplet-loss term. We compare our methods with DP through extensive evaluation. We show the advantages of our regularizers with favorable utility-privacy trade-off, faster training with the ability to tap into existing optimization approaches, and ensuring uniform treatment of under-represented subgroups.",
        "author": "Fatemehsadat Mireshghallah; Huseyin Inan; Marcello Hasegawa; Victor R\u00fchle; Taylor Berg-Kirkpatrick; Robert Sim",
        "authorids": "/f/fatemehsadat-mireshghallah/; /h/huseyin-inan/; /m/marcello-hasegawa/; /v/victor-ruhle/; /t/taylor-berg-kirkpatrick/; /r/robert-sim/",
        "bibtex": "@inproceedings{mireshghallah-etal-2021-privacy,\n    title = \"Privacy Regularization: Joint Privacy-Utility Optimization in {L}anguage{M}odels\",\n    author = {Mireshghallah, Fatemehsadat  and\n      Inan, Huseyin  and\n      Hasegawa, Marcello  and\n      R{\\\"u}hle, Victor  and\n      Berg-Kirkpatrick, Taylor  and\n      Sim, Robert},\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.298/\",\n    doi = \"10.18653/v1/2021.naacl-main.298\",\n    pages = \"3799--3807\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.298.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.298/",
        "pdf_size": 519478,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=404933390258824392&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "University of California San Diego; Microsoft Corporation; Microsoft Research; Microsoft Corporation; University of California San Diego; Microsoft Research",
        "aff_domain": "ucsd.edu;ucsd.edu;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "email": "ucsd.edu;ucsd.edu;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;1;1;0;1",
        "aff_unique_norm": "University of California, San Diego;Microsoft Corporation",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://ucsd.edu;https://www.microsoft.com",
        "aff_unique_abbr": "UCSD;Microsoft",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "San Diego;",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.68",
        "title": "Probabilistic Box Embeddings for Uncertain Knowledge Graph Reasoning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Knowledge bases often consist of facts which are harvested from a variety of sources, many of which are noisy and some of which conflict, resulting in a level of uncertainty for each triple. Knowledge bases are also often incomplete, prompting the use of embedding methods to generalize from known facts, however, existing embedding methods only model triple-level uncertainty, and reasoning results lack global consistency. To address these shortcomings, we propose BEUrRE, a novel uncertain knowledge graph embedding method with calibrated probabilistic semantics. BEUrRE models each entity as a box (i.e. axis-aligned hyperrectangle) and relations between two entities as affine transforms on the head and tail entity boxes. The geometry of the boxes allows for efficient calculation of intersections and volumes, endowing the model with calibrated probabilistic semantics and facilitating the incorporation of relational constraints. Extensive experiments on two benchmark datasets show that BEUrRE consistently outperforms baselines on confidence prediction and fact ranking due to its probabilistic calibration and ability to capture high-order dependencies among facts.",
        "author": "Xuelu Chen; Michael Boratko; Muhao Chen; Shib Sankar Dasgupta; Xiang Lorraine Li; Andrew McCallum",
        "authorids": "/x/xuelu-chen/; /m/michael-boratko/; /m/muhao-chen/; /s/shib-sankar-dasgupta/; /x/xiang-lorraine-li/; /a/andrew-mccallum/",
        "bibtex": "@inproceedings{chen-etal-2021-probabilistic,\n    title = \"Probabilistic Box Embeddings for Uncertain Knowledge Graph Reasoning\",\n    author = \"Chen, Xuelu  and\n      Boratko, Michael  and\n      Chen, Muhao  and\n      Dasgupta, Shib Sankar  and\n      Li, Xiang Lorraine  and\n      McCallum, Andrew\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.68/\",\n    doi = \"10.18653/v1/2021.naacl-main.68\",\n    pages = \"882--893\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.68.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.68/",
        "pdf_size": 427899,
        "gs_citation": 57,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12666694446066594302&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, UCLA; College of Information and Computer Sciences, UMass Amherst; Department of Computer Science, USC + Information Sciences Institute, USC; College of Information and Computer Sciences, UMass Amherst; College of Information and Computer Sciences, UMass Amherst; College of Information and Computer Sciences, UMass Amherst",
        "aff_domain": "cs.ucla.edu;iesl.cs.umass.edu;usc.edu;cs.umass.edu;cs.umass.edu;cs.umass.edu",
        "email": "cs.ucla.edu;iesl.cs.umass.edu;usc.edu;cs.umass.edu;cs.umass.edu;cs.umass.edu",
        "github": "https://github.com/stasl0217/beurre",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;2+2;1;1;1",
        "aff_unique_norm": "University of California, Los Angeles;University of Massachusetts Amherst;University of Southern California",
        "aff_unique_dep": "Department of Computer Science;College of Information and Computer Sciences;Department of Computer Science",
        "aff_unique_url": "https://www.ucla.edu;https://www.cics.umass.edu;https://www.usc.edu",
        "aff_unique_abbr": "UCLA;UMass Amherst;USC",
        "aff_campus_unique_index": "0;1;0+2;1;1;1",
        "aff_campus_unique": "Los Angeles;Amherst;ISI",
        "aff_country_unique_index": "0;0;0+0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.422",
        "title": "Probing Contextual Language Models for Common Ground with Visual Representations",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The success of large-scale contextual language models has attracted great interest in probing what is encoded in their representations. In this work, we consider a new question: to what extent contextual representations of concrete nouns are aligned with corresponding visual representations? We design a probing model that evaluates how effective are text-only representations in distinguishing between matching and non-matching visual representations. Our findings show that language representations alone provide a strong signal for retrieving image patches from the correct object categories. Moreover, they are effective in retrieving specific instances of image patches; textual context plays an important role in this process. Visually grounded language models slightly outperform text-only language models in instance retrieval, but greatly under-perform humans. We hope our analyses inspire future research in understanding and improving the visual capabilities of language models.",
        "author": "Gabriel Ilharco; Rowan Zellers; Ali Farhadi; Hannaneh Hajishirzi",
        "authorids": "/g/gabriel-ilharco/; /r/rowan-zellers/; /a/ali-farhadi/; /h/hannaneh-hajishirzi/",
        "bibtex": "@inproceedings{ilharco-etal-2021-probing,\n    title = \"Probing Contextual Language Models for Common Ground with Visual Representations\",\n    author = \"Ilharco, Gabriel  and\n      Zellers, Rowan  and\n      Farhadi, Ali  and\n      Hajishirzi, Hannaneh\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.422/\",\n    doi = \"10.18653/v1/2021.naacl-main.422\",\n    pages = \"5367--5377\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.422.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.422/",
        "pdf_size": 3539066,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10789773452739620842&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Paul G. Allen School of Computer Science & Engineering, University of Washington; Paul G. Allen School of Computer Science & Engineering, University of Washington; Paul G. Allen School of Computer Science & Engineering, University of Washington; Paul G. Allen School of Computer Science & Engineering, University of Washington",
        "aff_domain": "cs.washington.edu;cs.washington.edu;cs.washington.edu;cs.washington.edu",
        "email": "cs.washington.edu;cs.washington.edu;cs.washington.edu;cs.washington.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Washington",
        "aff_unique_dep": "Paul G. Allen School of Computer Science & Engineering",
        "aff_unique_url": "https://www.cs.washington.edu",
        "aff_unique_abbr": "UW",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Seattle",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.7",
        "title": "Probing Word Translations in the Transformer and Trading Decoder for Encoder Layers",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Due to its effectiveness and performance, the Transformer translation model has attracted wide attention, most recently in terms of probing-based approaches. Previous work focuses on using or probing source linguistic features in the encoder. To date, the way word translation evolves in Transformer layers has not yet been investigated. Naively, one might assume that encoder layers capture source information while decoder layers translate. In this work, we show that this is not quite the case: translation already happens progressively in encoder layers and even in the input embeddings. More surprisingly, we find that some of the lower decoder layers do not actually do that much decoding. We show all of this in terms of a probing approach where we project representations of the layer analyzed to the final trained and frozen classifier level of the Transformer decoder to measure word translation accuracy. Our findings motivate and explain a Transformer configuration change: if translation already happens in the encoder layers, perhaps we can increase the number of encoder layers, while decreasing the number of decoder layers, boosting decoding speed, without loss in translation quality? Our experiments show that this is indeed the case: we can increase speed by up to a factor 2.3 with small gains in translation quality, while an 18-4 deep encoder configuration boosts translation quality by +1.42 BLEU (En-De) at a speed-up of 1.4.",
        "author": "Hongfei Xu; Josef van Genabith; Qiuhui Liu; Deyi Xiong",
        "authorids": "/h/hongfei-xu/; /j/josef-van-genabith/; /q/qiuhui-liu/; /d/deyi-xiong/",
        "bibtex": "@inproceedings{xu-etal-2021-probing,\n    title = \"Probing Word Translations in the Transformer and Trading Decoder for Encoder Layers\",\n    author = \"Xu, Hongfei  and\n      van Genabith, Josef  and\n      Liu, Qiuhui  and\n      Xiong, Deyi\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.7/\",\n    doi = \"10.18653/v1/2021.naacl-main.7\",\n    pages = \"74--85\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.7.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.7/",
        "pdf_size": 384753,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16089700448810774528&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Saarland University / Saarland, Germany+German Research Center for Arti\ufb01cial Intelligence / Saarland, Germany; Saarland University / Saarland, Germany+German Research Center for Arti\ufb01cial Intelligence / Saarland, Germany; China Mobile Online Services / Henan, China; Tianjin University / Tianjin, China",
        "aff_domain": "foxmail.com;dfki.de;foxmail.com;tju.edu.cn",
        "email": "foxmail.com;dfki.de;foxmail.com;tju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0+1;2;3",
        "aff_unique_norm": "Saarland University;German Research Center for Artificial Intelligence;China Mobile Online Services;Tianjin University",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.uni-saarland.de;https://www.dfki.de/;http://www.chinamobile.com/;http://www.tju.edu.cn",
        "aff_unique_abbr": "UdS;DFKI;CMOS;Tianjin U",
        "aff_campus_unique_index": ";;1",
        "aff_campus_unique": ";Tianjin",
        "aff_country_unique_index": "0+0;0+0;1;1",
        "aff_country_unique": "Germany;China"
    },
    {
        "id": "2021.naacl-main.327",
        "title": "Probing for Bridging Inference in Transformer Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We probe pre-trained transformer language models for bridging inference. We first investigate individual attention heads in BERT and observe that attention heads at higher layers prominently focus on bridging relations in-comparison with the lower and middle layers, also, few specific attention heads concentrate consistently on bridging. More importantly, we consider language models as a whole in our second approach where bridging anaphora resolution is formulated as a masked token prediction task (Of-Cloze test). Our formulation produces optimistic results without any fine-tuning, which indicates that pre-trained language models substantially capture bridging inference. Our further investigation shows that the distance between anaphor-antecedent and the context provided to language models play an important role in the inference.",
        "author": "Onkar Pandit; Yufang Hou",
        "authorids": "/o/onkar-arun-pandit/; /y/yufang-hou/",
        "bibtex": "@inproceedings{pandit-hou-2021-probing,\n    title = \"Probing for Bridging Inference in Transformer Language Models\",\n    author = \"Pandit, Onkar  and\n      Hou, Yufang\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.327/\",\n    doi = \"10.18653/v1/2021.naacl-main.327\",\n    pages = \"4153--4163\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.327.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.327/",
        "pdf_size": 419043,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10348798858323799846&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "University of Lille, INRIA Lille, CNRS, Centrale Lille, UMR 9189-CRIStAL, F-59000, Lille, France; IBM Research Europe, Dublin, Ireland",
        "aff_domain": "inria.fr;ie.ibm.com",
        "email": "inria.fr;ie.ibm.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Lille;IBM Research Europe",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.univ-lille.fr;https://www.ibm.com/research/europe",
        "aff_unique_abbr": "UoL;IBM",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Lille;Dublin",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "France;Ireland"
    },
    {
        "id": "2021.naacl-main.389",
        "title": "Profiling of Intertextuality in Latin Literature Using Word Embeddings",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Identifying intertextual relationships between authors is of central importance to the study of literature. We report an empirical analysis of intertextuality in classical Latin literature using word embedding models. To enable quantitative evaluation of intertextual search methods, we curate a new dataset of 945 known parallels drawn from traditional scholarship on Latin epic poetry. We train an optimized word2vec model on a large corpus of lemmatized Latin, which achieves state-of-the-art performance for synonym detection and outperforms a widely used lexical method for intertextual search. We then demonstrate that training embeddings on very small corpora can capture salient aspects of literary style and apply this approach to replicate a previous intertextual study of the Roman historian Livy, which relied on hand-crafted stylometric features. Our results advance the development of core computational resources for a major premodern language and highlight a productive avenue for cross-disciplinary collaboration between the study of literature and NLP.",
        "author": "Patrick J. Burns; James A. Brofos; Kyle Li; Pramit Chaudhuri; Joseph P. Dexter",
        "authorids": "/p/patrick-j-burns/; /j/james-a-brofos/; /k/kyle-li/; /p/pramit-chaudhuri/; /j/joseph-p-dexter/",
        "bibtex": "@inproceedings{burns-etal-2021-profiling,\n    title = \"Profiling of Intertextuality in {L}atin Literature Using Word Embeddings\",\n    author = \"Burns, Patrick J.  and\n      Brofos, James A.  and\n      Li, Kyle  and\n      Chaudhuri, Pramit  and\n      Dexter, Joseph P.\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.389/\",\n    doi = \"10.18653/v1/2021.naacl-main.389\",\n    pages = \"4900--4907\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.389.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.389/",
        "pdf_size": 959826,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1479944324367166670&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "https://github.com/QuantitativeCriticismLab/NAACL-HLT-2021-Latin-Intertextuality",
        "project": "",
        "author_num": 5
    },
    {
        "id": "2021.naacl-main.341",
        "title": "Progressive Generation of Long Text with Pretrained Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Large-scale language models (LMs) pretrained on massive corpora of text, such as GPT-2, are powerful open-domain text generators. However, as our systematic examination reveals, it is still challenging for such models to generate coherent long passages of text (e.g., 1000 tokens), especially when the models are fine-tuned to the target domain on a small corpus. Previous planning-then-generation methods also fall short of producing such long text in various domains. To overcome the limitations, we propose a simple but effective method of generating text in a progressive manner, inspired by generating images from low to high resolution. Our method first produces domain-specific content keywords and then progressively refines them into complete passages in multiple stages. The simple design allows our approach to take advantage of pretrained LMs at each stage and effectively adapt to any target domain given only a small set of examples. We conduct a comprehensive empirical study with a broad set of evaluation metrics, and show that our approach significantly improves upon the fine-tuned large LMs and various planning-then-generation methods in terms of quality and sample efficiency. Human evaluation also validates that our model generations are more coherent.",
        "author": "Bowen Tan; Zichao Yang; Maruan Al-Shedivat; Eric Xing; Zhiting Hu",
        "authorids": "/b/bowen-tan/; /z/zichao-yang/; /m/maruan-al-shedivat/; /e/eric-xing/; /z/zhiting-hu/",
        "bibtex": "@inproceedings{tan-etal-2021-progressive,\n    title = \"Progressive Generation of Long Text with Pretrained Language Models\",\n    author = \"Tan, Bowen  and\n      Yang, Zichao  and\n      Al-Shedivat, Maruan  and\n      Xing, Eric  and\n      Hu, Zhiting\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.341/\",\n    doi = \"10.18653/v1/2021.naacl-main.341\",\n    pages = \"4313--4324\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.341.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.341/",
        "pdf_size": 497063,
        "gs_citation": 103,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=392979307463202975&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Carnegie Mellon University+Petuum Inc.+MBZUAI; Carnegie Mellon University+Petuum Inc.+MBZUAI; Carnegie Mellon University+Petuum Inc.+MBZUAI; Carnegie Mellon University+Petuum Inc.+MBZUAI+UC San Diego; Carnegie Mellon University+UC San Diego",
        "aff_domain": "andrew.cmu.edu;andrew.cmu.edu;andrew.cmu.edu;andrew.cmu.edu;ucsd.edu",
        "email": "andrew.cmu.edu;andrew.cmu.edu;andrew.cmu.edu;andrew.cmu.edu;ucsd.edu",
        "github": "https://github.com/tanyuqian/progressive-generation",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1+2;0+1+2;0+1+2;0+1+2+3;0+3",
        "aff_unique_norm": "Carnegie Mellon University;Petuum Inc.;Mohamed Bin Zayed University of Artificial Intelligence;University of California, San Diego",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.cmu.edu;https://www.petuum.com;https://www.mbzuai.ac.ae;https://www.ucsd.edu",
        "aff_unique_abbr": "CMU;;MBZUAI;UCSD",
        "aff_campus_unique_index": ";;;1;1",
        "aff_campus_unique": ";San Diego",
        "aff_country_unique_index": "0+0+1;0+0+1;0+0+1;0+0+1+0;0+0",
        "aff_country_unique": "United States;United Arab Emirates"
    },
    {
        "id": "2021.naacl-industry.10",
        "title": "Proteno: Text Normalization with Limited Data for Fast Deployment in Text to Speech Systems",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Developing Text Normalization (TN) systems for Text-to-Speech (TTS) on new languages is hard. We propose a novel architecture to facilitate it for multiple languages while using data less than 3% of the size of the data used by the state of the art results on English. We treat TN as a sequence classification problem and propose a granular tokenization mechanism that enables the system to learn majority of the classes and their normalizations from the training data itself. This is further combined with minimal precoded linguistic knowledge for other classes. We publish the first results on TN for TTS in Spanish and Tamil and also demonstrate that the performance of the approach is comparable with the previous work done on English. All annotated datasets used for experimentation will be released.",
        "author": "Shubhi Tyagi; Antonio Bonafonte; Jaime Lorenzo-Trueba; Javier Latorre",
        "authorids": "/s/shubhi-tyagi/; /a/antonio-bonafonte/; /j/jaime-lorenzo-trueba/; /j/javier-latorre/",
        "bibtex": "@inproceedings{tyagi-etal-2021-proteno,\n    title = \"Proteno: Text Normalization with Limited Data for Fast Deployment in Text to Speech Systems\",\n    author = \"Tyagi, Shubhi  and\n      Bonafonte, Antonio  and\n      Lorenzo-Trueba, Jaime  and\n      Latorre, Javier\",\n    editor = \"Kim, Young-bum  and\n      Li, Yunyao  and\n      Rambow, Owen\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-industry.10/\",\n    doi = \"10.18653/v1/2021.naacl-industry.10\",\n    pages = \"72--79\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-industry.10.pdf",
        "site": "https://aclanthology.org/2021.naacl-industry.10/",
        "pdf_size": 223080,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16486898970176644015&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4
    },
    {
        "id": "2021.naacl-main.308",
        "title": "Pruning-then-Expanding Model for Domain Adaptation of Neural Machine Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Domain Adaptation is widely used in practical applications of neural machine translation, which aims to achieve good performance on both general domain and in-domain data. However, the existing methods for domain adaptation usually suffer from catastrophic forgetting, large domain divergence, and model explosion. To address these three problems, we propose a method of \u201cdivide and conquer\u201d which is based on the importance of neurons or parameters for the translation model. In this method, we first prune the model and only keep the important neurons or parameters, making them responsible for both general-domain and in-domain translation. Then we further train the pruned model supervised by the original whole model with knowledge distillation. Last we expand the model to the original size and fine-tune the added parameters for the in-domain translation. We conducted experiments on different language pairs and domains and the results show that our method can achieve significant improvements compared with several strong baselines.",
        "author": "Shuhao Gu; Yang Feng; Wanying Xie",
        "authorids": "/s/shuhao-gu/; /y/yang-feng/; /w/wanying-xie/",
        "bibtex": "@inproceedings{gu-etal-2021-pruning,\n    title = \"Pruning-then-Expanding Model for Domain Adaptation of Neural Machine Translation\",\n    author = \"Gu, Shuhao  and\n      Feng, Yang  and\n      Xie, Wanying\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.308/\",\n    doi = \"10.18653/v1/2021.naacl-main.308\",\n    pages = \"3942--3952\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.308.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.308/",
        "pdf_size": 1918488,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18122126597434996357&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences (ICT/CAS) + University of Chinese Academy of Sciences; Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences (ICT/CAS) + University of Chinese Academy of Sciences; Beijing Language and Culture University, China",
        "aff_domain": "ict.ac.cn;ict.ac.cn;gmail.com",
        "email": "ict.ac.cn;ict.ac.cn;gmail.com",
        "github": "https://github.com/ictnlp/PTE-NMT",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;0+1;2",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences;Beijing Language and Culture University",
        "aff_unique_dep": "Institute of Computing Technology;;",
        "aff_unique_url": "http://www.cas.cn;http://www.ucas.ac.cn;http://www.blcu.edu.cn",
        "aff_unique_abbr": "CAS;UCAS;BLCU",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2021.naacl-main.123",
        "title": "Put Chatbot into Its Interlocutor\u2019s Shoes: New Framework to Learn Chatbot Responding with Intention",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Most chatbot literature that focuses on improving the fluency and coherence of a chatbot, is dedicated to making chatbots more human-like. However, very little work delves into what really separates humans from chatbots \u2013 humans intrinsically understand the effect their responses have on the interlocutor and often respond with an intention such as proposing an optimistic view to make the interlocutor feel better. This paper proposes an innovative framework to train chatbots to possess human-like intentions. Our framework includes a guiding chatbot and an interlocutor model that plays the role of humans. The guiding chatbot is assigned an intention and learns to induce the interlocutor to reply with responses matching the intention, for example, long responses, joyful responses, responses with specific words, etc. We examined our framework using three experimental setups and evaluated the guiding chatbot with four different metrics to demonstrate flexibility and performance advantages. Additionally, we performed trials with human interlocutors to substantiate the guiding chatbot\u2019s effectiveness in influencing the responses of humans to a certain extent. Code will be made available to the public.",
        "author": "Hsuan Su; Jiun-Hao Jhan; Fan-yun Sun; Saurav Sahay; Hung-yi Lee",
        "authorids": "/h/hsuan-su/; /j/jiun-hao-jhan/; /f/fan-yun-sun/; /s/saurav-sahay/; /h/hung-yi-lee/",
        "bibtex": "@inproceedings{su-etal-2021-put,\n    title = \"Put Chatbot into Its Interlocutor{'}s Shoes: New Framework to Learn Chatbot Responding with Intention\",\n    author = \"Su, Hsuan  and\n      Jhan, Jiun-Hao  and\n      Sun, Fan-yun  and\n      Sahay, Saurav  and\n      Lee, Hung-yi\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.123/\",\n    doi = \"10.18653/v1/2021.naacl-main.123\",\n    pages = \"1559--1569\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.123.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.123/",
        "pdf_size": 1170315,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13716843834476599597&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 4,
        "aff": "National Taiwan University; National Taiwan University; Stanford University + National Taiwan University; Intel Labs + National Taiwan University; National Taiwan University",
        "aff_domain": "ntu.edu.tw;ntu.edu.tw;stanford.edu;intel.com;ntu.edu.tw",
        "email": "ntu.edu.tw;ntu.edu.tw;stanford.edu;intel.com;ntu.edu.tw",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1+0;2+0;0",
        "aff_unique_norm": "National Taiwan University;Stanford University;Intel Corporation",
        "aff_unique_dep": ";;Intel Labs",
        "aff_unique_url": "https://www.ntu.edu.tw;https://www.stanford.edu;https://www.intel.com",
        "aff_unique_abbr": "NTU;Stanford;Intel",
        "aff_campus_unique_index": "0;0;1+0;0;0",
        "aff_campus_unique": "Taiwan;Stanford;",
        "aff_country_unique_index": "0;0;1+0;1+0;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2021.naacl-main.45",
        "title": "QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The problem of answering questions using knowledge from pre-trained language models (LMs) and knowledge graphs (KGs) presents two challenges: given a QA context (question and answer choice), methods need to (i) identify relevant knowledge from large KGs, and (ii) perform joint reasoning over the QA context and KG. Here we propose a new model, QA-GNN, which addresses the above challenges through two key innovations: (i) relevance scoring, where we use LMs to estimate the importance of KG nodes relative to the given QA context, and (ii) joint reasoning, where we connect the QA context and KG to form a joint graph, and mutually update their representations through graph-based message passing. We evaluate QA-GNN on the CommonsenseQA and OpenBookQA datasets, and show its improvement over existing LM and LM+KG models, as well as its capability to perform interpretable and structured reasoning, e.g., correctly handling negation in questions.",
        "author": "Michihiro Yasunaga; Hongyu Ren; Antoine Bosselut; Percy Liang; Jure Leskovec",
        "authorids": "/m/michihiro-yasunaga/; /h/hongyu-ren/; /a/antoine-bosselut/; /p/percy-liang/; /j/jure-leskovec/",
        "bibtex": "@inproceedings{yasunaga-etal-2021-qa,\n    title = \"{QA}-{GNN}: Reasoning with Language Models and Knowledge Graphs for Question Answering\",\n    author = \"Yasunaga, Michihiro  and\n      Ren, Hongyu  and\n      Bosselut, Antoine  and\n      Liang, Percy  and\n      Leskovec, Jure\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.45/\",\n    doi = \"10.18653/v1/2021.naacl-main.45\",\n    pages = \"535--546\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.45.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.45/",
        "pdf_size": 1673861,
        "gs_citation": 689,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17250260885715357161&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5
    },
    {
        "id": "2021.naacl-main.472",
        "title": "QMSum: A New Benchmark for Query-based Multi-domain Meeting Summarization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Meetings are a key component of human collaboration. As increasing numbers of meetings are recorded and transcribed, meeting summaries have become essential to remind those who may or may not have attended the meetings about the key decisions made and the tasks to be completed. However, it is hard to create a single short summary that covers all the content of a long meeting involving multiple people and topics. In order to satisfy the needs of different types of users, we define a new query-based multi-domain meeting summarization task, where models have to select and summarize relevant spans of meetings in response to a query, and we introduce QMSum, a new benchmark for this task. QMSum consists of 1,808 query-summary pairs over 232 meetings in multiple domains. Besides, we investigate a locate-then-summarize method and evaluate a set of strong summarization baselines on the task. Experimental results and manual analysis reveal that QMSum presents significant challenges in long meeting summarization for future research. Dataset is available at https://github.com/Yale-LILY/QMSum.",
        "author": "Ming Zhong; Da Yin; Tao Yu; Ahmad Zaidi; Mutethia Mutuma; Rahul Jha; Ahmed Hassan Awadallah; Asli Celikyilmaz; Yang Liu; Xipeng Qiu; Dragomir Radev",
        "authorids": "/m/ming-zhong/; /d/da-yin/; /t/tao-yu/; /a/ahmad-zaidi/; /m/mutethia-mutuma/; /r/rahul-jha/; /a/ahmed-hassan/; /a/asli-celikyilmaz/; /y/yang-liu-edinburgh/; /x/xipeng-qiu/; /d/dragomir-radev/",
        "bibtex": "@inproceedings{zhong-etal-2021-qmsum,\n    title = \"{QMS}um: A New Benchmark for Query-based Multi-domain Meeting Summarization\",\n    author = \"Zhong, Ming  and\n      Yin, Da  and\n      Yu, Tao  and\n      Zaidi, Ahmad  and\n      Mutuma, Mutethia  and\n      Jha, Rahul  and\n      Awadallah, Ahmed Hassan  and\n      Celikyilmaz, Asli  and\n      Liu, Yang  and\n      Qiu, Xipeng  and\n      Radev, Dragomir\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.472/\",\n    doi = \"10.18653/v1/2021.naacl-main.472\",\n    pages = \"5905--5921\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.472.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.472/",
        "pdf_size": 708432,
        "gs_citation": 341,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10185019600887478050&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Fudan University; University of California, Los Angeles; Yale University; Yale University; Yale University; Microsoft Research; Microsoft Research; Microsoft Research; Microsoft Cognitive Services Research; Fudan University; Yale University",
        "aff_domain": "fudan.edu.cn;cs.ucla.edu;yale.edu;yale.edu; ; ; ; ; ; ;",
        "email": "fudan.edu.cn;cs.ucla.edu;yale.edu;yale.edu; ; ; ; ; ; ;",
        "github": "https://github.com/Yale-LILY/QMSum",
        "project": "",
        "author_num": 11,
        "aff_unique_index": "0;1;2;2;2;3;3;3;4;0;2",
        "aff_unique_norm": "Fudan University;University of California, Los Angeles;Yale University;Microsoft Corporation;Microsoft",
        "aff_unique_dep": ";;;Microsoft Research;Cognitive Services Research",
        "aff_unique_url": "https://www.fudan.edu.cn;https://www.ucla.edu;https://www.yale.edu;https://www.microsoft.com/en-us/research;https://www.microsoft.com",
        "aff_unique_abbr": "Fudan;UCLA;Yale;MSR;Microsoft",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Los Angeles",
        "aff_country_unique_index": "0;1;1;1;1;1;1;1;1;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2021.naacl-main.292",
        "title": "QuadrupletBERT: An Efficient Model For Embedding-Based Large-Scale Retrieval",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The embedding-based large-scale query-document retrieval problem is a hot topic in the information retrieval (IR) field. Considering that pre-trained language models like BERT have achieved great success in a wide variety of NLP tasks, we present a QuadrupletBERT model for effective and efficient retrieval in this paper. Unlike most existing BERT-style retrieval models, which only focus on the ranking phase in retrieval systems, our model makes considerable improvements to the retrieval phase and leverages the distances between simple negative and hard negative instances to obtaining better embeddings. Experimental results demonstrate that our QuadrupletBERT achieves state-of-the-art results in embedding-based large-scale retrieval tasks.",
        "author": "Peiyang Liu; Sen Wang; Xi Wang; Wei Ye; Shikun Zhang",
        "authorids": "/p/peiyang-liu/; /s/sen-wang/; /x/xi-wang/; /w/wei-ye/; /s/shikun-zhang/",
        "bibtex": "@inproceedings{liu-etal-2021-quadrupletbert,\n    title = \"{Q}uadruplet{BERT}: An Efficient Model For Embedding-Based Large-Scale Retrieval\",\n    author = \"Liu, Peiyang  and\n      Wang, Sen  and\n      Wang, Xi  and\n      Ye, Wei  and\n      Zhang, Shikun\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.292/\",\n    doi = \"10.18653/v1/2021.naacl-main.292\",\n    pages = \"3734--3739\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.292.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.292/",
        "pdf_size": 825264,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5776243998248807175&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "National Engineering Research Center for Software Engineering, Peking University, Beijing, China + School of Software and Microelectronics, Peking University, Beijing, China; PX Securities, Beijing, China; School of Software and Microelectronics, Peking University, Beijing, China; National Engineering Research Center for Software Engineering, Peking University, Beijing, China + School of Software and Microelectronics, Peking University, Beijing, China; National Engineering Research Center for Software Engineering, Peking University, Beijing, China",
        "aff_domain": "pku.edu.cn;yeah.net;pku.edu.cn;pku.edu.cn;pku.edu.cn",
        "email": "pku.edu.cn;yeah.net;pku.edu.cn;pku.edu.cn;pku.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+0;1;0;0+0;0",
        "aff_unique_norm": "Peking University;PX Securities",
        "aff_unique_dep": "National Engineering Research Center for Software Engineering;",
        "aff_unique_url": "http://www.pku.edu.cn;",
        "aff_unique_abbr": "PKU;",
        "aff_campus_unique_index": "0+0;0;0+0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0+0;0;0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2021.naacl-main.253",
        "title": "Quality Estimation for Image Captions Based on Large-scale Human Evaluations",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Automatic image captioning has improved significantly over the last few years, but the problem is far from being solved, with state of the art models still often producing low quality captions when used in the wild. In this paper, we focus on the task of Quality Estimation (QE) for image captions, which attempts to model the caption quality from a human perspective and *without* access to ground-truth references, so that it can be applied at prediction time to detect low-quality captions produced on *previously unseen images*. For this task, we develop a human evaluation process that collects coarse-grained caption annotations from crowdsourced users, which is then used to collect a large scale dataset spanning more than 600k caption quality ratings. We then carefully validate the quality of the collected ratings and establish baseline models for this new QE task. Finally, we further collect fine-grained caption quality annotations from trained raters, and use them to demonstrate that QE models trained over the coarse ratings can effectively detect and filter out low-quality image captions, thereby improving the user experience from captioning systems.",
        "author": "Tomer Levinboim; Ashish V. Thapliyal; Piyush Sharma; Radu Soricut",
        "authorids": "/t/tomer-levinboim/; /a/ashish-v-thapliyal/; /p/piyush-sharma/; /r/radu-soricut/",
        "bibtex": "@inproceedings{levinboim-etal-2021-quality,\n    title = \"Quality Estimation for Image Captions Based on Large-scale Human Evaluations\",\n    author = \"Levinboim, Tomer  and\n      Thapliyal, Ashish V.  and\n      Sharma, Piyush  and\n      Soricut, Radu\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.253/\",\n    doi = \"10.18653/v1/2021.naacl-main.253\",\n    pages = \"3157--3166\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.253.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.253/",
        "pdf_size": 2063978,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6739503870557083308&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4
    },
    {
        "id": "2021.naacl-main.316",
        "title": "Quantitative Day Trading from Natural Language using Reinforcement Learning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "It is challenging to design profitable and practical trading strategies, as stock price movements are highly stochastic, and the market is heavily influenced by chaotic data across sources like news and social media. Existing NLP approaches largely treat stock prediction as a classification or regression problem and are not optimized to make profitable investment decisions. Further, they do not model the temporal dynamics of large volumes of diversely influential text to which the market responds quickly. Building on these shortcomings, we propose a deep reinforcement learning approach that makes time-aware decisions to trade stocks while optimizing profit using textual data. Our method outperforms state-of-the-art in terms of risk-adjusted returns in trading simulations on two benchmarks: Tweets (English) and financial news (Chinese) pertaining to two major indexes and four global stock markets. Through extensive experiments and studies, we build the case for our method as a tool for quantitative trading.",
        "author": "Ramit Sawhney; Arnav Wadhwa; Shivam Agarwal; Rajiv Ratn Shah",
        "authorids": "/r/ramit-sawhney/; /a/arnav-wadhwa/; /s/shivam-agarwal/; /r/rajiv-shah/",
        "bibtex": "@inproceedings{sawhney-etal-2021-quantitative,\n    title = \"Quantitative Day Trading from Natural Language using Reinforcement Learning\",\n    author = \"Sawhney, Ramit  and\n      Wadhwa, Arnav  and\n      Agarwal, Shivam  and\n      Shah, Rajiv Ratn\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.316/\",\n    doi = \"10.18653/v1/2021.naacl-main.316\",\n    pages = \"4018--4030\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.316.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.316/",
        "pdf_size": 1764652,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12849222833859806623&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "IIIT Delhi; MIDAS, IIIT Delhi; Manipal Institute of Technology; IIIT Delhi",
        "aff_domain": "iiitd.ac.in;gmail.com;gmail.com;iiitd.ac.in",
        "email": "iiitd.ac.in;gmail.com;gmail.com;iiitd.ac.in",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "International Institute of Information Technology, Delhi;IIIT Delhi;Manipal Institute of Technology",
        "aff_unique_dep": ";MIDAS;",
        "aff_unique_url": "https://www.iiitdelhi.ac.in;https://www.iiitdelhi.ac.in;https://mit manipal.edu",
        "aff_unique_abbr": "IIIT-D;IIITD;MIT Manipal",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Delhi;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2021.naacl-industry.20",
        "title": "Query2Prod2Vec: Grounded Word Embeddings for eCommerce",
        "track": "main",
        "status": "Industry",
        "award": true,
        "abstract": "We present Query2Prod2Vec, a model that grounds lexical representations for product search in product embeddings: in our model, meaning is a mapping between words and a latent space of products in a digital shop. We leverage shopping sessions to learn the underlying space and use merchandising annotations to build lexical analogies for evaluation: our experiments show that our model is more accurate than known techniques from the NLP and IR literature. Finally, we stress the importance of data efficiency for product search outside of retail giants, and highlight how Query2Prod2Vec fits with practical constraints faced by most practitioners.",
        "author": "Federico Bianchi; Jacopo Tagliabue; Bingqing Yu",
        "authorids": "/f/federico-bianchi/; /j/jacopo-tagliabue/; /b/bingqing-yu/",
        "bibtex": "@inproceedings{bianchi-etal-2021-query2prod2vec,\n    title = \"{Q}uery2{P}rod2{V}ec: Grounded Word Embeddings for e{C}ommerce\",\n    author = \"Bianchi, Federico  and\n      Tagliabue, Jacopo  and\n      Yu, Bingqing\",\n    editor = \"Kim, Young-bum  and\n      Li, Yunyao  and\n      Rambow, Owen\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-industry.20/\",\n    doi = \"10.18653/v1/2021.naacl-industry.20\",\n    pages = \"154--162\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-industry.20.pdf",
        "site": "https://aclanthology.org/2021.naacl-industry.20/",
        "pdf_size": 279847,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4591906093102704397&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Bocconi University; Coveo Labs; Coveo",
        "aff_domain": "unibocconi.it;coveo.com;coveo.com",
        "email": "unibocconi.it;coveo.com;coveo.com",
        "github": "https://github.com/coveooss/ecommerce-query-embeddings",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Bocconi University;Coveo",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.bocconi.edu;https://www.coveo.com",
        "aff_unique_abbr": "Bocconi;Coveo",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "Italy;Canada"
    },
    {
        "id": "2021.naacl-main.100",
        "title": "RECONSIDER: Improved Re-Ranking using Span-Focused Cross-Attention for Open Domain Question Answering",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "State-of-the-art Machine Reading Comprehension (MRC) models for Open-domain Question Answering (QA) are typically trained for span selection using distantly supervised positive examples and heuristically retrieved negative examples. This training scheme possibly explains empirical observations that these models achieve a high recall amongst their top few predictions, but a low overall accuracy, motivating the need for answer re-ranking. We develop a successful re-ranking approach (RECONSIDER) for span-extraction tasks that improves upon the performance of MRC models, even beyond large-scale pre-training. RECONSIDER is trained on positive and negative examples extracted from high confidence MRC model predictions, and uses in-passage span annotations to perform span-focused re-ranking over a smaller candidate set. As a result, RECONSIDER learns to eliminate close false positives, achieving a new extractive state of the art on four QA tasks, with 45.5% Exact Match accuracy on Natural Questions with real user questions, and 61.7% on TriviaQA. We will release all related data, models, and code.",
        "author": "Srinivasan Iyer; Sewon Min; Yashar Mehdad; Wen-tau Yih",
        "authorids": "/s/srinivasan-iyer/; /s/sewon-min/; /y/yashar-mehdad/; /w/wen-tau-yih/",
        "bibtex": "@inproceedings{iyer-etal-2021-reconsider,\n    title = \"{RECONSIDER}: Improved Re-Ranking using Span-Focused Cross-Attention for Open Domain Question Answering\",\n    author = \"Iyer, Srinivasan  and\n      Min, Sewon  and\n      Mehdad, Yashar  and\n      Yih, Wen-tau\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.100/\",\n    doi = \"10.18653/v1/2021.naacl-main.100\",\n    pages = \"1280--1287\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.100.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.100/",
        "pdf_size": 475952,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2438836476401128606&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Facebook AI; University of Washington; Facebook AI; Facebook AI",
        "aff_domain": "fb.com;cs.washington.edu;fb.com;fb.com",
        "email": "fb.com;cs.washington.edu;fb.com;fb.com",
        "github": "github.com/facebookresearch/reconsider",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Facebook;University of Washington",
        "aff_unique_dep": "Facebook AI;",
        "aff_unique_url": "https://www.facebook.com;https://www.washington.edu",
        "aff_unique_abbr": "Facebook AI;UW",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-demos.16",
        "title": "RESIN: A Dockerized Schema-Guided Cross-document Cross-lingual Cross-media Information Extraction and Event Tracking System",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "We present a new information extraction system that can automatically construct temporal event graphs from a collection of news documents from multiple sources, multiple languages (English and Spanish for our experiment), and multiple data modalities (speech, text, image and video). The system advances state-of-the-art from two aspects: (1) extending from sentence-level event extraction to cross-document cross-lingual cross-media event extraction, coreference resolution and temporal event tracking; (2) using human curated event schema library to match and enhance the extraction output. We have made the dockerlized system publicly available for research purpose at GitHub, with a demo video.",
        "author": "Haoyang Wen; Ying Lin; Tuan Lai; Xiaoman Pan; Sha Li; Xudong Lin; Ben Zhou; Manling Li; Haoyu Wang; Hongming Zhang; Xiaodong Yu; Alexander Dong; Zhenhailong Wang; Yi Fung; Piyush Mishra; Qing Lyu; D\u00eddac Sur\u00eds; Brian Chen; Susan Windisch Brown; Martha Palmer; Chris Callison-Burch; Carl Vondrick; Jiawei Han; Dan Roth; Shih-Fu Chang; Heng Ji",
        "authorids": "/h/haoyang-wen/; /y/ying-lin/; /t/tuan-lai/; /x/xiaoman-pan/; /s/sha-li/; /x/xudong-lin/; /b/ben-zhou/; /m/manling-li/; /h/haoyu-wang/; /h/hongming-zhang/; /x/xiaodong-yu/; /a/alexander-dong/; /z/zhenhailong-wang/; /y/yi-fung/; /p/piyush-mishra/; /q/qing-lyu/; /d/didac-suris/; /b/brian-chen/; /s/susan-windisch-brown/; /m/martha-palmer/; /c/chris-callison-burch/; /c/carl-vondrick/; /j/jiawei-han/; /d/dan-roth/; /s/shih-fu-chang/; /h/heng-ji/",
        "bibtex": "@inproceedings{wen-etal-2021-resin,\n    title = \"{RESIN}: A Dockerized Schema-Guided Cross-document Cross-lingual Cross-media Information Extraction and Event Tracking System\",\n    author = \"Wen, Haoyang  and\n      Lin, Ying  and\n      Lai, Tuan  and\n      Pan, Xiaoman  and\n      Li, Sha  and\n      Lin, Xudong  and\n      Zhou, Ben  and\n      Li, Manling  and\n      Wang, Haoyu  and\n      Zhang, Hongming  and\n      Yu, Xiaodong  and\n      Dong, Alexander  and\n      Wang, Zhenhailong  and\n      Fung, Yi  and\n      Mishra, Piyush  and\n      Lyu, Qing  and\n      Sur{\\'i}s, D{\\'i}dac  and\n      Chen, Brian  and\n      Brown, Susan Windisch  and\n      Palmer, Martha  and\n      Callison-Burch, Chris  and\n      Vondrick, Carl  and\n      Han, Jiawei  and\n      Roth, Dan  and\n      Chang, Shih-Fu  and\n      Ji, Heng\",\n    editor = \"Sil, Avi  and\n      Lin, Xi Victoria\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-demos.16/\",\n    doi = \"10.18653/v1/2021.naacl-demos.16\",\n    pages = \"133--143\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-demos.16.pdf",
        "site": "https://aclanthology.org/2021.naacl-demos.16/",
        "pdf_size": 603231,
        "gs_citation": 65,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3025810419188784283&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": ";;;;;;;;;;;;;;;;;;;;;;;;;",
        "aff_domain": ";;;;;;;;;;;;;;;;;;;;;;;;;",
        "email": ";;;;;;;;;;;;;;;;;;;;;;;;;",
        "github": "https://github.com/RESIN-KAIROS/RESIN-pipeline-public",
        "project": "http://blender.cs.illinois.edu/software/resin/resin.mp4",
        "author_num": 26
    },
    {
        "id": "2021.naacl-main.128",
        "title": "RST Parsing from Scratch",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We introduce a novel top-down end-to-end formulation of document level discourse parsing in the Rhetorical Structure Theory (RST) framework. In this formulation, we consider discourse parsing as a sequence of splitting decisions at token boundaries and use a seq2seq network to model the splitting decisions. Our framework facilitates discourse parsing from scratch without requiring discourse segmentation as a prerequisite; rather, it yields segmentation as part of the parsing process. Our unified parsing model adopts a beam search to decode the best tree structure by searching through a space of high scoring trees. With extensive experiments on the standard RST discourse treebank, we demonstrate that our parser outperforms existing methods by a good margin in both end-to-end parsing and parsing with gold segmentation. More importantly, it does so without using any handcrafted features, making it faster and easily adaptable to new languages and domains.",
        "author": "Thanh-Tung Nguyen; Xuan-Phi Nguyen; Shafiq Joty; Xiaoli Li",
        "authorids": "/t/thanh-tung-nguyen/; /x/xuan-phi-nguyen/; /s/shafiq-joty/; /x/xiaoli-li/",
        "bibtex": "@inproceedings{nguyen-etal-2021-rst,\n    title = \"{RST} Parsing from Scratch\",\n    author = \"Nguyen, Thanh-Tung  and\n      Nguyen, Xuan-Phi  and\n      Joty, Shafiq  and\n      Li, Xiaoli\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.128/\",\n    doi = \"10.18653/v1/2021.naacl-main.128\",\n    pages = \"1613--1625\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.128.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.128/",
        "pdf_size": 1034851,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9900746221342893212&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Nanyang Technological University+Institute for Infocomm Research, A-STAR; Nanyang Technological University+Institute for Infocomm Research, A-STAR; Salesforce Research Asia+Nanyang Technological University+Institute for Infocomm Research, A-STAR; Institute for Infocomm Research, A-STAR",
        "aff_domain": "e.ntu.edu.sg;e.ntu.edu.sg;ntu.edu.sg;i2r.a-star.edu.sg",
        "email": "e.ntu.edu.sg;e.ntu.edu.sg;ntu.edu.sg;i2r.a-star.edu.sg",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0+1;2+0+1;1",
        "aff_unique_norm": "Nanyang Technological University;Institute for Infocomm Research;Salesforce Research",
        "aff_unique_dep": ";;Research",
        "aff_unique_url": "https://www.ntu.edu.sg;https://www.i2r.a-star.edu.sg;https://research.salesforce.com",
        "aff_unique_abbr": "NTU;I2R;Salesforce Research Asia",
        "aff_campus_unique_index": ";;1",
        "aff_campus_unique": ";Asia",
        "aff_country_unique_index": "0+0;0+0;0+0+0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "2021.naacl-main.451",
        "title": "RTFE: A Recursive Temporal Fact Embedding Framework for Temporal Knowledge Graph Completion",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Static knowledge graph (SKG) embedding (SKGE) has been studied intensively in the past years. Recently, temporal knowledge graph (TKG) embedding (TKGE) has emerged. In this paper, we propose a Recursive Temporal Fact Embedding (RTFE) framework to transplant SKGE models to TKGs and to enhance the performance of existing TKGE models for TKG completion. Different from previous work which ignores the continuity of states of TKG in time evolution, we treat the sequence of graphs as a Markov chain, which transitions from the previous state to the next state. RTFE takes the SKGE to initialize the embeddings of TKG. Then it recursively tracks the state transition of TKG by passing updated parameters/features between timestamps. Specifically, at each timestamp, we approximate the state transition as the gradient update process. Since RTFE learns each timestamp recursively, it can naturally transit to future timestamps. Experiments on five TKG datasets show the effectiveness of RTFE.",
        "author": "Youri Xu; Haihong E; Meina Song; Wenyu Song; Xiaodong Lv; Wang Haotian; Yang Jinrui",
        "authorids": "/y/youri-xu/; /h/haihong-e/; /m/meina-song/; /w/wenyu-song/; /x/xiaodong-lv/; /w/wang-haotian/; /y/yang-jinrui/",
        "bibtex": "@inproceedings{xu-etal-2021-rtfe,\n    title = \"{RTFE}: A Recursive Temporal Fact Embedding Framework for Temporal Knowledge Graph Completion\",\n    author = \"Xu, Youri  and\n      E, Haihong  and\n      Song, Meina  and\n      Song, Wenyu  and\n      Lv, Xiaodong  and\n      Haotian, Wang  and\n      Jinrui, Yang\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.451/\",\n    doi = \"10.18653/v1/2021.naacl-main.451\",\n    pages = \"5671--5681\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.451.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.451/",
        "pdf_size": 1385868,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16273552037792567982&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "School of Computer Science, Beijing University of Posts and Telecommunications, China; School of Computer Science, Beijing University of Posts and Telecommunications, China; School of Computer Science, Beijing University of Posts and Telecommunications, China; School of Computer Science, Beijing University of Posts and Telecommunications, China; School of Computer Science, Beijing University of Posts and Telecommunications, China; School of Computer Science, Beijing University of Posts and Telecommunications, China; School of Computer Science, Beijing University of Posts and Telecommunications, China",
        "aff_domain": "bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn",
        "email": "bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "Beijing University of Posts and Telecommunications",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "http://www.bupt.edu.cn/",
        "aff_unique_abbr": "BUPT",
        "aff_campus_unique_index": "0;0;0;0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2021.naacl-main.408",
        "title": "ReadTwice: Reading Very Large Documents with Memories",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Knowledge-intensive tasks such as question answering often require assimilating information from different sections of large inputs such as books or article collections. We propose ReadTwice, a simple and effective technique that combines several strengths of prior approaches to model long-range dependencies with Transformers. The main idea is to read text in small segments, in parallel, summarizing each segment into a memory table to be used in a second read of the text. We show that the method outperforms models of comparable size on several question answering (QA) datasets and sets a new state of the art on the challenging NarrativeQA task, with questions about entire books.",
        "author": "Yury Zemlyanskiy; Joshua Ainslie; Michiel de Jong; Philip Pham; Ilya Eckstein; Fei Sha",
        "authorids": "/y/yury-zemlyanskiy/; /j/joshua-ainslie/; /m/michiel-de-jong/; /p/philip-pham/; /i/ilya-eckstein/; /f/fei-sha/",
        "bibtex": "@inproceedings{zemlyanskiy-etal-2021-readtwice,\n    title = \"{R}ead{T}wice: Reading Very Large Documents with Memories\",\n    author = \"Zemlyanskiy, Yury  and\n      Ainslie, Joshua  and\n      de Jong, Michiel  and\n      Pham, Philip  and\n      Eckstein, Ilya  and\n      Sha, Fei\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.408/\",\n    doi = \"10.18653/v1/2021.naacl-main.408\",\n    pages = \"5189--5195\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.408.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.408/",
        "pdf_size": 476654,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18360131197363658320&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "https://goo.gle/research-readtwice",
        "author_num": 6
    },
    {
        "id": "2021.naacl-main.247",
        "title": "Reading and Acting while Blindfolded: The Need for Semantics in Text Game Agents",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Text-based games simulate worlds and interact with players using natural language. Recent work has used them as a testbed for autonomous language-understanding agents, with the motivation being that understanding the meanings of words or semantics is a key component of how humans understand, reason, and act in these worlds. However, it remains unclear to what extent artificial agents utilize semantic understanding of the text. To this end, we perform experiments to systematically reduce the amount of semantic information available to a learning agent. Surprisingly, we find that an agent is capable of achieving high scores even in the complete absence of language semantics, indicating that the currently popular experimental setup and models may be poorly designed to understand and leverage game texts. To remedy this deficiency, we propose an inverse dynamics decoder to regularize the representation space and encourage exploration, which shows improved performance on several games including Zork I. We discuss the implications of our findings for designing future agents with stronger semantic understanding.",
        "author": "Shunyu Yao; Karthik Narasimhan; Matthew Hausknecht",
        "authorids": "/s/shunyu-yao/; /k/karthik-narasimhan/; /m/matthew-hausknecht/",
        "bibtex": "@inproceedings{yao-etal-2021-reading,\n    title = \"Reading and Acting while Blindfolded: The Need for Semantics in Text Game Agents\",\n    author = \"Yao, Shunyu  and\n      Narasimhan, Karthik  and\n      Hausknecht, Matthew\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.247/\",\n    doi = \"10.18653/v1/2021.naacl-main.247\",\n    pages = \"3097--3102\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.247.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.247/",
        "pdf_size": 399768,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8979330841836317751&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Princeton University; Princeton University + Microsoft Research; Microsoft Research",
        "aff_domain": "princeton.edu;princeton.edu;microsoft.com",
        "email": "princeton.edu;princeton.edu;microsoft.com",
        "github": "",
        "project": "https://blindfolded.cs.princeton.edu",
        "author_num": 3,
        "aff_unique_index": "0;0+1;1",
        "aff_unique_norm": "Princeton University;Microsoft Corporation",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.princeton.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "Princeton;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.372",
        "title": "Recent advances in neural metaphor processing: A linguistic, cognitive and social perspective",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Metaphor is an indispensable part of human cognition and everyday communication. Much research has been conducted elucidating metaphor processing in the mind/brain and the role it plays in communication. in recent years, metaphor processing systems have benefited greatly from these studies, as well as the rapid advances in deep learning for natural language processing (NLP). This paper provides a comprehensive review and discussion of recent developments in automated metaphor processing, in light of the findings about metaphor in the mind, language, and communication, and from the perspective of downstream NLP tasks.",
        "author": "Xiaoyu Tong; Ekaterina Shutova; Martha Lewis",
        "authorids": "/x/xiaoyu-tong/; /e/ekaterina-shutova/; /m/martha-lewis/",
        "bibtex": "@inproceedings{tong-etal-2021-recent,\n    title = \"Recent advances in neural metaphor processing: A linguistic, cognitive and social perspective\",\n    author = \"Tong, Xiaoyu  and\n      Shutova, Ekaterina  and\n      Lewis, Martha\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.372/\",\n    doi = \"10.18653/v1/2021.naacl-main.372\",\n    pages = \"4673--4686\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.372.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.372/",
        "pdf_size": 305182,
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10821080351674139770&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "ILLC, University of Amsterdam, the Netherlands; ILLC, University of Amsterdam, the Netherlands; Department of Engineering Mathematics, University of Bristol, United Kingdom+ILLC, University of Amsterdam, the Netherlands",
        "aff_domain": "student.uva.nl;uva.nl;bristol.ac.uk",
        "email": "student.uva.nl;uva.nl;bristol.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;1+0",
        "aff_unique_norm": "University of Amsterdam;University of Bristol",
        "aff_unique_dep": "ILLC;Department of Engineering Mathematics",
        "aff_unique_url": "https://www.uva.nl;https://www.bristol.ac.uk",
        "aff_unique_abbr": "UvA;UoB",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Amsterdam;",
        "aff_country_unique_index": "0;0;1+0",
        "aff_country_unique": "Netherlands;United Kingdom"
    },
    {
        "id": "2021.naacl-main.330",
        "title": "Redefining Absent Keyphrases and their Effect on Retrieval Effectiveness",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Neural keyphrase generation models have recently attracted much interest due to their ability to output absent keyphrases, that is, keyphrases that do not appear in the source text. In this paper, we discuss the usefulness of absent keyphrases from an Information Retrieval (IR) perspective, and show that the commonly drawn distinction between present and absent keyphrases is not made explicit enough. We introduce a finer-grained categorization scheme that sheds more light on the impact of absent keyphrases on scientific document retrieval. Under this scheme, we find that only a fraction (around 20%) of the words that make up keyphrases actually serves as document expansion, but that this small fraction of words is behind much of the gains observed in retrieval effectiveness. We also discuss how the proposed scheme can offer a new angle to evaluate the output of neural keyphrase generation models.",
        "author": "Florian Boudin; Ygor Gallina",
        "authorids": "/f/florian-boudin/; /y/ygor-gallina/",
        "bibtex": "@inproceedings{boudin-gallina-2021-redefining,\n    title = \"Redefining Absent Keyphrases and their Effect on Retrieval Effectiveness\",\n    author = \"Boudin, Florian  and\n      Gallina, Ygor\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.330/\",\n    doi = \"10.18653/v1/2021.naacl-main.330\",\n    pages = \"4185--4193\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.330.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.330/",
        "pdf_size": 271689,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15133440123297293444&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "LS2N, Universit\u00e9 de Nantes, France; LS2N, Universit\u00e9 de Nantes, France",
        "aff_domain": "univ-nantes.fr;univ-nantes.fr",
        "email": "univ-nantes.fr;univ-nantes.fr",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Universit\u00e9 de Nantes",
        "aff_unique_dep": "LS2N",
        "aff_unique_url": "https://www.univ-nantes.fr",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "2021.naacl-main.113",
        "title": "RefSum: Refactoring Neural Summarization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Although some recent works show potential complementarity among different state-of-the-art systems, few works try to investigate this problem in text summarization. Researchers in other areas commonly refer to the techniques of reranking or stacking to approach this problem. In this work, we highlight several limitations of previous methods, which motivates us to present a new framework Refactor that provides a unified view of text summarization and summaries combination. Experimentally, we perform a comprehensive evaluation that involves twenty-two base systems, four datasets, and three different application scenarios. Besides new state-of-the-art results on CNN/DailyMail dataset (46.18 ROUGE-1), we also elaborate on how our proposed method addresses the limitations of the traditional methods and the effectiveness of the Refactor model sheds light on insight for performance improvement. Our system can be directly used by other researchers as an off-the-shelf tool to achieve further performance improvements. We open-source all the code and provide a convenient interface to use it: https://github.com/yixinL7/Refactoring-Summarization.",
        "author": "Yixin Liu; Zi-Yi Dou; Pengfei Liu",
        "authorids": "/y/yixin-liu/; /z/zi-yi-dou/; /p/pengfei-liu/",
        "bibtex": "@inproceedings{liu-etal-2021-refsum,\n    title = \"{R}ef{S}um: Refactoring Neural Summarization\",\n    author = \"Liu, Yixin  and\n      Dou, Zi-Yi  and\n      Liu, Pengfei\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.113/\",\n    doi = \"10.18653/v1/2021.naacl-main.113\",\n    pages = \"1437--1448\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.113.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.113/",
        "pdf_size": 501327,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17655880173665460803&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "github": "https://github.com/yixinL7/Refactoring-Summarization",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.290",
        "title": "Refining Targeted Syntactic Evaluation of Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Targeted syntactic evaluation of subject-verb number agreement in English (TSE) evaluates language models\u2019 syntactic knowledge using hand-crafted minimal pairs of sentences that differ only in the main verb\u2019s conjugation. The method evaluates whether language models rate each grammatical sentence as more likely than its ungrammatical counterpart. We identify two distinct goals for TSE. First, evaluating the systematicity of a language model\u2019s syntactic knowledge: given a sentence, can it conjugate arbitrary verbs correctly? Second, evaluating a model\u2019s likely behavior: given a sentence, does the model concentrate its probability mass on correctly conjugated verbs, even if only on a subset of the possible verbs? We argue that current implementations of TSE do not directly capture either of these goals, and propose new metrics to capture each goal separately. Under our metrics, we find that TSE overestimates systematicity of language models, but that models score up to 40% better on verbs that they predict are likely in context.",
        "author": "Benjamin Newman; Kai-Siang Ang; Julia Gong; John Hewitt",
        "authorids": "/b/benjamin-newman/; /k/kai-siang-ang/; /j/julia-gong/; /j/john-hewitt/",
        "bibtex": "@inproceedings{newman-etal-2021-refining,\n    title = \"Refining Targeted Syntactic Evaluation of Language Models\",\n    author = \"Newman, Benjamin  and\n      Ang, Kai-Siang  and\n      Gong, Julia  and\n      Hewitt, John\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.290/\",\n    doi = \"10.18653/v1/2021.naacl-main.290\",\n    pages = \"3710--3723\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.290.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.290/",
        "pdf_size": 2872762,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9701242770026513623&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science, Stanford University; Department of Computer Science, Stanford University; Department of Computer Science, Stanford University; Department of Computer Science, Stanford University",
        "aff_domain": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.477",
        "title": "ReinforceBug: A Framework to Generate Adversarial Textual Examples",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Adversarial Examples (AEs) generated by perturbingining examples are useful in improving the robustness of Deep Learning (DL) based models. Most prior works generate AEs that are either unconscionable due to lexical errors or semantically and functionally deviant from original examples. In this paper, we present ReinforceBug, a reinforcement learning framework, that learns a policy that is transferable on unseen datasets and generates utility-preserving and transferable (on other models) AEs. Our experiments show that ReinforceBug is on average 10% more successful as compared to the state-of the-art attack TextFooler. Moreover, the target models have on average 73.64% confidence in wrong prediction, the generated AEs preserve the functional equivalence and semantic similarity (83.38%) to their original counterparts, and are transferable on other models with an average success rate of 46%",
        "author": "Bushra Sabir; Muhammad Ali Babar; Raj Gaire",
        "authorids": "/b/bushra-sabir/; /m/muhammad-ali-babar/; /r/raj-gaire/",
        "bibtex": "@inproceedings{sabir-etal-2021-reinforcebug,\n    title = \"{R}einforce{B}ug: A Framework to Generate Adversarial Textual Examples\",\n    author = \"Sabir, Bushra  and\n      Babar, Muhammad Ali  and\n      Gaire, Raj\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.477/\",\n    doi = \"10.18653/v1/2021.naacl-main.477\",\n    pages = \"5954--5964\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.477.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.477/",
        "pdf_size": 7597184,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5737077801909580642&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "University of Adelaide+CREST - The Centre for Research on Engineering Software Technologies+CSIRO Data61; University of Adelaide+CREST - The Centre for Research on Engineering Software Technologies; CSIRO Data61",
        "aff_domain": "; ; ",
        "email": "; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1+2;0+1;2",
        "aff_unique_norm": "University of Adelaide;The Centre for Research on Engineering Software Technologies;CSIRO",
        "aff_unique_dep": ";Centre for Research on Engineering Software Technologies;Data61",
        "aff_unique_url": "https://www.adelaide.edu.au;https://crest.anu.edu.au/;https://www.csiro.au",
        "aff_unique_abbr": "Adelaide;CREST;CSIRO",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0+0;0+0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "2021.naacl-main.53",
        "title": "Representing Numbers in NLP: a Survey and a Vision",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "NLP systems rarely give special consideration to numbers found in text. This starkly contrasts with the consensus in neuroscience that, in the brain, numbers are represented differently from words. We arrange recent NLP work on numeracy into a comprehensive taxonomy of tasks and methods. We break down the subjective notion of numeracy into 7 subtasks, arranged along two dimensions: granularity (exact vs approximate) and units (abstract vs grounded). We analyze the myriad representational choices made by over a dozen previously published number encoders and decoders. We synthesize best practices for representing numbers in text and articulate a vision for holistic numeracy in NLP, comprised of design trade-offs and a unified evaluation.",
        "author": "Avijit Thawani; Jay Pujara; Filip Ilievski; Pedro Szekely",
        "authorids": "/a/avijit-thawani/; /j/jay-pujara/; /f/filip-ilievski/; /p/pedro-szekely/",
        "bibtex": "@inproceedings{thawani-etal-2021-representing,\n    title = \"Representing Numbers in {NLP}: a Survey and a Vision\",\n    author = \"Thawani, Avijit  and\n      Pujara, Jay  and\n      Ilievski, Filip  and\n      Szekely, Pedro\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.53/\",\n    doi = \"10.18653/v1/2021.naacl-main.53\",\n    pages = \"644--656\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.53.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.53/",
        "pdf_size": 393520,
        "gs_citation": 133,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9808775490771688083&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "University of Southern California; University of Southern California; University of Southern California; University of Southern California",
        "aff_domain": "isi.edu;isi.edu;isi.edu;isi.edu",
        "email": "isi.edu;isi.edu;isi.edu;isi.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Southern California",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.usc.edu",
        "aff_unique_abbr": "USC",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.317",
        "title": "Restoring and Mining the Records of the Joseon Dynasty via Neural Language Modeling and Machine Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Understanding voluminous historical records provides clues on the past in various aspects, such as social and political issues and even natural science facts. However, it is generally difficult to fully utilize the historical records, since most of the documents are not written in a modern language and part of the contents are damaged over time. As a result, restoring the damaged or unrecognizable parts as well as translating the records into modern languages are crucial tasks. In response, we present a multi-task learning approach to restore and translate historical documents based on a self-attention mechanism, specifically utilizing two Korean historical records, ones of the most voluminous historical records in the world. Experimental results show that our approach significantly improves the accuracy of the translation task than baselines without multi-task learning. In addition, we present an in-depth exploratory analysis on our translated results via topic modeling, uncovering several significant historical events.",
        "author": "Kyeongpil Kang; Kyohoon Jin; Soyoung Yang; Soojin Jang; Jaegul Choo; Youngbin Kim",
        "authorids": "/k/kyeongpil-kang/; /k/kyohoon-jin/; /s/soyoung-yang/; /s/soojin-jang/; /j/jaegul-choo/; /y/youngbin-kim/",
        "bibtex": "@inproceedings{kang-etal-2021-restoring,\n    title = \"Restoring and Mining the Records of the {J}oseon Dynasty via Neural Language Modeling and Machine Translation\",\n    author = \"Kang, Kyeongpil  and\n      Jin, Kyohoon  and\n      Yang, Soyoung  and\n      Jang, Soojin  and\n      Choo, Jaegul  and\n      Kim, Youngbin\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.317/\",\n    doi = \"10.18653/v1/2021.naacl-main.317\",\n    pages = \"4031--4042\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.317.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.317/",
        "pdf_size": 3862827,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3749137563509795004&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Scatter Lab, Seoul, South Korea; Chung-Ang University, Seoul, South Korea; KAIST, Daejeon, South Korea; Chung-Ang University, Seoul, South Korea; KAIST, Daejeon, South Korea; Chung-Ang University, Seoul, South Korea",
        "aff_domain": "scatterlab.co.kr;cau.ac.kr;kaist.ac.kr;cau.ac.kr;kaist.ac.kr;cau.ac.kr",
        "email": "scatterlab.co.kr;cau.ac.kr;kaist.ac.kr;cau.ac.kr;kaist.ac.kr;cau.ac.kr",
        "github": "",
        "project": "https://support.google.com/websearch/answer/9690276",
        "author_num": 6,
        "aff_unique_index": "0;1;2;1;2;1",
        "aff_unique_norm": "Scatter Lab;Chung-Ang University;Korea Advanced Institute of Science and Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";http://www.cau.ac.kr;https://www.kaist.ac.kr",
        "aff_unique_abbr": ";CAU;KAIST",
        "aff_campus_unique_index": "0;0;1;0;1;0",
        "aff_campus_unique": "Seoul;Daejeon",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2021.naacl-main.188",
        "title": "Rethinking Network Pruning \u2013 under the Pre-train and Fine-tune Paradigm",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Transformer-based pre-trained language models have significantly improved the performance of various natural language processing (NLP) tasks in the recent years. While effective and prevalent, these models are usually prohibitively large for resource-limited deployment scenarios. A thread of research has thus been working on applying network pruning techniques under the pretrain-then-finetune paradigm widely adopted in NLP. However, the existing pruning results on benchmark transformers, such as BERT, are not as remarkable as the pruning results in the literature of convolutional neural networks (CNNs). In particular, common wisdom in pruning CNN states that sparse pruning technique compresses a model more than that obtained by reducing number of channels and layers, while existing works on sparse pruning of BERT yields inferior results than its small-dense counterparts such as TinyBERT. In this work, we aim to fill this gap by studying how knowledge are transferred and lost during the pre-train, fine-tune, and pruning process, and proposing a knowledge-aware sparse pruning process that achieves significantly superior results than existing literature. We show for the first time that sparse pruning compresses a BERT model significantly more than reducing its number of channels and layers. Experiments on multiple data sets of GLUE benchmark show that our method outperforms the leading competitors with a 20-times weight/FLOPs compression and neglectable loss in prediction accuracy.",
        "author": "Dongkuan Xu; Ian En-Hsu Yen; Jinxi Zhao; Zhibin Xiao",
        "authorids": "/d/dongkuan-xu/; /i/ian-en-hsu-yen/; /j/jinxi-zhao/; /z/zhibin-xiao/",
        "bibtex": "@inproceedings{xu-etal-2021-rethinking,\n    title = \"Rethinking Network Pruning {--} under the Pre-train and Fine-tune Paradigm\",\n    author = \"Xu, Dongkuan  and\n      Yen, Ian En-Hsu  and\n      Zhao, Jinxi  and\n      Xiao, Zhibin\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.188/\",\n    doi = \"10.18653/v1/2021.naacl-main.188\",\n    pages = \"2376--2382\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.188.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.188/",
        "pdf_size": 723042,
        "gs_citation": 63,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12708626152367200566&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "The Pennsylvania State University, State College, PA, USA; Moffett AI, Los Altos, CA, USA; Moffett AI, Los Altos, CA, USA; Moffett AI, Los Altos, CA, USA",
        "aff_domain": "psu.edu;moffett.ai;moffett.ai;moffett.ai",
        "email": "psu.edu;moffett.ai;moffett.ai;moffett.ai",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "The Pennsylvania State University;Moffett AI",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.psu.edu;",
        "aff_unique_abbr": "PSU;",
        "aff_campus_unique_index": "0;1;1;1",
        "aff_campus_unique": "State College;Los Altos",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.460",
        "title": "Rethinking Perturbations in Encoder-Decoders for Fast Training",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We often use perturbations to regularize neural models. For neural encoder-decoders, previous studies applied the scheduled sampling (Bengio et al., 2015) and adversarial perturbations (Sato et al., 2019) as perturbations but these methods require considerable computational time. Thus, this study addresses the question of whether these approaches are efficient enough for training time. We compare several perturbations in sequence-to-sequence problems with respect to computational time. Experimental results show that the simple techniques such as word dropout (Gal and Ghahramani, 2016) and random replacement of input tokens achieve comparable (or better) scores to the recently proposed perturbations, even though these simple methods are faster.",
        "author": "Sho Takase; Shun Kiyono",
        "authorids": "/s/sho-takase/; /s/shun-kiyono/",
        "bibtex": "@inproceedings{takase-kiyono-2021-rethinking,\n    title = \"Rethinking Perturbations in Encoder-Decoders for Fast Training\",\n    author = \"Takase, Sho  and\n      Kiyono, Shun\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.460/\",\n    doi = \"10.18653/v1/2021.naacl-main.460\",\n    pages = \"5767--5780\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.460.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.460/",
        "pdf_size": 536846,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3827738758701923170&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Tokyo Institute of Technology; RIKEN / Tohoku University",
        "aff_domain": "nlp.c.titech.ac.jp;riken.jp",
        "email": "nlp.c.titech.ac.jp;riken.jp",
        "github": "https://github.com/takase/rethink_perturbations",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Tokyo Institute of Technology;RIKEN",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.titech.ac.jp;https://www.riken.jp",
        "aff_unique_abbr": "Titech;RIKEN",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2021.naacl-main.250",
        "title": "Revisiting Document Representations for Large-Scale Zero-Shot Learning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Zero-shot learning aims to recognize unseen objects using their semantic representations. Most existing works use visual attributes labeled by humans, not suitable for large-scale applications. In this paper, we revisit the use of documents as semantic representations. We argue that documents like Wikipedia pages contain rich visual information, which however can easily be buried by the vast amount of non-visual sentences. To address this issue, we propose a semi-automatic mechanism for visual sentence extraction that leverages the document section headers and the clustering structure of visual sentences. The extracted visual sentences, after a novel weighting scheme to distinguish similar classes, essentially form semantic representations like visual attributes but need much less human effort. On the ImageNet dataset with over 10,000 unseen classes, our representations lead to a 64% relative improvement against the commonly used ones.",
        "author": "Jihyung Kil; Wei-Lun Chao",
        "authorids": "/j/jihyung-kil/; /w/wei-lun-chao/",
        "bibtex": "@inproceedings{kil-chao-2021-revisiting,\n    title = \"Revisiting Document Representations for Large-Scale Zero-Shot Learning\",\n    author = \"Kil, Jihyung  and\n      Chao, Wei-Lun\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.250/\",\n    doi = \"10.18653/v1/2021.naacl-main.250\",\n    pages = \"3117--3128\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.250.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.250/",
        "pdf_size": 2601938,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18330442953735582384&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "The Ohio State University; The Ohio State University",
        "aff_domain": "osu.edu;osu.edu",
        "email": "osu.edu;osu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "The Ohio State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.osu.edu",
        "aff_unique_abbr": "OSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.407",
        "title": "Revisiting Simple Neural Probabilistic Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent progress in language modeling has been driven not only by advances in neural architectures, but also through hardware and optimization improvements. In this paper, we revisit the neural probabilistic language model (NPLM) of Bengio et al. (2003), which simply concatenates word embeddings within a fixed window and passes the result through a feed-forward network to predict the next word. When scaled up to modern hardware, this model (despite its many limitations) performs much better than expected on word-level language model benchmarks. Our analysis reveals that the NPLM achieves lower perplexity than a baseline Transformer with short input contexts but struggles to handle long-term dependencies. Inspired by this result, we modify the Transformer by replacing its first self-attention layer with the NPLM\u2019s local concatenation layer, which results in small but consistent perplexity decreases across three word-level language modeling datasets.",
        "author": "Simeng Sun; Mohit Iyyer",
        "authorids": "/s/simeng-sun/; /m/mohit-iyyer/",
        "bibtex": "@inproceedings{sun-iyyer-2021-revisiting,\n    title = \"Revisiting Simple Neural Probabilistic Language Models\",\n    author = \"Sun, Simeng  and\n      Iyyer, Mohit\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.407/\",\n    doi = \"10.18653/v1/2021.naacl-main.407\",\n    pages = \"5181--5188\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.407.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.407/",
        "pdf_size": 389038,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16629391587477492079&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "College of Information and Computer Sciences, University of Massachusetts Amherst; College of Information and Computer Sciences, University of Massachusetts Amherst",
        "aff_domain": "cs.umass.edu;cs.umass.edu",
        "email": "cs.umass.edu;cs.umass.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Massachusetts Amherst",
        "aff_unique_dep": "College of Information and Computer Sciences",
        "aff_unique_url": "https://www.umass.edu",
        "aff_unique_abbr": "UMass Amherst",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Amherst",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.133",
        "title": "Revisiting the Weaknesses of Reinforcement Learning for Neural Machine Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Policy gradient algorithms have found wide adoption in NLP, but have recently become subject to criticism, doubting their suitability for NMT. Choshen et al. (2020) identify multiple weaknesses and suspect that their success is determined by the shape of output distributions rather than the reward. In this paper, we revisit these claims and study them under a wider range of configurations. Our experiments on in-domain and cross-domain adaptation reveal the importance of exploration and reward scaling, and provide empirical counter-evidence to these claims.",
        "author": "Samuel Kiegeland; Julia Kreutzer",
        "authorids": "/s/samuel-kiegeland/; /j/julia-kreutzer/",
        "bibtex": "@inproceedings{kiegeland-kreutzer-2021-revisiting,\n    title = \"Revisiting the Weaknesses of Reinforcement Learning for Neural Machine Translation\",\n    author = \"Kiegeland, Samuel  and\n      Kreutzer, Julia\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.133/\",\n    doi = \"10.18653/v1/2021.naacl-main.133\",\n    pages = \"1673--1681\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.133.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.133/",
        "pdf_size": 581131,
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5844768475574256915&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Heidelberg University; Google Research",
        "aff_domain": "cl.uni-heidelberg.de;google.com",
        "email": "cl.uni-heidelberg.de;google.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Heidelberg University;Google",
        "aff_unique_dep": ";Google Research",
        "aff_unique_url": "https://www.uni-heidelberg.de;https://research.google",
        "aff_unique_abbr": "Uni Heidelberg;Google Research",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Germany;United States"
    },
    {
        "id": "2021.naacl-main.98",
        "title": "Robust Question Answering Through Sub-part Alignment",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Current textual question answering (QA) models achieve strong performance on in-domain test sets, but often do so by fitting surface-level patterns, so they fail to generalize to out-of-distribution settings. To make a more robust and understandable QA system, we model question answering as an alignment problem. We decompose both the question and context into smaller units based on off-the-shelf semantic representations (here, semantic roles), and align the question to a subgraph of the context in order to find the answer. We formulate our model as a structured SVM, with alignment scores computed via BERT, and we can train end-to-end despite using beam search for approximate inference. Our use of explicit alignments allows us to explore a set of constraints with which we can prohibit certain types of bad model behavior arising in cross-domain settings. Furthermore, by investigating differences in scores across different potential answers, we can seek to understand what particular aspects of the input lead the model to choose the answer without relying on post-hoc explanation techniques. We train our model on SQuAD v1.1 and test it on several adversarial and out-of-domain datasets. The results show that our model is more robust than the standard BERT QA model, and constraints derived from alignment scores allow us to effectively trade off coverage and accuracy.",
        "author": "Jifan Chen; Greg Durrett",
        "authorids": "/j/jifan-chen/; /g/greg-durrett/",
        "bibtex": "@inproceedings{chen-durrett-2021-robust,\n    title = \"Robust Question Answering Through Sub-part Alignment\",\n    author = \"Chen, Jifan  and\n      Durrett, Greg\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.98/\",\n    doi = \"10.18653/v1/2021.naacl-main.98\",\n    pages = \"1251--1263\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.98.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.98/",
        "pdf_size": 831625,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6337933760529936059&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "The University of Texas at Austin; The University of Texas at Austin",
        "aff_domain": "cs.utexas.edu;cs.utexas.edu",
        "email": "cs.utexas.edu;cs.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-demos.6",
        "title": "Robustness Gym: Unifying the NLP Evaluation Landscape",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "Despite impressive performance on standard benchmarks, natural language processing (NLP) models are often brittle when deployed in real-world systems. In this work, we identify challenges with evaluating NLP systems and propose a solution in the form of Robustness Gym (RG), a simple and extensible evaluation toolkit that unifies 4 standard evaluation paradigms: subpopulations, transformations, evaluation sets, and adversarial attacks. By providing a common platform for evaluation, RG enables practitioners to compare results from disparate evaluation paradigms with a single click, and to easily develop and share novel evaluation methods using a built-in set of abstractions. RG is under active development and we welcome feedback & contributions from the community.",
        "author": "Karan Goel; Nazneen Fatema Rajani; Jesse Vig; Zachary Taschdjian; Mohit Bansal; Christopher R\u00e9",
        "authorids": "/k/karan-goel/; /n/nazneen-fatema-rajani/; /j/jesse-vig/; /z/zachary-taschdjian/; /m/mohit-bansal/; /c/christopher-re/",
        "bibtex": "@inproceedings{goel-etal-2021-robustness,\n    title = \"Robustness Gym: Unifying the {NLP} Evaluation Landscape\",\n    author = \"Goel, Karan  and\n      Rajani, Nazneen Fatema  and\n      Vig, Jesse  and\n      Taschdjian, Zachary  and\n      Bansal, Mohit  and\n      R{\\'e}, Christopher\",\n    editor = \"Sil, Avi  and\n      Lin, Xi Victoria\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-demos.6/\",\n    doi = \"10.18653/v1/2021.naacl-demos.6\",\n    pages = \"42--55\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-demos.6.pdf",
        "site": "https://aclanthology.org/2021.naacl-demos.6/",
        "pdf_size": 2061734,
        "gs_citation": 152,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4191922831262197090&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Stanford University; Salesforce Research; Salesforce Research; Salesforce Research; UNC Chapel-Hill; Stanford University",
        "aff_domain": "cs.stanford.edu; ; ; ; ; ",
        "email": "cs.stanford.edu; ; ; ; ; ",
        "github": "https://github.com/robustness-gym/robustness-gym",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;1;1;2;0",
        "aff_unique_norm": "Stanford University;Salesforce;University of North Carolina at Chapel Hill",
        "aff_unique_dep": ";Salesforce Research;",
        "aff_unique_url": "https://www.stanford.edu;https://research.salesforce.com;https://www.unc.edu",
        "aff_unique_abbr": "Stanford;Salesforce;UNC",
        "aff_campus_unique_index": "0;2;0",
        "aff_campus_unique": "Stanford;;Chapel Hill",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.466",
        "title": "RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In open-domain question answering, dense passage retrieval has become a new paradigm to retrieve relevant passages for finding answers. Typically, the dual-encoder architecture is adopted to learn dense representations of questions and passages for semantic matching. However, it is difficult to effectively train a dual-encoder due to the challenges including the discrepancy between training and inference, the existence of unlabeled positives and limited training data. To address these challenges, we propose an optimized training approach, called RocketQA, to improving dense passage retrieval. We make three major technical contributions in RocketQA, namely cross-batch negatives, denoised hard negatives and data augmentation. The experiment results show that RocketQA significantly outperforms previous state-of-the-art models on both MSMARCO and Natural Questions. We also conduct extensive experiments to examine the effectiveness of the three strategies in RocketQA. Besides, we demonstrate that the performance of end-to-end QA can be improved based on our RocketQA retriever.",
        "author": "Yingqi Qu; Yuchen Ding; Jing Liu; Kai Liu; Ruiyang Ren; Wayne Xin Zhao; Daxiang Dong; Hua Wu; Haifeng Wang",
        "authorids": "/y/yingqi-qu/; /y/yuchen-ding/; /j/jing-liu/; /k/kai-liu/; /r/ruiyang-ren/; /w/wayne-xin-zhao/; /d/daxiang-dong/; /h/hua-wu/; /h/haifeng-wang/",
        "bibtex": "@inproceedings{qu-etal-2021-rocketqa,\n    title = \"{R}ocket{QA}: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering\",\n    author = \"Qu, Yingqi  and\n      Ding, Yuchen  and\n      Liu, Jing  and\n      Liu, Kai  and\n      Ren, Ruiyang  and\n      Zhao, Wayne Xin  and\n      Dong, Daxiang  and\n      Wu, Hua  and\n      Wang, Haifeng\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.466/\",\n    doi = \"10.18653/v1/2021.naacl-main.466\",\n    pages = \"5835--5847\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.466.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.466/",
        "pdf_size": 1007756,
        "gs_citation": 664,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16865490844693172929&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Baidu Inc.; Baidu Inc.; Baidu Inc.; Baidu Inc.; Gaoling School of Arti\ufb01cial Intelligence, Renmin University of China; Gaoling School of Arti\ufb01cial Intelligence, Renmin University of China; Baidu Inc.; Baidu Inc.; Baidu Inc.",
        "aff_domain": "baidu.com;baidu.com;baidu.com;baidu.com;ruc.edu.cn;gmail.com;baidu.com;baidu.com;baidu.com",
        "email": "baidu.com;baidu.com;baidu.com;baidu.com;ruc.edu.cn;gmail.com;baidu.com;baidu.com;baidu.com",
        "github": "https://github.com/PaddlePaddle/Research/tree/master/NLP/NAACL2021-RocketQA",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;0;0;0;1;1;0;0;0",
        "aff_unique_norm": "Baidu Inc.;Renmin University of China",
        "aff_unique_dep": ";Gaoling School of Arti\ufb01cial Intelligence",
        "aff_unique_url": "https://www.baidu.com;http://www.ruc.edu.cn",
        "aff_unique_abbr": "Baidu;RUC",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2021.naacl-main.409",
        "title": "SCRIPT: Self-Critic PreTraining of Transformers",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We introduce Self-CRItic Pretraining Transformers (SCRIPT) for representation learning of text. The popular masked language modeling (MLM) pretraining methods like BERT replace some tokens with [MASK] and an encoder is trained to recover them, while ELECTRA trains a discriminator to detect replaced tokens proposed by a generator. In contrast, we train a language model as in MLM and further derive a discriminator or critic on top of the encoder without using any additional parameters. That is, the model itself is a critic. SCRIPT combines MLM training and discriminative training for learning rich representations and compute- and sample-efficiency. We demonstrate improved sample-efficiency in pretraining and enhanced representations evidenced by improved downstream task performance on GLUE and SQuAD over strong baselines. Also, the self-critic scores can be directly used as pseudo-log-likelihood for efficient scoring.",
        "author": "Erik Nijkamp; Bo Pang; Ying Nian Wu; Caiming Xiong",
        "authorids": "/e/erik-nijkamp/; /b/bo-pang/; /y/ying-nian-wu/; /c/caiming-xiong/",
        "bibtex": "@inproceedings{nijkamp-etal-2021-script,\n    title = \"{SCRIPT}: Self-Critic {P}re{T}raining of Transformers\",\n    author = \"Nijkamp, Erik  and\n      Pang, Bo  and\n      Wu, Ying Nian  and\n      Xiong, Caiming\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.409/\",\n    doi = \"10.18653/v1/2021.naacl-main.409\",\n    pages = \"5196--5202\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.409.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.409/",
        "pdf_size": 306175,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3958170342454769491&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "UCLA; UCLA; UCLA; Salesforce Research",
        "aff_domain": "ucla.edu;ucla.edu;stat.ucla.edu;salesforce.com",
        "email": "ucla.edu;ucla.edu;stat.ucla.edu;salesforce.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "University of California, Los Angeles;Salesforce",
        "aff_unique_dep": ";Salesforce Research",
        "aff_unique_url": "https://www.ucla.edu;https://research.salesforce.com",
        "aff_unique_abbr": "UCLA;Salesforce",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.455",
        "title": "SGG: Learning to Select, Guide, and Generate for Keyphrase Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Keyphrases, that concisely summarize the high-level topics discussed in a document, can be categorized into present keyphrase which explicitly appears in the source text and absent keyphrase which does not match any contiguous subsequence but is highly semantically related to the source. Most existing keyphrase generation approaches synchronously generate present and absent keyphrases without explicitly distinguishing these two categories. In this paper, a Select-Guide-Generate (SGG) approach is proposed to deal with present and absent keyphrases generation separately with different mechanisms. Specifically, SGG is a hierarchical neural network which consists of a pointing-based selector at low layer concentrated on present keyphrase generation, a selection-guided generator at high layer dedicated to absent keyphrase generation, and a guider in the middle to transfer information from selector to generator. Experimental results on four keyphrase generation benchmarks demonstrate the effectiveness of our model, which significantly outperforms the strong baselines for both present and absent keyphrases generation. Furthermore, we extend SGG to a title generation task which indicates its extensibility in natural language generation tasks.",
        "author": "Jing Zhao; Junwei Bao; Yifan Wang; Youzheng Wu; Xiaodong He; Bowen Zhou",
        "authorids": "/j/jing-zhao/; /j/junwei-bao/; /y/yifan-wang/; /y/youzheng-wu/; /x/xiaodong-he/; /b/bowen-zhou/",
        "bibtex": "@inproceedings{zhao-etal-2021-sgg,\n    title = \"{SGG}: Learning to Select, Guide, and Generate for Keyphrase Generation\",\n    author = \"Zhao, Jing  and\n      Bao, Junwei  and\n      Wang, Yifan  and\n      Wu, Youzheng  and\n      He, Xiaodong  and\n      Zhou, Bowen\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.455/\",\n    doi = \"10.18653/v1/2021.naacl-main.455\",\n    pages = \"5717--5726\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.455.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.455/",
        "pdf_size": 1988167,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9804267798247399629&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "JD AI Research; JD AI Research; JD AI Research; JD AI Research; JD AI Research; JD AI Research",
        "aff_domain": "jd.com;jd.com;jd.com;jd.com;jd.com;jd.com",
        "email": "jd.com;jd.com;jd.com;jd.com;jd.com;jd.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "JD AI Research",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.jd.com",
        "aff_unique_abbr": "JD AI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2021.naacl-main.30",
        "title": "SGL: Speaking the Graph Languages of Semantic Parsing via Multilingual Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Graph-based semantic parsing aims to represent textual meaning through directed graphs. As one of the most promising general-purpose meaning representations, these structures and their parsing have gained a significant interest momentum during recent years, with several diverse formalisms being proposed. Yet, owing to this very heterogeneity, most of the research effort has focused mainly on solutions specific to a given formalism. In this work, instead, we reframe semantic parsing towards multiple formalisms as Multilingual Neural Machine Translation (MNMT), and propose SGL, a many-to-many seq2seq architecture trained with an MNMT objective. Backed by several experiments, we show that this framework is indeed effective once the learning procedure is enhanced with large parallel corpora coming from Machine Translation: we report competitive performances on AMR and UCCA parsing, especially once paired with pre-trained architectures. Furthermore, we find that models trained under this configuration scale remarkably well to tasks such as cross-lingual AMR parsing: SGL outperforms all its competitors by a large margin without even explicitly seeing non-English to AMR examples at training time and, once these examples are included as well, sets an unprecedented state of the art in this task. We release our code and our models for research purposes at https://github.com/SapienzaNLP/sgl.",
        "author": "Luigi Procopio; Rocco Tripodi; Roberto Navigli",
        "authorids": "/l/luigi-procopio/; /r/rocco-tripodi/; /r/roberto-navigli/",
        "bibtex": "@inproceedings{procopio-etal-2021-sgl,\n    title = \"{SGL}: Speaking the Graph Languages of Semantic Parsing via Multilingual Translation\",\n    author = \"Procopio, Luigi  and\n      Tripodi, Rocco  and\n      Navigli, Roberto\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.30/\",\n    doi = \"10.18653/v1/2021.naacl-main.30\",\n    pages = \"325--337\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.30.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.30/",
        "pdf_size": 748925,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1810137071178162722&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Sapienza NLP Group, Department of Computer Science, Sapienza University of Rome; Sapienza NLP Group, Department of Computer Science, Sapienza University of Rome; Sapienza NLP Group, Department of Computer Science, Sapienza University of Rome",
        "aff_domain": "di.uniroma1.it;di.uniroma1.it;di.uniroma1.it",
        "email": "di.uniroma1.it;di.uniroma1.it;di.uniroma1.it",
        "github": "https://github.com/SapienzaNLP/sgl",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Sapienza University of Rome",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.uniroma1.it",
        "aff_unique_abbr": "Sapienza",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Rome",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Italy"
    },
    {
        "id": "2021.naacl-main.342",
        "title": "SOCCER: An Information-Sparse Discourse State Tracking Collection in the Sports Commentary Domain",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In the pursuit of natural language understanding, there has been a long standing interest in tracking state changes throughout narratives. Impressive progress has been made in modeling the state of transaction-centric dialogues and procedural texts. However, this problem has been less intensively studied in the realm of general discourse where ground truth descriptions of states may be loosely defined and state changes are less densely distributed over utterances. This paper proposes to turn to simplified, fully observable systems that show some of these properties: Sports events. We curated 2,263 soccer matches including time-stamped natural language commentary accompanied by discrete events such as a team scoring goals, switching players or being penalized with cards. We propose a new task formulation where, given paragraphs of commentary of a game at different timestamps, the system is asked to recognize the occurrence of in-game events. This domain allows for rich descriptions of state while avoiding the complexities of many other real-world settings. As an initial point of performance measurement, we include two baseline methods from the perspectives of sentence classification with temporal dependence and current state-of-the-art generative model, respectively, and demonstrate that even sophisticated existing methods struggle on the state tracking task when the definition of state broadens or non-event chatter becomes prevalent.",
        "author": "Ruochen Zhang; Carsten Eickhoff",
        "authorids": "/r/ruochen-zhang/; /c/carsten-eickhoff/",
        "bibtex": "@inproceedings{zhang-eickhoff-2021-soccer,\n    title = \"{SOCCER}: An Information-Sparse Discourse State Tracking Collection in the Sports Commentary Domain\",\n    author = \"Zhang, Ruochen  and\n      Eickhoff, Carsten\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.342/\",\n    doi = \"10.18653/v1/2021.naacl-main.342\",\n    pages = \"4325--4333\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.342.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.342/",
        "pdf_size": 888828,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6143274558544334941&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Brown University; Brown University",
        "aff_domain": "brown.edu;brown.edu",
        "email": "brown.edu;brown.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Brown University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.brown.edu",
        "aff_unique_abbr": "Brown",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.248",
        "title": "SOrT-ing VQA Models : Contrastive Gradient Learning for Improved Consistency",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent research in Visual Question Answering (VQA) has revealed state-of-the-art models to be inconsistent in their understanding of the world - they answer seemingly difficult questions requiring reasoning correctly but get simpler associated sub-questions wrong. These sub-questions pertain to lower level visual concepts in the image that models ideally should understand to be able to answer the reasoning question correctly. To address this, we first present a gradient-based interpretability approach to determine the questions most strongly correlated with the reasoning question on an image, and use this to evaluate VQA models on their ability to identify the relevant sub-questions needed to answer a reasoning question. Next, we propose a contrastive gradient learning based approach called Sub-question Oriented Tuning (SOrT) which encourages models to rank relevant sub-questions higher than irrelevant questions for an <image, reasoning-question> pair. We show that SOrT improves model consistency by up to 6.5% points over existing approaches, while also improving visual grounding and robustness to rephrasings of questions.",
        "author": "Sameer Dharur; Purva Tendulkar; Dhruv Batra; Devi Parikh; Ramprasaath R. Selvaraju",
        "authorids": "/s/sameer-dharur/; /p/purva-tendulkar/; /d/dhruv-batra/; /d/devi-parikh/; /r/ramprasaath-r-selvaraju/",
        "bibtex": "@inproceedings{dharur-etal-2021-sort,\n    title = \"{SO}r{T}-ing {VQA} Models : Contrastive Gradient Learning for Improved Consistency\",\n    author = \"Dharur, Sameer  and\n      Tendulkar, Purva  and\n      Batra, Dhruv  and\n      Parikh, Devi  and\n      R. Selvaraju, Ramprasaath\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.248/\",\n    doi = \"10.18653/v1/2021.naacl-main.248\",\n    pages = \"3103--3111\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.248.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.248/",
        "pdf_size": 2460036,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7602555613956372633&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Georgia Tech+University of California, San Diego; Georgia Tech+University of California, San Diego; Georgia Tech+Facebook AI Research; Georgia Tech+Facebook AI Research; Georgia Tech+Salesforce Research",
        "aff_domain": "gatech.edu;gatech.edu;gatech.edu;gatech.edu;gatech.edu",
        "email": "gatech.edu;gatech.edu;gatech.edu;gatech.edu;gatech.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0+1;0+2;0+2;0+3",
        "aff_unique_norm": "Georgia Institute of Technology;University of California, San Diego;Facebook;Salesforce",
        "aff_unique_dep": ";;Facebook AI Research;Salesforce Research",
        "aff_unique_url": "https://www.gatech.edu;https://www.ucsd.edu;https://research.facebook.com;https://research.salesforce.com",
        "aff_unique_abbr": "Georgia Tech;UCSD;FAIR;Salesforce",
        "aff_campus_unique_index": "1;1;;;",
        "aff_campus_unique": ";San Diego",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.47",
        "title": "SPARTA: Efficient Open-Domain Question Answering via Sparse Transformer Matching Retrieval",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We introduce SPARTA, a novel neural retrieval method that shows great promise in performance, generalization, and interpretability for open-domain question answering. Unlike many neural ranking methods that use dense vector nearest neighbor search, SPARTA learns a sparse representation that can be efficiently implemented as an Inverted Index. The resulting representation enables scalable neural retrieval that does not require expensive approximate vector search and leads to better performance than its dense counterpart. We validated our approaches on 4 open-domain question answering (OpenQA) tasks and 11 retrieval question answering (ReQA) tasks. SPARTA achieves new state-of-the-art results across a variety of open-domain question answering tasks in both English and Chinese datasets, including open SQuAD, CMRC and etc. Analysis also confirms that the proposed method creates human interpretable representation and allows flexible control over the trade-off between performance and efficiency.",
        "author": "Tiancheng Zhao; Xiaopeng Lu; Kyusong Lee",
        "authorids": "/t/tiancheng-zhao/; /x/xiaopeng-lu/; /k/kyusong-lee/",
        "bibtex": "@inproceedings{zhao-etal-2021-sparta,\n    title = \"{SPARTA}: Efficient Open-Domain Question Answering via Sparse Transformer Matching Retrieval\",\n    author = \"Zhao, Tiancheng  and\n      Lu, Xiaopeng  and\n      Lee, Kyusong\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.47/\",\n    doi = \"10.18653/v1/2021.naacl-main.47\",\n    pages = \"565--575\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.47.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.47/",
        "pdf_size": 470226,
        "gs_citation": 92,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4830504991433932793&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "SOCO Inc. + Language Technologies Institute, Carnegie Mellon University; Language Technologies Institute, Carnegie Mellon University; SOCO Inc.",
        "aff_domain": "soco.ai;soco.ai;andrew.cmu.edu",
        "email": "soco.ai;soco.ai;andrew.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;1;0",
        "aff_unique_norm": "SOCO Inc.;Carnegie Mellon University",
        "aff_unique_dep": ";Language Technologies Institute",
        "aff_unique_url": ";https://www.cmu.edu",
        "aff_unique_abbr": ";CMU",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Pittsburgh",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.364",
        "title": "SPARTQA: A Textual Question Answering Benchmark for Spatial Reasoning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "This paper proposes a question-answering (QA) benchmark for spatial reasoning on natural language text which contains more realistic spatial phenomena not covered by prior work and is challenging for state-of-the-art language models (LM). We propose a distant supervision method to improve on this task. Specifically, we design grammar and reasoning rules to automatically generate a spatial description of visual scenes and corresponding QA pairs. Experiments show that further pretraining LMs on these automatically generated data significantly improves LMs\u2019 capability on spatial understanding, which in turn helps to better solve two external datasets, bAbI, and boolQ. We hope that this work can foster investigations into more sophisticated models for spatial reasoning over text.",
        "author": "Roshanak Mirzaee; Hossein Rajaby Faghihi; Qiang Ning; Parisa Kordjamshidi",
        "authorids": "/r/roshanak-mirzaee/; /h/hossein-rajaby-faghihi/; /q/qiang-ning/; /p/parisa-kordjamshidi/",
        "bibtex": "@inproceedings{mirzaee-etal-2021-spartqa,\n    title = \"{SPARTQA}: A Textual Question Answering Benchmark for Spatial Reasoning\",\n    author = \"Mirzaee, Roshanak  and\n      Rajaby Faghihi, Hossein  and\n      Ning, Qiang  and\n      Kordjamshidi, Parisa\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.364/\",\n    doi = \"10.18653/v1/2021.naacl-main.364\",\n    pages = \"4582--4598\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.364.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.364/",
        "pdf_size": 1428274,
        "gs_citation": 93,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9915099857230319657&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Michigan State University; Michigan State University; Amazon; Michigan State University",
        "aff_domain": "msu.edu;msu.edu;amazon.com;msu.edu",
        "email": "msu.edu;msu.edu;amazon.com;msu.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Michigan State University;Amazon.com, Inc.",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.msu.edu;https://www.amazon.com",
        "aff_unique_abbr": "MSU;Amazon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.152",
        "title": "SPLAT: Speech-Language Joint Pre-Training for Spoken Language Understanding",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Spoken language understanding (SLU) requires a model to analyze input acoustic signal to understand its linguistic content and make predictions. To boost the models\u2019 performance, various pre-training methods have been proposed to learn rich representations from large-scale unannotated speech and text. However, the inherent disparities between the two modalities necessitate a mutual analysis. In this paper, we propose a novel semi-supervised learning framework, SPLAT, to jointly pre-train the speech and language modules. Besides conducting a self-supervised masked language modeling task on the two individual modules using unpaired speech and text, SPLAT aligns representations from the two modules in a shared latent space using a small amount of paired speech and text. Thus, during fine-tuning, the speech module alone can produce representations carrying both acoustic information and contextual semantic knowledge of an input acoustic signal. Experimental results verify the effectiveness of our approach on various SLU tasks. For example, SPLAT improves the previous state-of-the-art performance on the Spoken SQuAD dataset by more than 10%.",
        "author": "Yu-An Chung; Chenguang Zhu; Michael Zeng",
        "authorids": "/y/yu-an-chung/; /c/chenguang-zhu/; /m/michael-zeng/",
        "bibtex": "@inproceedings{chung-etal-2021-splat,\n    title = \"{SPLAT}: Speech-Language Joint Pre-Training for Spoken Language Understanding\",\n    author = \"Chung, Yu-An  and\n      Zhu, Chenguang  and\n      Zeng, Michael\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.152/\",\n    doi = \"10.18653/v1/2021.naacl-main.152\",\n    pages = \"1897--1907\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.152.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.152/",
        "pdf_size": 667543,
        "gs_citation": 81,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=513208677605873024&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "MIT Computer Science and Artificial Intelligence Laboratory; Microsoft Cognitive Services Group; Microsoft Cognitive Services Group",
        "aff_domain": "mit.edu;microsoft.com;microsoft.com",
        "email": "mit.edu;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Massachusetts Institute of Technology;Microsoft",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory;Cognitive Services Group",
        "aff_unique_url": "https://www.csail.mit.edu;https://www.microsoft.com",
        "aff_unique_abbr": "MIT CSAIL;Microsoft",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.369",
        "title": "Scalable and Interpretable Semantic Change Detection",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Several cluster-based methods for semantic change detection with contextual embeddings emerged recently. They allow a fine-grained analysis of word use change by aggregating embeddings into clusters that reflect the different usages of the word. However, these methods are unscalable in terms of memory consumption and computation time. Therefore, they require a limited set of target words to be picked in advance. This drastically limits the usability of these methods in open exploratory tasks, where each word from the vocabulary can be considered as a potential target. We propose a novel scalable method for word usage-change detection that offers large gains in processing time and significant memory savings while offering the same interpretability and better performance than unscalable methods. We demonstrate the applicability of the proposed method by analysing a large corpus of news articles about COVID-19.",
        "author": "Syrielle Montariol; Matej Martinc; Lidia Pivovarova",
        "authorids": "/s/syrielle-montariol/; /m/matej-martinc/; /l/lidia-pivovarova/",
        "bibtex": "@inproceedings{montariol-etal-2021-scalable,\n    title = \"Scalable and Interpretable Semantic Change Detection\",\n    author = \"Montariol, Syrielle  and\n      Martinc, Matej  and\n      Pivovarova, Lidia\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.369/\",\n    doi = \"10.18653/v1/2021.naacl-main.369\",\n    pages = \"4642--4652\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.369.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.369/",
        "pdf_size": 459959,
        "gs_citation": 82,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6889955644278733585&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "LISN - CNRS, Univ. Paris-Saclay + Societ\u00e9 G\u00e9n\u00e9rale; Jozef Stefan Institute; University of Helsinki",
        "aff_domain": "limsi.fr;ijs.si;helsinki.fi",
        "email": "limsi.fr;ijs.si;helsinki.fi",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;2;3",
        "aff_unique_norm": "CNRS;Soci\u00e9t\u00e9 G\u00e9n\u00e9rale;Jozef Stefan Institute;University of Helsinki",
        "aff_unique_dep": "LISN;;;",
        "aff_unique_url": "https://www.cnrs.fr;https://www.societegenerale.fr;https://www.ijs.si;https://www.helsinki.fi",
        "aff_unique_abbr": "CNRS;SG;JSI;UH",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;1;2",
        "aff_country_unique": "France;Slovenia;Finland"
    },
    {
        "id": "2021.naacl-main.370",
        "title": "Scalar Adjective Identification and Multilingual Ranking",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The intensity relationship that holds between scalar adjectives (e.g., nice < great < wonderful) is highly relevant for natural language inference and common-sense reasoning. Previous research on scalar adjective ranking has focused on English, mainly due to the availability of datasets for evaluation. We introduce a new multilingual dataset in order to promote research on scalar adjectives in new languages. We perform a series of experiments and set performance baselines on this dataset, using monolingual and multilingual contextual language models. Additionally, we introduce a new binary classification task for English scalar adjective identification which examines the models\u2019 ability to distinguish scalar from relational adjectives. We probe contextualised representations and report baseline results for future comparison on this task.",
        "author": "Aina Gar\u00ed Soler; Marianna Apidianaki",
        "authorids": "/a/aina-gari-soler/; /m/marianna-apidianaki/",
        "bibtex": "@inproceedings{gari-soler-apidianaki-2021-scalar,\n    title = \"Scalar Adjective Identification and Multilingual Ranking\",\n    author = \"Gar{\\'i} Soler, Aina  and\n      Apidianaki, Marianna\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.370/\",\n    doi = \"10.18653/v1/2021.naacl-main.370\",\n    pages = \"4653--4660\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.370.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.370/",
        "pdf_size": 274148,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2350622508772045171&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Universit\u00e9 Paris-Saclay+CNRS, LISN; Department of Digital Humanities, University of Helsinki",
        "aff_domain": "limsi.fr;helsinki.fi",
        "email": "limsi.fr;helsinki.fi",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;2",
        "aff_unique_norm": "Universit\u00e9 Paris-Saclay;CNRS;University of Helsinki",
        "aff_unique_dep": ";LISN;Department of Digital Humanities",
        "aff_unique_url": "https://www.universite-paris-saclay.fr;https://www.cnrs.fr;https://www.helsinki.fi",
        "aff_unique_abbr": "UPSaclay;CNRS;UH",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;1",
        "aff_country_unique": "France;Finland"
    },
    {
        "id": "2021.naacl-main.151",
        "title": "Searchable Hidden Intermediates for End-to-End Models of Decomposable Sequence Tasks",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "End-to-end approaches for sequence tasks are becoming increasingly popular. Yet for complex sequence tasks, like speech translation, systems that cascade several models trained on sub-tasks have shown to be superior, suggesting that the compositionality of cascaded systems simplifies learning and enables sophisticated search capabilities. In this work, we present an end-to-end framework that exploits compositionality to learn searchable hidden representations at intermediate stages of a sequence model using decomposed sub-tasks. These hidden intermediates can be improved using beam search to enhance the overall performance and can also incorporate external models at intermediate stages of the network to re-score or adapt towards out-of-domain data. One instance of the proposed framework is a Multi-Decoder model for speech translation that extracts the searchable hidden intermediates from a speech recognition sub-task. The model demonstrates the aforementioned benefits and outperforms the previous state-of-the-art by around +6 and +3 BLEU on the two test sets of Fisher-CallHome and by around +3 and +4 BLEU on the English-German and English-French test sets of MuST-C.",
        "author": "Siddharth Dalmia; Brian Yan; Vikas Raunak; Florian Metze; Shinji Watanabe",
        "authorids": "/s/siddharth-dalmia/; /b/brian-yan/; /v/vikas-raunak/; /f/florian-metze/; /s/shinji-watanabe/",
        "bibtex": "@inproceedings{dalmia-etal-2021-searchable,\n    title = \"Searchable Hidden Intermediates for End-to-End Models of Decomposable Sequence Tasks\",\n    author = \"Dalmia, Siddharth  and\n      Yan, Brian  and\n      Raunak, Vikas  and\n      Metze, Florian  and\n      Watanabe, Shinji\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.151/\",\n    doi = \"10.18653/v1/2021.naacl-main.151\",\n    pages = \"1882--1896\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.151.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.151/",
        "pdf_size": 968364,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13818293975386739757&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Language Technologies Institute, Carnegie Mellon University, USA; Language Technologies Institute, Carnegie Mellon University, USA; Language Technologies Institute, Carnegie Mellon University, USA; Language Technologies Institute, Carnegie Mellon University, USA; Language Technologies Institute, Carnegie Mellon University, USA",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu; ; ; ",
        "email": "cs.cmu.edu;cs.cmu.edu; ; ; ",
        "github": "https://github.com/espnet/espnet",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Language Technologies Institute",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.388",
        "title": "Self Promotion in US Congressional Tweets",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Prior studies have found that women self-promote less than men due to gender stereotypes. In this study we built a BERT-based NLP model to predict whether a Congressional tweet shows self-promotion or not and then used this model to examine whether a gender gap in self-promotion exists among Congressional tweets. After analyzing 2 million Congressional tweets from July 2017 to March 2021, controlling for a number of factors that include political party, chamber, age, number of terms in Congress, number of daily tweets, and number of followers, we found that women in Congress actually perform more self-promotion on Twitter, indicating a reversal of traditional gender norms where women self-promote less than men.",
        "author": "Jun Wang; Kelly Cui; Bei Yu",
        "authorids": "/j/jun-wang/; /k/kelly-cui/; /b/bei-yu/",
        "bibtex": "@inproceedings{wang-etal-2021-self,\n    title = \"Self Promotion in {US} Congressional Tweets\",\n    author = \"Wang, Jun  and\n      Cui, Kelly  and\n      Yu, Bei\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.388/\",\n    doi = \"10.18653/v1/2021.naacl-main.388\",\n    pages = \"4893--4899\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.388.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.388/",
        "pdf_size": 380334,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9872015404662130768&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Independent Researcher, Syracuse, New York; Leland High School, San Jose, California; School of Information Studies, Syracuse University, New York",
        "aff_domain": "gmail.com;gmail.com;syr.edu",
        "email": "gmail.com;gmail.com;syr.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Independent Researcher;Leland High School;Syracuse University",
        "aff_unique_dep": ";;School of Information Studies",
        "aff_unique_url": ";https://www.lelandhs.org;https://www.syracuse.edu",
        "aff_unique_abbr": ";;Syracuse",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";San Jose;New York",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.334",
        "title": "Self-Alignment Pretraining for Biomedical Entity Representations",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Despite the widespread success of self-supervised learning via masked language models (MLM), accurately capturing fine-grained semantic relationships in the biomedical domain remains a challenge. This is of paramount importance for entity-level tasks such as entity linking where the ability to model entity relations (especially synonymy) is pivotal. To address this challenge, we propose SapBERT, a pretraining scheme that self-aligns the representation space of biomedical entities. We design a scalable metric learning framework that can leverage UMLS, a massive collection of biomedical ontologies with 4M+ concepts. In contrast with previous pipeline-based hybrid systems, SapBERT offers an elegant one-model-for-all solution to the problem of medical entity linking (MEL), achieving a new state-of-the-art (SOTA) on six MEL benchmarking datasets. In the scientific domain, we achieve SOTA even without task-specific supervision. With substantial improvement over various domain-specific pretrained MLMs such as BioBERT, SciBERTand and PubMedBERT, our pretraining scheme proves to be both effective and robust.",
        "author": "Fangyu Liu; Ehsan Shareghi; Zaiqiao Meng; Marco Basaldella; Nigel Collier",
        "authorids": "/f/fangyu-liu/; /e/ehsan-shareghi/; /z/zaiqiao-meng/; /m/marco-basaldella/; /n/nigel-collier/",
        "bibtex": "@inproceedings{liu-etal-2021-self,\n    title = \"Self-Alignment Pretraining for Biomedical Entity Representations\",\n    author = \"Liu, Fangyu  and\n      Shareghi, Ehsan  and\n      Meng, Zaiqiao  and\n      Basaldella, Marco  and\n      Collier, Nigel\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.334/\",\n    doi = \"10.18653/v1/2021.naacl-main.334\",\n    pages = \"4228--4238\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.334.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.334/",
        "pdf_size": 14213952,
        "gs_citation": 371,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5826728826174879811&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Language Technology Lab, TAL, University of Cambridge + Amazon Alexa; Department of Data Science & AI, Monash University + Language Technology Lab, TAL, University of Cambridge; Language Technology Lab, TAL, University of Cambridge; Amazon Alexa; Language Technology Lab, TAL, University of Cambridge",
        "aff_domain": "cam.ac.uk;monash.edu;cam.ac.uk;amazon.co.uk;cam.ac.uk",
        "email": "cam.ac.uk;monash.edu;cam.ac.uk;amazon.co.uk;cam.ac.uk",
        "github": "https://github.com/cambridgeltl/sapbert",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;2+0;0;1;0",
        "aff_unique_norm": "University of Cambridge;Amazon;Monash University",
        "aff_unique_dep": "Language Technology Lab, TAL;Amazon Alexa;Department of Data Science & AI",
        "aff_unique_url": "https://www.cam.ac.uk;https://www.amazon.com/alexa;https://www.monash.edu",
        "aff_unique_abbr": "Cambridge;Amazon Alexa;Monash",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0+1;2+0;0;1;0",
        "aff_country_unique": "United Kingdom;United States;Australia"
    },
    {
        "id": "2021.naacl-main.319",
        "title": "Self-Supervised Contrastive Learning for Efficient User Satisfaction Prediction in Conversational Agents",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Turn-level user satisfaction is one of the most important performance metrics for conversational agents. It can be used to monitor the agent\u2019s performance and provide insights about defective user experiences. While end-to-end deep learning has shown promising results, having access to a large number of reliable annotated samples required by these methods remains challenging. In a large-scale conversational system, there is a growing number of newly developed skills, making the traditional data collection, annotation, and modeling process impractical due to the required annotation costs and the turnaround times. In this paper, we suggest a self-supervised contrastive learning approach that leverages the pool of unlabeled data to learn user-agent interactions. We show that the pre-trained models using the self-supervised objective are transferable to the user satisfaction prediction. In addition, we propose a novel few-shot transfer learning approach that ensures better transferability for very small sample sizes. The suggested few-shot method does not require any inner loop optimization process and is scalable to very large datasets and complex models. Based on our experiments using real data from a large-scale commercial system, the suggested approach is able to significantly reduce the required number of annotations, while improving the generalization on unseen skills.",
        "author": "Mohammad Kachuee; Hao Yuan; Young-Bum Kim; Sungjin Lee",
        "authorids": "/m/mohammad-kachuee/; /h/hao-yuan/; /y/young-bum-kim/; /s/sungjin-lee/",
        "bibtex": "@inproceedings{kachuee-etal-2021-self,\n    title = \"Self-Supervised Contrastive Learning for Efficient User Satisfaction Prediction in Conversational Agents\",\n    author = \"Kachuee, Mohammad  and\n      Yuan, Hao  and\n      Kim, Young-Bum  and\n      Lee, Sungjin\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.319/\",\n    doi = \"10.18653/v1/2021.naacl-main.319\",\n    pages = \"4053--4064\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.319.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.319/",
        "pdf_size": 551819,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11983296429529946636&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "UCLA Computer Science; Amazon Alexa AI; Amazon Alexa AI; Amazon Alexa AI",
        "aff_domain": "cs.ucla.edu;amazon.com;amazon.com;amazon.com",
        "email": "cs.ucla.edu;amazon.com;amazon.com;amazon.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "University of California, Los Angeles;Amazon",
        "aff_unique_dep": "Computer Science;Alexa AI",
        "aff_unique_url": "https://www.ucla.edu;https://www.amazon.com",
        "aff_unique_abbr": "UCLA;Amazon",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.95",
        "title": "Self-Supervised Test-Time Learning for Reading Comprehension",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent work on unsupervised question answering has shown that models can be trained with procedurally generated question-answer pairs and can achieve performance competitive with supervised methods. In this work, we consider the task of unsupervised reading comprehension and present a method that performs \u201ctest-time learning\u201d (TTL) on a given context (text passage), without requiring training on large-scale human-authored datasets containing context-question-answer triplets. This method operates directly on a single test context, uses self-supervision to train models on synthetically generated question-answer pairs, and then infers answers to unseen human-authored questions for this context. Our method achieves accuracies competitive with fully supervised methods and significantly outperforms current unsupervised methods. TTL methods with a smaller model are also competitive with the current state-of-the-art in unsupervised reading comprehension.",
        "author": "Pratyay Banerjee; Tejas Gokhale; Chitta Baral",
        "authorids": "/p/pratyay-banerjee/; /t/tejas-gokhale/; /c/chitta-baral/",
        "bibtex": "@inproceedings{banerjee-etal-2021-self,\n    title = \"Self-Supervised Test-Time Learning for Reading Comprehension\",\n    author = \"Banerjee, Pratyay  and\n      Gokhale, Tejas  and\n      Baral, Chitta\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.95/\",\n    doi = \"10.18653/v1/2021.naacl-main.95\",\n    pages = \"1200--1211\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.95.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.95/",
        "pdf_size": 1194294,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7170102090609189822&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Arizona State University; Arizona State University; Arizona State University",
        "aff_domain": "asu.edu;asu.edu;asu.edu",
        "email": "asu.edu;asu.edu;asu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Arizona State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.asu.edu",
        "aff_unique_abbr": "ASU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.311",
        "title": "Self-Training for Unsupervised Neural Machine Translation in Unbalanced Training Data Scenarios",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Unsupervised neural machine translation (UNMT) that relies solely on massive monolingual corpora has achieved remarkable results in several translation tasks. However, in real-world scenarios, massive monolingual corpora do not exist for some extremely low-resource languages such as Estonian, and UNMT systems usually perform poorly when there is not adequate training corpus for one language. In this paper, we first define and analyze the unbalanced training data scenario for UNMT. Based on this scenario, we propose UNMT self-training mechanisms to train a robust UNMT system and improve its performance in this case. Experimental results on several language pairs show that the proposed methods substantially outperform conventional UNMT systems.",
        "author": "Haipeng Sun; Rui Wang; Kehai Chen; Masao Utiyama; Eiichiro Sumita; Tiejun Zhao",
        "authorids": "/h/haipeng-sun/; /r/rui-wang/; /k/kehai-chen/; /m/masao-utiyama/; /e/eiichiro-sumita/; /t/tiejun-zhao/",
        "bibtex": "@inproceedings{sun-etal-2021-self,\n    title = \"Self-Training for Unsupervised Neural Machine Translation in Unbalanced Training Data Scenarios\",\n    author = \"Sun, Haipeng  and\n      Wang, Rui  and\n      Chen, Kehai  and\n      Utiyama, Masao  and\n      Sumita, Eiichiro  and\n      Zhao, Tiejun\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.311/\",\n    doi = \"10.18653/v1/2021.naacl-main.311\",\n    pages = \"3975--3981\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.311.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.311/",
        "pdf_size": 299188,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14408925455170734361&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Harbin Institute of Technology, Harbin, China+JD AI Research, Beijing, China; Shanghai Jiao Tong University, Shanghai, China; National Institute of Information and Communications Technology (NICT), Kyoto, Japan; National Institute of Information and Communications Technology (NICT), Kyoto, Japan; National Institute of Information and Communications Technology (NICT), Kyoto, Japan; Harbin Institute of Technology, Harbin, China",
        "aff_domain": "jd.com;gmail.com;nict.go.jp;nict.go.jp;nict.go.jp;hit.edu.cn",
        "email": "jd.com;gmail.com;nict.go.jp;nict.go.jp;nict.go.jp;hit.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;2;3;3;3;0",
        "aff_unique_norm": "Harbin Institute of Technology;JD AI Research;Shanghai Jiao Tong University;National Institute of Information and Communications Technology",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "http://www.hit.edu.cn/;;https://www.sjtu.edu.cn;https://www.nict.go.jp/",
        "aff_unique_abbr": "HIT;;SJTU;NICT",
        "aff_campus_unique_index": "0+1;2;3;3;3;0",
        "aff_campus_unique": "Harbin;Beijing;Shanghai;Kyoto",
        "aff_country_unique_index": "0+0;0;1;1;1;0",
        "aff_country_unique": "China;Japan"
    },
    {
        "id": "2021.naacl-main.66",
        "title": "Self-Training with Weak Supervision",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "State-of-the-art deep neural networks require large-scale labeled training data that is often expensive to obtain or not available for many tasks. Weak supervision in the form of domain-specific rules has been shown to be useful in such settings to automatically generate weakly labeled training data. However, learning with weak rules is challenging due to their inherent heuristic and noisy nature. An additional challenge is rule coverage and overlap, where prior work on weak supervision only considers instances that are covered by weak rules, thus leaving valuable unlabeled data behind. In this work, we develop a weak supervision framework (ASTRA) that leverages all the available data for a given task. To this end, we leverage task-specific unlabeled data through self-training with a model (student) that considers contextualized representations and predicts pseudo-labels for instances that may not be covered by weak rules. We further develop a rule attention network (teacher) that learns how to aggregate student pseudo-labels with weak rule labels, conditioned on their fidelity and the underlying context of an instance. Finally, we construct a semi-supervised learning objective for end-to-end training with unlabeled data, domain-specific rules, and a small amount of labeled data. Extensive experiments on six benchmark datasets for text classification demonstrate the effectiveness of our approach with significant improvements over state-of-the-art baselines.",
        "author": "Giannis Karamanolakis; Subhabrata Mukherjee; Guoqing Zheng; Ahmed Hassan Awadallah",
        "authorids": "/g/giannis-karamanolakis/; /s/subhabrata-mukherjee/; /g/guoqing-zheng/; /a/ahmed-hassan/",
        "bibtex": "@inproceedings{karamanolakis-etal-2021-self,\n    title = \"Self-Training with Weak Supervision\",\n    author = \"Karamanolakis, Giannis  and\n      Mukherjee, Subhabrata  and\n      Zheng, Guoqing  and\n      Awadallah, Ahmed Hassan\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.66/\",\n    doi = \"10.18653/v1/2021.naacl-main.66\",\n    pages = \"845--863\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.66.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.66/",
        "pdf_size": 1621552,
        "gs_citation": 102,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16760382474323728416&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Columbia University, New York; Microsoft Research; Microsoft Research; Microsoft Research",
        "aff_domain": "cs.columbia.edu;microsoft.com;microsoft.com;microsoft.com",
        "email": "cs.columbia.edu;microsoft.com;microsoft.com;microsoft.com",
        "github": "https://github.com/microsoft/ASTRA",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "Columbia University;Microsoft Corporation",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.columbia.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "Columbia;MSR",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "New York;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.426",
        "title": "Self-training Improves Pre-training for Natural Language Understanding",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Unsupervised pre-training has led to much recent progress in natural language understanding. In this paper, we study self-training as another way to leverage unlabeled data through semi-supervised learning. To obtain additional data for a specific task, we introduce SentAugment, a data augmentation method which computes task-specific query embeddings from labeled data to retrieve sentences from a bank of billions of unlabeled sentences crawled from the web. Unlike previous semi-supervised methods, our approach does not require in-domain unlabeled data and is therefore more generally applicable. Experiments show that self-training is complementary to strong RoBERTa baselines on a variety of tasks. Our augmentation approach leads to scalable and effective self-training with improvements of up to 2.6% on standard text classification benchmarks. Finally, we also show strong gains on knowledge-distillation and few-shot learning.",
        "author": "Jingfei Du; Edouard Grave; Beliz Gunel; Vishrav Chaudhary; Onur Celebi; Michael Auli; Veselin Stoyanov; Alexis Conneau",
        "authorids": "/j/jingfei-du/; /e/edouard-grave/; /b/beliz-gunel/; /v/vishrav-chaudhary/; /o/onur-celebi/; /m/michael-auli/; /v/veselin-stoyanov/; /a/alexis-conneau/",
        "bibtex": "@inproceedings{du-etal-2021-self,\n    title = \"Self-training Improves Pre-training for Natural Language Understanding\",\n    author = \"Du, Jingfei  and\n      Grave, Edouard  and\n      Gunel, Beliz  and\n      Chaudhary, Vishrav  and\n      Celebi, Onur  and\n      Auli, Michael  and\n      Stoyanov, Veselin  and\n      Conneau, Alexis\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.426/\",\n    doi = \"10.18653/v1/2021.naacl-main.426\",\n    pages = \"5408--5418\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.426.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.426/",
        "pdf_size": 1765118,
        "gs_citation": 183,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8353188978644599903&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Facebook AI\u2020; Facebook AI\u2020; Stanford University\u2021; Facebook AI\u2020; Facebook AI\u2020; Facebook AI\u2020; Facebook AI\u2020; Facebook AI\u2020",
        "aff_domain": "fb.com;fb.com;stanford.edu;fb.com;fb.com;fb.com;fb.com;fb.com",
        "email": "fb.com;fb.com;stanford.edu;fb.com;fb.com;fb.com;fb.com;fb.com",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;1;0;0;0;0;0",
        "aff_unique_norm": "Facebook;Stanford University",
        "aff_unique_dep": "Facebook AI;",
        "aff_unique_url": "https://www.facebook.com;https://www.stanford.edu",
        "aff_unique_abbr": "Facebook AI;Stanford",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Stanford",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.215",
        "title": "Semantic Frame Forecast",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "This paper introduces Semantic Frame Forecast, a task that predicts the semantic frames that will occur in the next 10, 100, or even 1,000 sentences in a running story. Prior work focused on predicting the immediate future of a story, such as one to a few sentences ahead. However, when novelists write long stories, generating a few sentences is not enough to help them gain high-level insight to develop the follow-up story. In this paper, we formulate a long story as a sequence of \u201cstory blocks,\u201d where each block contains a fixed number of sentences (e.g., 10, 100, or 200). This formulation allows us to predict the follow-up story arc beyond the scope of a few sentences. We represent a story block using the term frequencies (TF) of semantic frames in it, normalized by each frame\u2019s inverse document frequency (IDF). We conduct semantic frame forecast experiments on 4,794 books from the Bookcorpus and 7,962 scientific abstracts from CODA-19, with block sizes ranging from 5 to 1,000 sentences. The results show that automated models can forecast the follow-up story blocks better than the random, prior, and replay baselines, indicating the feasibility of the task. We also learn that the models using the frame representation as features outperform all the existing approaches when the block size is over 150 sentences. The human evaluation also shows that the proposed frame representation, when visualized as word clouds, is comprehensible, representative, and specific to humans.",
        "author": "Chieh-Yang Huang; Ting-Hao Huang",
        "authorids": "/c/chieh-yang-huang/; /t/ting-hao-huang/",
        "bibtex": "@inproceedings{huang-huang-2021-semantic,\n    title = \"Semantic Frame Forecast\",\n    author = \"Huang, Chieh-Yang  and\n      Huang, Ting-Hao\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.215/\",\n    doi = \"10.18653/v1/2021.naacl-main.215\",\n    pages = \"2702--2713\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.215.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.215/",
        "pdf_size": 821797,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10448953314668745225&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Pennsylvania State University, University Park, PA 16802, USA; Pennsylvania State University, University Park, PA 16802, USA",
        "aff_domain": "psu.edu;psu.edu",
        "email": "psu.edu;psu.edu",
        "github": "https://github.com/appleternity/FrameForecasting",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Pennsylvania State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.psu.edu",
        "aff_unique_abbr": "PSU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "University Park",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.249",
        "title": "Semi-Supervised Policy Initialization for Playing Games with Language Hints",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Using natural language as a hint can supply an additional reward for playing sparse-reward games. Achieving a goal should involve several different hints, while the given hints are usually incomplete. Those unmentioned latent hints still rely on the sparse reward signal, and make the learning process difficult. In this paper, we propose semi-supervised initialization (SSI) that allows the agent to learn from various possible hints before training under different tasks. Experiments show that SSI not only helps to learn faster (1.2x) but also has a higher success rate (11% relative improvement) of the final policy.",
        "author": "Tsu-Jui Fu; William Yang Wang",
        "authorids": "/t/tsu-jui-fu/; /w/william-yang-wang/",
        "bibtex": "@inproceedings{fu-wang-2021-semi,\n    title = \"Semi-Supervised Policy Initialization for Playing Games with Language Hints\",\n    author = \"Fu, Tsu-Jui  and\n      Wang, William Yang\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.249/\",\n    doi = \"10.18653/v1/2021.naacl-main.249\",\n    pages = \"3112--3116\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.249.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.249/",
        "pdf_size": 1320679,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=609205965775297207&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "UC Santa Barbara; UC Santa Barbara",
        "aff_domain": "cs.ucsb.edu;cs.ucsb.edu",
        "email": "cs.ucsb.edu;cs.ucsb.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Santa Barbara",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ucsb.edu",
        "aff_unique_abbr": "UCSB",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Santa Barbara",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.252",
        "title": "SentSim: Crosslingual Semantic Evaluation of Machine Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Machine translation (MT) is currently evaluated in one of two ways: in a monolingual fashion, by comparison with the system output to one or more human reference translations, or in a trained crosslingual fashion, by building a supervised model to predict quality scores from human-labeled data. In this paper, we propose a more cost-effective, yet well performing unsupervised alternative SentSim: relying on strong pretrained multilingual word and sentence representations, we directly compare the source with the machine translated sentence, thus avoiding the need for both reference translations and labelled training data. The metric builds on state-of-the-art embedding-based approaches \u2013 namely BERTScore and Word Mover\u2019s Distance \u2013 by incorporating a notion of sentence semantic similarity. By doing so, it achieves better correlation with human scores on different datasets. We show that it outperforms these and other metrics in the standard monolingual setting (MT-reference translation), a well as in the source-MT bilingual setting, where it performs on par with glass-box approaches to quality estimation that rely on MT model information.",
        "author": "Yurun Song; Junchen Zhao; Lucia Specia",
        "authorids": "/y/yurun-song/; /j/junchen-zhao/; /l/lucia-specia/",
        "bibtex": "@inproceedings{song-etal-2021-sentsim,\n    title = \"{S}ent{S}im: Crosslingual Semantic Evaluation of Machine Translation\",\n    author = \"Song, Yurun  and\n      Zhao, Junchen  and\n      Specia, Lucia\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.252/\",\n    doi = \"10.18653/v1/2021.naacl-main.252\",\n    pages = \"3143--3156\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.252.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.252/",
        "pdf_size": 503657,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14611769696422888971&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Imperial College London, UK; University of California, Irvine, CA, US; Imperial College London, UK",
        "aff_domain": "imperial.ac.uk;uci.edu;imperial.ac.uk",
        "email": "imperial.ac.uk;uci.edu;imperial.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Imperial College London;University of California, Irvine",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.imperial.ac.uk;https://www.uci.edu",
        "aff_unique_abbr": "ICL;UCI",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Irvine",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "2021.naacl-main.375",
        "title": "Seq2Emo: A Sequence to Multi-Label Emotion Classification Model",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Multi-label emotion classification is an important task in NLP and is essential to many applications. In this work, we propose a sequence-to-emotion (Seq2Emo) approach, which implicitly models emotion correlations in a bi-directional decoder. Experiments on SemEval\u201918 and GoEmotions datasets show that our approach outperforms state-of-the-art methods (without using external data). In particular, Seq2Emo outperforms the binary relevance (BR) and classifier chain (CC) approaches in a fair setting.",
        "author": "Chenyang Huang; Amine Trabelsi; Xuebin Qin; Nawshad Farruque; Lili Mou; Osmar Za\u00efane",
        "authorids": "/c/chenyang-huang/; /a/amine-trabelsi/; /x/xuebin-qin/; /n/nawshad-farruque/; /l/lili-mou/; /o/osmar-r-zaiane/",
        "bibtex": "@inproceedings{huang-etal-2021-seq2emo,\n    title = \"{S}eq2{E}mo: A Sequence to Multi-Label Emotion Classification Model\",\n    author = {Huang, Chenyang  and\n      Trabelsi, Amine  and\n      Qin, Xuebin  and\n      Farruque, Nawshad  and\n      Mou, Lili  and\n      Za{\\\"i}ane, Osmar},\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.375/\",\n    doi = \"10.18653/v1/2021.naacl-main.375\",\n    pages = \"4717--4724\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.375.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.375/",
        "pdf_size": 469544,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16437716276708990359&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Alberta Machine Intelligence Institute; Alberta Machine Intelligence Institute; Alberta Machine Intelligence Institute; Alberta Machine Intelligence Institute; Alberta Machine Intelligence Institute; Alberta Machine Intelligence Institute",
        "aff_domain": "ualberta.ca;ualberta.ca;ualberta.ca;ualberta.ca;gmail.com;ualberta.ca",
        "email": "ualberta.ca;ualberta.ca;ualberta.ca;ualberta.ca;gmail.com;ualberta.ca",
        "github": "https://github.com/chenyangh/Seq2Emo",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Alberta Machine Intelligence Institute",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ami.ualberta.ca/",
        "aff_unique_abbr": "AMII",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2021.naacl-main.441",
        "title": "ShadowGNN: Graph Projection Neural Network for Text-to-SQL Parser",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Given a database schema, Text-to-SQL aims to translate a natural language question into the corresponding SQL query. Under the setup of cross-domain, traditional semantic parsing models struggle to adapt to unseen database schemas. To improve the model generalization capability for rare and unseen schemas, we propose a new architecture, ShadowGNN, which processes schemas at abstract and semantic levels. By ignoring names of semantic items in databases, abstract schemas are exploited in a well-designed graph projection neural network to obtain delexicalized representation of question and schema. Based on the domain-independent representations, a relation-aware transformer is utilized to further extract logical linking between question and schema. Finally, a SQL decoder with context-free grammar is applied. On the challenging Text-to-SQL benchmark Spider, empirical results show that ShadowGNN outperforms state-of-the-art models. When the annotated data is extremely limited (only 10% training set), ShadowGNN gets over absolute 5% performance gain, which shows its powerful generalization ability. Our implementation will be open-sourced at https://github.com/WowCZ/shadowgnn",
        "author": "Zhi Chen; Lu Chen; Yanbin Zhao; Ruisheng Cao; Zihan Xu; Su Zhu; Kai Yu",
        "authorids": "/z/zhi-chen/; /l/lu-chen/; /y/yanbin-zhao/; /r/ruisheng-cao/; /z/zihan-xu/; /s/su-zhu/; /k/kai-yu/",
        "bibtex": "@inproceedings{chen-etal-2021-shadowgnn,\n    title = \"{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser\",\n    author = \"Chen, Zhi  and\n      Chen, Lu  and\n      Zhao, Yanbin  and\n      Cao, Ruisheng  and\n      Xu, Zihan  and\n      Zhu, Su  and\n      Yu, Kai\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.441/\",\n    doi = \"10.18653/v1/2021.naacl-main.441\",\n    pages = \"5567--5577\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.441.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.441/",
        "pdf_size": 527042,
        "gs_citation": 58,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6029916257896516242&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "X-LANCE Lab, Department of Computer Science and Engineering, MoE Key Lab of Arti\ufb01cial Intelligence, AI Institute, Shanghai Jiao Tong University, Shanghai Jiao Tong University, Shanghai, China; X-LANCE Lab, Department of Computer Science and Engineering, MoE Key Lab of Arti\ufb01cial Intelligence, AI Institute, Shanghai Jiao Tong University, Shanghai Jiao Tong University, Shanghai, China; X-LANCE Lab, Department of Computer Science and Engineering, MoE Key Lab of Arti\ufb01cial Intelligence, AI Institute, Shanghai Jiao Tong University, Shanghai Jiao Tong University, Shanghai, China; X-LANCE Lab, Department of Computer Science and Engineering, MoE Key Lab of Arti\ufb01cial Intelligence, AI Institute, Shanghai Jiao Tong University, Shanghai Jiao Tong University, Shanghai, China; X-LANCE Lab, Department of Computer Science and Engineering, MoE Key Lab of Arti\ufb01cial Intelligence, AI Institute, Shanghai Jiao Tong University, Shanghai Jiao Tong University, Shanghai, China; AISpeech Co., Ltd., Suzhou, China; X-LANCE Lab, Department of Computer Science and Engineering, MoE Key Lab of Arti\ufb01cial Intelligence, AI Institute, Shanghai Jiao Tong University, Shanghai Jiao Tong University, Shanghai, China",
        "aff_domain": "sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn; ; ;aispeech.com;sjtu.edu.cn",
        "email": "sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn; ; ;aispeech.com;sjtu.edu.cn",
        "github": "https://github.com/WowCZ/shadowgnn",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;1;0",
        "aff_unique_norm": "Shanghai Jiao Tong University;AISpeech Co., Ltd.",
        "aff_unique_dep": "Department of Computer Science and Engineering;",
        "aff_unique_url": "https://www.sjtu.edu.cn;",
        "aff_unique_abbr": "SJTU;",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Shanghai;",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2021.naacl-industry.13",
        "title": "Should we find another model?: Improving Neural Machine Translation Performance with ONE-Piece Tokenization Method without Model Modification",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Most of the recent Natural Language Processing(NLP) studies are based on the Pretrain-Finetuning Approach (PFA), but in small and medium-sized enterprises or companies with insufficient hardware there are many limitations to servicing NLP application software using such technology due to slow speed and insufficient memory. The latest PFA technologies require large amounts of data, especially for low-resource languages, making them much more difficult to work with. We propose a new tokenization method, ONE-Piece, to address this limitation that combines the morphology-considered subword tokenization method and the vocabulary method used after probing for an existing method that has not been carefully considered before. Our proposed method can also be used without modifying the model structure. We experiment by applying ONE-Piece to Korean, a morphologically-rich and low-resource language. We derive an optimal subword tokenization result for Korean-English machine translation by conducting a case study that combines the subword tokenization method, morphological segmentation, and vocabulary method. Through comparative experiments with all the tokenization methods currently used in NLP research, ONE-Piece achieves performance comparable to the current Korean-English machine translation state-of-the-art model.",
        "author": "Chanjun Park; Sugyeong Eo; Hyeonseok Moon; Heuiseok Lim",
        "authorids": "/c/chanjun-park/; /s/sugyeong-eo/; /h/hyeonseok-moon/; /h/heui-seok-lim/",
        "bibtex": "@inproceedings{park-etal-2021-find,\n    title = \"Should we find another model?: Improving Neural Machine Translation Performance with {ONE}-Piece Tokenization Method without Model Modification\",\n    author = \"Park, Chanjun  and\n      Eo, Sugyeong  and\n      Moon, Hyeonseok  and\n      Lim, Heuiseok\",\n    editor = \"Kim, Young-bum  and\n      Li, Yunyao  and\n      Rambow, Owen\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-industry.13/\",\n    doi = \"10.18653/v1/2021.naacl-industry.13\",\n    pages = \"97--104\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-industry.13.pdf",
        "site": "https://aclanthology.org/2021.naacl-industry.13/",
        "pdf_size": 1609890,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3551282377662352005&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Korea University; Korea University; Korea University; Korea University",
        "aff_domain": "korea.ac.kr;korea.ac.kr;korea.ac.kr;korea.ac.kr",
        "email": "korea.ac.kr;korea.ac.kr;korea.ac.kr;korea.ac.kr",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Korea University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.korea.ac.kr",
        "aff_unique_abbr": "KU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2021.naacl-main.470",
        "title": "Sliding Selector Network with Dynamic Memory for Extractive Summarization of Long Documents",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Neural-based summarization models suffer from the length limitation of text encoder. Long documents have to been truncated before they are sent to the model, which results in huge loss of summary-relevant contents. To address this issue, we propose the sliding selector network with dynamic memory for extractive summarization of long-form documents, which employs a sliding window to extract summary sentences segment by segment. Moreover, we adopt memory mechanism to preserve and update the history information dynamically, allowing the semantic flow across different windows. Experimental results on two large-scale datasets that consist of scientific papers demonstrate that our model substantially outperforms previous state-of-the-art models. Besides, we perform qualitative and quantitative investigations on how our model works and where the performance gain comes from.",
        "author": "Peng Cui; Le Hu",
        "authorids": "/p/peng-cui/; /l/le-hu/",
        "bibtex": "@inproceedings{cui-hu-2021-sliding,\n    title = \"Sliding Selector Network with Dynamic Memory for Extractive Summarization of Long Documents\",\n    author = \"Cui, Peng  and\n      Hu, Le\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.470/\",\n    doi = \"10.18653/v1/2021.naacl-main.470\",\n    pages = \"5881--5891\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.470.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.470/",
        "pdf_size": 769271,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14210707208755198716&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China",
        "aff_domain": "insun.hit.edu.cn;insun.hit.edu.cn",
        "email": "insun.hit.edu.cn;insun.hit.edu.cn",
        "github": "https://github.com/pcui-nlp/SSN_DM",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Harbin Institute of Technology",
        "aff_unique_dep": "School of Computer Science and Technology",
        "aff_unique_url": "http://www.hit.edu.cn/",
        "aff_unique_abbr": "HIT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Harbin",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2021.naacl-main.29",
        "title": "SmBoP: Semi-autoregressive Bottom-up Semantic Parsing",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The de-facto standard decoding method for semantic parsing in recent years has been to autoregressively decode the abstract syntax tree of the target program using a top-down depth-first traversal. In this work, we propose an alternative approach: a Semi-autoregressive Bottom-up Parser (SmBoP) that constructs at decoding step t the top-K sub-trees of height \u2264 t. Our parser enjoys several benefits compared to top-down autoregressive parsing. From an efficiency perspective, bottom-up parsing allows to decode all sub-trees of a certain height in parallel, leading to logarithmic runtime complexity rather than linear. From a modeling perspective, a bottom-up parser learns representations for meaningful semantic sub-programs at each step, rather than for semantically-vacuous partial trees. We apply SmBoP on Spider, a challenging zero-shot semantic parsing benchmark, and show that SmBoP leads to a 2.2x speed-up in decoding time and a ~5x speed-up in training time, compared to a semantic parser that uses autoregressive decoding. SmBoP obtains 71.1 denotation accuracy on Spider, establishing a new state-of-the-art, and 69.5 exact match, comparable to the 69.6 exact match of the autoregressive RAT-SQL+GraPPa.",
        "author": "Ohad Rubin; Jonathan Berant",
        "authorids": "/o/ohad-rubin/; /j/jonathan-berant/",
        "bibtex": "@inproceedings{rubin-berant-2021-smbop,\n    title = \"{S}m{B}o{P}: Semi-autoregressive Bottom-up Semantic Parsing\",\n    author = \"Rubin, Ohad  and\n      Berant, Jonathan\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.29/\",\n    doi = \"10.18653/v1/2021.naacl-main.29\",\n    pages = \"311--324\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.29.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.29/",
        "pdf_size": 668418,
        "gs_citation": 158,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9908374039925139936&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Tel Aviv University; Tel Aviv University + Allen Institute for AI",
        "aff_domain": "mail.tau.ac.il;cs.tau.ac.il",
        "email": "mail.tau.ac.il;cs.tau.ac.il",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "Tel Aviv University;Allen Institute for AI",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tau.ac.il;https://allenai.org",
        "aff_unique_abbr": "TAU;AI2",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+1",
        "aff_country_unique": "Israel;United States"
    },
    {
        "id": "2021.naacl-main.312",
        "title": "Smart-Start Decoding for Neural Machine Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Most current neural machine translation models adopt a monotonic decoding order of either left-to-right or right-to-left. In this work, we propose a novel method that breaks up the limitation of these decoding orders, called Smart-Start decoding. More specifically, our method first predicts a median word. It starts to decode the words on the right side of the median word and then generates words on the left. We evaluate the proposed Smart-Start decoding method on three datasets. Experimental results show that the proposed method can significantly outperform strong baseline models.",
        "author": "Jian Yang; Shuming Ma; Dongdong Zhang; Juncheng Wan; Zhoujun Li; Ming Zhou",
        "authorids": "/j/jian-yang/; /s/shuming-ma/; /d/dongdong-zhang/; /j/juncheng-wan/; /z/zhoujun-li/; /m/ming-zhou/",
        "bibtex": "@inproceedings{yang-etal-2021-smart,\n    title = \"Smart-Start Decoding for Neural Machine Translation\",\n    author = \"Yang, Jian  and\n      Ma, Shuming  and\n      Zhang, Dongdong  and\n      Wan, Juncheng  and\n      Li, Zhoujun  and\n      Zhou, Ming\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.312/\",\n    doi = \"10.18653/v1/2021.naacl-main.312\",\n    pages = \"3982--3988\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.312.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.312/",
        "pdf_size": 373226,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17338436717233817530&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "State Key Lab of Software Development Environment, Beihang University; Microsoft Research Asia; Microsoft Research Asia; Shanghai Jiao Tong University; State Key Lab of Software Development Environment, Beihang University; Microsoft Research Asia",
        "aff_domain": "buaa.edu.cn;microsoft.com;microsoft.com;apex.sjtu.edu.cn;buaa.edu.cn;microsoft.com",
        "email": "buaa.edu.cn;microsoft.com;microsoft.com;apex.sjtu.edu.cn;buaa.edu.cn;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;1;2;0;1",
        "aff_unique_norm": "Beihang University;Microsoft Research;Shanghai Jiao Tong University",
        "aff_unique_dep": "State Key Lab of Software Development Environment;Research;",
        "aff_unique_url": "http://www.buaa.edu.cn;https://www.microsoft.com/en-us/research/group/asia;https://www.sjtu.edu.cn",
        "aff_unique_abbr": "Beihang;MSR Asia;SJTU",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Asia",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2021.naacl-main.210",
        "title": "Smoothing and Shrinking the Sparse Seq2Seq Search Space",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Current sequence-to-sequence models are trained to minimize cross-entropy and use softmax to compute the locally normalized probabilities over target sequences. While this setup has led to strong results in a variety of tasks, one unsatisfying aspect is its length bias: models give high scores to short, inadequate hypotheses and often make the empty string the argmax\u2014the so-called cat got your tongue problem. Recently proposed entmax-based sparse sequence-to-sequence models present a possible solution, since they can shrink the search space by assigning zero probability to bad hypotheses, but their ability to handle word-level tasks with transformers has never been tested. In this work, we show that entmax-based models effectively solve the cat got your tongue problem, removing a major source of model error for neural machine translation. In addition, we generalize label smoothing, a critical regularization technique, to the broader family of Fenchel-Young losses, which includes both cross-entropy and the entmax losses. Our resulting label-smoothed entmax loss models set a new state of the art on multilingual grapheme-to-phoneme conversion and deliver improvements and better calibration properties on cross-lingual morphological inflection and machine translation for 7 language pairs.",
        "author": "Ben Peters; Andr\u00e9 F. T. Martins",
        "authorids": "/b/ben-peters/; /a/andre-f-t-martins/",
        "bibtex": "@inproceedings{peters-martins-2021-smoothing,\n    title = \"Smoothing and Shrinking the Sparse {S}eq2{S}eq Search Space\",\n    author = \"Peters, Ben  and\n      Martins, Andr{\\'e} F. T.\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.210/\",\n    doi = \"10.18653/v1/2021.naacl-main.210\",\n    pages = \"2642--2654\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.210.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.210/",
        "pdf_size": 511505,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14072636355988941984&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Instituto de Telecomunicac\u00f5es, Instituto Superior T\u00e9cnico, Lisbon, Portugal+LUMLIS (Lisbon ELLIS Unit), Lisbon, Portugal+Unbabel, Lisbon, Portugal; Instituto de Telecomunicac\u00f5es, Instituto Superior T\u00e9cnico, Lisbon, Portugal+LUMLIS (Lisbon ELLIS Unit), Lisbon, Portugal",
        "aff_domain": "gmail.com;tecnico.ulisboa.pt",
        "email": "gmail.com;tecnico.ulisboa.pt",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1+2;0+1",
        "aff_unique_norm": "Instituto Superior T\u00e9cnico;Lisbon ELLIS Unit;Unbabel",
        "aff_unique_dep": "Instituto de Telecomunicac\u00f5es;;",
        "aff_unique_url": "https://www.ist.utl.pt;;https://www.unbabel.com",
        "aff_unique_abbr": "IST;LUMLIS;",
        "aff_campus_unique_index": "0+0;0+0",
        "aff_campus_unique": "Lisbon;",
        "aff_country_unique_index": "0+0+0;0+0",
        "aff_country_unique": "Portugal"
    },
    {
        "id": "2021.naacl-main.150",
        "title": "Source and Target Bidirectional Knowledge Distillation for End-to-end Speech Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "A conventional approach to improving the performance of end-to-end speech translation (E2E-ST) models is to leverage the source transcription via pre-training and joint training with automatic speech recognition (ASR) and neural machine translation (NMT) tasks. However, since the input modalities are different, it is difficult to leverage source language text successfully. In this work, we focus on sequence-level knowledge distillation (SeqKD) from external text-based NMT models. To leverage the full potential of the source language information, we propose backward SeqKD, SeqKD from a target-to-source backward NMT model. To this end, we train a bilingual E2E-ST model to predict paraphrased transcriptions as an auxiliary task with a single decoder. The paraphrases are generated from the translations in bitext via back-translation. We further propose bidirectional SeqKD in which SeqKD from both forward and backward NMT models is combined. Experimental evaluations on both autoregressive and non-autoregressive models show that SeqKD in each direction consistently improves the translation performance, and the effectiveness is complementary regardless of the model capacity.",
        "author": "Hirofumi Inaguma; Tatsuya Kawahara; Shinji Watanabe",
        "authorids": "/h/hirofumi-inaguma/; /t/tatsuya-kawahara/; /s/shinji-watanabe/",
        "bibtex": "@inproceedings{inaguma-etal-2021-source,\n    title = \"Source and Target Bidirectional Knowledge Distillation for End-to-end Speech Translation\",\n    author = \"Inaguma, Hirofumi  and\n      Kawahara, Tatsuya  and\n      Watanabe, Shinji\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.150/\",\n    doi = \"10.18653/v1/2021.naacl-main.150\",\n    pages = \"1872--1881\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.150.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.150/",
        "pdf_size": 397987,
        "gs_citation": 47,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13803657281850384139&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Kyoto University, Japan; Kyoto University, Japan; Johns Hopkins University, USA",
        "aff_domain": "sap.ist.i.kyoto-u.ac.jp; ; ",
        "email": "sap.ist.i.kyoto-u.ac.jp; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Kyoto University;Johns Hopkins University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.kyoto-u.ac.jp;https://www.jhu.edu",
        "aff_unique_abbr": "Kyoto U;JHU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "Japan;United States"
    },
    {
        "id": "2021.naacl-main.413",
        "title": "SpanPredict: Extraction of Predictive Document Spans with Neural Attention",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In many natural language processing applications, identifying predictive text can be as important as the predictions themselves. When predicting medical diagnoses, for example, identifying predictive content in clinical notes not only enhances interpretability, but also allows unknown, descriptive (i.e., text-based) risk factors to be identified. We here formalize this problem as predictive extraction and address it using a simple mechanism based on linear attention. Our method preserves differentiability, allowing scalable inference via stochastic gradient descent. Further, the model decomposes predictions into a sum of contributions of distinct text spans. Importantly, we require only document labels, not ground-truth spans. Results show that our model identifies semantically-cohesive spans and assigns them scores that agree with human ratings, while preserving classification performance.",
        "author": "Vivek Subramanian; Matthew Engelhard; Sam Berchuck; Liqun Chen; Ricardo Henao; Lawrence Carin",
        "authorids": "/v/vivek-subramanian/; /m/matthew-engelhard/; /s/sam-berchuck/; /l/liqun-chen/; /r/ricardo-henao/; /l/lawrence-carin/",
        "bibtex": "@inproceedings{subramanian-etal-2021-spanpredict,\n    title = \"{S}pan{P}redict: Extraction of Predictive Document Spans with Neural Attention\",\n    author = \"Subramanian, Vivek  and\n      Engelhard, Matthew  and\n      Berchuck, Sam  and\n      Chen, Liqun  and\n      Henao, Ricardo  and\n      Carin, Lawrence\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.413/\",\n    doi = \"10.18653/v1/2021.naacl-main.413\",\n    pages = \"5234--5258\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.413.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.413/",
        "pdf_size": 4139762,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1703261205418334109&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Duke University; Duke University; Duke University; Duke University; Duke University; Duke University",
        "aff_domain": "duke.edu;duke.edu;duke.edu;duke.edu;duke.edu;duke.edu",
        "email": "duke.edu;duke.edu;duke.edu;duke.edu;duke.edu;duke.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Duke University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.duke.edu",
        "aff_unique_abbr": "Duke",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.63",
        "title": "Spoken Language Understanding for Task-oriented Dialogue Systems with Augmented Memory Networks",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Spoken language understanding, usually including intent detection and slot filling, is a core component to build a spoken dialog system. Recent research shows promising results by jointly learning of those two tasks based on the fact that slot filling and intent detection are sharing semantic knowledge. Furthermore, attention mechanism boosts joint learning to achieve state-of-the-art results. However, current joint learning models ignore the following important facts: 1. Long-term slot context is not traced effectively, which is crucial for future slot filling. 2. Slot tagging and intent detection could be mutually rewarding, but bi-directional interaction between slot filling and intent detection remains seldom explored. In this paper, we propose a novel approach to model long-term slot context and to fully utilize the semantic correlation between slots and intents. We adopt a key-value memory network to model slot context dynamically and to track more important slot tags decoded before, which are then fed into our decoder for slot tagging. Furthermore, gated memory information is utilized to perform intent detection, mutually improving both tasks through global optimization. Experiments on benchmark ATIS and Snips datasets show that our model achieves state-of-the-art performance and outperforms other methods, especially for the slot filling task.",
        "author": "Jie Wu; Ian Harris; Hongzhi Zhao",
        "authorids": "/j/jie-wu/; /i/ian-harris/; /h/hongzhi-zhao/",
        "bibtex": "@inproceedings{wu-etal-2021-spoken,\n    title = \"Spoken Language Understanding for Task-oriented Dialogue Systems with Augmented Memory Networks\",\n    author = \"Wu, Jie  and\n      Harris, Ian  and\n      Zhao, Hongzhi\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.63/\",\n    doi = \"10.18653/v1/2021.naacl-main.63\",\n    pages = \"797--806\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.63.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.63/",
        "pdf_size": 513155,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18111216754864530290&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "University of California Irvine; University of California Irvine; Beijing Jiaotong University",
        "aff_domain": "uci.edu;ics.uci.edu;bjtu.edu.cn",
        "email": "uci.edu;ics.uci.edu;bjtu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of California, Irvine;Beijing Jiaotong University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uci.edu;http://www.njtu.edu.cn/en",
        "aff_unique_abbr": "UCI;BJTU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Irvine;",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2021.naacl-main.186",
        "title": "Static Embeddings as Efficient Knowledge Bases?",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent research investigates factual knowledge stored in large pretrained language models (PLMs). Instead of structural knowledge base (KB) queries, masked sentences such as \u201cParis is the capital of [MASK]\u201d are used as probes. The good performance on this analysis task has been interpreted as PLMs becoming potential repositories of factual knowledge. In experiments across ten linguistically diverse languages, we study knowledge contained in static embeddings. We show that, when restricting the output space to a candidate set, simple nearest neighbor matching using static embeddings performs better than PLMs. E.g., static embeddings perform 1.6% points better than BERT while just using 0.3% of energy for training. One important factor in their good comparative performance is that static embeddings are standardly learned for a large vocabulary. In contrast, BERT exploits its more sophisticated, but expensive ability to compose meaningful representations from a much smaller subword vocabulary.",
        "author": "Philipp Dufter; Nora Kassner; Hinrich Sch\u00fctze",
        "authorids": "/p/philipp-dufter/; /n/nora-kassner/; /h/hinrich-schutze/",
        "bibtex": "@inproceedings{dufter-etal-2021-static,\n    title = \"Static Embeddings as Efficient Knowledge Bases?\",\n    author = {Dufter, Philipp  and\n      Kassner, Nora  and\n      Sch{\\\"u}tze, Hinrich},\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.186/\",\n    doi = \"10.18653/v1/2021.naacl-main.186\",\n    pages = \"2353--2363\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.186.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.186/",
        "pdf_size": 340353,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16346614015121889904&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Center for Information and Language Processing (CIS), LMU Munich, Germany; Center for Information and Language Processing (CIS), LMU Munich, Germany; Center for Information and Language Processing (CIS), LMU Munich, Germany",
        "aff_domain": "cis.lmu.de;cis.lmu.de; ",
        "email": "cis.lmu.de;cis.lmu.de; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "LMU Munich",
        "aff_unique_dep": "Center for Information and Language Processing (CIS)",
        "aff_unique_url": "https://www.lmu.de",
        "aff_unique_abbr": "LMU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Munich",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2021.naacl-main.329",
        "title": "Stay Together: A System for Single and Split-antecedent Anaphora Resolution",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The state-of-the-art on basic, single-antecedent anaphora has greatly improved in recent years. Researchers have therefore started to pay more attention to more complex cases of anaphora such as split-antecedent anaphora, as in \u201cTime-Warner is considering a legal challenge to Telecommunications Inc\u2019s plan to buy half of Showtime Networks Inc\u2013a move that could lead to all-out war between the two powerful companies\u201d. Split-antecedent anaphora is rarer and more complex to resolve than single-antecedent anaphora; as a result, it is not annotated in many datasets designed to test coreference, and previous work on resolving this type of anaphora was carried out in unrealistic conditions that assume gold mentions and/or gold split-antecedent anaphors are available. These systems also focus on split-antecedent anaphors only. In this work, we introduce a system that resolves both single and split-antecedent anaphors, and evaluate it in a more realistic setting that uses predicted mentions. We also start addressing the question of how to evaluate single and split-antecedent anaphors together using standard coreference evaluation metrics.",
        "author": "Juntao Yu; Nafise Sadat Moosavi; Silviu Paun; Massimo Poesio",
        "authorids": "/j/juntao-yu/; /n/nafise-sadat-moosavi/; /s/silviu-paun/; /m/massimo-poesio/",
        "bibtex": "@inproceedings{yu-etal-2021-stay,\n    title = \"Stay Together: A System for Single and Split-antecedent Anaphora Resolution\",\n    author = \"Yu, Juntao  and\n      Moosavi, Nafise Sadat  and\n      Paun, Silviu  and\n      Poesio, Massimo\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.329/\",\n    doi = \"10.18653/v1/2021.naacl-main.329\",\n    pages = \"4174--4184\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.329.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.329/",
        "pdf_size": 398151,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8127865076928497215&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Queen Mary University of London; UKP Lab, Technische Universit\u00e4t Darmstadt; Queen Mary University of London; Queen Mary University of London",
        "aff_domain": "qmul.ac.uk;ukp.informatik.tu-darmstadt.de;qmul.ac.uk;qmul.ac.uk",
        "email": "qmul.ac.uk;ukp.informatik.tu-darmstadt.de;qmul.ac.uk;qmul.ac.uk",
        "github": "https://github.com/juntaoy/dali-full-anaphora",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Queen Mary University of London;Technische Universit\u00e4t Darmstadt",
        "aff_unique_dep": ";UKP Lab",
        "aff_unique_url": "https://www.qmul.ac.uk;https://www.tu-darmstadt.de",
        "aff_unique_abbr": "QMUL;TU Darmstadt",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "London;",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "United Kingdom;Germany"
    },
    {
        "id": "2021.naacl-main.109",
        "title": "Structure-Aware Abstractive Conversation Summarization via Discourse and Action Graphs",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Abstractive conversation summarization has received much attention recently. However, these generated summaries often suffer from insufficient, redundant, or incorrect content, largely due to the unstructured and complex characteristics of human-human interactions. To this end, we propose to explicitly model the rich structures in conversations for more precise and accurate conversation summarization, by first incorporating discourse relations between utterances and action triples (\u201cwho-doing-what\u201d) in utterances through structured graphs to better encode conversations, and then designing a multi-granularity decoder to generate summaries by combining all levels of information. Experiments show that our proposed models outperform state-of-the-art methods and generalize well in other domains in terms of both automatic evaluations and human judgments. We have publicly released our code at https://github.com/GT-SALT/Structure-Aware-BART.",
        "author": "Jiaao Chen; Diyi Yang",
        "authorids": "/j/jiaao-chen/; /d/diyi-yang/",
        "bibtex": "@inproceedings{chen-yang-2021-structure,\n    title = \"Structure-Aware Abstractive Conversation Summarization via Discourse and Action Graphs\",\n    author = \"Chen, Jiaao  and\n      Yang, Diyi\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.109/\",\n    doi = \"10.18653/v1/2021.naacl-main.109\",\n    pages = \"1380--1391\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.109.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.109/",
        "pdf_size": 837860,
        "gs_citation": 101,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15191938076304532102&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "School of Interactive Computing, Georgia Institute of Technology; School of Interactive Computing, Georgia Institute of Technology",
        "aff_domain": "gatech.edu;gatech.edu",
        "email": "gatech.edu;gatech.edu",
        "github": "https://github.com/GT-SALT/Structure-Aware-BART",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Georgia Institute of Technology",
        "aff_unique_dep": "School of Interactive Computing",
        "aff_unique_url": "https://www.gatech.edu",
        "aff_unique_abbr": "Georgia Tech",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Atlanta",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.105",
        "title": "Structure-Grounded Pretraining for Text-to-SQL",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Learning to capture text-table alignment is essential for tasks like text-to-SQL. A model needs to correctly recognize natural language references to columns and values and to ground them in the given database schema. In this paper, we present a novel weakly supervised Structure-Grounded pretraining framework (STRUG) for text-to-SQL that can effectively learn to capture text-table alignment based on a parallel text-table corpus. We identify a set of novel pretraining tasks: column grounding, value grounding and column-value mapping, and leverage them to pretrain a text-table encoder. Additionally, to evaluate different methods under more realistic text-table alignment settings, we create a new evaluation set Spider-Realistic based on Spider dev set with explicit mentions of column names removed, and adopt eight existing text-to-SQL datasets for cross-database evaluation. STRUG brings significant improvement over BERTLARGE in all settings. Compared with existing pretraining methods such as GRAPPA, STRUG achieves similar performance on Spider, and outperforms all baselines on more realistic sets. All the code and data used in this work will be open-sourced to facilitate future research.",
        "author": "Xiang Deng; Ahmed Hassan Awadallah; Christopher Meek; Oleksandr Polozov; Huan Sun; Matthew Richardson",
        "authorids": "/x/xiang-deng/; /a/ahmed-hassan/; /c/christopher-meek/; /o/oleksandr-polozov/; /h/huan-sun/; /m/matthew-richardson/",
        "bibtex": "@inproceedings{deng-etal-2021-structure,\n    title = \"Structure-Grounded Pretraining for Text-to-{SQL}\",\n    author = \"Deng, Xiang  and\n      Awadallah, Ahmed Hassan  and\n      Meek, Christopher  and\n      Polozov, Oleksandr  and\n      Sun, Huan  and\n      Richardson, Matthew\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.105/\",\n    doi = \"10.18653/v1/2021.naacl-main.105\",\n    pages = \"1337--1350\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.105.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.105/",
        "pdf_size": 1782423,
        "gs_citation": 172,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13815399357448269615&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "The Ohio State University; Microsoft Research, Redmond; Microsoft Research, Redmond; Microsoft Research, Redmond; The Ohio State University; Microsoft Research, Redmond",
        "aff_domain": "osu.edu;microsoft.com;microsoft.com;microsoft.com;osu.edu;microsoft.com",
        "email": "osu.edu;microsoft.com;microsoft.com;microsoft.com;osu.edu;microsoft.com",
        "github": "",
        "project": "https://aka.ms/strug",
        "author_num": 6,
        "aff_unique_index": "0;1;1;1;0;1",
        "aff_unique_norm": "The Ohio State University;Microsoft Research",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.osu.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "OSU;MSR",
        "aff_campus_unique_index": "1;1;1;1",
        "aff_campus_unique": ";Redmond",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.171",
        "title": "StylePTB: A Compositional Benchmark for Fine-grained Controllable Text Style Transfer",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Text style transfer aims to controllably generate text with targeted stylistic changes while maintaining core meaning from the source sentence constant. Many of the existing style transfer benchmarks primarily focus on individual high-level semantic changes (e.g. positive to negative), which enable controllability at a high level but do not offer fine-grained control involving sentence structure, emphasis, and content of the sentence. In this paper, we introduce a large-scale benchmark, StylePTB, with (1) paired sentences undergoing 21 fine-grained stylistic changes spanning atomic lexical, syntactic, semantic, and thematic transfers of text, as well as (2) compositions of multiple transfers which allow modeling of fine-grained stylistic changes as building blocks for more complex, high-level transfers. By benchmarking existing methods on StylePTB, we find that they struggle to model fine-grained changes and have an even more difficult time composing multiple styles. As a result, StylePTB brings novel challenges that we hope will encourage future research in controllable text style transfer, compositional models, and learning disentangled representations. Solving these challenges would present important steps towards controllable text generation.",
        "author": "Yiwei Lyu; Paul Pu Liang; Hai Pham; Eduard Hovy; Barnab\u00e1s P\u00f3czos; Ruslan Salakhutdinov; Louis-Philippe Morency",
        "authorids": "/y/yiwei-lyu/; /p/paul-pu-liang/; /h/hai-pham/; /e/eduard-hovy/; /b/barnabas-poczos/; /r/ruslan-salakhutdinov/; /l/louis-philippe-morency/",
        "bibtex": "@inproceedings{lyu-etal-2021-styleptb,\n    title = \"{S}tyle{PTB}: A Compositional Benchmark for Fine-grained Controllable Text Style Transfer\",\n    author = \"Lyu, Yiwei  and\n      Liang, Paul Pu  and\n      Pham, Hai  and\n      Hovy, Eduard  and\n      P{\\'o}czos, Barnab{\\'a}s  and\n      Salakhutdinov, Ruslan  and\n      Morency, Louis-Philippe\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.171/\",\n    doi = \"10.18653/v1/2021.naacl-main.171\",\n    pages = \"2116--2138\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.171.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.171/",
        "pdf_size": 1198681,
        "gs_citation": 50,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6811370837155574842&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Machine Learning Department, Carnegie Mellon University; Machine Learning Department, Carnegie Mellon University; Language Technologies Institute, Carnegie Mellon University; Language Technologies Institute, Carnegie Mellon University; Machine Learning Department, Carnegie Mellon University; Machine Learning Department, Carnegie Mellon University; Language Technologies Institute, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu; ; ; ; ",
        "email": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu; ; ; ; ",
        "github": "https://github.com/lvyiwei1/StylePTB/",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Machine Learning Department",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Pittsburgh",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.176",
        "title": "Suicide Ideation Detection via Social and Temporal User Representations using Hyperbolic Learning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent psychological studies indicate that individuals exhibiting suicidal ideation increasingly turn to social media rather than mental health practitioners. Personally contextualizing the buildup of such ideation is critical for accurate identification of users at risk. In this work, we propose a framework jointly leveraging a user\u2019s emotional history and social information from a user\u2019s neighborhood in a network to contextualize the interpretation of the latest tweet of a user on Twitter. Reflecting upon the scale-free nature of social network relationships, we propose the use of Hyperbolic Graph Convolution Networks, in combination with the Hawkes process to learn the historical emotional spectrum of a user in a time-sensitive manner. Our system significantly outperforms state-of-the-art methods on this task, showing the benefits of both socially and personally contextualized representations.",
        "author": "Ramit Sawhney; Harshit Joshi; Rajiv Ratn Shah; Lucie Flek",
        "authorids": "/r/ramit-sawhney/; /h/harshit-joshi/; /r/rajiv-shah/; /l/lucie-flek/",
        "bibtex": "@inproceedings{sawhney-etal-2021-suicide,\n    title = \"Suicide Ideation Detection via Social and Temporal User Representations using Hyperbolic Learning\",\n    author = \"Sawhney, Ramit  and\n      Joshi, Harshit  and\n      Shah, Rajiv Ratn  and\n      Flek, Lucie\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.176/\",\n    doi = \"10.18653/v1/2021.naacl-main.176\",\n    pages = \"2176--2190\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.176.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.176/",
        "pdf_size": 1339437,
        "gs_citation": 56,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2854833184807599246&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4
    },
    {
        "id": "2021.naacl-main.232",
        "title": "Supertagging-based Parsing with Linear Context-free Rewriting Systems",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We present the first supertagging-based parser for linear context-free rewriting systems (LCFRS). It utilizes neural classifiers and outperforms previous LCFRS-based parsers in both accuracy and parsing speed by a wide margin. Our results keep up with the best (general) discontinuous parsers, particularly the scores for discontinuous constituents establish a new state of the art. The heart of our approach is an efficient lexicalization procedure which induces a lexical LCFRS from any discontinuous treebank. We describe a modification to usual chart-based LCFRS parsing that accounts for supertagging and introduce a procedure that transforms lexical LCFRS derivations into equivalent parse trees of the original treebank. Our approach is evaluated on the English Discontinuous Penn Treebank and the German treebanks Negra and Tiger.",
        "author": "Thomas Ruprecht; Richard M\u00f6rbitz",
        "authorids": "/t/thomas-ruprecht/; /r/richard-morbitz/",
        "bibtex": "@inproceedings{ruprecht-morbitz-2021-supertagging,\n    title = \"Supertagging-based Parsing with Linear Context-free Rewriting Systems\",\n    author = {Ruprecht, Thomas  and\n      M{\\\"o}rbitz, Richard},\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.232/\",\n    doi = \"10.18653/v1/2021.naacl-main.232\",\n    pages = \"2923--2935\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.232.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.232/",
        "pdf_size": 283968,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6334031064807303389&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2
    },
    {
        "id": "2021.naacl-main.263",
        "title": "Supervised Neural Clustering via Latent Structured Output Learning: Application to Question Intents",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Previous pre-neural work on structured prediction has produced very effective supervised clustering algorithms using linear classifiers, e.g., structured SVM or perceptron. However, these cannot exploit the representation learning ability of neural networks, which would make supervised clustering even more powerful, i.e., general clustering patterns can be learned automatically. In this paper, we design neural networks based on latent structured prediction loss and Transformer models to approach supervised clustering. We tested our methods on the task of automatically recreating categories of intents from publicly available question intent corpora. The results show that our approach delivers 95.65% of F1, outperforming the state of the art by 17.24%.",
        "author": "Iryna Haponchyk; Alessandro Moschitti",
        "authorids": "/i/iryna-haponchyk/; /a/alessandro-moschitti/",
        "bibtex": "@inproceedings{haponchyk-moschitti-2021-supervised,\n    title = \"Supervised Neural Clustering via Latent Structured Output Learning: Application to Question Intents\",\n    author = \"Haponchyk, Iryna  and\n      Moschitti, Alessandro\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.263/\",\n    doi = \"10.18653/v1/2021.naacl-main.263\",\n    pages = \"3364--3374\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.263.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.263/",
        "pdf_size": 362607,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1135347672099027674&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "DISI, University of Trento; Amazon Alexa AI",
        "aff_domain": "gmail.com;amazon.com",
        "email": "gmail.com;amazon.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Trento;Amazon",
        "aff_unique_dep": "DISI;Alexa AI",
        "aff_unique_url": "https://www.unitn.it;https://www.amazon.com",
        "aff_unique_abbr": ";Amazon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Italy;United States"
    },
    {
        "id": "2021.naacl-main.427",
        "title": "Supporting Clustering with Contrastive Learning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Unsupervised clustering aims at discovering the semantic categories of data according to some distance measured in the representation space. However, different categories often overlap with each other in the representation space at the beginning of the learning process, which poses a significant challenge for distance-based clustering in achieving good separation between different categories. To this end, we propose Supporting Clustering with Contrastive Learning (SCCL) \u2013 a novel framework to leverage contrastive learning to promote better separation. We assess the performance of SCCL on short text clustering and show that SCCL significantly advances the state-of-the-art results on most benchmark datasets with 3%-11% improvement on Accuracy and 4%-15% improvement on Normalized Mutual Information. Furthermore, our quantitative analysis demonstrates the effectiveness of SCCL in leveraging the strengths of both bottom-up instance discrimination and top-down clustering to achieve better intra-cluster and inter-cluster distances when evaluated with the ground truth cluster labels.",
        "author": "Dejiao Zhang; Feng Nan; Xiaokai Wei; Shang-Wen Li; Henghui Zhu; Kathleen McKeown; Ramesh Nallapati; Andrew O. Arnold; Bing Xiang",
        "authorids": "/d/dejiao-zhang/; /f/feng-nan/; /x/xiaokai-wei/; /s/shang-wen-li/; /h/henghui-zhu/; /k/kathleen-mckeown/; /r/ramesh-nallapati/; /a/andrew-o-arnold/; /b/bing-xiang/",
        "bibtex": "@inproceedings{zhang-etal-2021-supporting,\n    title = \"Supporting Clustering with Contrastive Learning\",\n    author = \"Zhang, Dejiao  and\n      Nan, Feng  and\n      Wei, Xiaokai  and\n      Li, Shang-Wen  and\n      Zhu, Henghui  and\n      McKeown, Kathleen  and\n      Nallapati, Ramesh  and\n      Arnold, Andrew O.  and\n      Xiang, Bing\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.427/\",\n    doi = \"10.18653/v1/2021.naacl-main.427\",\n    pages = \"5419--5430\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.427.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.427/",
        "pdf_size": 1274617,
        "gs_citation": 247,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4879949805438285038&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "AWS AI; Columbia University; AWS AI; Columbia University; AWS AI; AWS AI+Columbia University; AWS AI; AWS AI; AWS AI",
        "aff_domain": "amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com",
        "email": "amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com",
        "github": "",
        "project": "https://arxiv.org/abs/2103.12953",
        "author_num": 9,
        "aff_unique_index": "0;1;0;1;0;0+1;0;0;0",
        "aff_unique_norm": "Amazon Web Services;Columbia University",
        "aff_unique_dep": "AWS AI;",
        "aff_unique_url": "https://aws.amazon.com;https://www.columbia.edu",
        "aff_unique_abbr": "AWS;Columbia",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0+0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-demos.14",
        "title": "Supporting Spanish Writers using Automated Feedback",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "We present a tool that provides automated feedback to students studying Spanish writing. The feedback is given for four categories: topic development, coherence, writing conventions, and essay organization. The tool is made freely available via a Google Docs add-on. A small user study with third-level students in Mexico shows that students found the tool generally helpful and that most of them plan to continue using it as they work to improve their writing skills.",
        "author": "Aoife Cahill; James Bruno; James Ramey; Gilmar Ayala Meneses; Ian Blood; Florencia Tolentino; Tamar Lavee; Slava Andreyev",
        "authorids": "/a/aoife-cahill/; /j/james-bruno/; /j/james-ramey/; /g/gilmar-ayala-meneses/; /i/ian-blood/; /f/florencia-tolentino/; /t/tamar-lavee/; /s/slava-andreyev/",
        "bibtex": "@inproceedings{cahill-etal-2021-supporting,\n    title = \"Supporting {S}panish Writers using Automated Feedback\",\n    author = \"Cahill, Aoife  and\n      Bruno, James  and\n      Ramey, James  and\n      Ayala Meneses, Gilmar  and\n      Blood, Ian  and\n      Tolentino, Florencia  and\n      Lavee, Tamar  and\n      Andreyev, Slava\",\n    editor = \"Sil, Avi  and\n      Lin, Xi Victoria\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-demos.14/\",\n    doi = \"10.18653/v1/2021.naacl-demos.14\",\n    pages = \"116--124\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-demos.14.pdf",
        "site": "https://aclanthology.org/2021.naacl-demos.14/",
        "pdf_size": 1387971,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17039452010735350779&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": ";;;;;;;",
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "github": "",
        "project": "",
        "author_num": 8
    },
    {
        "id": "2021.naacl-main.345",
        "title": "Swords: A Benchmark for Lexical Substitution with Improved Data Coverage and Quality",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We release a new benchmark for lexical substitution, the task of finding appropriate substitutes for a target word in a context. For writing, lexical substitution systems can assist humans by suggesting words that humans cannot easily think of. However, existing benchmarks depend on human recall as the only source of data, and therefore lack coverage of the substitutes that would be most helpful to humans. Furthermore, annotators often provide substitutes of low quality, which are not actually appropriate in the given context. We collect higher-coverage and higher-quality data by framing lexical substitution as a classification problem, guided by the intuition that it is easier for humans to judge the appropriateness of candidate substitutes than conjure them from memory. To this end, we use a context-free thesaurus to produce candidates and rely on human judgement to determine contextual appropriateness. Compared to the previous largest benchmark, our Swords benchmark has 3x as many substitutes per target word for the same level of quality, and its substitutes are 1.4x more appropriate (based on human judgement) for the same number of substitutes.",
        "author": "Mina Lee; Chris Donahue; Robin Jia; Alexander Iyabor; Percy Liang",
        "authorids": "/m/mina-lee/; /c/chris-donahue/; /r/robin-jia/; /a/alexander-iyabor/; /p/percy-liang/",
        "bibtex": "@inproceedings{lee-etal-2021-swords,\n    title = \"Swords: A Benchmark for Lexical Substitution with Improved Data Coverage and Quality\",\n    author = \"Lee, Mina  and\n      Donahue, Chris  and\n      Jia, Robin  and\n      Iyabor, Alexander  and\n      Liang, Percy\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.345/\",\n    doi = \"10.18653/v1/2021.naacl-main.345\",\n    pages = \"4362--4379\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.345.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.345/",
        "pdf_size": 642464,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12544317355899136843&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Stanford University; Stanford University; Stanford University; Stanford University; Stanford University",
        "aff_domain": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.270",
        "title": "TABBIE: Pretrained Representations of Tabular Data",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Existing work on tabular representation-learning jointly models tables and associated text using self-supervised objective functions derived from pretrained language models such as BERT. While this joint pretraining improves tasks involving paired tables and text (e.g., answering questions about tables), we show that it underperforms on tasks that operate over tables without any associated text (e.g., populating missing cells). We devise a simple pretraining objective (corrupt cell detection) that learns exclusively from tabular data and reaches the state-of-the-art on a suite of table-based prediction tasks. Unlike competing approaches, our model (TABBIE) provides embeddings of all table substructures (cells, rows, and columns), and it also requires far less compute to train. A qualitative analysis of our model\u2019s learned cell, column, and row representations shows that it understands complex table semantics and numerical trends.",
        "author": "Hiroshi Iida; Dung Thai; Varun Manjunatha; Mohit Iyyer",
        "authorids": "/h/hiroshi-iida/; /d/dung-thai/; /v/varun-manjunatha/; /m/mohit-iyyer/",
        "bibtex": "@inproceedings{iida-etal-2021-tabbie,\n    title = \"{TABBIE}: Pretrained Representations of Tabular Data\",\n    author = \"Iida, Hiroshi  and\n      Thai, Dung  and\n      Manjunatha, Varun  and\n      Iyyer, Mohit\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.270/\",\n    doi = \"10.18653/v1/2021.naacl-main.270\",\n    pages = \"3446--3456\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.270.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.270/",
        "pdf_size": 6605612,
        "gs_citation": 200,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17873492528932515384&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Sony Corporation; UMass Amherst; Adobe Research; UMass Amherst",
        "aff_domain": "sony.com;cs.umass.edu;adobe.com;cs.umass.edu",
        "email": "sony.com;cs.umass.edu;adobe.com;cs.umass.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;2;1",
        "aff_unique_norm": "Sony Corporation;University of Massachusetts Amherst;Adobe",
        "aff_unique_dep": ";;Adobe Research",
        "aff_unique_url": "https://www.sony.com;https://www.umass.edu;https://research.adobe.com",
        "aff_unique_abbr": "Sony;UMass Amherst;Adobe",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Amherst",
        "aff_country_unique_index": "0;1;1;1",
        "aff_country_unique": "Japan;United States"
    },
    {
        "id": "2021.naacl-main.428",
        "title": "TITA: A Two-stage Interaction and Topic-Aware Text Matching Model",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In this paper, we focus on the problem of keyword and document matching by considering different relevance levels. In our recommendation system, different people follow different hot keywords with interest. We need to attach documents to each keyword and then distribute the documents to people who follow these keywords. The ideal documents should have the same topic with the keyword, which we call topic-aware relevance. In other words, topic-aware relevance documents are better than partially-relevance ones in this application. However, previous tasks never define topic-aware relevance clearly. To tackle this problem, we define a three-level relevance in keyword-document matching task: topic-aware relevance, partially-relevance and irrelevance. To capture the relevance between the short keyword and the document at above-mentioned three levels, we should not only combine the latent topic of the document with its deep neural representation, but also model complex interactions between the keyword and the document. To this end, we propose a Two-stage Interaction and Topic-Aware text matching model (TITA). In terms of \u201ctopic-aware\u201d, we introduce neural topic model to analyze the topic of the document and then use it to further encode the document. In terms of \u201ctwo-stage interaction\u201d, we propose two successive stages to model complex interactions between the keyword and the document. Extensive experiments reveal that TITA outperforms other well-designed baselines and shows excellent performance in our recommendation system.",
        "author": "Xingwu Sun; Yanling Cui; Hongyin Tang; Qiuyu Zhu; Fuzheng Zhang; Beihong Jin",
        "authorids": "/x/xingwu-sun/; /y/yanling-cui/; /h/hongyin-tang/; /q/qiuyu-zhu/; /f/fuzheng-zhang/; /b/beihong-jin/",
        "bibtex": "@inproceedings{sun-etal-2021-tita,\n    title = \"{TITA}: A Two-stage Interaction and Topic-Aware Text Matching Model\",\n    author = \"Sun, Xingwu  and\n      Cui, Yanling  and\n      Tang, Hongyin  and\n      Zhu, Qiuyu  and\n      Zhang, Fuzheng  and\n      Jin, Beihong\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.428/\",\n    doi = \"10.18653/v1/2021.naacl-main.428\",\n    pages = \"5431--5440\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.428.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.428/",
        "pdf_size": 823350,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16946245262345243867&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Meituan Inc., Beijing, China; State Key Laboratory of Computer Sciences, Institute of Software, Chinese Academy of Sciences + University of Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Sciences, Institute of Software, Chinese Academy of Sciences + University of Chinese Academy of Sciences, Beijing, China; Meituan Inc., Beijing, China; Meituan Inc., Beijing, China; State Key Laboratory of Computer Sciences, Institute of Software, Chinese Academy of Sciences + University of Chinese Academy of Sciences, Beijing, China",
        "aff_domain": "gmail.com; ; ; ; ;iscas.ac.cn",
        "email": "gmail.com; ; ; ; ;iscas.ac.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1+2;1+2;0;0;1+2",
        "aff_unique_norm": "Meituan Inc.;Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": ";Institute of Software;",
        "aff_unique_url": "https://www.meituan.com;http://www.ios.ac.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "Meituan;CAS;UCAS",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0;0+0;0+0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2021.naacl-main.463",
        "title": "TR-BERT: Dynamic Token Reduction for Accelerating BERT Inference",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Existing pre-trained language models (PLMs) are often computationally expensive in inference, making them impractical in various resource-limited real-world applications. To address this issue, we propose a dynamic token reduction approach to accelerate PLMs\u2019 inference, named TR-BERT, which could flexibly adapt the layer number of each token in inference to avoid redundant calculation. Specially, TR-BERT formulates the token reduction process as a multi-step token selection problem and automatically learns the selection strategy via reinforcement learning. The experimental results on several downstream NLP tasks show that TR-BERT is able to speed up BERT by 2-5 times to satisfy various performance demands. Moreover, TR-BERT can also achieve better performance with less computation in a suite of long-text tasks since its token-level layer number adaption greatly accelerates the self-attention operation in PLMs. The source code and experiment details of this paper can be obtained from https://github.com/thunlp/TR-BERT.",
        "author": "Deming Ye; Yankai Lin; Yufei Huang; Maosong Sun",
        "authorids": "/d/deming-ye/; /y/yankai-lin/; /y/yufei-huang/; /m/maosong-sun/",
        "bibtex": "@inproceedings{ye-etal-2021-tr,\n    title = \"{TR}-{BERT}: Dynamic Token Reduction for Accelerating {BERT} Inference\",\n    author = \"Ye, Deming  and\n      Lin, Yankai  and\n      Huang, Yufei  and\n      Sun, Maosong\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.463/\",\n    doi = \"10.18653/v1/2021.naacl-main.463\",\n    pages = \"5798--5809\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.463.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.463/",
        "pdf_size": 544735,
        "gs_citation": 68,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9197560692248211370&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Computer Science and Technology, Tsinghua University, Beijing, China+Institute for Artificial Intelligence, Tsinghua University, Beijing, China+Beijing National Research Center for Information Science and Technology+State Key Lab on Intelligent Technology and Systems, Tsinghua University, Beijing, China; Pattern Recognition Center, WeChat AI, Tencent Inc.; Department of Computer Science and Technology, Tsinghua University, Beijing, China+Institute for Artificial Intelligence, Tsinghua University, Beijing, China+Beijing National Research Center for Information Science and Technology+State Key Lab on Intelligent Technology and Systems, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China+Institute for Artificial Intelligence, Tsinghua University, Beijing, China+Beijing National Research Center for Information Science and Technology+State Key Lab on Intelligent Technology and Systems, Tsinghua University, Beijing, China+Beijing Academy of Arti\ufb01cial Intelligence",
        "aff_domain": "mails.tsinghua.edu.cn; ; ;tsinghua.edu.cn",
        "email": "mails.tsinghua.edu.cn; ; ;tsinghua.edu.cn",
        "github": "https://github.com/thunlp/TR-BERT",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+0+1+0;2;0+0+1+0;0+0+1+0+3",
        "aff_unique_norm": "Tsinghua University;Beijing National Research Center for Information Science and Technology;Tencent Inc.;Beijing Academy of Artificial Intelligence",
        "aff_unique_dep": "Department of Computer Science and Technology;;Pattern Recognition Center, WeChat AI;",
        "aff_unique_url": "https://www.tsinghua.edu.cn;;https://www.tencent.com;https://www.baaic.cn",
        "aff_unique_abbr": "THU;;Tencent;BAAI",
        "aff_campus_unique_index": "0+0+0;0+0+0;0+0+0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0+0+0+0;0;0+0+0+0;0+0+0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2021.naacl-main.148",
        "title": "Target-Aware Data Augmentation for Stance Detection",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The goal of stance detection is to identify whether the author of a text is in favor of, neutral or against a specific target. Despite substantial progress on this task, one of the remaining challenges is the scarcity of annotations. Data augmentation is commonly used to address annotation scarcity by generating more training samples. However, the augmented sentences that are generated by existing methods are either less diversified or inconsistent with the given target and stance label. In this paper, we formulate the data augmentation of stance detection as a conditional masked language modeling task and augment the dataset by predicting the masked word conditioned on both its context and the auxiliary sentence that contains target and label information. Moreover, we propose another simple yet effective method that generates target-aware sentence by replacing a target mention with the other. Experimental results show that our proposed methods significantly outperforms previous augmentation methods on 11 targets.",
        "author": "Yingjie Li; Cornelia Caragea",
        "authorids": "/y/yingjie-li/; /c/cornelia-caragea/",
        "bibtex": "@inproceedings{li-caragea-2021-target,\n    title = \"Target-Aware Data Augmentation for Stance Detection\",\n    author = \"Li, Yingjie  and\n      Caragea, Cornelia\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.148/\",\n    doi = \"10.18653/v1/2021.naacl-main.148\",\n    pages = \"1850--1860\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.148.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.148/",
        "pdf_size": 284104,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6431091677437496218&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "University of Illinois at Chicago; University of Illinois at Chicago",
        "aff_domain": "uic.edu;uic.edu",
        "email": "uic.edu;uic.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Illinois at Chicago",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uic.edu",
        "aff_unique_abbr": "UIC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Chicago",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.145",
        "title": "Target-specified Sequence Labeling with Multi-head Self-attention for Target-oriented Opinion Words Extraction",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Opinion target extraction and opinion term extraction are two fundamental tasks in Aspect Based Sentiment Analysis (ABSA). Many recent works on ABSA focus on Target-oriented Opinion Words (or Terms) Extraction (TOWE), which aims at extracting the corresponding opinion words for a given opinion target. TOWE can be further applied to Aspect-Opinion Pair Extraction (AOPE) which aims at extracting aspects (i.e., opinion targets) and opinion terms in pairs. In this paper, we propose Target-Specified sequence labeling with Multi-head Self-Attention (TSMSA) for TOWE, in which any pre-trained language model with multi-head self-attention can be integrated conveniently. As a case study, we also develop a Multi-Task structure named MT-TSMSA for AOPE by combining our TSMSA with an aspect and opinion term extraction module. Experimental results indicate that TSMSA outperforms the benchmark methods on TOWE significantly; meanwhile, the performance of MT-TSMSA is similar or even better than state-of-the-art AOPE baseline models.",
        "author": "Yuhao Feng; Yanghui Rao; Yuyao Tang; Ninghua Wang; He Liu",
        "authorids": "/y/yuhao-feng/; /y/yanghui-rao/; /y/yuyao-tang/; /n/ninghua-wang/; /h/he-liu/",
        "bibtex": "@inproceedings{feng-etal-2021-target,\n    title = \"Target-specified Sequence Labeling with Multi-head Self-attention for Target-oriented Opinion Words Extraction\",\n    author = \"Feng, Yuhao  and\n      Rao, Yanghui  and\n      Tang, Yuyao  and\n      Wang, Ninghua  and\n      Liu, He\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.145/\",\n    doi = \"10.18653/v1/2021.naacl-main.145\",\n    pages = \"1805--1815\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.145.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.145/",
        "pdf_size": 798506,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6125207806085424728&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; Tencent Inc, Shenzhen, China; Tencent Inc, Shenzhen, China; Tencent Inc, Shenzhen, China",
        "aff_domain": "mail2.sysu.edu.cn;mail.sysu.edu.cn;tencent.com;tencent.com;tencent.com",
        "email": "mail2.sysu.edu.cn;mail.sysu.edu.cn;tencent.com;tencent.com;tencent.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;1;1",
        "aff_unique_norm": "Sun Yat-sen University;Tencent",
        "aff_unique_dep": "School of Computer Science and Engineering;",
        "aff_unique_url": "http://www.sysu.edu.cn;https://www.tencent.com",
        "aff_unique_abbr": "SYSU;Tencent",
        "aff_campus_unique_index": "0;0;1;1;1",
        "aff_campus_unique": "Guangzhou;Shenzhen",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2021.naacl-main.424",
        "title": "Targeted Adversarial Training for Natural Language Understanding",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We present a simple yet effective Targeted Adversarial Training (TAT) algorithm to improve adversarial training for natural language understanding. The key idea is to introspect current mistakes and prioritize adversarial training steps to where the model errs the most. Experiments show that TAT can significantly improve accuracy over standard adversarial training on GLUE and attain new state-of-the-art zero-shot results on XNLI. Our code will be released upon acceptance of the paper.",
        "author": "Lis Pereira; Xiaodong Liu; Hao Cheng; Hoifung Poon; Jianfeng Gao; Ichiro Kobayashi",
        "authorids": "/l/lis-pereira/; /x/xiaodong-liu/; /h/hao-cheng/; /h/hoifung-poon/; /j/jianfeng-gao/; /i/ichiro-kobayashi/",
        "bibtex": "@inproceedings{pereira-etal-2021-targeted,\n    title = \"Targeted Adversarial Training for Natural Language Understanding\",\n    author = \"Pereira, Lis  and\n      Liu, Xiaodong  and\n      Cheng, Hao  and\n      Poon, Hoifung  and\n      Gao, Jianfeng  and\n      Kobayashi, Ichiro\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.424/\",\n    doi = \"10.18653/v1/2021.naacl-main.424\",\n    pages = \"5385--5393\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.424.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.424/",
        "pdf_size": 530109,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13974012326495132581&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Ochanomizu University+Microsoft Research; Microsoft Research; Microsoft Research; Microsoft Research; Microsoft Research; Ochanomizu University",
        "aff_domain": "ocha.ac.jp;microsoft.com;microsoft.com;microsoft.com;microsoft.com;ocha.ac.jp",
        "email": "ocha.ac.jp;microsoft.com;microsoft.com;microsoft.com;microsoft.com;ocha.ac.jp",
        "github": "https://github.com/namisan/mt-dnn",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;1;1;1;1;0",
        "aff_unique_norm": "Ochanomizu University;Microsoft Corporation",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.ochanomizu-u.ac.jp;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "Ochanomizu U;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;1;1;1;1;0",
        "aff_country_unique": "Japan;United States"
    },
    {
        "id": "2021.naacl-main.335",
        "title": "TaxoClass: Hierarchical Multi-Label Text Classification Using Only Class Names",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Hierarchical multi-label text classification (HMTC) aims to tag each document with a set of classes from a taxonomic class hierarchy. Most existing HMTC methods train classifiers using massive human-labeled documents, which are often too costly to obtain in real-world applications. In this paper, we explore to conduct HMTC based on only class surface names as supervision signals. We observe that to perform HMTC, human experts typically first pinpoint a few most essential classes for the document as its \u201ccore classes\u201d, and then check core classes\u2019 ancestor classes to ensure the coverage. To mimic human experts, we propose a novel HMTC framework, named TaxoClass. Specifically, TaxoClass (1) calculates document-class similarities using a textual entailment model, (2) identifies a document\u2019s core classes and utilizes confident core classes to train a taxonomy-enhanced classifier, and (3) generalizes the classifier via multi-label self-training. Our experiments on two challenging datasets show TaxoClass can achieve around 0.71 Example-F1 using only class names, outperforming the best previous method by 25%.",
        "author": "Jiaming Shen; Wenda Qiu; Yu Meng; Jingbo Shang; Xiang Ren; Jiawei Han",
        "authorids": "/j/jiaming-shen/; /w/wenda-qiu/; /y/yu-meng/; /j/jingbo-shang/; /x/xiang-ren/; /j/jiawei-han/",
        "bibtex": "@inproceedings{shen-etal-2021-taxoclass,\n    title = \"{T}axo{C}lass: Hierarchical Multi-Label Text Classification Using Only Class Names\",\n    author = \"Shen, Jiaming  and\n      Qiu, Wenda  and\n      Meng, Yu  and\n      Shang, Jingbo  and\n      Ren, Xiang  and\n      Han, Jiawei\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.335/\",\n    doi = \"10.18653/v1/2021.naacl-main.335\",\n    pages = \"4239--4249\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.335.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.335/",
        "pdf_size": 1219647,
        "gs_citation": 77,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13754707551226789419&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "University of Illinois at Urbana-Champaign, IL, USA; University of Illinois at Urbana-Champaign, IL, USA; University of Illinois at Urbana-Champaign, IL, USA; University of California, San Diego, CA, USA; University of Southern California, CA, USA; University of Illinois at Urbana-Champaign, IL, USA",
        "aff_domain": "illinois.edu;illinois.edu;illinois.edu;ucsd.edu;usc.edu;illinois.edu",
        "email": "illinois.edu;illinois.edu;illinois.edu;ucsd.edu;usc.edu;illinois.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;1;2;0",
        "aff_unique_norm": "University of Illinois at Urbana-Champaign;University of California, San Diego;University of Southern California",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://illinois.edu;https://ucsd.edu;https://www.usc.edu",
        "aff_unique_abbr": "UIUC;UCSD;USC",
        "aff_campus_unique_index": "0;0;0;1;2;0",
        "aff_campus_unique": "Urbana-Champaign;San Diego;Los Angeles",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-industry.23",
        "title": "Technical Question Answering across Tasks and Domains",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Building automatic technical support system is an important yet challenge task. Conceptually, to answer a user question on a technical forum, a human expert has to first retrieve relevant documents, and then read them carefully to identify the answer snippet. Despite huge success the researchers have achieved in coping with general domain question answering (QA), much less attentions have been paid for investigating technical QA. Specifically, existing methods suffer from several unique challenges (i) the question and answer rarely overlaps substantially and (ii) very limited data size. In this paper, we propose a novel framework of deep transfer learning to effectively address technical QA across tasks and domains. To this end, we present an adjustable joint learning approach for document retrieval and reading comprehension tasks. Our experiments on the TechQA demonstrates superior performance compared with state-of-the-art methods.",
        "author": "Wenhao Yu; Lingfei Wu; Yu Deng; Qingkai Zeng; Ruchi Mahindru; Sinem Guven; Meng Jiang",
        "authorids": "/w/wenhao-yu/; /l/lingfei-wu/; /y/yu-deng/; /q/qingkai-zeng/; /r/ruchi-mahindru/; /s/sinem-guven/; /m/meng-jiang/",
        "bibtex": "@inproceedings{yu-etal-2021-technical,\n    title = \"Technical Question Answering across Tasks and Domains\",\n    author = \"Yu, Wenhao  and\n      Wu, Lingfei  and\n      Deng, Yu  and\n      Zeng, Qingkai  and\n      Mahindru, Ruchi  and\n      Guven, Sinem  and\n      Jiang, Meng\",\n    editor = \"Kim, Young-bum  and\n      Li, Yunyao  and\n      Rambow, Owen\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-industry.23/\",\n    doi = \"10.18653/v1/2021.naacl-industry.23\",\n    pages = \"178--186\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-industry.23.pdf",
        "site": "https://aclanthology.org/2021.naacl-industry.23/",
        "pdf_size": 1489829,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1230506210266131043&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "University of Notre Dame, Notre Dame, IN, USA+IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; University of Notre Dame, Notre Dame, IN, USA+IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; University of Notre Dame, Notre Dame, IN, USA+IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA",
        "aff_domain": "nd.edu;us.ibm.com;us.ibm.com;nd.edu;us.ibm.com;us.ibm.com;nd.edu",
        "email": "nd.edu;us.ibm.com;us.ibm.com;nd.edu;us.ibm.com;us.ibm.com;nd.edu",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;1;1;0+1;1;1;0+1",
        "aff_unique_norm": "University of Notre Dame;IBM Thomas J. Watson Research Center",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.nd.edu;https://www.ibm.com/research/watson",
        "aff_unique_abbr": "Notre Dame;IBM Watson",
        "aff_campus_unique_index": "0+1;1;1;0+1;1;1;0+1",
        "aff_campus_unique": "Notre Dame;Yorktown Heights",
        "aff_country_unique_index": "0+0;0;0;0+0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.70",
        "title": "Template Filling with Generative Transformers",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Template filling is generally tackled by a pipeline of two separate supervised systems \u2013 one for role-filler extraction and another for template/event recognition. Since pipelines consider events in isolation, they can suffer from error propagation. We introduce a framework based on end-to-end generative transformers for this task (i.e., GTT). It naturally models the dependence between entities both within a single event and across the multiple events described in a document. Experiments demonstrate that this framework substantially outperforms pipeline-based approaches, and other neural end-to-end baselines that do not model between-event dependencies. We further show that our framework specifically improves performance on documents containing multiple events.",
        "author": "Xinya Du; Alexander Rush; Claire Cardie",
        "authorids": "/x/xinya-du/; /a/alexander-m-rush/; /c/claire-cardie/",
        "bibtex": "@inproceedings{du-etal-2021-template,\n    title = \"Template Filling with Generative Transformers\",\n    author = \"Du, Xinya  and\n      Rush, Alexander  and\n      Cardie, Claire\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.70/\",\n    doi = \"10.18653/v1/2021.naacl-main.70\",\n    pages = \"909--914\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.70.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.70/",
        "pdf_size": 518587,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6097036375476222294&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Department of Computer Science, Cornell University; Department of Computer Science, Cornell University; Department of Computer Science, Cornell University",
        "aff_domain": "cs.cornell.edu;cornell.edu;cs.cornell.edu",
        "email": "cs.cornell.edu;cornell.edu;cs.cornell.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Cornell University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.cornell.edu",
        "aff_unique_abbr": "Cornell",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.202",
        "title": "Temporal Knowledge Graph Completion using a Linear Temporal Regularizer and Multivector Embeddings",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Representation learning approaches for knowledge graphs have been mostly designed for static data. However, many knowledge graphs involve evolving data, e.g., the fact (The President of the United States is Barack Obama) is valid only from 2009 to 2017. This introduces important challenges for knowledge representation learning since the knowledge graphs change over time. In this paper, we present a novel time-aware knowledge graph embebdding approach, TeLM, which performs 4th-order tensor factorization of a Temporal knowledge graph using a Linear temporal regularizer and Multivector embeddings. Moreover, we investigate the effect of the temporal dataset\u2019s time granularity on temporal knowledge graph completion. Experimental results demonstrate that our proposed models trained with the linear temporal regularizer achieve the state-of-the-art performances on link prediction over four well-established temporal knowledge graph completion benchmarks.",
        "author": "Chengjin Xu; Yung-Yu Chen; Mojtaba Nayyeri; Jens Lehmann",
        "authorids": "/c/chengjin-xu/; /y/yung-yu-chen/; /m/mojtaba-nayyeri/; /j/jens-lehmann/",
        "bibtex": "@inproceedings{xu-etal-2021-temporal,\n    title = \"Temporal Knowledge Graph Completion using a Linear Temporal Regularizer and Multivector Embeddings\",\n    author = \"Xu, Chengjin  and\n      Chen, Yung-Yu  and\n      Nayyeri, Mojtaba  and\n      Lehmann, Jens\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.202/\",\n    doi = \"10.18653/v1/2021.naacl-main.202\",\n    pages = \"2569--2578\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.202.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.202/",
        "pdf_size": 534952,
        "gs_citation": 126,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3360805939590005277&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "University of Bonn / Germany; University of Bonn / Germany; University of Bonn / Germany; University of Bonn / Germany + Fraunhofer IAIS/ Germany",
        "aff_domain": "iai.uni-bonn.de;uni-bonn.de;iai.uni-bonn.de;iais.fraunhofer.de",
        "email": "iai.uni-bonn.de;uni-bonn.de;iai.uni-bonn.de;iais.fraunhofer.de",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0+1",
        "aff_unique_norm": "University of Bonn;Fraunhofer Institute for Applied Information Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uni-bonn.de;https://www.iais.fraunhofer.de/",
        "aff_unique_abbr": "UBonn;Fraunhofer IAIS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0+0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2021.naacl-main.107",
        "title": "Temporal Reasoning on Implicit Events from Distant Supervision",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We propose TRACIE, a novel temporal reasoning dataset that evaluates the degree to which systems understand implicit events\u2014events that are not mentioned explicitly in natural language text but can be inferred from it. This introduces a new challenge in temporal reasoning research, where prior work has focused on explicitly mentioned events. Human readers can infer implicit events via commonsense reasoning, resulting in a more comprehensive understanding of the situation and, consequently, better reasoning about time. We find, however, that state-of-the-art models struggle when predicting temporal relationships between implicit and explicit events. To address this, we propose a neuro-symbolic temporal reasoning model, SymTime, which exploits distant supervision signals from large-scale text and uses temporal rules to combine start times and durations to infer end times. SymTime outperforms strong baseline systems on TRACIE by 5%, and by 11% in a zero prior knowledge training setting. Our approach also generalizes to other temporal reasoning tasks, as evidenced by a gain of 1%-9% on MATRES, an explicit event benchmark.",
        "author": "Ben Zhou; Kyle Richardson; Qiang Ning; Tushar Khot; Ashish Sabharwal; Dan Roth",
        "authorids": "/b/ben-zhou/; /k/kyle-richardson/; /q/qiang-ning/; /t/tushar-khot/; /a/ashish-sabharwal/; /d/dan-roth/",
        "bibtex": "@inproceedings{zhou-etal-2021-temporal,\n    title = \"Temporal Reasoning on Implicit Events from Distant Supervision\",\n    author = \"Zhou, Ben  and\n      Richardson, Kyle  and\n      Ning, Qiang  and\n      Khot, Tushar  and\n      Sabharwal, Ashish  and\n      Roth, Dan\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.107/\",\n    doi = \"10.18653/v1/2021.naacl-main.107\",\n    pages = \"1361--1371\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.107.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.107/",
        "pdf_size": 591218,
        "gs_citation": 83,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10712170070160406881&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Allen Institute for AI+University of Pennsylvania; Allen Institute for AI; Amazon; Allen Institute for AI; Allen Institute for AI; University of Pennsylvania",
        "aff_domain": "cis.upenn.edu;allenai.org;amazon.com;allenai.org;allenai.org;cis.upenn.edu",
        "email": "cis.upenn.edu;allenai.org;amazon.com;allenai.org;allenai.org;cis.upenn.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;0;2;0;0;1",
        "aff_unique_norm": "Allen Institute for AI;University of Pennsylvania;Amazon.com, Inc.",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://allenai.org;https://www.upenn.edu;https://www.amazon.com",
        "aff_unique_abbr": "AI2;UPenn;Amazon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.414",
        "title": "Text Editing by Command",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "A prevailing paradigm in neural text generation is one-shot generation, where text is produced in a single step. The one-shot setting is inadequate, however, when the constraints the user wishes to impose on the generated text are dynamic, especially when authoring longer documents. We address this limitation with an interactive text generation setting in which the user interacts with the system by issuing commands to edit existing text. To this end, we propose a novel text editing task, and introduce WikiDocEdits, a dataset of single-sentence edits crawled from Wikipedia. We show that our Interactive Editor, a transformer-based model trained on this dataset, outperforms baselines and obtains positive results in both automatic and human evaluations. We present empirical and qualitative analyses of this model\u2019s performance.",
        "author": "Felix Faltings; Michel Galley; Gerold Hintz; Chris Brockett; Chris Quirk; Jianfeng Gao; Bill Dolan",
        "authorids": "/f/felix-faltings/; /m/michel-galley/; /g/gerold-hintz/; /c/chris-brockett/; /c/chris-quirk/; /j/jianfeng-gao/; /w/william-b-dolan/",
        "bibtex": "@inproceedings{faltings-etal-2021-text,\n    title = \"Text Editing by Command\",\n    author = \"Faltings, Felix  and\n      Galley, Michel  and\n      Hintz, Gerold  and\n      Brockett, Chris  and\n      Quirk, Chris  and\n      Gao, Jianfeng  and\n      Dolan, Bill\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.414/\",\n    doi = \"10.18653/v1/2021.naacl-main.414\",\n    pages = \"5259--5274\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.414.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.414/",
        "pdf_size": 659065,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15105225606423010288&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Computer Science, ETH Z\u00fcrich\u2666; Microsoft Research\u2660; Microsoft Research\u2660; Microsoft Research\u2660; Microsoft Research\u2660; Microsoft Research\u2660; Microsoft Research\u2660",
        "aff_domain": "student.ethz.ch;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "email": "student.ethz.ch;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "github": "",
        "project": "http://microsoft.com/research/project/interactive-document-generation",
        "author_num": 7,
        "aff_unique_index": "0;1;1;1;1;1;1",
        "aff_unique_norm": "ETH Z\u00fcrich;Microsoft Research",
        "aff_unique_dep": "Department of Computer Science;",
        "aff_unique_url": "https://www.ethz.ch;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "ETHZ;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;1;1;1;1",
        "aff_country_unique": "Switzerland;United States"
    },
    {
        "id": "2021.naacl-main.35",
        "title": "Text Generation from Discourse Representation Structures",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We propose neural models to generate text from formal meaning representations based on Discourse Representation Structures (DRSs). DRSs are document-level representations which encode rich semantic detail pertaining to rhetorical relations, presupposition, and co-reference within and across sentences. We formalize the task of neural DRS-to-text generation and provide modeling solutions for the problems of condition ordering and variable naming which render generation from DRSs non-trivial. Our generator relies on a novel sibling treeLSTM model which is able to accurately represent DRS structures and is more generally suited to trees with wide branches. We achieve competitive performance (59.48 BLEU) on the GMB benchmark against several strong baselines.",
        "author": "Jiangming Liu; Shay B. Cohen; Mirella Lapata",
        "authorids": "/j/jiangming-liu/; /s/shay-b-cohen/; /m/mirella-lapata/",
        "bibtex": "@inproceedings{liu-etal-2021-text,\n    title = \"Text Generation from Discourse Representation Structures\",\n    author = \"Liu, Jiangming  and\n      Cohen, Shay B.  and\n      Lapata, Mirella\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.35/\",\n    doi = \"10.18653/v1/2021.naacl-main.35\",\n    pages = \"397--415\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.35.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.35/",
        "pdf_size": 533725,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8250456362132304291&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Institute for Language, Cognition and Computation, School of Informatics, University of Edinburgh; Institute for Language, Cognition and Computation, School of Informatics, University of Edinburgh; Institute for Language, Cognition and Computation, School of Informatics, University of Edinburgh",
        "aff_domain": "ed.ac.uk;inf.ed.ac.uk;inf.ed.ac.uk",
        "email": "ed.ac.uk;inf.ed.ac.uk;inf.ed.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Edinburgh",
        "aff_unique_dep": "School of Informatics",
        "aff_unique_url": "https://www.ed.ac.uk",
        "aff_unique_abbr": "Edinburgh",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Edinburgh",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2021.naacl-main.99",
        "title": "Text Modular Networks: Learning to Decompose Tasks in the Language of Existing Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We propose a general framework called Text Modular Networks(TMNs) for building interpretable systems that learn to solve complex tasks by decomposing them into simpler ones solvable by existing models. To ensure solvability of simpler tasks, TMNs learn the textual input-output behavior (i.e., language) of existing models through their datasets. This differs from prior decomposition-based approaches which, besides being designed specifically for each complex task, produce decompositions independent of existing sub-models. Specifically, we focus on Question Answering (QA) and show how to train a next-question generator to sequentially produce sub-questions targeting appropriate sub-models, without additional human annotation. These sub-questions and answers provide a faithful natural language explanation of the model\u2019s reasoning. We use this framework to build ModularQA, a system that can answer multi-hop reasoning questions by decomposing them into sub-questions answerable by a neural factoid single-span QA model and a symbolic calculator. Our experiments show that ModularQA is more versatile than existing explainable systems for DROP and HotpotQA datasets, is more robust than state-of-the-art blackbox (uninterpretable) systems, and generates more understandable and trustworthy explanations compared to prior work.",
        "author": "Tushar Khot; Daniel Khashabi; Kyle Richardson; Peter Clark; Ashish Sabharwal",
        "authorids": "/t/tushar-khot/; /d/daniel-khashabi/; /k/kyle-richardson/; /p/peter-clark/; /a/ashish-sabharwal/",
        "bibtex": "@inproceedings{khot-etal-2021-text,\n    title = \"Text Modular Networks: Learning to Decompose Tasks in the Language of Existing Models\",\n    author = \"Khot, Tushar  and\n      Khashabi, Daniel  and\n      Richardson, Kyle  and\n      Clark, Peter  and\n      Sabharwal, Ashish\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.99/\",\n    doi = \"10.18653/v1/2021.naacl-main.99\",\n    pages = \"1264--1279\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.99.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.99/",
        "pdf_size": 1044459,
        "gs_citation": 94,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9959077401438622520&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "https://github.com/allenai/modularqa",
        "project": "",
        "author_num": 5
    },
    {
        "id": "2021.naacl-demos.13",
        "title": "TextEssence: A Tool for Interactive Analysis of Semantic Shifts Between Corpora",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "Embeddings of words and concepts capture syntactic and semantic regularities of language; however, they have seen limited use as tools to study characteristics of different corpora and how they relate to one another. We introduce TextEssence, an interactive system designed to enable comparative analysis of corpora using embeddings. TextEssence includes visual, neighbor-based, and similarity-based modes of embedding analysis in a lightweight, web-based interface. We further propose a new measure of embedding confidence based on nearest neighborhood overlap, to assist in identifying high-quality embeddings for corpus analysis. A case study on COVID-19 scientific literature illustrates the utility of the system. TextEssence can be found at https://textessence.github.io.",
        "author": "Denis Newman-Griffis; Venkatesh Sivaraman; Adam Perer; Eric Fosler-Lussier; Harry Hochheiser",
        "authorids": "/d/denis-newman-griffis/; /v/venkatesh-sivaraman/; /a/adam-perer/; /e/eric-fosler-lussier/; /h/harry-hochheiser/",
        "bibtex": "@inproceedings{newman-griffis-etal-2021-textessence,\n    title = \"{T}ext{E}ssence: A Tool for Interactive Analysis of Semantic Shifts Between Corpora\",\n    author = \"Newman-Griffis, Denis  and\n      Sivaraman, Venkatesh  and\n      Perer, Adam  and\n      Fosler-Lussier, Eric  and\n      Hochheiser, Harry\",\n    editor = \"Sil, Avi  and\n      Lin, Xi Victoria\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-demos.13/\",\n    doi = \"10.18653/v1/2021.naacl-demos.13\",\n    pages = \"106--115\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-demos.13.pdf",
        "site": "https://aclanthology.org/2021.naacl-demos.13/",
        "pdf_size": 859719,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13573902981131410373&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Biomedical Informatics, University of Pittsburgh; Human-Computer Interaction Institute, Carnegie Mellon University; Department of Computer Science and Engineering, The Ohio State University; Department of Computer Science and Engineering, The Ohio State University; Department of Biomedical Informatics, University of Pittsburgh + Intelligent Systems Program, University of Pittsburgh",
        "aff_domain": "pitt.edu;andrew.cmu.edu; ; ; ",
        "email": "pitt.edu;andrew.cmu.edu; ; ; ",
        "github": "",
        "project": "https://textessence.github.io",
        "author_num": 5,
        "aff_unique_index": "0;1;2;2;0+0",
        "aff_unique_norm": "University of Pittsburgh;Carnegie Mellon University;The Ohio State University",
        "aff_unique_dep": "Department of Biomedical Informatics;Human-Computer Interaction Institute;Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.pitt.edu;https://www.cmu.edu;https://www.osu.edu",
        "aff_unique_abbr": "Pitt;CMU;OSU",
        "aff_campus_unique_index": "1;",
        "aff_campus_unique": ";Pittsburgh",
        "aff_country_unique_index": "0;0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.92",
        "title": "The Curious Case of Hallucinations in Neural Machine Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In this work, we study hallucinations in Neural Machine Translation (NMT), which lie at an extreme end on the spectrum of NMT pathologies. Firstly, we connect the phenomenon of hallucinations under source perturbation to the Long-Tail theory of Feldman, and present an empirically validated hypothesis that explains hallucinations under source perturbation. Secondly, we consider hallucinations under corpus-level noise (without any source perturbation) and demonstrate that two prominent types of natural hallucinations (detached and oscillatory outputs) could be generated and explained through specific corpus-level noise patterns. Finally, we elucidate the phenomenon of hallucination amplification in popular data-generation processes such as Backtranslation and sequence-level Knowledge Distillation. We have released the datasets and code to replicate our results.",
        "author": "Vikas Raunak; Arul Menezes; Marcin Junczys-Dowmunt",
        "authorids": "/v/vikas-raunak/; /a/arul-menezes/; /m/marcin-junczys-dowmunt/",
        "bibtex": "@inproceedings{raunak-etal-2021-curious,\n    title = \"The Curious Case of Hallucinations in Neural Machine Translation\",\n    author = \"Raunak, Vikas  and\n      Menezes, Arul  and\n      Junczys-Dowmunt, Marcin\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.92/\",\n    doi = \"10.18653/v1/2021.naacl-main.92\",\n    pages = \"1172--1183\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.92.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.92/",
        "pdf_size": 1073411,
        "gs_citation": 225,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=340475041174580227&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Microsoft, USA; Microsoft, USA; Microsoft, USA",
        "aff_domain": "microsoft.com;microsoft.com;microsoft.com",
        "email": "microsoft.com;microsoft.com;microsoft.com",
        "github": "https://github.com/vyraun/hallucinations",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Microsoft Corporation",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.microsoft.com",
        "aff_unique_abbr": "Microsoft",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.49",
        "title": "The Importance of Modeling Social Factors of Language: Theory and Practice",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Natural language processing (NLP) applications are now more powerful and ubiquitous than ever before. With rapidly developing (neural) models and ever-more available data, current NLP models have access to more information than any human speaker during their life. Still, it would be hard to argue that NLP models have reached human-level capacity. In this position paper, we argue that the reason for the current limitations is a focus on information content while ignoring language\u2019s social factors. We show that current NLP systems systematically break down when faced with interpreting the social factors of language. This limits applications to a subset of information-related tasks and prevents NLP from reaching human-level performance. At the same time, systems that incorporate even a minimum of social factors already show remarkable improvements. We formalize a taxonomy of seven social factors based on linguistic theory and exemplify current failures and emerging successes for each of them. We suggest that the NLP community address social factors to get closer to the goal of human-like language understanding.",
        "author": "Dirk Hovy; Diyi Yang",
        "authorids": "/d/dirk-hovy/; /d/diyi-yang/",
        "bibtex": "@inproceedings{hovy-yang-2021-importance,\n    title = \"The Importance of Modeling Social Factors of Language: Theory and Practice\",\n    author = \"Hovy, Dirk  and\n      Yang, Diyi\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.49/\",\n    doi = \"10.18653/v1/2021.naacl-main.49\",\n    pages = \"588--602\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.49.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.49/",
        "pdf_size": 369714,
        "gs_citation": 208,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9919035510665727468&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Bocconi University; Georgia Institute of Technology",
        "aff_domain": "unibocconi.it;gatech.edu",
        "email": "unibocconi.it;gatech.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Bocconi University;Georgia Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.bocconi.edu;https://www.gatech.edu",
        "aff_unique_abbr": "Bocconi;Georgia Tech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Italy;United States"
    },
    {
        "id": "2021.naacl-main.178",
        "title": "The structure of online social networks modulates the rate of lexical change",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "New words are regularly introduced to communities, yet not all of these words persist in a community\u2019s lexicon. Among the many factors contributing to lexical change, we focus on the understudied effect of social networks. We conduct a large-scale analysis of over 80k neologisms in 4420 online communities across a decade. Using Poisson regression and survival analysis, our study demonstrates that the community\u2019s network structure plays a significant role in lexical change. Apart from overall size, properties including dense connections, the lack of local clusters, and more external contacts promote lexical innovation and retention. Unlike offline communities, these topic-based communities do not experience strong lexical leveling despite increased contact but accommodate more niche words. Our work provides support for the sociolinguistic hypothesis that lexical change is partially shaped by the structure of the underlying network but also uncovers findings specific to online communities.",
        "author": "Jian Zhu; David Jurgens",
        "authorids": "/j/jian-zhu/; /d/david-jurgens/",
        "bibtex": "@inproceedings{zhu-jurgens-2021-structure,\n    title = \"The structure of online social networks modulates the rate of lexical change\",\n    author = \"Zhu, Jian  and\n      Jurgens, David\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.178/\",\n    doi = \"10.18653/v1/2021.naacl-main.178\",\n    pages = \"2201--2218\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.178.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.178/",
        "pdf_size": 1873859,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2991761906842470885&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Linguistics, University of Michigan; School of Information, University of Michigan",
        "aff_domain": "umich.edu;umich.edu",
        "email": "umich.edu;umich.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "Department of Linguistics",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Ann Arbor",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.362",
        "title": "Time-Stamped Language Model: Teaching Language Models to Understand The Flow of Events",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Tracking entities throughout a procedure described in a text is challenging due to the dynamic nature of the world described in the process. Firstly, we propose to formulate this task as a question answering problem. This enables us to use pre-trained transformer-based language models on other QA benchmarks by adapting those to the procedural text understanding. Secondly, since the transformer-based language models cannot encode the flow of events by themselves, we propose a Time-Stamped Language Model (TSLM) to encode event information in LMs architecture by introducing the timestamp encoding. Our model evaluated on the Propara dataset shows improvements on the published state-of-the-art results with a 3.1% increase in F1 score. Moreover, our model yields better results on the location prediction task on the NPN-Cooking dataset. This result indicates that our approach is effective for procedural text understanding in general.",
        "author": "Hossein Rajaby Faghihi; Parisa Kordjamshidi",
        "authorids": "/h/hossein-rajaby-faghihi/; /p/parisa-kordjamshidi/",
        "bibtex": "@inproceedings{rajaby-faghihi-kordjamshidi-2021-time,\n    title = \"Time-Stamped Language Model: Teaching Language Models to Understand The Flow of Events\",\n    author = \"Rajaby Faghihi, Hossein  and\n      Kordjamshidi, Parisa\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.362/\",\n    doi = \"10.18653/v1/2021.naacl-main.362\",\n    pages = \"4560--4570\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.362.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.362/",
        "pdf_size": 493430,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10701531073997993843&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 9,
        "aff": "Michigan State University; Michigan State University",
        "aff_domain": "msu.edu;msu.edu",
        "email": "msu.edu;msu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Michigan State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.msu.edu",
        "aff_unique_abbr": "MSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.403",
        "title": "Too Much in Common: Shifting of Embeddings in Transformer Language Models and its Implications",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The success of language models based on the Transformer architecture appears to be inconsistent with observed anisotropic properties of representations learned by such models. We resolve this by showing, contrary to previous studies, that the representations do not occupy a narrow cone, but rather drift in common directions. At any training step, all of the embeddings except for the ground-truth target embedding are updated with gradient in the same direction. Compounded over the training set, the embeddings drift and share common components, manifested in their shape in all the models we have empirically tested. Our experiments show that isotropy can be restored using a simple transformation.",
        "author": "Daniel Bi\u015b; Maksim Podkorytov; Xiuwen Liu",
        "authorids": "/d/daniel-bis/; /m/maksim-podkorytov/; /x/xiuwen-liu/",
        "bibtex": "@inproceedings{bis-etal-2021-much,\n    title = \"Too Much in Common: Shifting of Embeddings in Transformer Language Models and its Implications\",\n    author = \"Bi{\\'s}, Daniel  and\n      Podkorytov, Maksim  and\n      Liu, Xiuwen\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.403/\",\n    doi = \"10.18653/v1/2021.naacl-main.403\",\n    pages = \"5117--5130\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.403.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.403/",
        "pdf_size": 5469912,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2387902950001966155&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Florida State University; Florida State University; Florida State University",
        "aff_domain": "cs.fsu.edu;cs.fsu.edu;cs.fsu.edu",
        "email": "cs.fsu.edu;cs.fsu.edu;cs.fsu.edu",
        "github": "https://github.com/danielbis/tooMuchInCommon",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Florida State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.fsu.edu",
        "aff_unique_abbr": "FSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.300",
        "title": "Topic Model or Topic Twaddle? Re-evaluating Semantic Interpretability Measures",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "When developing topic models, a critical question that should be asked is: How well will this model work in an applied setting? Because standard performance evaluation of topic interpretability uses automated measures modeled on human evaluation tests that are dissimilar to applied usage, these models\u2019 generalizability remains in question. In this paper, we probe the issue of validity in topic model evaluation and assess how informative coherence measures are for specialized collections used in an applied setting. Informed by the literature, we propose four understandings of interpretability. We evaluate these using a novel experimental framework reflective of varied applied settings, including human evaluations using open labeling, typical of applied research. These evaluations show that for some specialized collections, standard coherence measures may not inform the most appropriate topic model or the optimal number of topics, and current interpretability performance validation methods are challenged as a means to confirm model quality in the absence of ground truth data.",
        "author": "Caitlin Doogan; Wray Buntine",
        "authorids": "/c/caitlin-doogan/; /w/wray-buntine/",
        "bibtex": "@inproceedings{doogan-buntine-2021-topic,\n    title = \"Topic Model or Topic Twaddle? Re-evaluating Semantic Interpretability Measures\",\n    author = \"Doogan, Caitlin  and\n      Buntine, Wray\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.300/\",\n    doi = \"10.18653/v1/2021.naacl-main.300\",\n    pages = \"3824--3848\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.300.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.300/",
        "pdf_size": 1474196,
        "gs_citation": 90,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4617321254429354250&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Faculty of Information Technology, Monash University, Australia; Faculty of Information Technology, Monash University, Australia",
        "aff_domain": "dmonash.edu;dmonash.edu",
        "email": "dmonash.edu;dmonash.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Monash University",
        "aff_unique_dep": "Faculty of Information Technology",
        "aff_unique_url": "https://www.monash.edu",
        "aff_unique_abbr": "Monash",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "2021.naacl-main.93",
        "title": "Towards Continual Learning for Multilingual Machine Translation via Vocabulary Substitution",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We propose a straightforward vocabulary adaptation scheme to extend the language capacity of multilingual machine translation models, paving the way towards efficient continual learning for multilingual machine translation. Our approach is suitable for large-scale datasets, applies to distant languages with unseen scripts, incurs only minor degradation on the translation performance for the original language pairs and provides competitive performance even in the case where we only possess monolingual data for the new languages.",
        "author": "Xavier Garcia; Noah Constant; Ankur Parikh; Orhan Firat",
        "authorids": "/x/xavier-garcia/; /n/noah-constant/; /a/ankur-parikh/; /o/orhan-firat/",
        "bibtex": "@inproceedings{garcia-etal-2021-towards,\n    title = \"Towards Continual Learning for Multilingual Machine Translation via Vocabulary Substitution\",\n    author = \"Garcia, Xavier  and\n      Constant, Noah  and\n      Parikh, Ankur  and\n      Firat, Orhan\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.93/\",\n    doi = \"10.18653/v1/2021.naacl-main.93\",\n    pages = \"1184--1192\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.93.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.93/",
        "pdf_size": 612181,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8067390231801482180&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Google Research; Google Research; Google Research; Google Research",
        "aff_domain": "google.com;google.com;google.com;google.com",
        "email": "google.com;google.com;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google Research",
        "aff_unique_url": "https://research.google",
        "aff_unique_abbr": "Google Research",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Mountain View",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.158",
        "title": "Towards Few-shot Fact-Checking via Perplexity",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Few-shot learning has drawn researchers\u2019 attention to overcome the problem of data scarcity. Recently, large pre-trained language models have shown great performance in few-shot learning for various downstream tasks, such as question answering and machine translation. Nevertheless, little exploration has been made to achieve few-shot learning for the fact-checking task. However, fact-checking is an important problem, especially when the amount of information online is growing exponentially every day. In this paper, we propose a new way of utilizing the powerful transfer learning ability of a language model via a perplexity score. The most notable strength of our methodology lies in its capability in few-shot learning. With only two training samples, our methodology can already outperform the Major Class baseline by more than an absolute 10% on the F1-Macro metric across multiple datasets. Through experiments, we empirically verify the plausibility of the rather surprising usage of the perplexity score in the context of fact-checking and highlight the strength of our few-shot methodology by comparing it to strong fine-tuning-based baseline models. Moreover, we construct and publicly release two new fact-checking datasets related to COVID-19.",
        "author": "Nayeon Lee; Yejin Bang; Andrea Madotto; Pascale Fung",
        "authorids": "/n/nayeon-lee/; /y/yejin-bang/; /a/andrea-madotto/; /p/pascale-fung/",
        "bibtex": "@inproceedings{lee-etal-2021-towards,\n    title = \"Towards Few-shot Fact-Checking via Perplexity\",\n    author = \"Lee, Nayeon  and\n      Bang, Yejin  and\n      Madotto, Andrea  and\n      Fung, Pascale\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.158/\",\n    doi = \"10.18653/v1/2021.naacl-main.158\",\n    pages = \"1971--1981\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.158.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.158/",
        "pdf_size": 499899,
        "gs_citation": 91,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5194292296507534582&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4
    },
    {
        "id": "2021.naacl-main.71",
        "title": "Towards Interpreting and Mitigating Shortcut Learning Behavior of NLU models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent studies indicate that NLU models are prone to rely on shortcut features for prediction, without achieving true language understanding. As a result, these models fail to generalize to real-world out-of-distribution data. In this work, we show that the words in the NLU training set can be modeled as a long-tailed distribution. There are two findings: 1) NLU models have strong preference for features located at the head of the long-tailed distribution, and 2) Shortcut features are picked up during very early few iterations of the model training. These two observations are further employed to formulate a measurement which can quantify the shortcut degree of each training sample. Based on this shortcut measurement, we propose a shortcut mitigation framework LGTR, to suppress the model from making overconfident predictions for samples with large shortcut degree. Experimental results on three NLU benchmarks demonstrate that our long-tailed distribution explanation accurately reflects the shortcut learning behavior of NLU models. Experimental analysis further indicates that LGTR can improve the generalization accuracy on OOD data, while preserving the accuracy on in-distribution data.",
        "author": "Mengnan Du; Varun Manjunatha; Rajiv Jain; Ruchi Deshpande; Franck Dernoncourt; Jiuxiang Gu; Tong Sun; Xia Hu",
        "authorids": "/m/mengnan-du/; /v/varun-manjunatha/; /r/rajiv-jain/; /r/ruchi-deshpande/; /f/franck-dernoncourt/; /j/jiuxiang-gu/; /t/tong-sun/; /x/xia-hu/",
        "bibtex": "@inproceedings{du-etal-2021-towards,\n    title = \"Towards Interpreting and Mitigating Shortcut Learning Behavior of {NLU} models\",\n    author = \"Du, Mengnan  and\n      Manjunatha, Varun  and\n      Jain, Rajiv  and\n      Deshpande, Ruchi  and\n      Dernoncourt, Franck  and\n      Gu, Jiuxiang  and\n      Sun, Tong  and\n      Hu, Xia\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.71/\",\n    doi = \"10.18653/v1/2021.naacl-main.71\",\n    pages = \"915--929\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.71.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.71/",
        "pdf_size": 955332,
        "gs_citation": 107,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13274884401287880980&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Texas A&M University; Adobe Research; Adobe Document Cloud; Adobe Research; Adobe Research; Adobe Research; Adobe Research; Texas A&M University",
        "aff_domain": "tamu.edu;adobe.com;adobe.com;adobe.com;adobe.com;adobe.com;adobe.com;cse.tamu.edu",
        "email": "tamu.edu;adobe.com;adobe.com;adobe.com;adobe.com;adobe.com;adobe.com;cse.tamu.edu",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;1;1;1;1;1;1;0",
        "aff_unique_norm": "Texas A&M University;Adobe",
        "aff_unique_dep": ";Adobe Research",
        "aff_unique_url": "https://www.tamu.edu;https://research.adobe.com",
        "aff_unique_abbr": "TAMU;Adobe",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.94",
        "title": "Towards Modeling the Style of Translators in Neural Machine Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "One key ingredient of neural machine translation is the use of large datasets from different domains and resources (e.g. Europarl, TED talks). These datasets contain documents translated by professional translators using different but consistent translation styles. Despite that, the model is usually trained in a way that neither explicitly captures the variety of translation styles present in the data nor translates new data in different and controllable styles. In this work, we investigate methods to augment the state of the art Transformer model with translator information that is available in part of the training data. We show that our style-augmented translation models are able to capture the style variations of translators and to generate translations with different styles on new data. Indeed, the generated variations differ significantly, up to +4.5 BLEU score difference. Despite that, human evaluation confirms that the translations are of the same quality.",
        "author": "Yue Wang; Cuong Hoang; Marcello Federico",
        "authorids": "/y/yue-wang/; /c/cuong-hoang/; /m/marcello-federico/",
        "bibtex": "@inproceedings{wang-etal-2021-towards,\n    title = \"Towards Modeling the Style of Translators in Neural Machine Translation\",\n    author = \"Wang, Yue  and\n      Hoang, Cuong  and\n      Federico, Marcello\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.94/\",\n    doi = \"10.18653/v1/2021.naacl-main.94\",\n    pages = \"1193--1199\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.94.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.94/",
        "pdf_size": 293912,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6403649970380163068&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Amazon AI; Amazon AI; Amazon AI",
        "aff_domain": "amazon.com;amazon.com;amazon.com",
        "email": "amazon.com;amazon.com;amazon.com",
        "github": "",
        "project": "https://www.ted.com/participate/translate/guidelines",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Amazon",
        "aff_unique_dep": "Amazon AI",
        "aff_unique_url": "https://www.amazon.com",
        "aff_unique_abbr": "Amazon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.456",
        "title": "Towards Sentiment and Emotion aided Multi-modal Speech Act Classification in Twitter",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Speech Act Classification determining the communicative intent of an utterance has been investigated widely over the years as a standalone task. This holds true for discussion in any fora including social media platform such as Twitter. But the emotional state of the tweeter which has a considerable effect on the communication has not received the attention it deserves. Closely related to emotion is sentiment, and understanding of one helps understand the other. In this work, we firstly create a new multi-modal, emotion-TA (\u2018TA\u2019 means tweet act, i.e., speech act in Twitter) dataset called EmoTA collected from open-source Twitter dataset. We propose a Dyadic Attention Mechanism (DAM) based multi-modal, adversarial multi-tasking framework. DAM incorporates intra-modal and inter-modal attention to fuse multiple modalities and learns generalized features across all the tasks. Experimental results indicate that the proposed framework boosts the performance of the primary task, i.e., TA classification (TAC) by benefitting from the two secondary tasks, i.e., Sentiment and Emotion Analysis compared to its uni-modal and single task TAC (tweet act classification) variants.",
        "author": "Tulika Saha; Apoorva Upadhyaya; Sriparna Saha; Pushpak Bhattacharyya",
        "authorids": "/t/tulika-saha/; /a/apoorva-upadhyaya/; /s/sriparna-saha/; /p/pushpak-bhattacharyya/",
        "bibtex": "@inproceedings{saha-etal-2021-towards,\n    title = \"Towards Sentiment and Emotion aided Multi-modal Speech Act Classification in {T}witter\",\n    author = \"Saha, Tulika  and\n      Upadhyaya, Apoorva  and\n      Saha, Sriparna  and\n      Bhattacharyya, Pushpak\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.456/\",\n    doi = \"10.18653/v1/2021.naacl-main.456\",\n    pages = \"5727--5737\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.456.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.456/",
        "pdf_size": 1202016,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14977407546994745070&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science and Engineering, Indian Institute of Technology Patna, India; Department of Computer Science and Engineering, Indian Institute of Technology Patna, India; Department of Computer Science and Engineering, Indian Institute of Technology Patna, India; Department of Computer Science and Engineering, Indian Institute of Technology Patna, India",
        "aff_domain": "gmail.com; ;gmail.com;gmail.com",
        "email": "gmail.com; ;gmail.com;gmail.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Indian Institute of Technology Patna",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.iitp.ac.in",
        "aff_unique_abbr": "IIT Patna",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Patna",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2021.naacl-main.189",
        "title": "Towards a Comprehensive Understanding and Accurate Evaluation of Societal Biases in Pre-Trained Transformers",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The ease of access to pre-trained transformers has enabled developers to leverage large-scale language models to build exciting applications for their users. While such pre-trained models offer convenient starting points for researchers and developers, there is little consideration for the societal biases captured within these model risking perpetuation of racial, gender, and other harmful biases when these models are deployed at scale. In this paper, we investigate gender and racial bias across ubiquitous pre-trained language models, including GPT-2, XLNet, BERT, RoBERTa, ALBERT and DistilBERT. We evaluate bias within pre-trained transformers using three metrics: WEAT, sequence likelihood, and pronoun ranking. We conclude with an experiment demonstrating the ineffectiveness of word-embedding techniques, such as WEAT, signaling the need for more robust bias testing in transformers.",
        "author": "Andrew Silva; Pradyumna Tambwekar; Matthew Gombolay",
        "authorids": "/a/andrew-silva/; /p/pradyumna-tambwekar/; /m/matthew-gombolay/",
        "bibtex": "@inproceedings{silva-etal-2021-towards,\n    title = \"Towards a Comprehensive Understanding and Accurate Evaluation of Societal Biases in Pre-Trained Transformers\",\n    author = \"Silva, Andrew  and\n      Tambwekar, Pradyumna  and\n      Gombolay, Matthew\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.189/\",\n    doi = \"10.18653/v1/2021.naacl-main.189\",\n    pages = \"2383--2389\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.189.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.189/",
        "pdf_size": 183696,
        "gs_citation": 78,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1628328203049868602&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "School of Interactive Computing, Georgia Institute of Technology; School of Interactive Computing, Georgia Institute of Technology; School of Interactive Computing, Georgia Institute of Technology",
        "aff_domain": "gatech.edu;gatech.edu;gatech.edu",
        "email": "gatech.edu;gatech.edu;gatech.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Georgia Institute of Technology",
        "aff_unique_dep": "School of Interactive Computing",
        "aff_unique_url": "https://www.gatech.edu",
        "aff_unique_abbr": "Georgia Tech",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Atlanta",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.459",
        "title": "Training Data Augmentation for Code-Mixed Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Machine translation of user-generated code-mixed inputs to English is of crucial importance in applications like web search and targeted advertising. We address the scarcity of parallel training data for training such models by designing a strategy of converting existing non-code-mixed parallel data sources to code-mixed parallel data. We present an m-BERT based procedure whose core learnable component is a ternary sequence labeling model, that can be trained with a limited code-mixed corpus alone. We show a 5.8 point increase in BLEU on heavily code-mixed sentences by training a translation model using our data augmentation strategy on an Hindi-English code-mixed translation task.",
        "author": "Abhirut Gupta; Aditya Vavre; Sunita Sarawagi",
        "authorids": "/a/abhirut-gupta/; /a/aditya-vavre/; /s/sunita-sarawagi/",
        "bibtex": "@inproceedings{gupta-etal-2021-training,\n    title = \"Training Data Augmentation for Code-Mixed Translation\",\n    author = \"Gupta, Abhirut  and\n      Vavre, Aditya  and\n      Sarawagi, Sunita\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.459/\",\n    doi = \"10.18653/v1/2021.naacl-main.459\",\n    pages = \"5760--5766\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.459.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.459/",
        "pdf_size": 413821,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3256416900091162524&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Google Research; IIT Bombay; IIT Bombay",
        "aff_domain": "google.com;cse.iitb.ac.in;cse.iitb.ac.in",
        "email": "google.com;cse.iitb.ac.in;cse.iitb.ac.in",
        "github": "https://github.com/gentaiscool/code-switching-papers",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Google;Indian Institute of Technology Bombay",
        "aff_unique_dep": "Google Research;",
        "aff_unique_url": "https://research.google;https://www.iitb.ac.in",
        "aff_unique_abbr": "Google Research;IITB",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Mountain View;Mumbai",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "United States;India"
    },
    {
        "id": "2021.naacl-industry.35",
        "title": "Training Language Models under Resource Constraints for Adversarial Advertisement Detection",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Advertising on e-commerce and social media sites deliver ad impressions at web scale on a daily basis driving value to both shoppers and advertisers. This scale necessitates programmatic ways of detecting unsuitable content in ads to safeguard customer experience and trust. This paper focusses on techniques for training text classification models under resource constraints, built as part of automated solutions for advertising content moderation. We show how weak supervision, curriculum learning and multi-lingual training can be applied effectively to fine-tune BERT and its variants for text classification tasks in conjunction with different data augmentation strategies. Our extensive experiments on multiple languages show that these techniques detect adversarial ad categories with a substantial gain in precision at high recall threshold over the baseline.",
        "author": "Eshwar Shamanna Girishekar; Shiv Surya; Nishant Nikhil; Dyut Kumar Sil; Sumit Negi; Aruna Rajan",
        "authorids": "/e/eshwar-shamanna-girishekar/; /s/shiv-surya/; /n/nishant-nikhil/; /d/dyut-kumar-sil/; /s/sumit-negi/; /a/aruna-rajan/",
        "bibtex": "@inproceedings{shamanna-girishekar-etal-2021-training,\n    title = \"Training Language Models under Resource Constraints for Adversarial Advertisement Detection\",\n    author = \"Shamanna Girishekar, Eshwar  and\n      Surya, Shiv  and\n      Nikhil, Nishant  and\n      Sil, Dyut Kumar  and\n      Negi, Sumit  and\n      Rajan, Aruna\",\n    editor = \"Kim, Young-bum  and\n      Li, Yunyao  and\n      Rambow, Owen\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-industry.35/\",\n    doi = \"10.18653/v1/2021.naacl-industry.35\",\n    pages = \"280--287\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-industry.35.pdf",
        "site": "https://aclanthology.org/2021.naacl-industry.35/",
        "pdf_size": 429405,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2357622894684340415&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Amazon; Amazon; Amazon+\u2217; Amazon; Amazon; Amazon",
        "aff_domain": "amazon.com;amazon.com;gmail.com;amazon.com;amazon.com;amazon.com",
        "email": "amazon.com;amazon.com;gmail.com;amazon.com;amazon.com;amazon.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Amazon.com, Inc.;",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.amazon.com;",
        "aff_unique_abbr": "Amazon;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "2021.naacl-main.325",
        "title": "Translational NLP: A New Paradigm and General Principles for Natural Language Processing Research",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Natural language processing (NLP) research combines the study of universal principles, through basic science, with applied science targeting specific use cases and settings. However, the process of exchange between basic NLP and applications is often assumed to emerge naturally, resulting in many innovations going unapplied and many important questions left unstudied. We describe a new paradigm of Translational NLP, which aims to structure and facilitate the processes by which basic and applied NLP research inform one another. Translational NLP thus presents a third research paradigm, focused on understanding the challenges posed by application needs and how these challenges can drive innovation in basic science and technology design. We show that many significant advances in NLP research have emerged from the intersection of basic principles with application needs, and present a conceptual framework outlining the stakeholders and key questions in translational research. Our framework provides a roadmap for developing Translational NLP as a dedicated research area, and identifies general translational principles to facilitate exchange between basic and applied research.",
        "author": "Denis Newman-Griffis; Jill Fain Lehman; Carolyn Ros\u00e9; Harry Hochheiser",
        "authorids": "/d/denis-newman-griffis/; /j/jill-fain-lehman/; /c/carolyn-rose/; /h/harry-hochheiser/",
        "bibtex": "@inproceedings{newman-griffis-etal-2021-translational,\n    title = \"Translational {NLP}: A New Paradigm and General Principles for Natural Language Processing Research\",\n    author = \"Newman-Griffis, Denis  and\n      Lehman, Jill Fain  and\n      Ros{\\'e}, Carolyn  and\n      Hochheiser, Harry\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.325/\",\n    doi = \"10.18653/v1/2021.naacl-main.325\",\n    pages = \"4125--4138\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.325.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.325/",
        "pdf_size": 569477,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9647108170026627905&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Biomedical Informatics, University of Pittsburgh, USA; Human-Computer Interaction Institute, Carnegie Mellon University, USA; Language Technologies Institute, Carnegie Mellon University, USA; Department of Biomedical Informatics, University of Pittsburgh, USA",
        "aff_domain": "pitt.edu;cs.cmu.edu;cs.cmu.edu;pitt.edu",
        "email": "pitt.edu;cs.cmu.edu;cs.cmu.edu;pitt.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "University of Pittsburgh;Carnegie Mellon University",
        "aff_unique_dep": "Department of Biomedical Informatics;Human-Computer Interaction Institute",
        "aff_unique_url": "https://www.pitt.edu;https://www.cmu.edu",
        "aff_unique_abbr": "Pitt;CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.386",
        "title": "TuringAdvice: A Generative and Dynamic Evaluation of Language Use",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We propose TuringAdvice, a new challenge task and dataset for language understanding models. Given a written situation that a real person is currently facing, a model must generate helpful advice in natural language. Our evaluation framework tests a fundamental aspect of human language understanding: our ability to use language to resolve open-ended situations by communicating with each other. Empirical results show that today\u2019s models struggle at TuringAdvice, even multibillion parameter models finetuned on 600k in-domain training examples. The best model, T5, writes advice that is at least as helpful as human-written advice in only 14% of cases; a much larger non-finetunable GPT3 model does even worse at 4%. This low performance reveals language understanding errors that are hard to spot outside of a generative setting, showing much room for progress.",
        "author": "Rowan Zellers; Ari Holtzman; Elizabeth Clark; Lianhui Qin; Ali Farhadi; Yejin Choi",
        "authorids": "/r/rowan-zellers/; /a/ari-holtzman/; /e/elizabeth-clark/; /l/lianhui-qin/; /a/ali-farhadi/; /y/yejin-choi/",
        "bibtex": "@inproceedings{zellers-etal-2021-turingadvice,\n    title = \"{T}uring{A}dvice: A Generative and Dynamic Evaluation of Language Use\",\n    author = \"Zellers, Rowan  and\n      Holtzman, Ari  and\n      Clark, Elizabeth  and\n      Qin, Lianhui  and\n      Farhadi, Ali  and\n      Choi, Yejin\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.386/\",\n    doi = \"10.18653/v1/2021.naacl-main.386\",\n    pages = \"4856--4880\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.386.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.386/",
        "pdf_size": 747072,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1526713735878678629&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Paul G. Allen School of Computer Science & Engineering, University of Washington\u2660; Allen Institute for Arti\ufb01cial Intelligence\u2665; Paul G. Allen School of Computer Science & Engineering, University of Washington\u2660; Paul G. Allen School of Computer Science & Engineering, University of Washington\u2660; Paul G. Allen School of Computer Science & Engineering, University of Washington\u2660; Paul G. Allen School of Computer Science & Engineering, University of Washington\u2660\u2665",
        "aff_domain": "uw.edu;uw.edu;allenai.org;uw.edu;cs.washington.edu;cs.washington.edu",
        "email": "uw.edu;uw.edu;allenai.org;uw.edu;cs.washington.edu;cs.washington.edu",
        "github": "",
        "project": "rowanzellers.com/advice",
        "author_num": 6,
        "aff_unique_index": "0;1;0;0;0;0",
        "aff_unique_norm": "University of Washington;Allen Institute for Artificial Intelligence",
        "aff_unique_dep": "Paul G. Allen School of Computer Science & Engineering;",
        "aff_unique_url": "https://www.cs.washington.edu;https://allenai.org",
        "aff_unique_abbr": "UW;AI2",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Seattle;",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.203",
        "title": "UDALM: Unsupervised Domain Adaptation through Language Modeling",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In this work we explore Unsupervised Domain Adaptation (UDA) of pretrained language models for downstream tasks. We introduce UDALM, a fine-tuning procedure, using a mixed classification and Masked Language Model loss, that can adapt to the target domain distribution in a robust and sample efficient manner. Our experiments show that performance of models trained with the mixed loss scales with the amount of available target data and the mixed loss can be effectively used as a stopping criterion during UDA training. Furthermore, we discuss the relationship between A-distance and the target error and explore some limitations of the Domain Adversarial Training approach. Our method is evaluated on twelve domain pairs of the Amazon Reviews Sentiment dataset, yielding 91.74% accuracy, which is an 1.11% absolute improvement over the state-of-the-art.",
        "author": "Constantinos Karouzos; Georgios Paraskevopoulos; Alexandros Potamianos",
        "authorids": "/c/constantinos-karouzos/; /g/georgios-paraskevopoulos/; /a/alexandros-potamianos/",
        "bibtex": "@inproceedings{karouzos-etal-2021-udalm,\n    title = \"{UDALM}: Unsupervised Domain Adaptation through Language Modeling\",\n    author = \"Karouzos, Constantinos  and\n      Paraskevopoulos, Georgios  and\n      Potamianos, Alexandros\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.203/\",\n    doi = \"10.18653/v1/2021.naacl-main.203\",\n    pages = \"2579--2590\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.203.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.203/",
        "pdf_size": 970070,
        "gs_citation": 73,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7387748706768207277&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "School of ECE, National Technical University of Athens, Athens, Greece+Signal Analysis and Interpretation Laboratory (SAIL), USC, Los Angeles, CA, USA+Behavioral Signal Technologies, Los Angeles, CA, USA+Institute for Language and Speech Processing, Athena Research Center, Athens, Greece; School of ECE, National Technical University of Athens, Athens, Greece+Institute for Language and Speech Processing, Athena Research Center, Athens, Greece; School of ECE, National Technical University of Athens, Athens, Greece+Signal Analysis and Interpretation Laboratory (SAIL), USC, Los Angeles, CA, USA+Behavioral Signal Technologies, Los Angeles, CA, USA",
        "aff_domain": "gmail.com;central.ntua.gr;central.ntua.gr",
        "email": "gmail.com;central.ntua.gr;central.ntua.gr",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1+2+3;0+3;0+1+2",
        "aff_unique_norm": "National Technical University of Athens;University of Southern California;Behavioral Signal Technologies;Athena Research Center",
        "aff_unique_dep": "School of ECE;Signal Analysis and Interpretation Laboratory;;Institute for Language and Speech Processing",
        "aff_unique_url": "https://www.ntua.gr;https://www.usc.edu;;https://www.athenarc.gr",
        "aff_unique_abbr": "NTUA;USC;;",
        "aff_campus_unique_index": "0+1+1+0;0+0;0+1+1",
        "aff_campus_unique": "Athens;Los Angeles",
        "aff_country_unique_index": "0+1+1+0;0+0;0+1+1",
        "aff_country_unique": "Greece;United States"
    },
    {
        "id": "2021.naacl-main.139",
        "title": "UmlsBERT: Clinical Domain Knowledge Augmentation of Contextual Embeddings Using the Unified Medical Language System Metathesaurus",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Contextual word embedding models, such as BioBERT and Bio_ClinicalBERT, have achieved state-of-the-art results in biomedical natural language processing tasks by focusing their pre-training process on domain-specific corpora. However, such models do not take into consideration structured expert domain knowledge from a knowledge base. We introduce UmlsBERT, a contextual embedding model that integrates domain knowledge during the pre-training process via a novel knowledge augmentation strategy. More specifically, the augmentation on UmlsBERT with the Unified Medical Language System (UMLS) Metathesaurus is performed in two ways: i) connecting words that have the same underlying \u2018concept\u2019 in UMLS and ii) leveraging semantic type knowledge in UMLS to create clinically meaningful input embeddings. By applying these two strategies, UmlsBERT can encode clinical domain knowledge into word embeddings and outperform existing domain-specific models on common named-entity recognition (NER) and clinical natural language inference tasks.",
        "author": "George Michalopoulos; Yuanxin Wang; Hussam Kaka; Helen Chen; Alexander Wong",
        "authorids": "/g/george-michalopoulos/; /y/yuanxin-wang/; /h/hussam-kaka/; /h/helen-chen/; /a/alexander-wong/",
        "bibtex": "@inproceedings{michalopoulos-etal-2021-umlsbert,\n    title = \"{U}mls{BERT}: Clinical Domain Knowledge Augmentation of Contextual Embeddings Using the {U}nified {M}edical {L}anguage {S}ystem {M}etathesaurus\",\n    author = \"Michalopoulos, George  and\n      Wang, Yuanxin  and\n      Kaka, Hussam  and\n      Chen, Helen  and\n      Wong, Alexander\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.139/\",\n    doi = \"10.18653/v1/2021.naacl-main.139\",\n    pages = \"1744--1753\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.139.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.139/",
        "pdf_size": 748809,
        "gs_citation": 157,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1559606394630138568&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "University of Waterloo, Waterloo, Canada; University of Waterloo, Waterloo, Canada; University of Waterloo, Waterloo, Canada; University of Waterloo, Waterloo, Canada; University of Waterloo, Waterloo, Canada",
        "aff_domain": "uwaterloo.ca;uwaterloo.ca;uwaterloo.ca;uwaterloo.ca;uwaterloo.ca",
        "email": "uwaterloo.ca;uwaterloo.ca;uwaterloo.ca;uwaterloo.ca;uwaterloo.ca",
        "github": "https://github.com/gmichalo/UmlsBERT",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Waterloo",
        "aff_unique_dep": "",
        "aff_unique_url": "https://uwaterloo.ca",
        "aff_unique_abbr": "UW",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Waterloo",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2021.naacl-main.383",
        "title": "Understanding Factuality in Abstractive Summarization with FRANK: A Benchmark for Factuality Metrics",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Modern summarization models generate highly fluent but often factually unreliable outputs. This motivated a surge of metrics attempting to measure the factuality of automatically generated summaries. Due to the lack of common benchmarks, these metrics cannot be compared. Moreover, all these methods treat factuality as a binary concept and fail to provide deeper insights on the kinds of inconsistencies made by different systems. To address these limitations, we devise a typology of factual errors and use it to collect human annotations of generated summaries from state-of-the-art summarization systems for the CNN/DM and XSum datasets. Through these annotations we identify the proportion of different categories of factual errors and benchmark factuality metrics, showing their correlation with human judgement as well as their specific strengths and weaknesses.",
        "author": "Artidoro Pagnoni; Vidhisha Balachandran; Yulia Tsvetkov",
        "authorids": "/a/artidoro-pagnoni/; /v/vidhisha-balachandran/; /y/yulia-tsvetkov/",
        "bibtex": "@inproceedings{pagnoni-etal-2021-understanding,\n    title = \"Understanding Factuality in Abstractive Summarization with {FRANK}: A Benchmark for Factuality Metrics\",\n    author = \"Pagnoni, Artidoro  and\n      Balachandran, Vidhisha  and\n      Tsvetkov, Yulia\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.383/\",\n    doi = \"10.18653/v1/2021.naacl-main.383\",\n    pages = \"4812--4829\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.383.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.383/",
        "pdf_size": 3067543,
        "gs_citation": 321,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2020313251861237881&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Language Technologies Institute, Carnegie Mellon University; Language Technologies Institute, Carnegie Mellon University; Language Technologies Institute, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "github": "https://github.com/artidoro/frank",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Language Technologies Institute",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.86",
        "title": "Understanding Hard Negatives in Noise Contrastive Estimation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The choice of negative examples is important in noise contrastive estimation. Recent works find that hard negatives\u2014highest-scoring incorrect examples under the model\u2014are effective in practice, but they are used without a formal justification. We develop analytical tools to understand the role of hard negatives. Specifically, we view the contrastive loss as a biased estimator of the gradient of the cross-entropy loss, and show both theoretically and empirically that setting the negative distribution to be the model distribution results in bias reduction. We also derive a general form of the score function that unifies various architectures used in text retrieval. By combining hard negatives with appropriate score functions, we obtain strong results on the challenging task of zero-shot entity linking.",
        "author": "Wenzheng Zhang; Karl Stratos",
        "authorids": "/w/wenzheng-zhang/; /k/karl-stratos/",
        "bibtex": "@inproceedings{zhang-stratos-2021-understanding,\n    title = \"Understanding Hard Negatives in Noise Contrastive Estimation\",\n    author = \"Zhang, Wenzheng  and\n      Stratos, Karl\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.86/\",\n    doi = \"10.18653/v1/2021.naacl-main.86\",\n    pages = \"1090--1101\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.86.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.86/",
        "pdf_size": 509088,
        "gs_citation": 55,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13324511383699407180&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science, Rutgers University; Department of Computer Science, Rutgers University",
        "aff_domain": "rutgers.edu;rutgers.edu",
        "email": "rutgers.edu;rutgers.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Rutgers University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.rutgers.edu",
        "aff_unique_abbr": "Rutgers",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.102",
        "title": "Understanding by Understanding Not: Modeling Negation in Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Negation is a core construction in natural language. Despite being very successful on many tasks, state-of-the-art pre-trained language models often handle negation incorrectly. To improve language models in this regard, we propose to augment the language modeling objective with an unlikelihood objective that is based on negated generic sentences from a raw text corpus. By training BERT with the resulting combined objective we reduce the mean top 1 error rate to 4% on the negated LAMA dataset. We also see some improvements on the negated NLI benchmarks.",
        "author": "Arian Hosseini; Siva Reddy; Dzmitry Bahdanau; R Devon Hjelm; Alessandro Sordoni; Aaron Courville",
        "authorids": "/a/arian-hosseini/; /s/siva-reddy/; /d/dzmitry-bahdanau/; /r/r-devon-hjelm/; /a/alessandro-sordoni/; /a/aaron-courville/",
        "bibtex": "@inproceedings{hosseini-etal-2021-understanding,\n    title = \"Understanding by Understanding Not: Modeling Negation in Language Models\",\n    author = \"Hosseini, Arian  and\n      Reddy, Siva  and\n      Bahdanau, Dzmitry  and\n      Hjelm, R Devon  and\n      Sordoni, Alessandro  and\n      Courville, Aaron\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.102/\",\n    doi = \"10.18653/v1/2021.naacl-main.102\",\n    pages = \"1301--1312\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.102.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.102/",
        "pdf_size": 411550,
        "gs_citation": 96,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2232308230288870639&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Mila/Universit\u00e9 de Montr\u00e9al; Mila/McGill University; Element AI a ServiceNow Company; Mila/Universit\u00e9 de Montr\u00e9al+Microsoft Research; Microsoft Research; Mila/Universit\u00e9 de Montr\u00e9al",
        "aff_domain": "gmail.com; ; ; ; ; ",
        "email": "gmail.com; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;2;0+3;3;0",
        "aff_unique_norm": "Universit\u00e9 de Montr\u00e9al;McGill University;Element AI;Microsoft Corporation",
        "aff_unique_dep": "Mila;Mila;;Microsoft Research",
        "aff_unique_url": "https://www.umontreal.ca;https://www.mcgill.ca;https://www.elementai.com;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "UdeM;McGill;;MSR",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Montr\u00e9al;",
        "aff_country_unique_index": "0;0;0;0+1;1;0",
        "aff_country_unique": "Canada;United States"
    },
    {
        "id": "2021.naacl-main.302",
        "title": "UniDrop: A Simple yet Effective Technique to Improve Transformer without Extra Cost",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Transformer architecture achieves great success in abundant natural language processing tasks. The over-parameterization of the Transformer model has motivated plenty of works to alleviate its overfitting for superior performances. With some explorations, we find simple techniques such as dropout, can greatly boost model performance with a careful design. Therefore, in this paper, we integrate different dropout techniques into the training of Transformer models. Specifically, we propose an approach named UniDrop to unites three different dropout techniques from fine-grain to coarse-grain, i.e., feature dropout, structure dropout, and data dropout. Theoretically, we demonstrate that these three dropouts play different roles from regularization perspectives. Empirically, we conduct experiments on both neural machine translation and text classification benchmark datasets. Extensive results indicate that Transformer with UniDrop can achieve around 1.5 BLEU improvement on IWSLT14 translation tasks, and better accuracy for the classification even using strong pre-trained RoBERTa as backbone.",
        "author": "Zhen Wu; Lijun Wu; Qi Meng; Yingce Xia; Shufang Xie; Tao Qin; Xinyu Dai; Tie-Yan Liu",
        "authorids": "/z/zhen-wu/; /l/lijun-wu/; /q/qi-meng/; /y/yingce-xia/; /s/shufang-xie/; /t/tao-qin/; /x/xinyu-dai/; /t/tie-yan-liu/",
        "bibtex": "@inproceedings{wu-etal-2021-unidrop,\n    title = \"{U}ni{D}rop: A Simple yet Effective Technique to Improve Transformer without Extra Cost\",\n    author = \"Wu, Zhen  and\n      Wu, Lijun  and\n      Meng, Qi  and\n      Xia, Yingce  and\n      Xie, Shufang  and\n      Qin, Tao  and\n      Dai, Xinyu  and\n      Liu, Tie-Yan\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.302/\",\n    doi = \"10.18653/v1/2021.naacl-main.302\",\n    pages = \"3865--3878\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.302.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.302/",
        "pdf_size": 797637,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1575375273526086731&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "National Key Laboratory for Novel Software Technology, Nanjing University; Microsoft Research Asia; Microsoft Research Asia; Microsoft Research Asia; Microsoft Research Asia; Microsoft Research Asia; National Key Laboratory for Novel Software Technology, Nanjing University; Microsoft Research Asia",
        "aff_domain": "smail.nju.edu.cn;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;nju.edu.cn;microsoft.com",
        "email": "smail.nju.edu.cn;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;nju.edu.cn;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;1;1;1;1;1;0;1",
        "aff_unique_norm": "Nanjing University;Microsoft Research",
        "aff_unique_dep": "National Key Laboratory for Novel Software Technology;Research",
        "aff_unique_url": "http://www.nju.edu.cn;https://www.microsoft.com/en-us/research/group/asia",
        "aff_unique_abbr": "Nanjing University;MSR Asia",
        "aff_campus_unique_index": "1;1;1;1;1;1",
        "aff_campus_unique": ";Asia",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2021.naacl-main.211",
        "title": "Unified Pre-training for Program Understanding and Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Code summarization and generation empower conversion between programming language (PL) and natural language (NL), while code translation avails the migration of legacy code from one PL to another. This paper introduces PLBART, a sequence-to-sequence model capable of performing a broad spectrum of program and language understanding and generation tasks. PLBART is pre-trained on an extensive collection of Java and Python functions and associated NL text via denoising autoencoding. Experiments on code summarization in the English language, code generation, and code translation in seven programming languages show that PLBART outperforms or rivals state-of-the-art models. Moreover, experiments on discriminative tasks, e.g., program repair, clone detection, and vulnerable code detection, demonstrate PLBART\u2019s effectiveness in program understanding. Furthermore, analysis reveals that PLBART learns program syntax, style (e.g., identifier naming convention), logical flow (e.g., \u201cif\u201c block inside an \u201celse\u201c block is equivalent to \u201celse if\u201c block) that are crucial to program semantics and thus excels even with limited annotations.",
        "author": "Wasi Ahmad; Saikat Chakraborty; Baishakhi Ray; Kai-Wei Chang",
        "authorids": "/w/wasi-ahmad/; /s/saikat-chakraborty/; /b/baishakhi-ray/; /k/kai-wei-chang/",
        "bibtex": "@inproceedings{ahmad-etal-2021-unified,\n    title = \"Unified Pre-training for Program Understanding and Generation\",\n    author = \"Ahmad, Wasi  and\n      Chakraborty, Saikat  and\n      Ray, Baishakhi  and\n      Chang, Kai-Wei\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.211/\",\n    doi = \"10.18653/v1/2021.naacl-main.211\",\n    pages = \"2655--2668\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.211.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.211/",
        "pdf_size": 394379,
        "gs_citation": 924,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5217588768308087736&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "University of California, Los Angeles; Columbia University; Columbia University; University of California, Los Angeles",
        "aff_domain": "cs.ucla.edu;cs.columbia.edu;cs.columbia.edu;cs.ucla.edu",
        "email": "cs.ucla.edu;cs.columbia.edu;cs.columbia.edu;cs.ucla.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "University of California, Los Angeles;Columbia University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ucla.edu;https://www.columbia.edu",
        "aff_unique_abbr": "UCLA;Columbia",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.31",
        "title": "Unifying Cross-Lingual Semantic Role Labeling with Heterogeneous Linguistic Resources",
        "track": "main",
        "status": "Long",
        "award": true,
        "abstract": "While cross-lingual techniques are finding increasing success in a wide range of Natural Language Processing tasks, their application to Semantic Role Labeling (SRL) has been strongly limited by the fact that each language adopts its own linguistic formalism, from PropBank for English to AnCora for Spanish and PDT-Vallex for Czech, inter alia. In this work, we address this issue and present a unified model to perform cross-lingual SRL over heterogeneous linguistic resources. Our model implicitly learns a high-quality mapping for different formalisms across diverse languages without resorting to word alignment and/or translation techniques. We find that, not only is our cross-lingual system competitive with the current state of the art but that it is also robust to low-data scenarios. Most interestingly, our unified model is able to annotate a sentence in a single forward pass with all the inventories it was trained with, providing a tool for the analysis and comparison of linguistic theories across different languages. We release our code and model at https://github.com/SapienzaNLP/unify-srl.",
        "author": "Simone Conia; Andrea Bacciu; Roberto Navigli",
        "authorids": "/s/simone-conia/; /a/andrea-bacciu/; /r/roberto-navigli/",
        "bibtex": "@inproceedings{conia-etal-2021-unifying,\n    title = \"Unifying Cross-Lingual Semantic Role Labeling with Heterogeneous Linguistic Resources\",\n    author = \"Conia, Simone  and\n      Bacciu, Andrea  and\n      Navigli, Roberto\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.31/\",\n    doi = \"10.18653/v1/2021.naacl-main.31\",\n    pages = \"338--351\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.31.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.31/",
        "pdf_size": 1961360,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16231793149348577217&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Sapienza NLP Group, Department of Computer Science, Sapienza University of Rome; Sapienza NLP Group, Department of Computer Science, Sapienza University of Rome; Sapienza NLP Group, Department of Computer Science, Sapienza University of Rome",
        "aff_domain": "uniroma1.it;uniroma1.it;uniroma1.it",
        "email": "uniroma1.it;uniroma1.it;uniroma1.it",
        "github": "https://github.com/SapienzaNLP/unify-srl",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Sapienza University of Rome",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.uniroma1.it",
        "aff_unique_abbr": "Sapienza",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Rome",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Italy"
    },
    {
        "id": "2021.naacl-main.291",
        "title": "Universal Adversarial Attacks with Natural Triggers for Text Classification",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent work has demonstrated the vulnerability of modern text classifiers to universal adversarial attacks, which are input-agnostic sequences of words added to text processed by classifiers. Despite being successful, the word sequences produced in such attacks are often ungrammatical and can be easily distinguished from natural text. We develop adversarial attacks that appear closer to natural English phrases and yet confuse classification systems when added to benign inputs. We leverage an adversarially regularized autoencoder (ARAE) to generate triggers and propose a gradient-based search that aims to maximize the downstream classifier\u2019s prediction loss. Our attacks effectively reduce model accuracy on classification tasks while being less identifiable than prior models as per automatic detection metrics and human-subject studies. Our aim is to demonstrate that adversarial attacks can be made harder to detect than previously thought and to enable the development of appropriate defenses.",
        "author": "Liwei Song; Xinwei Yu; Hsuan-Tung Peng; Karthik Narasimhan",
        "authorids": "/l/liwei-song/; /x/xinwei-yu/; /h/hsuan-tung-peng/; /k/karthik-narasimhan/",
        "bibtex": "@inproceedings{song-etal-2021-universal,\n    title = \"Universal Adversarial Attacks with Natural Triggers for Text Classification\",\n    author = \"Song, Liwei  and\n      Yu, Xinwei  and\n      Peng, Hsuan-Tung  and\n      Narasimhan, Karthik\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.291/\",\n    doi = \"10.18653/v1/2021.naacl-main.291\",\n    pages = \"3724--3733\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.291.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.291/",
        "pdf_size": 691627,
        "gs_citation": 92,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10312480835358862111&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Princeton University; Princeton University; Princeton University; Princeton University",
        "aff_domain": "princeton.edu;princeton.edu;princeton.edu;princeton.edu",
        "email": "princeton.edu;princeton.edu;princeton.edu;princeton.edu",
        "github": "https://github.com/Hsuan-Tung/universal_attack_natural_trigger",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Princeton University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.princeton.edu",
        "aff_unique_abbr": "Princeton",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.440",
        "title": "Universal Semantic Tagging for English and Mandarin Chinese",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Universal Semantic Tagging aims to provide lightweight unified analysis for all languages at the word level. Though the proposed annotation scheme is conceptually promising, the feasibility is only examined in four Indo\u2013European languages. This paper is concerned with extending the annotation scheme to handle Mandarin Chinese and empirically study the plausibility of unifying meaning representations for multiple languages. We discuss a set of language-specific semantic phenomena, propose new annotation specifications and build a richly annotated corpus. The corpus consists of 1100 English\u2013Chinese parallel sentences, where compositional semantic analysis is available for English, and another 1000 Chinese sentences which has enriched syntactic analysis. By means of the new annotations, we also evaluate a series of neural tagging models to gauge how successful semantic tagging can be: accuracies of 92.7% and 94.6% are obtained for Chinese and English respectively. The English tagging performance is remarkably better than the state-of-the-art by 7.7%.",
        "author": "Wenxi Li; Yiyang Hou; Yajie Ye; Li Liang; Weiwei Sun",
        "authorids": "/w/wenxi-li/; /y/yiyang-hou/; /y/yajie-ye/; /l/li-liang/; /w/weiwei-sun-sd/",
        "bibtex": "@inproceedings{li-etal-2021-universal,\n    title = \"Universal Semantic Tagging for {E}nglish and {M}andarin {C}hinese\",\n    author = \"Li, Wenxi  and\n      Hou, Yiyang  and\n      Ye, Yajie  and\n      Liang, Li  and\n      Sun, Weiwei\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.440/\",\n    doi = \"10.18653/v1/2021.naacl-main.440\",\n    pages = \"5554--5566\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.440.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.440/",
        "pdf_size": 628104,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16456791891132648356&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Department of Chinese Language and Literature, Peking University; Department of Chinese Language and Literature, Peking University; Wangxuan Institute of Computer Technology, Peking University; Department of Chinese Language and Literature, Peking University + Tencent; Department of Computer Science and Technology, University of Cambridge",
        "aff_domain": "pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn;cam.ac.uk",
        "email": "pku.edu.cn;pku.edu.cn;pku.edu.cn;pku.edu.cn;cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0+1;2",
        "aff_unique_norm": "Peking University;Tencent Holdings Limited;University of Cambridge",
        "aff_unique_dep": "Department of Chinese Language and Literature;;Department of Computer Science and Technology",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.tencent.com;https://www.cam.ac.uk",
        "aff_unique_abbr": "Peking U;Tencent;Cambridge",
        "aff_campus_unique_index": "0;0;0;2",
        "aff_campus_unique": "Beijing;;Cambridge",
        "aff_country_unique_index": "0;0;0;0+0;1",
        "aff_country_unique": "China;United Kingdom"
    },
    {
        "id": "2021.naacl-main.445",
        "title": "Unsupervised Concept Representation Learning for Length-Varying Text Similarity",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Measuring document similarity plays an important role in natural language processing tasks. Most existing document similarity approaches suffer from the information gap caused by context and vocabulary mismatches when comparing varying-length texts. In this paper, we propose an unsupervised concept representation learning approach to address the above issues. Specifically, we propose a novel Concept Generation Network (CGNet) to learn concept representations from the perspective of the entire text corpus. Moreover, a concept-based document matching method is proposed to leverage advances in the recognition of local phrase features and corpus-level concept features. Extensive experiments on real-world data sets demonstrate that new method can achieve a considerable improvement in comparing length-varying texts. In particular, our model achieved 6.5% better F1 Score compared to the best of the baseline models for a concept-project benchmark dataset.",
        "author": "Xuchao Zhang; Bo Zong; Wei Cheng; Jingchao Ni; Yanchi Liu; Haifeng Chen",
        "authorids": "/x/xuchao-zhang/; /b/bo-zong/; /w/wei-cheng/; /j/jingchao-ni/; /y/yanchi-liu/; /h/haifeng-chen/",
        "bibtex": "@inproceedings{zhang-etal-2021-unsupervised,\n    title = \"Unsupervised Concept Representation Learning for Length-Varying Text Similarity\",\n    author = \"Zhang, Xuchao  and\n      Zong, Bo  and\n      Cheng, Wei  and\n      Ni, Jingchao  and\n      Liu, Yanchi  and\n      Chen, Haifeng\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.445/\",\n    doi = \"10.18653/v1/2021.naacl-main.445\",\n    pages = \"5611--5620\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.445.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.445/",
        "pdf_size": 518286,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15352935596963487957&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "NEC Laboratories America, Princeton, NJ, USA; NEC Laboratories America, Princeton, NJ, USA; NEC Laboratories America, Princeton, NJ, USA; NEC Laboratories America, Princeton, NJ, USA; NEC Laboratories America, Princeton, NJ, USA; NEC Laboratories America, Princeton, NJ, USA",
        "aff_domain": "nec-labs.com;nec-labs.com;nec-labs.com;nec-labs.com;nec-labs.com;nec-labs.com",
        "email": "nec-labs.com;nec-labs.com;nec-labs.com;nec-labs.com;nec-labs.com;nec-labs.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "NEC Laboratories America",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.nec-labs.com",
        "aff_unique_abbr": "NEC Labs America",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Princeton",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.469",
        "title": "Unsupervised Multi-hop Question Answering by Question Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Obtaining training data for multi-hop question answering (QA) is time-consuming and resource-intensive. We explore the possibility to train a well-performed multi-hop QA model without referencing any human-labeled multi-hop question-answer pairs, i.e., unsupervised multi-hop QA. We propose MQA-QG, an unsupervised framework that can generate human-like multi-hop training data from both homogeneous and heterogeneous data sources. MQA-QG generates questions by first selecting/generating relevant information from each data source and then integrating the multiple information to form a multi-hop question. Using only generated training data, we can train a competent multi-hop QA which achieves 61% and 83% of the supervised learning performance for the HybridQA and the HotpotQA dataset, respectively. We also show that pretraining the QA system with the generated data would greatly reduce the demand for human-annotated training data. Our codes are publicly available at https://github.com/teacherpeterpan/Unsupervised-Multi-hop-QA.",
        "author": "Liangming Pan; Wenhu Chen; Wenhan Xiong; Min-Yen Kan; William Yang Wang",
        "authorids": "/l/liangming-pan/; /w/wenhu-chen/; /w/wenhan-xiong/; /m/min-yen-kan/; /w/william-yang-wang/",
        "bibtex": "@inproceedings{pan-etal-2021-unsupervised,\n    title = \"Unsupervised Multi-hop Question Answering by Question Generation\",\n    author = \"Pan, Liangming  and\n      Chen, Wenhu  and\n      Xiong, Wenhan  and\n      Kan, Min-Yen  and\n      Wang, William Yang\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.469/\",\n    doi = \"10.18653/v1/2021.naacl-main.469\",\n    pages = \"5866--5880\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.469.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.469/",
        "pdf_size": 883866,
        "gs_citation": 66,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13956384354176259933&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "School of Computing, National University of Singapore, Singapore; University of California, Santa Barbara, CA, USA; University of California, Santa Barbara, CA, USA; School of Computing, National University of Singapore, Singapore; University of California, Santa Barbara, CA, USA",
        "aff_domain": "u.nus.edu;cs.ucsb.edu;cs.ucsb.edu;comp.nus.edu.sg;cs.ucsb.edu",
        "email": "u.nus.edu;cs.ucsb.edu;cs.ucsb.edu;comp.nus.edu.sg;cs.ucsb.edu",
        "github": "https://github.com/teacherpeterpan/Unsupervised-Multi-hop-QA",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;1;0;1",
        "aff_unique_norm": "National University of Singapore;University of California, Santa Barbara",
        "aff_unique_dep": "School of Computing;",
        "aff_unique_url": "https://www.nus.edu.sg;https://www.ucsb.edu",
        "aff_unique_abbr": "NUS;UCSB",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Santa Barbara",
        "aff_country_unique_index": "0;1;1;0;1",
        "aff_country_unique": "Singapore;United States"
    },
    {
        "id": "2021.naacl-main.420",
        "title": "Unsupervised Vision-and-Language Pre-training Without Parallel Images and Captions",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Pre-trained contextual vision-and-language (V&L) models have achieved impressive performance on various benchmarks. However, existing models require a large amount of parallel image-caption data for pre-training. Such data are costly to collect and require cumbersome curation. Inspired by unsupervised machine translation, we investigate if a strong V&L representation model can be learned through unsupervised pre-training without image-caption corpora. In particular, we propose to conduct \u201cmask-and-predict\u201d pre-training on text-only and image-only corpora and introduce the object tags detected by an object recognition model as anchor points to bridge two modalities. We find that such a simple approach achieves performance close to a model pre-trained with aligned data, on four English V&L benchmarks. Our work challenges the widely held notion that aligned data is necessary for V&L pre-training, while significantly reducing the amount of supervision needed for V&L models.",
        "author": "Liunian Harold Li; Haoxuan You; Zhecan Wang; Alireza Zareian; Shih-Fu Chang; Kai-Wei Chang",
        "authorids": "/l/liunian-harold-li/; /h/haoxuan-you/; /z/zhecan-wang/; /a/alireza-zareian/; /s/shih-fu-chang/; /k/kai-wei-chang/",
        "bibtex": "@inproceedings{li-etal-2021-unsupervised,\n    title = \"Unsupervised Vision-and-Language Pre-training Without Parallel Images and Captions\",\n    author = \"Li, Liunian Harold  and\n      You, Haoxuan  and\n      Wang, Zhecan  and\n      Zareian, Alireza  and\n      Chang, Shih-Fu  and\n      Chang, Kai-Wei\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.420/\",\n    doi = \"10.18653/v1/2021.naacl-main.420\",\n    pages = \"5339--5350\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.420.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.420/",
        "pdf_size": 4498312,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16117576977079681342&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "University of California, Los Angeles\u2020; Columbia University\u2217\u25e6+University of California, Los Angeles\u2020; Columbia University\u2217\u25e6+University of California, Los Angeles\u2020; Columbia University\u25e6; Columbia University\u25e6; University of California, Los Angeles\u2020",
        "aff_domain": "cs.ucla.edu;columbia.edu;columbia.edu;columbia.edu;columbia.edu;cs.ucla.edu",
        "email": "cs.ucla.edu;columbia.edu;columbia.edu;columbia.edu;columbia.edu;cs.ucla.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1+0;1+0;1;1;0",
        "aff_unique_norm": "University of California, Los Angeles;Columbia University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ucla.edu;https://www.columbia.edu",
        "aff_unique_abbr": "UCLA;Columbia",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0;0+0;0+0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.438",
        "title": "User-Generated Text Corpus for Evaluating Japanese Morphological Analysis and Lexical Normalization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Morphological analysis (MA) and lexical normalization (LN) are both important tasks for Japanese user-generated text (UGT). To evaluate and compare different MA/LN systems, we have constructed a publicly available Japanese UGT corpus. Our corpus comprises 929 sentences annotated with morphological and normalization information, along with category information we classified for frequent UGT-specific phenomena. Experiments on the corpus demonstrated the low performance of existing MA/LN methods for non-general words and non-standard forms, indicating that the corpus would be a challenging benchmark for further research on UGT.",
        "author": "Shohei Higashiyama; Masao Utiyama; Taro Watanabe; Eiichiro Sumita",
        "authorids": "/s/shohei-higashiyama/; /m/masao-utiyama/; /t/taro-watanabe/; /e/eiichiro-sumita/",
        "bibtex": "@inproceedings{higashiyama-etal-2021-user,\n    title = \"User-Generated Text Corpus for Evaluating {J}apanese Morphological Analysis and Lexical Normalization\",\n    author = \"Higashiyama, Shohei  and\n      Utiyama, Masao  and\n      Watanabe, Taro  and\n      Sumita, Eiichiro\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.438/\",\n    doi = \"10.18653/v1/2021.naacl-main.438\",\n    pages = \"5532--5541\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.438.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.438/",
        "pdf_size": 474408,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10398896025525421719&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "National Institute of Information and Communications Technology, Kyoto, Japan+ Nara Institute of Science and Technology, Nara, Japan; National Institute of Information and Communications Technology, Kyoto, Japan; Nara Institute of Science and Technology, Nara, Japan; National Institute of Information and Communications Technology, Kyoto, Japan",
        "aff_domain": "nict.go.jp;nict.go.jp;is.naist.jp;nict.go.jp",
        "email": "nict.go.jp;nict.go.jp;is.naist.jp;nict.go.jp",
        "github": "https://github.com/shigashiyama/jlexnorm",
        "project": "https://pj.ninjal.ac.jp/corpus_center/bccwj/en/subscription.html",
        "author_num": 4,
        "aff_unique_index": "0+1;0;1;0",
        "aff_unique_norm": "National Institute of Information and Communications Technology;Nara Institute of Science and Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.nict.go.jp/;https://www.nist.go.jp",
        "aff_unique_abbr": "NICT;NIST",
        "aff_campus_unique_index": "0+1;0;1;0",
        "aff_campus_unique": "Kyoto;Nara",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2021.naacl-main.206",
        "title": "Variance-reduced First-order Meta-learning for Natural Language Processing Tasks",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "First-order meta-learning algorithms have been widely used in practice to learn initial model parameters that can be quickly adapted to new tasks due to their efficiency and effectiveness. However, existing studies find that meta-learner can overfit to some specific adaptation when we have heterogeneous tasks, leading to significantly degraded performance. In Natural Language Processing (NLP) applications, datasets are often diverse and each task has its unique characteristics. Therefore, to address the overfitting issue when applying first-order meta-learning to NLP applications, we propose to reduce the variance of the gradient estimator used in task adaptation. To this end, we develop a variance-reduced first-order meta-learning algorithm. The core of our algorithm is to introduce a novel variance reduction term to the gradient estimation when performing the task adaptation. Experiments on two NLP applications: few-shot text classification and multi-domain dialog state tracking demonstrate the superior performance of our proposed method.",
        "author": "Lingxiao Wang; Kevin Huang; Tengyu Ma; Quanquan Gu; Jing Huang",
        "authorids": "/l/ling-xiao-wang/; /k/kevin-huang/; /t/tengyu-ma/; /q/quanquan-gu/; /j/jing-huang/",
        "bibtex": "@inproceedings{wang-etal-2021-variance,\n    title = \"Variance-reduced First-order Meta-learning for Natural Language Processing Tasks\",\n    author = \"Wang, Lingxiao  and\n      Huang, Kevin  and\n      Ma, Tengyu  and\n      Gu, Quanquan  and\n      Huang, Jing\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.206/\",\n    doi = \"10.18653/v1/2021.naacl-main.206\",\n    pages = \"2609--2615\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.206.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.206/",
        "pdf_size": 368794,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17903201382618215513&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Department of Computer Science, University of California, Los Angeles; JD AI Research, Mountain View, CA; Department of Computer Science, Stanford University; Department of Computer Science, University of California, Los Angeles; JD AI Research, Mountain View, CA",
        "aff_domain": "cs.ucla.edu;jd.com;stanford.edu;cs.ucla.edu;jd.com",
        "email": "cs.ucla.edu;jd.com;stanford.edu;cs.ucla.edu;jd.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;2;0;1",
        "aff_unique_norm": "University of California, Los Angeles;JD AI Research;Stanford University",
        "aff_unique_dep": "Department of Computer Science;;Department of Computer Science",
        "aff_unique_url": "https://www.ucla.edu;;https://www.stanford.edu",
        "aff_unique_abbr": "UCLA;;Stanford",
        "aff_campus_unique_index": "0;1;2;0;1",
        "aff_campus_unique": "Los Angeles;Mountain View;Stanford",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.196",
        "title": "Video Question Answering with Phrases via Semantic Roles",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Video Question Answering (VidQA) evaluation metrics have been limited to a single-word answer or selecting a phrase from a fixed set of phrases. These metrics limit the VidQA models\u2019 application scenario. In this work, we leverage semantic roles derived from video descriptions to mask out certain phrases, to introduce VidQAP which poses VidQA as a fill-in-the-phrase task. To enable evaluation of answer phrases, we compute the relative improvement of the predicted answer compared to an empty string. To reduce the influence of language bias in VidQA datasets, we retrieve a video having a different answer for the same question. To facilitate research, we construct ActivityNet-SRL-QA and Charades-SRL-QA and benchmark them by extending three vision-language models. We perform extensive analysis and ablative studies to guide future work. Code and data are public.",
        "author": "Arka Sadhu; Kan Chen; Ram Nevatia",
        "authorids": "/a/arka-sadhu/; /k/kan-chen/; /r/ram-nevatia/",
        "bibtex": "@inproceedings{sadhu-etal-2021-video,\n    title = \"Video Question Answering with Phrases via Semantic Roles\",\n    author = \"Sadhu, Arka  and\n      Chen, Kan  and\n      Nevatia, Ram\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.196/\",\n    doi = \"10.18653/v1/2021.naacl-main.196\",\n    pages = \"2460--2478\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.196.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.196/",
        "pdf_size": 2607996,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17964962279336991652&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "University of Southern California; Facebook Inc.; University of Southern California",
        "aff_domain": "usc.edu;fb.com;usc.edu",
        "email": "usc.edu;fb.com;usc.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Southern California;Facebook",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.usc.edu;https://www.facebook.com",
        "aff_unique_abbr": "USC;FB",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.119",
        "title": "Video-aided Unsupervised Grammar Induction",
        "track": "main",
        "status": "Long",
        "award": true,
        "abstract": "We investigate video-aided grammar induction, which learns a constituency parser from both unlabeled text and its corresponding video. Existing methods of multi-modal grammar induction focus on grammar induction from text-image pairs, with promising results showing that the information from static images is useful in induction. However, videos provide even richer information, including not only static objects but also actions and state changes useful for inducing verb phrases. In this paper, we explore rich features (e.g. action, object, scene, audio, face, OCR and speech) from videos, taking the recent Compound PCFG model as the baseline. We further propose a Multi-Modal Compound PCFG model (MMC-PCFG) to effectively aggregate these rich features from different modalities. Our proposed MMC-PCFG is trained end-to-end and outperforms each individual modality and previous state-of-the-art systems on three benchmarks, i.e. DiDeMo, YouCook2 and MSRVTT, confirming the effectiveness of leveraging video information for unsupervised grammar induction.",
        "author": "Songyang Zhang; Linfeng Song; Lifeng Jin; Kun Xu; Dong Yu; Jiebo Luo",
        "authorids": "/s/songyang-zhang/; /l/linfeng-song/; /l/lifeng-jin/; /k/kun-xu/; /d/dong-yu/; /j/jiebo-luo/",
        "bibtex": "@inproceedings{zhang-etal-2021-video,\n    title = \"Video-aided Unsupervised Grammar Induction\",\n    author = \"Zhang, Songyang  and\n      Song, Linfeng  and\n      Jin, Lifeng  and\n      Xu, Kun  and\n      Yu, Dong  and\n      Luo, Jiebo\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.119/\",\n    doi = \"10.18653/v1/2021.naacl-main.119\",\n    pages = \"1513--1524\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.119.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.119/",
        "pdf_size": 1109900,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5084385779758855396&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "University of Rochester, Rochester, NY, USA+Tencent AI Lab, Bellevue, WA, USA; Tencent AI Lab, Bellevue, WA, USA; Tencent AI Lab, Bellevue, WA, USA; Tencent AI Lab, Bellevue, WA, USA; Tencent AI Lab, Bellevue, WA, USA; University of Rochester, Rochester, NY, USA",
        "aff_domain": "ur.rochester.edu;tencent.com;tencent.com;tencent.com;tencent.com;cs.rochester.edu",
        "email": "ur.rochester.edu;tencent.com;tencent.com;tencent.com;tencent.com;cs.rochester.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;1;1;1;1;0",
        "aff_unique_norm": "University of Rochester;Tencent",
        "aff_unique_dep": ";AI Lab",
        "aff_unique_url": "https://www.rochester.edu;https://ai.tencent.com",
        "aff_unique_abbr": "U of R;Tencent AI Lab",
        "aff_campus_unique_index": "0+1;1;1;1;1;0",
        "aff_campus_unique": "Rochester;Bellevue",
        "aff_country_unique_index": "0+0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.198",
        "title": "WEC: Deriving a Large-scale Cross-document Event Coreference dataset from Wikipedia",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Cross-document event coreference resolution is a foundational task for NLP applications involving multi-text processing. However, existing corpora for this task are scarce and relatively small, while annotating only modest-size clusters of documents belonging to the same topic. To complement these resources and enhance future research, we present Wikipedia Event Coreference (WEC), an efficient methodology for gathering a large-scale dataset for cross-document event coreference from Wikipedia, where coreference links are not restricted within predefined topics. We apply this methodology to the English Wikipedia and extract our large-scale WEC-Eng dataset. Notably, our dataset creation method is generic and can be applied with relatively little effort to other Wikipedia languages. To set baseline results, we develop an algorithm that adapts components of state-of-the-art models for within-document coreference resolution to the cross-document setting. Our model is suitably efficient and outperforms previously published state-of-the-art results for the task.",
        "author": "Alon Eirew; Arie Cattan; Ido Dagan",
        "authorids": "/a/alon-eirew/; /a/arie-cattan/; /i/ido-dagan/",
        "bibtex": "@inproceedings{eirew-etal-2021-wec,\n    title = \"{WEC}: Deriving a Large-scale Cross-document Event Coreference dataset from {W}ikipedia\",\n    author = \"Eirew, Alon  and\n      Cattan, Arie  and\n      Dagan, Ido\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.198/\",\n    doi = \"10.18653/v1/2021.naacl-main.198\",\n    pages = \"2498--2510\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.198.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.198/",
        "pdf_size": 1226818,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14002567213154822946&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Bar Ilan University, Ramat-Gan, Israel+Intel Labs, Israel; Bar Ilan University, Ramat-Gan, Israel; Bar Ilan University, Ramat-Gan, Israel",
        "aff_domain": "intel.com;gmail.com;cs.biu.ac.il",
        "email": "intel.com;gmail.com;cs.biu.ac.il",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;0;0",
        "aff_unique_norm": "Bar Ilan University;Intel Labs",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.biu.ac.il;https://www.intel.com/content/www/us/en/research/labs.html",
        "aff_unique_abbr": "BIU;Intel",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Ramat-Gan;",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "2021.naacl-main.169",
        "title": "WRIME: A New Dataset for Emotional Intensity Estimation with Subjective and Objective Annotations",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We annotate 17,000 SNS posts with both the writer\u2019s subjective emotional intensity and the reader\u2019s objective one to construct a Japanese emotion analysis dataset. In this study, we explore the difference between the emotional intensity of the writer and that of the readers with this dataset. We found that the reader cannot fully detect the emotions of the writer, especially anger and trust. In addition, experimental results in estimating the emotional intensity show that it is more difficult to estimate the writer\u2019s subjective labels than the readers\u2019. The large gap between the subjective and objective emotions imply the complexity of the mapping from a post to the subjective emotion intensities, which also leads to a lower performance with machine learning models.",
        "author": "Tomoyuki Kajiwara; Chenhui Chu; Noriko Takemura; Yuta Nakashima; Hajime Nagahara",
        "authorids": "/t/tomoyuki-kajiwara/; /c/chenhui-chu/; /n/noriko-takemura/; /y/yuta-nakashima/; /h/hajime-nagahara/",
        "bibtex": "@inproceedings{kajiwara-etal-2021-wrime,\n    title = \"{WRIME}: A New Dataset for Emotional Intensity Estimation with Subjective and Objective Annotations\",\n    author = \"Kajiwara, Tomoyuki  and\n      Chu, Chenhui  and\n      Takemura, Noriko  and\n      Nakashima, Yuta  and\n      Nagahara, Hajime\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.169/\",\n    doi = \"10.18653/v1/2021.naacl-main.169\",\n    pages = \"2095--2104\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.169.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.169/",
        "pdf_size": 1157174,
        "gs_citation": 62,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3547855106561722514&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "https://github.com/ids-cv/wrime",
        "project": "",
        "author_num": 5
    },
    {
        "id": "2021.naacl-main.181",
        "title": "What About the Precedent: An Information-Theoretic Analysis of Common Law",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In common law, the outcome of a new case is determined mostly by precedent cases, rather than by existing statutes. However, how exactly does the precedent influence the outcome of a new case? Answering this question is crucial for guaranteeing fair and consistent judicial decision-making. We are the first to approach this question computationally by comparing two longstanding jurisprudential views; Halsbury\u2019s, who believes that the arguments of the precedent are the main determinant of the outcome, and Goodhart\u2019s, who believes that what matters most is the precedent\u2019s facts. We base our study on the corpus of legal cases from the European Court of Human Rights (ECtHR), which allows us to access not only the case itself, but also cases cited in the judges\u2019 arguments (i.e. the precedent cases). Taking an information-theoretic view, and modelling the question as a case out-come classification task, we find that the precedent\u2019s arguments share 0.38 nats of information with the case\u2019s outcome, whereas precedent\u2019s facts only share 0.18 nats of information (i.e.,58% less); suggesting Halsbury\u2019s view may be more accurate in this specific court. We found however in a qualitative analysis that there are specific statues where Goodhart\u2019s view dominates, and present some evidence these are the ones where the legal concept at hand is less straightforward.",
        "author": "Josef Valvoda; Tiago Pimentel; Niklas Stoehr; Ryan Cotterell; Simone Teufel",
        "authorids": "/j/josef-valvoda/; /t/tiago-pimentel/; /n/niklas-stoehr/; /r/ryan-cotterell/; /s/simone-teufel/",
        "bibtex": "@inproceedings{valvoda-etal-2021-precedent,\n    title = \"What About the Precedent: An Information-Theoretic Analysis of Common Law\",\n    author = \"Valvoda, Josef  and\n      Pimentel, Tiago  and\n      Stoehr, Niklas  and\n      Cotterell, Ryan  and\n      Teufel, Simone\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.181/\",\n    doi = \"10.18653/v1/2021.naacl-main.181\",\n    pages = \"2275--2288\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.181.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.181/",
        "pdf_size": 885500,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3420930871647993073&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "University of Cambridge; University of Cambridge; University of Cambridge; ETH Z\u00fcrich; University of Cambridge",
        "aff_domain": "cam.ac.uk;cam.ac.uk; ; ; ",
        "email": "cam.ac.uk;cam.ac.uk; ; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "University of Cambridge;ETH Z\u00fcrich",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cam.ac.uk;https://www.ethz.ch",
        "aff_unique_abbr": "Cambridge;ETHZ",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;0;0;1;0",
        "aff_country_unique": "United Kingdom;Switzerland"
    },
    {
        "id": "2021.naacl-main.385",
        "title": "What Will it Take to Fix Benchmarking in Natural Language Understanding?",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Evaluation for many natural language understanding (NLU) tasks is broken: Unreliable and biased systems score so highly on standard benchmarks that there is little room for researchers who develop better systems to demonstrate their improvements. The recent trend to abandon IID benchmarks in favor of adversarially-constructed, out-of-distribution test sets ensures that current models will perform poorly, but ultimately only obscures the abilities that we want our benchmarks to measure. In this position paper, we lay out four criteria that we argue NLU benchmarks should meet. We argue most current benchmarks fail at these criteria, and that adversarial data collection does not meaningfully address the causes of these failures. Instead, restoring a healthy evaluation ecosystem will require significant progress in the design of benchmark datasets, the reliability with which they are annotated, their size, and the ways they handle social bias.",
        "author": "Samuel R. Bowman; George Dahl",
        "authorids": "/s/samuel-bowman/; /g/george-dahl/",
        "bibtex": "@inproceedings{bowman-dahl-2021-will,\n    title = \"What Will it Take to Fix Benchmarking in Natural Language Understanding?\",\n    author = \"Bowman, Samuel R.  and\n      Dahl, George\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.385/\",\n    doi = \"10.18653/v1/2021.naacl-main.385\",\n    pages = \"4843--4855\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.385.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.385/",
        "pdf_size": 264799,
        "gs_citation": 189,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15066484312916710901&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "New York University; Google Research, Brain Team",
        "aff_domain": "nyu.edu;google.com",
        "email": "nyu.edu;google.com",
        "github": "",
        "project": "https://dynabench.org/about",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "New York University;Google",
        "aff_unique_dep": ";Google Research",
        "aff_unique_url": "https://www.nyu.edu;https://research.google",
        "aff_unique_abbr": "NYU;Google",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.382",
        "title": "What\u2019s in a Summary? Laying the Groundwork for Advances in Hospital-Course Summarization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Summarization of clinical narratives is a long-standing research problem. Here, we introduce the task of hospital-course summarization. Given the documentation authored throughout a patient\u2019s hospitalization, generate a paragraph that tells the story of the patient admission. We construct an English, text-to-text dataset of 109,000 hospitalizations (2M source notes) and their corresponding summary proxy: the clinician-authored \u201cBrief Hospital Course\u201d paragraph written as part of a discharge note. Exploratory analyses reveal that the BHC paragraphs are highly abstractive with some long extracted fragments; are concise yet comprehensive; differ in style and content organization from the source notes; exhibit minimal lexical cohesion; and represent silver-standard references. Our analysis identifies multiple implications for modeling this complex, multi-document summarization task.",
        "author": "Griffin Adams; Emily Alsentzer; Mert Ketenci; Jason Zucker; No\u00e9mie Elhadad",
        "authorids": "/g/griffin-adams/; /e/emily-alsentzer/; /m/mert-ketenci/; /j/jason-zucker/; /n/noemie-elhadad/",
        "bibtex": "@inproceedings{adams-etal-2021-whats,\n    title = \"What{'}s in a Summary? Laying the Groundwork for Advances in Hospital-Course Summarization\",\n    author = \"Adams, Griffin  and\n      Alsentzer, Emily  and\n      Ketenci, Mert  and\n      Zucker, Jason  and\n      Elhadad, No{\\'e}mie\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.382/\",\n    doi = \"10.18653/v1/2021.naacl-main.382\",\n    pages = \"4794--4811\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.382.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.382/",
        "pdf_size": 1044138,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7812205835710488112&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Columbia University; Harvard-MIT\u2019s Health Science and Technology + Columbia University; Columbia University; Columbia University; Columbia University",
        "aff_domain": "columbia.edu;mit.edu;columbia.edu;cumc.columbia.edu;columbia.edu",
        "email": "columbia.edu;mit.edu;columbia.edu;cumc.columbia.edu;columbia.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1+0;0;0;0",
        "aff_unique_norm": "Columbia University;Harvard-MIT Health Sciences and Technology",
        "aff_unique_dep": ";Health Sciences and Technology",
        "aff_unique_url": "https://www.columbia.edu;https://hst.mit.edu",
        "aff_unique_abbr": "Columbia;HST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.38",
        "title": "When Being Unseen from mBERT is just the Beginning: Handling New Languages With Multilingual Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Transfer learning based on pretraining language models on a large amount of raw data has become a new norm to reach state-of-the-art performance in NLP. Still, it remains unclear how this approach should be applied for unseen languages that are not covered by any available large-scale multilingual language model and for which only a small amount of raw data is generally available. In this work, by comparing multilingual and monolingual models, we show that such models behave in multiple ways on unseen languages. Some languages greatly benefit from transfer learning and behave similarly to closely related high resource languages whereas others apparently do not. Focusing on the latter, we show that this failure to transfer is largely related to the impact of the script used to write such languages. We show that transliterating those languages significantly improves the potential of large-scale multilingual language models on downstream tasks. This result provides a promising direction towards making these massively multilingual models useful for a new set of unseen languages.",
        "author": "Benjamin Muller; Antonios Anastasopoulos; Beno\u00eet Sagot; Djam\u00e9 Seddah",
        "authorids": "/b/benjamin-muller/; /a/antonios-anastasopoulos/; /b/benoit-sagot/; /d/djame-seddah/",
        "bibtex": "@inproceedings{muller-etal-2021-unseen,\n    title = \"When Being Unseen from m{BERT} is just the Beginning: Handling New Languages With Multilingual Language Models\",\n    author = \"Muller, Benjamin  and\n      Anastasopoulos, Antonios  and\n      Sagot, Beno{\\^i}t  and\n      Seddah, Djam{\\'e}\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.38/\",\n    doi = \"10.18653/v1/2021.naacl-main.38\",\n    pages = \"448--462\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.38.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.38/",
        "pdf_size": 395834,
        "gs_citation": 153,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2723320077757229105&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Inria, Paris, France+Sorbonne Universit\u00e9, Paris, France; Department of Computer Science, George Mason University, USA; Inria, Paris, France; Inria, Paris, France",
        "aff_domain": "inria.fr;gmu.edu;inria.fr;inria.fr",
        "email": "inria.fr;gmu.edu;inria.fr;inria.fr",
        "github": "https://github.com/benjamin-mlr/mbert-unseen-languages.git",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;2;0;0",
        "aff_unique_norm": "Inria;Sorbonne Universit\u00e9;George Mason University",
        "aff_unique_dep": ";;Department of Computer Science",
        "aff_unique_url": "https://www.inria.fr;https://www.sorbonne-universite.fr;https://www.gmu.edu",
        "aff_unique_abbr": "Inria;Sorbonne U;GMU",
        "aff_campus_unique_index": "0+0;0;0",
        "aff_campus_unique": "Paris;",
        "aff_country_unique_index": "0+0;1;0;0",
        "aff_country_unique": "France;United States"
    },
    {
        "id": "2021.naacl-industry.22",
        "title": "When and Why a Model Fails? A Human-in-the-loop Error Detection Framework for Sentiment Analysis",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Although deep neural networks have been widely employed and proven effective in sentiment analysis tasks, it remains challenging for model developers to assess their models for erroneous predictions that might exist prior to deployment. Once deployed, emergent errors can be hard to identify in prediction run-time and impossible to trace back to their sources. To address such gaps, in this paper we propose an error detection framework for sentiment analysis based on explainable features. We perform global-level feature validation with human-in-the-loop assessment, followed by an integration of global and local-level feature contribution analysis. Experimental results show that, given limited human-in-the-loop intervention, our method is able to identify erroneous model predictions on unseen data with high precision.",
        "author": "Zhe Liu; Yufan Guo; Jalal Mahmud",
        "authorids": "/z/zhe-liu/; /y/yufan-guo/; /j/jalal-mahmud/",
        "bibtex": "@inproceedings{liu-etal-2021-model,\n    title = \"When and Why a Model Fails? A Human-in-the-loop Error Detection Framework for Sentiment Analysis\",\n    author = \"Liu, Zhe  and\n      Guo, Yufan  and\n      Mahmud, Jalal\",\n    editor = \"Kim, Young-bum  and\n      Li, Yunyao  and\n      Rambow, Owen\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-industry.22/\",\n    doi = \"10.18653/v1/2021.naacl-industry.22\",\n    pages = \"170--177\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-industry.22.pdf",
        "site": "https://aclanthology.org/2021.naacl-industry.22/",
        "pdf_size": 655542,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1915417419870316917&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "IBM Research - Almaden; Amazon Alexa AI + IBM Research - Almaden; IBM Research - Almaden",
        "aff_domain": "us.ibm.com;amazon.com;us.ibm.com",
        "email": "us.ibm.com;amazon.com;us.ibm.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1+0;0",
        "aff_unique_norm": "IBM Research;Amazon",
        "aff_unique_dep": ";Alexa AI",
        "aff_unique_url": "https://www.ibm.com/research;https://www.amazon.com",
        "aff_unique_abbr": "IBM;Amazon",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Almaden;",
        "aff_country_unique_index": "0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-industry.1",
        "title": "When does text prediction benefit from additional context? An exploration of contextual signals for chat and email messages",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Email and chat communication tools are increasingly important for completing daily tasks. Accurate real-time phrase completion can save time and bolster productivity. Modern text prediction algorithms are based on large language models which typically rely on the prior words in a message to predict a completion. We examine how additional contextual signals (from previous messages, time, and subject) affect the performance of a commercial text prediction model. We compare contextual text prediction in chat and email messages from two of the largest commercial platforms Microsoft Teams and Outlook, finding that contextual signals contribute to performance differently between these scenarios. On emails, time context is most beneficial with small relative gains of 2% over baseline. Whereas, in chat scenarios, using a tailored set of previous messages as context yields relative improvements over the baseline between 9.3% and 18.6% across various critical service-oriented text prediction metrics.",
        "author": "Stojan Trajanovski; Chad Atalla; Kunho Kim; Vipul Agarwal; Milad Shokouhi; Chris Quirk",
        "authorids": "/s/stojan-trajanovski/; /c/chad-atalla/; /k/kunho-kim/; /v/vipul-agarwal/; /m/milad-shokouhi/; /c/chris-quirk/",
        "bibtex": "@inproceedings{trajanovski-etal-2021-text,\n    title = \"When does text prediction benefit from additional context? An exploration of contextual signals for chat and email messages\",\n    author = \"Trajanovski, Stojan  and\n      Atalla, Chad  and\n      Kim, Kunho  and\n      Agarwal, Vipul  and\n      Shokouhi, Milad  and\n      Quirk, Chris\",\n    editor = \"Kim, Young-bum  and\n      Li, Yunyao  and\n      Rambow, Owen\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-industry.1/\",\n    doi = \"10.18653/v1/2021.naacl-industry.1\",\n    pages = \"1--9\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-industry.1.pdf",
        "site": "https://aclanthology.org/2021.naacl-industry.1/",
        "pdf_size": 501556,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6826257113495509578&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Microsoft; Microsoft; Microsoft; Microsoft; Microsoft; Microsoft",
        "aff_domain": "microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "email": "microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Microsoft Corporation",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.microsoft.com",
        "aff_unique_abbr": "Microsoft",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.143",
        "title": "Why Do Document-Level Polarity Classifiers Fail?",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Machine learning solutions are often criticized for the lack of explanation of their successes and failures. Understanding which instances are misclassified and why is essential to improve the learning process. This work helps to fill this gap by proposing a methodology to characterize, quantify and measure the impact of hard instances in the task of polarity classification of movie reviews. We characterize such instances into two categories: neutrality, where the text does not convey a clear polarity, and discrepancy, where the polarity of the text is the opposite of its true rating. We quantify the number of hard instances in polarity classification of movie reviews and provide empirical evidence about the need to pay attention to such problematic instances, as they are much harder to classify, for both machine and human classifiers. To the best of our knowledge, this is the first systematic analysis of the impact of hard instances in polarity detection from well-formed textual reviews.",
        "author": "Karen Martins; Pedro O.S Vaz-de-Melo; Rodrygo Santos",
        "authorids": "/k/karen-martins/; /p/pedro-o-s-vaz-de-melo/; /r/rodrygo-santos/",
        "bibtex": "@inproceedings{martins-etal-2021-document,\n    title = \"Why Do Document-Level Polarity Classifiers Fail?\",\n    author = \"Martins, Karen  and\n      Vaz-de-Melo, Pedro O.S  and\n      Santos, Rodrygo\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.143/\",\n    doi = \"10.18653/v1/2021.naacl-main.143\",\n    pages = \"1782--1794\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.143.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.143/",
        "pdf_size": 441580,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18063172680610116914&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "CS Department, UFMG, Belo Horizonte, MG, Brazil; CS Department, UFMG, Belo Horizonte, MG, Brazil; CS Department, UFMG, Belo Horizonte, MG, Brazil",
        "aff_domain": "dcc.ufmg.br;dcc.ufmg.br;dcc.ufmg.br",
        "email": "dcc.ufmg.br;dcc.ufmg.br;dcc.ufmg.br",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Universidade Federal de Minas Gerais",
        "aff_unique_dep": "CS Department",
        "aff_unique_url": "https://www.ufmg.br",
        "aff_unique_abbr": "UFMG",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Belo Horizonte",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Brazil"
    },
    {
        "id": "2021.naacl-main.177",
        "title": "WikiTalkEdit: A Dataset for modeling Editors\u2019 behaviors on Wikipedia",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "This study introduces and analyzes WikiTalkEdit, a dataset of conversations and edit histories from Wikipedia, for research in online cooperation and conversation modeling. The dataset comprises dialog triplets from the Wikipedia Talk pages, and editing actions on the corresponding articles being discussed. We show how the data supports the classic understanding of style matching, where positive emotion and the use of first-person pronouns predict a positive emotional change in a Wikipedia contributor. However, they do not predict editorial behavior. On the other hand, feedback invoking evidentiality and criticism, and references to Wikipedia\u2019s community norms, is more likely to persuade the contributor to perform edits but is less likely to lead to a positive emotion. We developed baseline classifiers trained on pre-trained RoBERTa features that can predict editorial change with an F1 score of .54, as compared to an F1 score of .66 for predicting emotional change. A diagnostic analysis of persisting errors is also provided. We conclude with possible applications and recommendations for future work. The dataset is publicly available for the research community at https://github.com/kj2013/WikiTalkEdit/.",
        "author": "Kokil Jaidka; Andrea Ceolin; Iknoor Singh; Niyati Chhaya; Lyle Ungar",
        "authorids": "/k/kokil-jaidka/; /a/andrea-ceolin/; /i/iknoor-singh/; /n/niyati-chhaya/; /l/lyle-ungar/",
        "bibtex": "@inproceedings{jaidka-etal-2021-wikitalkedit,\n    title = \"{W}iki{T}alk{E}dit: A Dataset for modeling Editors' behaviors on {W}ikipedia\",\n    author = \"Jaidka, Kokil  and\n      Ceolin, Andrea  and\n      Singh, Iknoor  and\n      Chhaya, Niyati  and\n      Ungar, Lyle\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.177/\",\n    doi = \"10.18653/v1/2021.naacl-main.177\",\n    pages = \"2191--2200\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.177.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.177/",
        "pdf_size": 1381581,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3587891290425558168&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Communications and New Media, National University of Singapore; Department of Linguistics, University of Pennsylvania; Department of Computer Science, University of Shef\ufb01eld; Big Data Experience Lab, Adobe Research India Pvt. Ltd.; Department of Computer & Information Science, University of Pennsylvania",
        "aff_domain": "nus.edu.sg; ; ; ; ",
        "email": "nus.edu.sg; ; ; ; ",
        "github": "https://github.com/kj2013/WikiTalkEdit/",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;2;3;1",
        "aff_unique_norm": "National University of Singapore;University of Pennsylvania;University of Sheffield;Adobe Research India Pvt. Ltd.",
        "aff_unique_dep": "Department of Communications and New Media;Department of Linguistics;Department of Computer Science;Big Data Experience Lab",
        "aff_unique_url": "https://www.nus.edu.sg;https://www.upenn.edu;https://www.sheffield.ac.uk;https://research.adobe.com/",
        "aff_unique_abbr": "NUS;UPenn;Sheffield;Adobe Research India",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;2;3;1",
        "aff_country_unique": "Singapore;United States;United Kingdom;India"
    },
    {
        "id": "2021.naacl-main.286",
        "title": "Wikipedia Entities as Rendezvous across Languages: Grounding Multilingual Language Models by Predicting Wikipedia Hyperlinks",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Masked language models have quickly become the de facto standard when processing text. Recently, several approaches have been proposed to further enrich word representations with external knowledge sources such as knowledge graphs. However, these models are devised and evaluated in a monolingual setting only. In this work, we propose a language-independent entity prediction task as an intermediate training procedure to ground word representations on entity semantics and bridge the gap across different languages by means of a shared vocabulary of entities. We show that our approach effectively injects new lexical-semantic knowledge into neural models, improving their performance on different semantic tasks in the zero-shot crosslingual setting. As an additional advantage, our intermediate training does not require any supplementary input, allowing our models to be applied to new datasets right away. In our experiments, we use Wikipedia articles in up to 100 languages and already observe consistent gains compared to strong baselines when predicting entities using only the English Wikipedia. Further adding extra languages lead to improvements in most tasks up to a certain point, but overall we found it non-trivial to scale improvements in model transferability by training on ever increasing amounts of Wikipedia languages.",
        "author": "Iacer Calixto; Alessandro Raganato; Tommaso Pasini",
        "authorids": "/i/iacer-calixto/; /a/alessandro-raganato/; /t/tommaso-pasini/",
        "bibtex": "@inproceedings{calixto-etal-2021-wikipedia,\n    title = \"{W}ikipedia Entities as Rendezvous across Languages: Grounding Multilingual Language Models by Predicting {W}ikipedia Hyperlinks\",\n    author = \"Calixto, Iacer  and\n      Raganato, Alessandro  and\n      Pasini, Tommaso\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.286/\",\n    doi = \"10.18653/v1/2021.naacl-main.286\",\n    pages = \"3651--3661\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.286.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.286/",
        "pdf_size": 652815,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18281318314889121235&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Center for Data Science, New York University+ILLC, University of Amsterdam; Department of Digital Humanities, University of Helsinki, Finland; Department of Computer Science, University of Copenhagen+University of Rome \u201cLa Sapienza\u201d",
        "aff_domain": "nyu.edu;helsinki.fi;di.ku.dk",
        "email": "nyu.edu;helsinki.fi;di.ku.dk",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;2;3+4",
        "aff_unique_norm": "New York University;University of Amsterdam;University of Helsinki;University of Copenhagen;University of Rome La Sapienza",
        "aff_unique_dep": "Center for Data Science;ILLC;Department of Digital Humanities;Department of Computer Science;",
        "aff_unique_url": "https://www.nyu.edu;https://www.uva.nl;https://www.helsinki.fi;https://www.ku.dk;https://www.uniroma1.it",
        "aff_unique_abbr": "NYU;UvA;UH;UCPH;La Sapienza",
        "aff_campus_unique_index": "0+1;",
        "aff_campus_unique": "New York;Amsterdam;",
        "aff_country_unique_index": "0+1;2;3+4",
        "aff_country_unique": "United States;Netherlands;Finland;Denmark;Italy"
    },
    {
        "id": "2021.naacl-main.351",
        "title": "Word Complexity is in the Eye of the Beholder",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Lexical complexity is a highly subjective notion, yet this factor is often neglected in lexical simplification and readability systems which use a \u201done-size-fits-all\u201d approach. In this paper, we investigate which aspects contribute to the notion of lexical complexity in various groups of readers, focusing on native and non-native speakers of English, and how the notion of complexity changes depending on the proficiency level of a non-native reader. To facilitate reproducibility of our approach and foster further research into these aspects, we release a dataset of complex words annotated by readers with different backgrounds.",
        "author": "Sian Gooding; Ekaterina Kochmar; Seid Muhie Yimam; Chris Biemann",
        "authorids": "/s/sian-gooding/; /e/ekaterina-kochmar/; /s/seid-muhie-yimam/; /c/chris-biemann/",
        "bibtex": "@inproceedings{gooding-etal-2021-word,\n    title = \"Word Complexity is in the Eye of the Beholder\",\n    author = \"Gooding, Sian  and\n      Kochmar, Ekaterina  and\n      Yimam, Seid Muhie  and\n      Biemann, Chris\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.351/\",\n    doi = \"10.18653/v1/2021.naacl-main.351\",\n    pages = \"4439--4449\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.351.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.351/",
        "pdf_size": 658660,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5104180917787726844&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Dept of Computer Science and Technology, University of Cambridge; Dept of Computer Science, University of Bath; Language Technology group, Universit\u00e4t Hamburg, Germany; Language Technology group, Universit\u00e4t Hamburg, Germany",
        "aff_domain": "cam.ac.uk;bath.ac.uk;informatik.uni-hamburg.de;informatik.uni-hamburg.de",
        "email": "cam.ac.uk;bath.ac.uk;informatik.uni-hamburg.de;informatik.uni-hamburg.de",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;2;2",
        "aff_unique_norm": "University of Cambridge;University of Bath;Universit\u00e4t Hamburg",
        "aff_unique_dep": "Department of Computer Science and Technology;Dept of Computer Science;Language Technology group",
        "aff_unique_url": "https://www.cam.ac.uk;https://www.bath.ac.uk;https://www.uni-hamburg.de",
        "aff_unique_abbr": "Cambridge;Bath;",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;0;1;1",
        "aff_country_unique": "United Kingdom;Germany"
    },
    {
        "id": "2021.naacl-main.153",
        "title": "Worldly Wise (WoW) - Cross-Lingual Knowledge Fusion for Fact-based Visual Spoken-Question Answering",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Although Question-Answering has long been of research interest, its accessibility to users through a speech interface and its support to multiple languages have not been addressed in prior studies. Towards these ends, we present a new task and a synthetically-generated dataset to do Fact-based Visual Spoken-Question Answering (FVSQA). FVSQA is based on the FVQA dataset, which requires a system to retrieve an entity from Knowledge Graphs (KGs) to answer a question about an image. In FVSQA, the question is spoken rather than typed. Three sub-tasks are proposed: (1) speech-to-text based, (2) end-to-end, without speech-to-text as an intermediate component, and (3) cross-lingual, in which the question is spoken in a language different from that in which the KG is recorded. The end-to-end and cross-lingual tasks are the first to require world knowledge from a multi-relational KG as a differentiable layer in an end-to-end spoken language understanding task, hence the proposed reference implementation is called Worldly-Wise (WoW).WoW is shown to perform end-to-end cross-lingual FVSQA at same levels of accuracy across 3 languages - English, Hindi, and Turkish.",
        "author": "Kiran Ramnath; Leda Sari; Mark Hasegawa-Johnson; Chang Yoo",
        "authorids": "/k/kiran-ramnath/; /l/leda-sari/; /m/mark-hasegawa-johnson/; /c/chang-yoo/",
        "bibtex": "@inproceedings{ramnath-etal-2021-worldly,\n    title = \"Worldly Wise ({W}o{W}) - Cross-Lingual Knowledge Fusion for Fact-based Visual Spoken-Question Answering\",\n    author = \"Ramnath, Kiran  and\n      Sari, Leda  and\n      Hasegawa-Johnson, Mark  and\n      Yoo, Chang\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.153/\",\n    doi = \"10.18653/v1/2021.naacl-main.153\",\n    pages = \"1908--1919\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.153.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.153/",
        "pdf_size": 2685809,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16682232684502054090&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 5,
        "aff": "University of Illinois, Urbana-Champaign; University of Illinois, Urbana-Champaign; University of Illinois, Urbana-Champaign; KAIST",
        "aff_domain": "illinois.edu;illinois.edu;illinois.edu;kaist.ac.kr",
        "email": "illinois.edu;illinois.edu;illinois.edu;kaist.ac.kr",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "University of Illinois;Korea Advanced Institute of Science and Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://illinois.edu;https://www.kaist.ac.kr",
        "aff_unique_abbr": "UIUC;KAIST",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Urbana-Champaign;",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "United States;South Korea"
    },
    {
        "id": "2021.naacl-main.242",
        "title": "X-Class: Text Classification with Extremely Weak Supervision",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In this paper, we explore text classification with extremely weak supervision, i.e., only relying on the surface text of class names. This is a more challenging setting than the seed-driven weak supervision, which allows a few seed words per class. We opt to attack this problem from a representation learning perspective\u2014ideal document representations should lead to nearly the same results between clustering and the desired classification. In particular, one can classify the same corpus differently (e.g., based on topics and locations), so document representations should be adaptive to the given class names. We propose a novel framework X-Class to realize the adaptive representations. Specifically, we first estimate class representations by incrementally adding the most similar word to each class until inconsistency arises. Following a tailored mixture of class attention mechanisms, we obtain the document representation via a weighted average of contextualized word representations. With the prior of each document assigned to its nearest class, we then cluster and align the documents to classes. Finally, we pick the most confident documents from each cluster to train a text classifier. Extensive experiments demonstrate that X-Class can rival and even outperform seed-driven weakly supervised methods on 7 benchmark datasets.",
        "author": "Zihan Wang; Dheeraj Mekala; Jingbo Shang",
        "authorids": "/z/zihan-wang/; /d/dheeraj-mekala/; /j/jingbo-shang/",
        "bibtex": "@inproceedings{wang-etal-2021-x,\n    title = \"{X}-Class: Text Classification with Extremely Weak Supervision\",\n    author = \"Wang, Zihan  and\n      Mekala, Dheeraj  and\n      Shang, Jingbo\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.242/\",\n    doi = \"10.18653/v1/2021.naacl-main.242\",\n    pages = \"3043--3053\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.242.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.242/",
        "pdf_size": 1357946,
        "gs_citation": 94,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9662453466742945438&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science and Engineering, University of California San Diego, CA, USA + Hal\u0131c\u0131o\u011flu Data Science Institute, University of California San Diego, CA, USA; Department of Computer Science and Engineering, University of California San Diego, CA, USA + Hal\u0131c\u0131o\u011flu Data Science Institute, University of California San Diego, CA, USA; Department of Computer Science and Engineering, University of California San Diego, CA, USA + Hal\u0131c\u0131o\u011flu Data Science Institute, University of California San Diego, CA, USA",
        "aff_domain": "ucsd.edu;ucsd.edu;ucsd.edu",
        "email": "ucsd.edu;ucsd.edu;ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+0;0+0;0+0",
        "aff_unique_norm": "University of California San Diego",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0+0;0+0;0+0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.283",
        "title": "X-METRA-ADA: Cross-lingual Meta-Transfer learning Adaptation to Natural Language Understanding and Question Answering",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Multilingual models, such as M-BERT and XLM-R, have gained increasing popularity, due to their zero-shot cross-lingual transfer learning capabilities. However, their generalization ability is still inconsistent for typologically diverse languages and across different benchmarks. Recently, meta-learning has garnered attention as a promising technique for enhancing transfer learning under low-resource scenarios: particularly for cross-lingual transfer in Natural Language Understanding (NLU). In this work, we propose X-METRA-ADA, a cross-lingual MEta-TRAnsfer learning ADAptation approach for NLU. Our approach adapts MAML, an optimization-based meta-learning approach, to learn to adapt to new languages. We extensively evaluate our framework on two challenging cross-lingual NLU tasks: multilingual task-oriented dialog and typologically diverse question answering. We show that our approach outperforms naive fine-tuning, reaching competitive performance on both tasks for most languages. Our analysis reveals that X-METRA-ADA can leverage limited data for faster adaptation.",
        "author": "Meryem M\u2019hamdi; Doo Soon Kim; Franck Dernoncourt; Trung Bui; Xiang Ren; Jonathan May",
        "authorids": "/m/meryem-mhamdi/; /d/doo-soon-kim/; /f/franck-dernoncourt/; /t/trung-bui/; /x/xiang-ren/; /j/jonathan-may/",
        "bibtex": "@inproceedings{mhamdi-etal-2021-x,\n    title = \"{X}-{METRA}-{ADA}: Cross-lingual Meta-Transfer learning Adaptation to Natural Language Understanding and Question Answering\",\n    author = \"M{'}hamdi, Meryem  and\n      Kim, Doo Soon  and\n      Dernoncourt, Franck  and\n      Bui, Trung  and\n      Ren, Xiang  and\n      May, Jonathan\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.283/\",\n    doi = \"10.18653/v1/2021.naacl-main.283\",\n    pages = \"3617--3632\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.283.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.283/",
        "pdf_size": 2375690,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9072830761921313388&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Information Sciences Institute, University of Southern California; Adobe Research; Adobe Research; Adobe Research; Information Sciences Institute, University of Southern California; Information Sciences Institute, University of Southern California",
        "aff_domain": "isi.edu;isi.edu;isi.edu;adobe.com;adobe.com;adobe.com",
        "email": "isi.edu;isi.edu;isi.edu;adobe.com;adobe.com;adobe.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;1;1;0;0",
        "aff_unique_norm": "University of Southern California;Adobe",
        "aff_unique_dep": "Information Sciences Institute;Adobe Research",
        "aff_unique_url": "https://www.usc.edu;https://research.adobe.com",
        "aff_unique_abbr": "USC;Adobe",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.46",
        "title": "XOR QA: Cross-lingual Open-Retrieval Question Answering",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Multilingual question answering tasks typically assume that answers exist in the same language as the question. Yet in practice, many languages face both information scarcity\u2014where languages have few reference articles\u2014and information asymmetry\u2014where questions reference concepts from other cultures. This work extends open-retrieval question answering to a cross-lingual setting enabling questions from one language to be answered via answer content from another language. We construct a large-scale dataset built on 40K information-seeking questions across 7 diverse non-English languages that TyDi QA could not find same-language answers for. Based on this dataset, we introduce a task framework, called Cross-lingual Open-Retrieval Question Answering (XOR QA), that consists of three new tasks involving cross-lingual document retrieval from multilingual and English resources. We establish baselines with state-of-the-art machine translation systems and cross-lingual pretrained models. Experimental results suggest that XOR QA is a challenging task that will facilitate the development of novel techniques for multilingual question answering. Our data and code are available at https://nlp.cs.washington.edu/xorqa/.",
        "author": "Akari Asai; Jungo Kasai; Jonathan Clark; Kenton Lee; Eunsol Choi; Hannaneh Hajishirzi",
        "authorids": "/a/akari-asai/; /j/jungo-kasai/; /j/jonathan-h-clark/; /k/kenton-lee/; /e/eunsol-choi/; /h/hannaneh-hajishirzi/",
        "bibtex": "@inproceedings{asai-etal-2021-xor,\n    title = \"{XOR} {QA}: Cross-lingual Open-Retrieval Question Answering\",\n    author = \"Asai, Akari  and\n      Kasai, Jungo  and\n      Clark, Jonathan  and\n      Lee, Kenton  and\n      Choi, Eunsol  and\n      Hajishirzi, Hannaneh\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.46/\",\n    doi = \"10.18653/v1/2021.naacl-main.46\",\n    pages = \"547--564\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.46.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.46/",
        "pdf_size": 1491331,
        "gs_citation": 161,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1239154868633864483&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "University of Washington; University of Washington; Google Research; Google Research; The University of Texas at Austin; University of Washington+Allen Institute for AI",
        "aff_domain": "cs.washington.edu;cs.washington.edu;google.com;google.com;cs.utexas.edu;cs.washington.edu",
        "email": "cs.washington.edu;cs.washington.edu;google.com;google.com;cs.utexas.edu;cs.washington.edu",
        "github": "",
        "project": "https://nlp.cs.washington.edu/xorqa/",
        "author_num": 6,
        "aff_unique_index": "0;0;1;1;2;0+3",
        "aff_unique_norm": "University of Washington;Google;University of Texas at Austin;Allen Institute for AI",
        "aff_unique_dep": ";Google Research;;",
        "aff_unique_url": "https://www.washington.edu;https://research.google;https://www.utexas.edu;https://allenai.org",
        "aff_unique_abbr": "UW;Google Research;UT Austin;AI2",
        "aff_campus_unique_index": "1;1;2;",
        "aff_campus_unique": ";Mountain View;Austin",
        "aff_country_unique_index": "0;0;0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.246",
        "title": "You Sound Like Someone Who Watches Drama Movies: Towards Predicting Movie Preferences from Conversational Interactions",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The increasing popularity of voice-based personal assistants provides new opportunities for conversational recommendation. One particularly interesting area is movie recommendation, which can benefit from an open-ended interaction with the user, through a natural conversation. We explore one promising direction for conversational recommendation: mapping a conversational user, for whom there is limited or no data available, to most similar external reviewers, whose preferences are known, by representing the conversation as a user\u2019s interest vector, and adapting collaborative filtering techniques to estimate the current user\u2019s preferences for new movies. We call our proposed method ConvExtr (Conversational Collaborative Filtering using External Data), which 1) infers a user\u2019s sentiment towards an entity from the conversation context, and 2) transforms the ratings of \u201csimilar\u201d external reviewers to predict the current user\u2019s preferences. We implement these steps by adapting contextual sentiment prediction techniques, and domain adaptation, respectively. To evaluate our method, we develop and make available a finely annotated dataset of movie recommendation conversations, which we call MovieSent. Our results demonstrate that ConvExtr can improve the accuracy of predicting users\u2019 ratings for new movies by exploiting conversation content and external data.",
        "author": "Sergey Volokhin; Joyce Ho; Oleg Rokhlenko; Eugene Agichtein",
        "authorids": "/s/sergey-volokhin/; /j/joyce-ho/; /o/oleg-rokhlenko/; /e/eugene-agichtein/",
        "bibtex": "@inproceedings{volokhin-etal-2021-sound,\n    title = \"You Sound Like Someone Who Watches Drama Movies: Towards Predicting Movie Preferences from Conversational Interactions\",\n    author = \"Volokhin, Sergey  and\n      Ho, Joyce  and\n      Rokhlenko, Oleg  and\n      Agichtein, Eugene\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.246/\",\n    doi = \"10.18653/v1/2021.naacl-main.246\",\n    pages = \"3091--3096\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.246.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.246/",
        "pdf_size": 725251,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=297102929074605281&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Emory University, GA USA+Amazon, Seattle, WA, USA; Emory University, GA USA+Amazon, Seattle, WA, USA; Amazon, Seattle, WA, USA; Emory University, GA USA+Amazon, Seattle, WA, USA",
        "aff_domain": "emory.edu;emory.edu;amazon.com;amazon.com",
        "email": "emory.edu;emory.edu;amazon.com;amazon.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0+1;1;0+1",
        "aff_unique_norm": "Emory University;Amazon",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.emory.edu;https://www.amazon.com",
        "aff_unique_abbr": "Emory;Amazon",
        "aff_campus_unique_index": "0+1;0+1;1;0+1",
        "aff_campus_unique": "Georgia;Seattle",
        "aff_country_unique_index": "0+0;0+0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.272",
        "title": "ZS-BERT: Towards Zero-Shot Relation Extraction with Attribute Representation Learning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "While relation extraction is an essential task in knowledge acquisition and representation, and new-generated relations are common in the real world, less effort is made to predict unseen relations that cannot be observed at the training stage. In this paper, we formulate the zero-shot relation extraction problem by incorporating the text description of seen and unseen relations. We propose a novel multi-task learning model, Zero-Shot BERT (ZS-BERT), to directly predict unseen relations without hand-crafted attribute labeling and multiple pairwise classifications. Given training instances consisting of input sentences and the descriptions of their seen relations, ZS-BERT learns two functions that project sentences and relations into an embedding space by jointly minimizing the distances between them and classifying seen relations. By generating the embeddings of unseen relations and new-coming sentences based on such two functions, we use nearest neighbor search to obtain the prediction of unseen relations. Experiments conducted on two well-known datasets exhibit that ZS-BERT can outperform existing methods by at least 13.54% improvement on F1 score.",
        "author": "Chih-Yao Chen; Cheng-Te Li",
        "authorids": "/c/chih-yao-chen/; /c/cheng-te-li/",
        "bibtex": "@inproceedings{chen-li-2021-zs,\n    title = \"{ZS}-{BERT}: Towards Zero-Shot Relation Extraction with Attribute Representation Learning\",\n    author = \"Chen, Chih-Yao  and\n      Li, Cheng-Te\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.272/\",\n    doi = \"10.18653/v1/2021.naacl-main.272\",\n    pages = \"3470--3479\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.272.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.272/",
        "pdf_size": 850039,
        "gs_citation": 96,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7259375119514143225&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Program of Data Science, National Taiwan University, Taipei, Taiwan; Institute of Data Science, National Cheng Kung University, Tainan, Taiwan",
        "aff_domain": "ntu.edu.tw;ncku.edu.tw",
        "email": "ntu.edu.tw;ncku.edu.tw",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "National Taiwan University;National Cheng Kung University",
        "aff_unique_dep": "Program of Data Science;Institute of Data Science",
        "aff_unique_url": "https://www.ntu.edu.tw;https://www.ncku.edu.tw",
        "aff_unique_abbr": "NTU;NCKU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Taiwan",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2021.naacl-main.41",
        "title": "mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The recent \u201cText-to-Text Transfer Transformer\u201d (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent \u201caccidental translation\u201d in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available.",
        "author": "Linting Xue; Noah Constant; Adam Roberts; Mihir Kale; Rami Al-Rfou; Aditya Siddhant; Aditya Barua; Colin Raffel",
        "authorids": "/l/linting-xue/; /n/noah-constant/; /a/adam-roberts/; /m/mihir-kale/; /r/rami-al-rfou/; /a/aditya-siddhant/; /a/aditya-barua/; /c/colin-raffel/",
        "bibtex": "@inproceedings{xue-etal-2021-mt5,\n    title = \"m{T}5: A Massively Multilingual Pre-trained Text-to-Text Transformer\",\n    author = \"Xue, Linting  and\n      Constant, Noah  and\n      Roberts, Adam  and\n      Kale, Mihir  and\n      Al-Rfou, Rami  and\n      Siddhant, Aditya  and\n      Barua, Aditya  and\n      Raffel, Colin\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.41/\",\n    doi = \"10.18653/v1/2021.naacl-main.41\",\n    pages = \"483--498\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.41.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.41/",
        "pdf_size": 610847,
        "gs_citation": 2724,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10382762269124313526&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Google Research; Google Research; Google Research; Google Research; Google Research; Google Research; Google Research; Google Research",
        "aff_domain": "google.com;google.com;google.com; ; ; ; ;google.com",
        "email": "google.com;google.com;google.com; ; ; ; ;google.com",
        "github": "",
        "project": "https://goo.gle/mt5-code",
        "author_num": 8,
        "aff_unique_index": "0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google Research",
        "aff_unique_url": "https://research.google",
        "aff_unique_abbr": "Google Research",
        "aff_campus_unique_index": "0;0;0;0;0;0;0;0",
        "aff_campus_unique": "Mountain View",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.287",
        "title": "multiPRover: Generating Multiple Proofs for Improved Interpretability in Rule Reasoning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We focus on a type of linguistic formal reasoning where the goal is to reason over explicit knowledge in the form of natural language facts and rules (Clark et al., 2020). A recent work, named PRover (Saha et al., 2020), performs such reasoning by answering a question and also generating a proof graph that explains the answer. However, compositional reasoning is not always unique and there may be multiple ways of reaching the correct answer. Thus, in our work, we address a new and challenging problem of generating multiple proof graphs for reasoning over natural language rule-bases. Each proof provides a different rationale for the answer, thereby improving the interpretability of such reasoning systems. In order to jointly learn from all proof graphs and exploit the correlations between multiple proofs for a question, we pose this task as a set generation problem over structured output spaces where each proof is represented as a directed graph. We propose two variants of a proof-set generation model, multiPRover. Our first model, Multilabel-multiPRover, generates a set of proofs via multi-label classification and implicit conditioning between the proofs; while the second model, Iterative-multiPRover, generates proofs iteratively by explicitly conditioning on the previously generated proofs. Experiments on multiple synthetic, zero-shot, and human-paraphrased datasets reveal that both multiPRover models significantly outperform PRover on datasets containing multiple gold proofs. Iterative-multiPRover obtains state-of-the-art proof F1 in zero-shot scenarios where all examples have single correct proofs. It also generalizes better to questions requiring higher depths of reasoning where multiple proofs are more frequent.",
        "author": "Swarnadeep Saha; Prateek Yadav; Mohit Bansal",
        "authorids": "/s/swarnadeep-saha/; /p/prateek-yadav/; /m/mohit-bansal/",
        "bibtex": "@inproceedings{saha-etal-2021-multiprover,\n    title = \"multi{PR}over: Generating Multiple Proofs for Improved Interpretability in Rule Reasoning\",\n    author = \"Saha, Swarnadeep  and\n      Yadav, Prateek  and\n      Bansal, Mohit\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.287/\",\n    doi = \"10.18653/v1/2021.naacl-main.287\",\n    pages = \"3662--3677\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.287.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.287/",
        "pdf_size": 1100931,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3222675970792936082&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "UNC Chapel Hill; UNC Chapel Hill; UNC Chapel Hill",
        "aff_domain": "cs.unc.edu;cs.unc.edu;cs.unc.edu",
        "email": "cs.unc.edu;cs.unc.edu;cs.unc.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of North Carolina at Chapel Hill",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.unc.edu",
        "aff_unique_abbr": "UNC",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Chapel Hill",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.303",
        "title": "tWT\u2013WT: A Dataset to Assert the Role of Target Entities for Detecting Stance of Tweets",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The stance detection task aims at detecting the stance of a tweet or a text for a target. These targets can be named entities or free-form sentences (claims). Though the task involves reasoning of the tweet with respect to a target, we find that it is possible to achieve high accuracy on several publicly available Twitter stance detection datasets without looking at the target sentence. Specifically, a simple tweet classification model achieved human-level performance on the WT\u2013WT dataset and more than two-third accuracy on various other datasets. We investigate the existence of biases in such datasets to find the potential spurious correlations of sentiment-stance relations and lexical choice associated with the stance category. Furthermore, we propose a new large dataset free of such biases and demonstrate its aptness on the existing stance detection systems. Our empirical findings show much scope for research on the stance detection task and proposes several considerations for creating future stance detection datasets.",
        "author": "Ayush Kaushal; Avirup Saha; Niloy Ganguly",
        "authorids": "/a/ayush-kaushal/; /a/avirup-saha/; /n/niloy-ganguly/",
        "bibtex": "@inproceedings{kaushal-etal-2021-twt,\n    title = \"t{WT}{--}{WT}: A Dataset to Assert the Role of Target Entities for Detecting Stance of Tweets\",\n    author = \"Kaushal, Ayush  and\n      Saha, Avirup  and\n      Ganguly, Niloy\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.303/\",\n    doi = \"10.18653/v1/2021.naacl-main.303\",\n    pages = \"3879--3889\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.303.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.303/",
        "pdf_size": 1039233,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1731599236721160796&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Indian Institute of Technology Kharagpur; Indian Institute of Technology Kharagpur; Indian Institute of Technology Kharagpur",
        "aff_domain": "gmail.com;gmail.com;cse.iitkgp.ac.in",
        "email": "gmail.com;gmail.com;cse.iitkgp.ac.in",
        "github": "https://github.com/Ayushk4/bias-stance",
        "project": "https://github.com/Ayushk4/stance-dataset",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Indian Institute of Technology Kharagpur",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iitkgp.ac.in",
        "aff_unique_abbr": "IIT Kharagpur",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Kharagpur",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2021.naacl-main.346",
        "title": "\u201cI\u2019m Not Mad\u201d: Commonsense Implications of Negation and Contradiction",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Natural language inference requires reasoning about contradictions, negations, and their commonsense implications. Given a simple premise (e.g., \u201cI\u2019m mad at you\u201d), humans can reason about the varying shades of contradictory statements ranging from straightforward negations (\u201cI\u2019m not mad at you\u201d) to commonsense contradictions (\u201cI\u2019m happy\u201d). Moreover, these negated or contradictory statements shift the commonsense implications of the original premise in interesting and nontrivial ways. For example, while \u201cI\u2019m mad\u201d implies \u201cI\u2019m unhappy about something,\u201d negating the premise does not necessarily negate the corresponding commonsense implications. In this paper, we present the first comprehensive study focusing on commonsense implications of negated statements and contradictions. We introduce ANION, a new commonsense knowledge graph with 624K if-then rules focusing on negated and contradictory events. We then present joint generative and discriminative inference models for this new resource, providing novel empirical insights on how logical negations and commonsense contradictions reshape the commonsense implications of their original premises.",
        "author": "Liwei Jiang; Antoine Bosselut; Chandra Bhagavatula; Yejin Choi",
        "authorids": "/l/liwei-jiang/; /a/antoine-bosselut/; /c/chandra-bhagavatula/; /y/yejin-choi/",
        "bibtex": "@inproceedings{jiang-etal-2021-im,\n    title = \"{\\textquotedblleft}{I}{'}m Not Mad{\\textquotedblright}: Commonsense Implications of Negation and Contradiction\",\n    author = \"Jiang, Liwei  and\n      Bosselut, Antoine  and\n      Bhagavatula, Chandra  and\n      Choi, Yejin\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.346/\",\n    doi = \"10.18653/v1/2021.naacl-main.346\",\n    pages = \"4380--4397\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.346.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.346/",
        "pdf_size": 552755,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4949786110329555469&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Paul G. Allen School of Computer Science & Engineering, University of Washington + Allen Institute for Arti\ufb01cial Intelligence; Allen Institute for Arti\ufb01cial Intelligence + Stanford University; Allen Institute for Arti\ufb01cial Intelligence; Paul G. Allen School of Computer Science & Engineering, University of Washington + Allen Institute for Arti\ufb01cial Intelligence",
        "aff_domain": "cs.washington.edu;allenai.org; ;cs.washington.edu",
        "email": "cs.washington.edu;allenai.org; ;cs.washington.edu",
        "github": "https://github.com/liweijiang/anion",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;1+2;1;0+1",
        "aff_unique_norm": "University of Washington;Allen Institute for Artificial Intelligence;Stanford University",
        "aff_unique_dep": "Paul G. Allen School of Computer Science & Engineering;Artificial Intelligence;",
        "aff_unique_url": "https://www.cs.washington.edu;https://allenai.org;https://www.stanford.edu",
        "aff_unique_abbr": "UW;AI2;Stanford",
        "aff_campus_unique_index": "0;2;0",
        "aff_campus_unique": "Seattle;;Stanford",
        "aff_country_unique_index": "0+0;0+0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2021.naacl-main.60",
        "title": "\u201cNice Try, Kiddo\u201d: Investigating Ad Hominems in Dialogue Responses",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Ad hominem attacks are those that target some feature of a person\u2019s character instead of the position the person is maintaining. These attacks are harmful because they propagate implicit biases and diminish a person\u2019s credibility. Since dialogue systems respond directly to user input, it is important to study ad hominems in dialogue responses. To this end, we propose categories of ad hominems, compose an annotated dataset, and build a classifier to analyze human and dialogue system responses to English Twitter posts. We specifically compare responses to Twitter topics about marginalized communities (#BlackLivesMatter, #MeToo) versus other topics (#Vegan, #WFH), because the abusive language of ad hominems could further amplify the skew of power away from marginalized populations. Furthermore, we propose a constrained decoding technique that uses salient n-gram similarity as a soft constraint for top-k sampling to reduce the amount of ad hominems generated. Our results indicate that 1) responses from both humans and DialoGPT contain more ad hominems for discussions around marginalized communities, 2) different quantities of ad hominems in the training data can influence the likelihood of generating ad hominems, and 3) we can use constrained decoding techniques to reduce ad hominems in generated dialogue responses.",
        "author": "Emily Sheng; Kai-Wei Chang; Prem Natarajan; Nanyun Peng",
        "authorids": "/e/emily-sheng/; /k/kai-wei-chang/; /p/prem-natarajan/; /n/nanyun-peng/",
        "bibtex": "@inproceedings{sheng-etal-2021-nice,\n    title = \"{\\textquotedblleft}Nice Try, Kiddo{\\textquotedblright}: Investigating Ad Hominems in Dialogue Responses\",\n    author = \"Sheng, Emily  and\n      Chang, Kai-Wei  and\n      Natarajan, Prem  and\n      Peng, Nanyun\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.60/\",\n    doi = \"10.18653/v1/2021.naacl-main.60\",\n    pages = \"750--767\"\n}",
        "pdf": "https://aclanthology.org/2021.naacl-main.60.pdf",
        "site": "https://aclanthology.org/2021.naacl-main.60/",
        "pdf_size": 449445,
        "gs_citation": 59,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8835553084502646656&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Information Sciences Institute, University of Southern California; Computer Science Department, University of California, Los Angeles; Information Sciences Institute, University of Southern California; Information Sciences Institute, University of Southern California + Computer Science Department, University of California, Los Angeles",
        "aff_domain": "isi.edu;cs.ucla.edu;isi.edu;cs.ucla.edu",
        "email": "isi.edu;cs.ucla.edu;isi.edu;cs.ucla.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;0+1",
        "aff_unique_norm": "University of Southern California;University of California, Los Angeles",
        "aff_unique_dep": "Information Sciences Institute;Computer Science Department",
        "aff_unique_url": "https://www.usc.edu;https://www.ucla.edu",
        "aff_unique_abbr": "USC;UCLA",
        "aff_campus_unique_index": "0;0;0;0+0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0;0;0;0+0",
        "aff_country_unique": "United States"
    }
]