[
    {
        "id": "23bc92acb9",
        "title": "3D Fragment Reassembly Using Integrated Template Guidance and Fracture-Region Matching",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Kang Zhang, Wuyi Yu, Mary Manhein, Warren Waggenspack, Xin Li",
        "author": "Kang Zhang; Wuyi Yu; Mary Manhein; Warren Waggenspack; Xin Li",
        "abstract": "This paper studies matching of fragmented objects to recompose their original geometry. Solving this geometric reassembly problem has direct applications in archaeology and forensic investigation in the computer-aided restoration of damaged artifacts and evidence. We develop a new algorithm to effectively integrate both guidance from a template and from matching of adjacent pieces' fracture-regions. First, we compute partial matchings between fragments and a template, and pairwise matchings among fragments. Many potential matches are obtained and then selected/refined in a multi-piece matching stage to maximize global groupwise matching consistency. This pipeline is effective in composing fragmented thin-shell objects containing small pieces, whose pairwise matching is usually unreliable and ambiguous and hence their reassembly remains challenging to the existing algorithms.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Zhang_3D_Fragment_Reassembly_ICCV_2015_paper.pdf",
        "aff": "School of Electrical Engineering and Computer Science, Louisiana State University; School of Electrical Engineering and Computer Science, Louisiana State University; Dept. Geography & Anthropology, Louisiana State University; Dept. Mechanical & Industrial Engineering, Louisiana State University; School of Electrical Engineering and Computer Science, Louisiana State University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1356984,
        "gs_citation": 86,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6381840478323100568&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "cct.lsu.edu; ; ; ;cct.lsu.edu",
        "email": "cct.lsu.edu; ; ; ;cct.lsu.edu",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Zhang_3D_Fragment_Reassembly_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Louisiana State University",
        "aff_unique_dep": "School of Electrical Engineering and Computer Science",
        "aff_unique_url": "https://www.lsu.edu",
        "aff_unique_abbr": "LSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Zhang_2015_ICCV,\n    \n    author = {\n    Zhang,\n    Kang and Yu,\n    Wuyi and Manhein,\n    Mary and Waggenspack,\n    Warren and Li,\n    Xin\n},\n    title = {\n    3D Fragment Reassembly Using Integrated Template Guidance and Fracture-Region Matching\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "ca90ddff68",
        "title": "3D Hand Pose Estimation Using Randomized Decision Forest With Segmentation Index Points",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Peiyi Li, Haibin Ling, Xi Li, Chunyuan Liao",
        "author": "Peiyi Li; Haibin Ling; Xi Li; Chunyuan Liao",
        "abstract": "In this paper, we propose a real-time 3D hand pose estimation algorithm using the randomized decision forest framework. Our algorithm takes a depth image as input and generates a set of skeletal joints as output. Previous decision forest-based methods often give labels to all points in a point cloud at a very early stage and vote for the joint locations. By contrast, our algorithm only tracks a set of more flexible virtual landmark points, named segmentation index points (SIPs), before reaching the final decision at a leaf node. Roughly speaking, a SIP represents the centroid of a subset of skeletal joints, which are to be located at the leaves of the branch expanded from the SIP. Inspired by recent latent regression forest-based hand pose estimation framework (Tang et al. 2014), we integrate SIP into the framework with several important improvements: First, we devise a new forest growing strategy, whose decision is made using a randomized feature guided by SIPs. Second, we speed-up the training procedure since only SIPs, not the skeletal joints, are estimated at non-leaf nodes. Third, the experimental results on public benchmark datasets show clearly the advantage of the proposed algorithm over previous state-of-the-art methods, and our algorithm runs at 55.5 fps on a normal CPU without parallelism.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Li_3D_Hand_Pose_ICCV_2015_paper.pdf",
        "aff": "Meitu HiScene Lab, HiScene Information Technologies, Shanghai, China+Computer and Information Sciences Department, Temple University, Philadelphia, PA USA; Computer and Information Sciences Department, Temple University, Philadelphia, PA USA; College of Computer Science, Zhejiang University, Hangzhou, China; Meitu HiScene Lab, HiScene Information Technologies, Shanghai, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1014658,
        "gs_citation": 70,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5511109007433794374&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "hiscene.com;temple.edu;zju.edu.cn;hiscene.com",
        "email": "hiscene.com;temple.edu;zju.edu.cn;hiscene.com",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Li_3D_Hand_Pose_ICCV_2015_paper.html",
        "aff_unique_index": "0+1;1;2;0",
        "aff_unique_norm": "HiScene Information Technologies;Temple University;Zhejiang University",
        "aff_unique_dep": "Meitu HiScene Lab;Computer and Information Sciences Department;College of Computer Science",
        "aff_unique_url": ";https://www.temple.edu;http://www.zju.edu.cn",
        "aff_unique_abbr": ";Temple;ZJU",
        "aff_campus_unique_index": "1;1;2",
        "aff_campus_unique": ";Philadelphia;Hangzhou",
        "aff_country_unique_index": "0+1;1;0;0",
        "aff_country_unique": "China;United States",
        "bibtex": "@InProceedings{Li_2015_ICCV,\n    \n    author = {\n    Li,\n    Peiyi and Ling,\n    Haibin and Li,\n    Xi and Liao,\n    Chunyuan\n},\n    title = {\n    3D Hand Pose Estimation Using Randomized Decision Forest With Segmentation Index Points\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "680fcfa26e",
        "title": "3D Object Reconstruction From Hand-Object Interactions",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Dimitrios Tzionas, Juergen Gall",
        "author": "Dimitrios Tzionas; Juergen Gall",
        "abstract": "Recent advances have enabled 3d object reconstruction approaches using a single off-the-shelf RGB-D camera. Although these approaches are successful for a wide range of object classes, they rely on stable and distinctive geometric or texture features. Many objects like mechanical parts, toys, household or decorative articles, however, are textureless and characterized by minimalistic shapes that are simple and symmetric. Existing in-hand scanning systems and 3d reconstruction techniques fail for such symmetric objects in the absence of highly distinctive features. In this work, we show that extracting 3d hand motion for in-hand scanning effectively facilitates the reconstruction of even featureless and highly symmetric objects and we present an approach that fuses the rich additional information of hands into a 3d reconstruction pipeline, significantly contributing to the state-of-the-art of in-hand scanning.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Tzionas_3D_Object_Reconstruction_ICCV_2015_paper.pdf",
        "aff": "University of Bonn, Bonn, Germany + MPI for Intelligent Systems, Tübingen, Germany; University of Bonn, Bonn, Germany",
        "project": "http://skanect.occipital.com/",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1934826,
        "gs_citation": 90,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9561115001744405149&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 20,
        "aff_domain": "iai.uni-bonn.de;iai.uni-bonn.de",
        "email": "iai.uni-bonn.de;iai.uni-bonn.de",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Tzionas_3D_Object_Reconstruction_ICCV_2015_paper.html",
        "aff_unique_index": "0+1;0",
        "aff_unique_norm": "University of Bonn;Max Planck Institute for Intelligent Systems",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uni-bonn.de;https://www.mpituebingen.mpg.de",
        "aff_unique_abbr": "UBonn;MPI-IS",
        "aff_campus_unique_index": "0+1;0",
        "aff_campus_unique": "Bonn;Tübingen",
        "aff_country_unique_index": "0+0;0",
        "aff_country_unique": "Germany",
        "bibtex": "@InProceedings{Tzionas_2015_ICCV,\n    \n    author = {\n    Tzionas,\n    Dimitrios and Gall,\n    Juergen\n},\n    title = {\n    3D Object Reconstruction From Hand-Object Interactions\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "80d68768f9",
        "title": "3D Surface Profilometry Using Phase Shifting of De Bruijn Pattern",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Matea Ðonlić, Tomislav Petković, Tomislav Pribanić",
        "author": "Matea Donlic; Tomislav Petkovic; Tomislav Pribanic",
        "abstract": "A novel structured light method for color 3D surface profilometry is proposed. The proposed method does not require color calibration of a camera-projector pair and may be used for reconstruction of both dynamic and static scenes. The method uses a structured light pattern that is a combination of a De Bruijn color sequence and of a sinusoidal fringe. For dynamic scenes a Hessian ridge detector and a Gaussian mixture model are combined to extract stripe centers and to identify color. Stripes are then uniquely identified using dynamic programming based on the Smith-Waterman algorithm and a De Bruijn window property. For static scenes phase-shifting and De Bruijn window property are combined to obtain a high accuracy reconstruction. We have tested the proposed method on multiple objects with challenging surfaces and different albedos that demonstrate usability and robustness of the method.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Donlic_3D_Surface_Profilometry_ICCV_2015_paper.pdf",
        "aff": "University of Zagreb Faculty of Electrical Engineering and Computing; University of Zagreb Faculty of Electrical Engineering and Computing; University of Zagreb Faculty of Electrical Engineering and Computing",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3569695,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3500194769921760782&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "fer.hr;fer.hr;fer.hr",
        "email": "fer.hr;fer.hr;fer.hr",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Donlic_3D_Surface_Profilometry_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Zagreb",
        "aff_unique_dep": "Faculty of Electrical Engineering and Computing",
        "aff_unique_url": "https://www.feezagreb.unizg.hr/",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Croatia",
        "bibtex": "@InProceedings{Donlic_2015_ICCV,\n    \n    author = {\n    Donlic,\n    Matea and Petkovic,\n    Tomislav and Pribanic,\n    Tomislav\n},\n    title = {\n    3D Surface Profilometry Using Phase Shifting of De Bruijn Pattern\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "c8ac619d1a",
        "title": "3D Time-Lapse Reconstruction From Internet Photos",
        "session": "3d vision",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "Ricardo Martin-Brualla, David Gallup, Steven M. Seitz",
        "author": "Ricardo Martin-Brualla; David Gallup; Steven M. Seitz",
        "abstract": "Given an Internet photo collection of a landmark, we compute a 3D time-lapse video sequence where a virtual camera moves continuously in time and space. While previous work assumed a static camera, the addition of camera motion during the time-lapse creates a very compelling impression of parallax.  Achieving this goal, however, requires addressing multiple technical challenges, including solving for time-varying depth maps, regularizing 3D point color profiles over time, and reconstructing high quality, hole-free images at every frame from the projected profiles. Our results show photorealistic time-lapses of skylines and natural scenes over many years, with dramatic parallax effects.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Martin-Brualla_3D_Time-Lapse_Reconstruction_ICCV_2015_paper.pdf",
        "aff": "University of Washington1; Google Inc.2; University of Washington1+Google Inc.2",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1627128,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17318709865665070630&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff_domain": "cs.washington.edu;google.com;cs.washington.edu",
        "email": "cs.washington.edu;google.com;cs.washington.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Martin-Brualla_3D_Time-Lapse_Reconstruction_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;0+1",
        "aff_unique_norm": "University of Washington;Google",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.washington.edu;https://www.google.com",
        "aff_unique_abbr": "UW;Google",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;0;0+0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Martin-Brualla_2015_ICCV,\n    \n    author = {\n    Martin-Brualla,\n    Ricardo and Gallup,\n    David and Seitz,\n    Steven M.\n},\n    title = {\n    3D Time-Lapse Reconstruction From Internet Photos\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "c4ab758772",
        "title": "3D-Assisted Feature Synthesis for Novel Views of an Object",
        "session": "3d representations for recognition and localization",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "Hao Su, Fan Wang, Eric Yi, Leonidas J. Guibas",
        "author": "Hao Su; Fan Wang; Eric Yi; Leonidas J. Guibas",
        "abstract": "Comparing two images from different views has been a long-standing challenging problem in computer vision, as visual features are not stable under large view point changes. In this paper, given a single input image of an object, we synthesize its features for other views, leveraging an existing modestly-sized 3D model collection of related but not identical objects.To accomplish this, we study the relationship of image patches between different views of the same object, seeking what we call surrogate patches -- patches in one view whose feature content predicts well the features of a patch in another view. Based upon these surrogate relationships, we can create feature sets for all views of the latent object on a per patch basis, providing us an augmented multi-view representation of the object. We provide theoretical and empirical analysis of the feature synthesis process, and evaluate the augmented features in fine-grained image retrieval/recognition and instance retrieval tasks. Experimental results show that our synthesized features do enable view-independent comparison between images and perform significantly better than other traditional approaches in this respect.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Su_3D-Assisted_Feature_Synthesis_ICCV_2015_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2490415,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9498461510010693699&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Su_3D-Assisted_Feature_Synthesis_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Su_2015_ICCV,\n    \n    author = {\n    Su,\n    Hao and Wang,\n    Fan and Yi,\n    Eric and Guibas,\n    Leonidas J.\n},\n    title = {\n    3D-Assisted Feature Synthesis for Novel Views of an Object\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "60887d3072",
        "title": "A Collaborative Filtering Approach to Real-Time Hand Pose Estimation",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Chiho Choi, Ayan Sinha, Joon Hee Choi, Sujin Jang, Karthik Ramani",
        "author": "Chiho Choi; Ayan Sinha; Joon Hee Choi; Sujin Jang; Karthik Ramani",
        "abstract": "Collaborative filtering aims to predict unknown user ratings in a recommender system by collectively assessing known user preferences. In this paper, we first draw analogies between collaborative filtering and the pose estimation problem. Specifically, we recast the hand pose estimation problem as the cold-start problem for a new user with unknown item ratings in a recommender system. Inspired by fast and accurate matrix factorization techniques for collaborative filtering, we develop a real-time algorithm for estimating the hand pose from RGB-D data of a commercial depth camera. First, we efficiently identify nearest neighbors using local shape descriptors in the RGB-D domain from a library of hand poses with known pose parameter values. We then use this information to evaluate the unknown pose parameters using a joint matrix factorization and completion (JMFC) approach. Our quantitative and qualitative results suggest that our approach is robust to variation in hand configurations while achieving real time performance (29 FPS) on a standard computer.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Choi_A_Collaborative_Filtering_ICCV_2015_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1148613,
        "gs_citation": 71,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13856720417828107929&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Choi_A_Collaborative_Filtering_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Choi_2015_ICCV,\n    \n    author = {\n    Choi,\n    Chiho and Sinha,\n    Ayan and Choi,\n    Joon Hee and Jang,\n    Sujin and Ramani,\n    Karthik\n},\n    title = {\n    A Collaborative Filtering Approach to Real-Time Hand Pose Estimation\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "962dfe1b47",
        "title": "A Comprehensive Multi-Illuminant Dataset for Benchmarking of the Intrinsic Image Algorithms",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Shida Beigpour, Andreas Kolb, Sven Kunz",
        "author": "Shida Beigpour; Andreas Kolb; Sven Kunz",
        "abstract": "In this paper, we provide a new, real photo dataset with precise ground-truth for intrinsic image research. Prior ground-truth datasets have been restricted to rather simple illumination conditions and scene geometries, or have been enhanced using image synthesis methods. The dataset provided in this paper is based on complex multi-illuminant scenarios under multi-colored illumination conditions and challenging cast shadows. We provide full per-pixel intrinsic ground-truth data for these scenarios, i.e. reflectance, specularity, shading, and illumination for scenes as well as preliminary depth information.  Furthermore, we evaluate 3 state-of-the-art intrinsic image recovery methods, using our dataset.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Beigpour_A_Comprehensive_Multi-Illuminant_ICCV_2015_paper.pdf",
        "aff": "Chair of Computer Graphics and Multimedia Systems, University of Siegen; Chair of Computer Graphics and Multimedia Systems, University of Siegen; ",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1030575,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13646690048883822267&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "uni-siegen.de;uni-siegen.de;gmail.com",
        "email": "uni-siegen.de;uni-siegen.de;gmail.com",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Beigpour_A_Comprehensive_Multi-Illuminant_ICCV_2015_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Siegen",
        "aff_unique_dep": "Chair of Computer Graphics and Multimedia Systems",
        "aff_unique_url": "https://www.uni-siegen.de",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany",
        "bibtex": "@InProceedings{Beigpour_2015_ICCV,\n    \n    author = {\n    Beigpour,\n    Shida and Kolb,\n    Andreas and Kunz,\n    Sven\n},\n    title = {\n    A Comprehensive Multi-Illuminant Dataset for Benchmarking of the Intrinsic Image Algorithms\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "7f1888d76d",
        "title": "A Data-Driven Metric for Comprehensive Evaluation of Saliency Models",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Jia Li, Changqun Xia, Yafei Song, Shu Fang, Xiaowu Chen",
        "author": "Jia Li; Changqun Xia; Yafei Song; Shu Fang; Xiaowu Chen",
        "abstract": "In the past decades, hundreds of saliency models have been proposed for fixation prediction, along with dozens of evaluation metrics. However, existing metrics, which are often heuristically designed, may draw conflict conclusions in comparing saliency models. As a consequence, it becomes somehow confusing on the selection of metrics in comparing new models with state-of-the-arts. To address this problem, we propose a data-driven metric for comprehensive evaluation of saliency models. Instead of heuristically designing such a metric, we first conduct extensive subjective tests to find how saliency maps are assessed by the human-being. Based on the user data collected in the tests, nine representative evaluation metrics are directly compared by quantizing their performances in assessing saliency maps. Moreover, we propose to learn a data-driven metric by using Convolutional Neural Network. Compared with existing metrics, experimental results show that the data-driven metric performs the most consistently with the human-being in evaluating saliency maps as well as saliency models.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Li_A_Data-Driven_Metric_ICCV_2015_paper.pdf",
        "aff": "State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University + International Research Institute for Multidisciplinary Science, Beihang University; State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University; State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University; Cooperative Medianet Innovation Center, School of EE & CS, Peking University; State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University + International Research Institute for Multidisciplinary Science, Beihang University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 891401,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8821218506480321248&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "buaa.edu.cn; ; ; ;buaa.edu.cn",
        "email": "buaa.edu.cn; ; ; ;buaa.edu.cn",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Li_A_Data-Driven_Metric_ICCV_2015_paper.html",
        "aff_unique_index": "0+0;0;0;1;0+0",
        "aff_unique_norm": "Beihang University;Peking University",
        "aff_unique_dep": "School of Computer Science and Engineering;School of EE & CS",
        "aff_unique_url": "http://www.buaa.edu.cn;http://www.pku.edu.cn",
        "aff_unique_abbr": "Beihang;PKU",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0+0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Li_2015_ICCV,\n    \n    author = {\n    Li,\n    Jia and Xia,\n    Changqun and Song,\n    Yafei and Fang,\n    Shu and Chen,\n    Xiaowu\n},\n    title = {\n    A Data-Driven Metric for Comprehensive Evaluation of Saliency Models\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "9f4aff673e",
        "title": "A Deep Visual Correspondence Embedding Model for Stereo Matching Costs",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Zhuoyuan Chen, Xun Sun, Liang Wang, Yinan Yu, Chang Huang",
        "author": "Zhuoyuan Chen; Xun Sun; Liang Wang; Yinan Yu; Chang Huang",
        "abstract": "This paper presents a data-driven matching cost for stereo matching. A novel deep visual correspondence embedding model is trained via Convolutional Neural Network on a large set of stereo images with ground truth disparities. This deep embedding model leverages appearance data to learn visual similarity relationships between corresponding image patches, and explicitly maps intensity values into an embedding feature space to measure pixel dissimilarities. Experimental results on KITTI and Middlebury data sets demonstrate the effectiveness of our model. First, we prove that the new measure of pixel dissimilarity outperforms traditional matching costs. Furthermore, when integrated with a global stereo framework, our method ranks top 3 among all two-frame algorithms on the KITTI benchmark. Finally, cross-validation results show that our model is able to make correct predictions for unseen data which are outside of its labeled training set.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Chen_A_Deep_Visual_ICCV_2015_paper.pdf",
        "aff": "Baidu Research – Institute of Deep Learning; Baidu Research – Institute of Deep Learning; Baidu Research – Institute of Deep Learning; Horizon Robotics; Horizon Robotics",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1004024,
        "gs_citation": 246,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14843861252929294852&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "baidu.com;baidu.com;baidu.com;horizon-robotics.com;horizon-robotics.com",
        "email": "baidu.com;baidu.com;baidu.com;horizon-robotics.com;horizon-robotics.com",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Chen_A_Deep_Visual_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;1;1",
        "aff_unique_norm": "Baidu Research;Horizon Robotics",
        "aff_unique_dep": "Institute of Deep Learning;",
        "aff_unique_url": "https://research.baidu.com;https://www.horizon-robotics.com/",
        "aff_unique_abbr": "Baidu;Horizon Robotics",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Chen_2015_ICCV,\n    \n    author = {\n    Chen,\n    Zhuoyuan and Sun,\n    Xun and Wang,\n    Liang and Yu,\n    Yinan and Huang,\n    Chang\n},\n    title = {\n    A Deep Visual Correspondence Embedding Model for Stereo Matching Costs\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "27c64c8ae0",
        "title": "A Gaussian Process Latent Variable Model for BRDF Inference",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Stamatios Georgoulis, Vincent Vanweddingen, Marc Proesmans, Luc Van Gool",
        "author": "Stamatios Georgoulis; Vincent Vanweddingen; Marc Proesmans; Luc Van Gool",
        "abstract": "The problem of estimating a full BRDF from partial observations has already been studied using either parametric or non-parametric approaches. The goal in each case is to best match this sparse set of input measurements. In this paper we address the problem of inferring higher order reflectance information starting from the minimal input of a single BRDF slice. We begin from the prototypical case of a homogeneous sphere, lit by a head-on light source, which only holds information about less than 0.001% of the whole BRDF domain. We propose a novel method to infer the higher dimensional properties of the material's BRDF, based on the statistical distribution of known material characteristics observed in real-life samples. We evaluated our method based on a large set of experiments generated from real-world BRDFs and newly measured materials. Although inferring higher dimensional BRDFs from such modest training is not a trivial problem, our method performs better than state-of-the-art parametric, semi-parametric and non-parametric approaches. Finally, we discuss interesting applications on material re-lighting, and flash-based photography.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Georgoulis_A_Gaussian_Process_ICCV_2015_paper.pdf",
        "aff": "ESAT-PSI/VISICS, KU Leuven; ESAT-PSI/VISICS, KU Leuven; ESAT-PSI/VISICS, KU Leuven; ESAT-PSI/VISICS, KU Leuven+CVL, ETH Zurich",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1562008,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=981793854885000726&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "esat.kuleuven.be;esat.kuleuven.be;esat.kuleuven.be;esat.kuleuven.be",
        "email": "esat.kuleuven.be;esat.kuleuven.be;esat.kuleuven.be;esat.kuleuven.be",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Georgoulis_A_Gaussian_Process_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0+1",
        "aff_unique_norm": "KU Leuven;ETH Zurich",
        "aff_unique_dep": "ESAT-PSI/VISICS;Computer Vision Laboratory",
        "aff_unique_url": "https://www.kuleuven.be;https://www.ethz.ch",
        "aff_unique_abbr": "KU Leuven;ETHZ",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0+1",
        "aff_country_unique": "Belgium;Switzerland",
        "bibtex": "@InProceedings{Georgoulis_2015_ICCV,\n    \n    author = {\n    Georgoulis,\n    Stamatios and Vanweddingen,\n    Vincent and Proesmans,\n    Marc and Van Gool,\n    Luc\n},\n    title = {\n    A Gaussian Process Latent Variable Model for BRDF Inference\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "3647a0d516",
        "title": "A Groupwise Multilinear Correspondence Optimization for 3D Faces",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Timo Bolkart, Stefanie Wuhrer",
        "author": "Timo Bolkart; Stefanie Wuhrer",
        "abstract": "Multilinear face models are widely used to model the space of human faces with expressions. For databases of 3D human faces of different identities performing multiple expressions, these statistical shape models decouple identity and expression variations. To compute a high-quality multilinear face model, the quality of the registration of the database of 3D face scans used for training is essential. Meanwhile, a multilinear face model can be used as an effective prior to register 3D face scans, which are typically noisy and incomplete. Inspired by the minimum description length approach, we propose the first method to jointly optimize a multilinear model and the registration of the 3D scans used for training. Given an initial registration, our approach fully automatically improves the registration by optimizing an objective function that measures the compactness of the multilinear model, resulting in a sparse model. We choose a continuous representation for each face shape that allows to use a quasi-Newton method in parameter space for optimization. We show that our approach is computationally significantly more efficient and leads to correspondences of higher quality than existing methods based on linear statistical models. This allows us to evaluate our approach on large standard 3D face databases and in the presence of noisy initializations.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Bolkart_A_Groupwise_Multilinear_ICCV_2015_paper.pdf",
        "aff": "Saarland University, Germany; Inria Rhône-Alpes, France",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3282048,
        "gs_citation": 77,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2863610817366199665&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff_domain": "mmci.uni-saarland.de;inria.fr",
        "email": "mmci.uni-saarland.de;inria.fr",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Bolkart_A_Groupwise_Multilinear_ICCV_2015_paper.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Saarland University;Inria",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uni-saarland.de;https://www.inria.fr",
        "aff_unique_abbr": "UdS;Inria",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Rhône-Alpes",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Germany;France",
        "bibtex": "@InProceedings{Bolkart_2015_ICCV,\n    \n    author = {\n    Bolkart,\n    Timo and Wuhrer,\n    Stefanie\n},\n    title = {\n    A Groupwise Multilinear Correspondence Optimization for 3D Faces\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "27f2ed0d69",
        "title": "A Linear Generalized Camera Calibration From Three Intersecting Reference Planes",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Mai Nishimura, Shohei Nobuhara, Takashi Matsuyama, Shinya Shimizu, Kensaku Fujii",
        "author": "Mai Nishimura; Shohei Nobuhara; Takashi Matsuyama; Shinya Shimizu; Kensaku Fujii",
        "abstract": "This paper presents a new generalized (or ray-pixel, raxel) camera calibration algorithm for camera systems involving distortions by unknown refraction and reflection processes. The key idea is use of intersections of calibration planes, while conventional methods utilized collinearity constraints of points on the planes. We show that intersections of calibration planes can realize a simple linear algorithm, and that our method can be applied to any ray-distributions while conventional methods require knowing the ray-distribution class in advance. Evaluations using synthesized and real datasets demonstrate the performance of our method quantitatively and qualitatively.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Nishimura_A_Linear_Generalized_ICCV_2015_paper.pdf",
        "aff": "Graduate School of Informatics, Kyoto University, Japan + NTT Software Innovation Center, NTT Corporation, Japan; Graduate School of Informatics, Kyoto University, Japan; Graduate School of Informatics, Kyoto University, Japan; NTT Media Intelligence Laboratories, NTT Corporation, Japan; NTT Media Intelligence Laboratories, NTT Corporation, Japan",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3518589,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3205915230152841078&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "vision.kuee.kyoto-u.ac.jp;vision.kuee.kyoto-u.ac.jp;vision.kuee.kyoto-u.ac.jp;lab.ntt.co.jp;lab.ntt.co.jp",
        "email": "vision.kuee.kyoto-u.ac.jp;vision.kuee.kyoto-u.ac.jp;vision.kuee.kyoto-u.ac.jp;lab.ntt.co.jp;lab.ntt.co.jp",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Nishimura_A_Linear_Generalized_ICCV_2015_paper.html",
        "aff_unique_index": "0+1;0;0;1;1",
        "aff_unique_norm": "Kyoto University;NTT Corporation",
        "aff_unique_dep": "Graduate School of Informatics;NTT Software Innovation Center",
        "aff_unique_url": "https://www.kyoto-u.ac.jp;https://www.ntt.co.jp",
        "aff_unique_abbr": "Kyoto U;NTT",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Kyoto;",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "Japan",
        "bibtex": "@InProceedings{Nishimura_2015_ICCV,\n    \n    author = {\n    Nishimura,\n    Mai and Nobuhara,\n    Shohei and Matsuyama,\n    Takashi and Shimizu,\n    Shinya and Fujii,\n    Kensaku\n},\n    title = {\n    A Linear Generalized Camera Calibration From Three Intersecting Reference Planes\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "0cc3e03870",
        "title": "A Matrix Decomposition Perspective to Multiple Graph Matching",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Junchi Yan, Hongteng Xu, Hongyuan Zha, Xiaokang Yang, Huanxi Liu, Stephen Chu",
        "author": "Junchi Yan; Hongteng Xu; Hongyuan Zha; Xiaokang Yang; Huanxi Liu; Stephen Chu",
        "abstract": "Graph matching has a wide spectrum of real-world applications and in general is known NP-hard. In many vision tasks, one realistic problem arises for finding the global node mappings across a batch of corrupted weighted graphs. This paper is an attempt to connect graph matching, especially multi-graph matching to the matrix decomposition model and its relevant on-the-shelf convex optimization algorithms. Our method aims to extract the common inliers and their synchronized permutations from disordered weighted graphs in the presence of deformation and outliers. Under the proposed framework, several variants can be derived in the hope of accommodating to other types of noises. Experimental results on both synthetic data and real images empirically show that the proposed paradigm exhibits several interesting behaviors and in many cases performs competitively with the state-of-the-arts.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Yan_A_Matrix_Decomposition_ICCV_2015_paper.pdf",
        "aff": "Shanghai Jiao Tong University+IBM Research – China; College of Computing+School of Electronic Engineering, Georgia Institute of Technology; Software Engineering Institute, East China Normal University+College of Computing, Georgia Institute of Technology; Shanghai Jiao Tong University; Shanghai Jiao Tong University; IBM Research – China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 902989,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5060001548922420900&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 7,
        "aff_domain": "sjtu.edu.cn;gatech.edu;cc.gatech.edu;sjtu.edu.cn;sjtu.edu.cn;us.ibm.com",
        "email": "sjtu.edu.cn;gatech.edu;cc.gatech.edu;sjtu.edu.cn;sjtu.edu.cn;us.ibm.com",
        "author_num": 6,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Yan_A_Matrix_Decomposition_ICCV_2015_paper.html",
        "aff_unique_index": "0+1;2+3;4+3;0;0;1",
        "aff_unique_norm": "Shanghai Jiao Tong University;IBM Research;College of Computing;Georgia Institute of Technology;East China Normal University",
        "aff_unique_dep": ";China;;School of Electronic Engineering;Software Engineering Institute",
        "aff_unique_url": "https://www.sjtu.edu.cn;https://www.ibm.com/research/global/china;;https://www.gatech.edu;http://www.ecnu.edu.cn",
        "aff_unique_abbr": "SJTU;IBM;;Georgia Tech;ECNU",
        "aff_campus_unique_index": ";1;1",
        "aff_campus_unique": ";Atlanta",
        "aff_country_unique_index": "0+0;2;0+2;0;0;0",
        "aff_country_unique": "China;;United States",
        "bibtex": "@InProceedings{Yan_2015_ICCV,\n    \n    author = {\n    Yan,\n    Junchi and Xu,\n    Hongteng and Zha,\n    Hongyuan and Yang,\n    Xiaokang and Liu,\n    Huanxi and Chu,\n    Stephen\n},\n    title = {\n    A Matrix Decomposition Perspective to Multiple Graph Matching\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "dee01b7d79",
        "title": "A Multiscale Variable-Grouping Framework for MRF Energy Minimization",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Omer Meir, Meirav Galun, Stav Yagev, Ronen Basri, Irad Yavneh",
        "author": "Omer Meir; Meirav Galun; Stav Yagev; Ronen Basri; Irad Yavneh",
        "abstract": "We present a multiscale approach for minimizing the energy associated with Markov Random Fields (MRFs) with energy functions that include arbitrary pairwise potentials. The MRF is represented on a hierarchy of successively coarser scales, where the problem on each scale is itself an MRF with suitably defined potentials. These representations are used to construct an efficient multiscale algorithm that seeks a minimal-energy solution to the original problem. The algorithm is iterative and features a bidirectional crosstalk between fine and coarse representations. We use consistency criteria to guarantee that the energy is nonincreasing throughout the iterative process. The algorithm is evaluated on real-world datasets, achieving competitive performance in relatively short run-times.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Meir_A_Multiscale_Variable-Grouping_ICCV_2015_paper.pdf",
        "aff": "Weizmann Institute of Science; Weizmann Institute of Science; Weizmann Institute of Science; Weizmann Institute of Science; Technion",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 502633,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4767814740111537714&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "weizmann.ac.il;weizmann.ac.il;weizmann.ac.il;weizmann.ac.il;cs.technion.ac.il",
        "email": "weizmann.ac.il;weizmann.ac.il;weizmann.ac.il;weizmann.ac.il;cs.technion.ac.il",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Meir_A_Multiscale_Variable-Grouping_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0;1",
        "aff_unique_norm": "Weizmann Institute of Science;Technion - Israel Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.weizmann.org.il;https://www.technion.ac.il/en/",
        "aff_unique_abbr": "Weizmann;Technion",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Israel",
        "bibtex": "@InProceedings{Meir_2015_ICCV,\n    \n    author = {\n    Meir,\n    Omer and Galun,\n    Meirav and Yagev,\n    Stav and Basri,\n    Ronen and Yavneh,\n    Irad\n},\n    title = {\n    A Multiscale Variable-Grouping Framework for MRF Energy Minimization\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "3c32b46339",
        "title": "A Nonparametric Bayesian Approach Toward Stacked Convolutional Independent Component Analysis",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Sotirios P. Chatzis, Dimitrios Kosmopoulos",
        "author": "Sotirios P. Chatzis; Dimitrios Kosmopoulos",
        "abstract": "Unsupervised feature learning algorithms based on convolutional formulations of independent components analysis (ICA) have been demonstrated to yield state-of-the-art results in several action recognition benchmarks. However, existing approaches do not allow for the number of latent components (features) to be automatically inferred from the data in an unsupervised manner. This is a significant disadvantage of the state-of-the-art, as it results in considerable burden imposed on researchers and practitioners, who must resort to tedious cross-validation procedures to obtain the optimal number of latent features. To resolve these issues, in this paper we introduce a convolutional nonparametric Bayesian sparse ICA architecture for overcomplete feature learning from high-dimensional data. Our method utilizes an Indian buffet process prior to facilitate inference of the appropriate number of latent features under a hybrid variational inference algorithm, scalable to massive datasets. As we show, our model can be naturally used to obtain deep unsupervised hierarchical feature extractors, by greedily stacking successive model layers, similar to existing approaches. In addition, inference for this model is completely heuristics-free; thus, it obviates the need of tedious parameter tuning, which is a major challenge most deep learning approaches are faced with. We evaluate our method on several action recognition benchmarks, and exhibit its advantages over the state-of-the-art.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Chatzis_A_Nonparametric_Bayesian_ICCV_2015_paper.pdf",
        "aff": "Cyprus University of Technology; University of Patras",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 436197,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8609991189272427744&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "eecei.cut.ac.cy;upatras.gr",
        "email": "eecei.cut.ac.cy;upatras.gr",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Chatzis_A_Nonparametric_Bayesian_ICCV_2015_paper.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Cyprus University of Technology;University of Patras",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cut.ac.cy;https://www.upatras.gr",
        "aff_unique_abbr": "CUT;UPatras",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Cyprus;Greece",
        "bibtex": "@InProceedings{Chatzis_2015_ICCV,\n    \n    author = {\n    Chatzis,\n    Sotirios P. and Kosmopoulos,\n    Dimitrios\n},\n    title = {\n    A Nonparametric Bayesian Approach Toward Stacked Convolutional Independent Component Analysis\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "a13c6ba702",
        "title": "A Novel Representation of Parts for Accurate 3D Object Detection and Tracking in Monocular Images",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Alberto Crivellaro, Mahdi Rad, Yannick Verdie, Kwang Moo Yi, Pascal Fua, Vincent Lepetit",
        "author": "Alberto Crivellaro; Mahdi Rad; Yannick Verdie; Kwang Moo Yi; Pascal Fua; Vincent Lepetit",
        "abstract": "We present a method that estimates in real-time and under challenging conditions the 3D pose of a known object.  Our method relies only on grayscale images since depth cameras fail  on metallic objects; it can handle  poorly textured objects, and cluttered, changing  environments; the pose it  predicts degrades gracefully in  presence  of   large  occlusions.   As  a  result,  by   contrast  with  the state-of-the-art,  our  method  is  suitable  for  practical  Augmented  Reality applications even  in industrial environments.   To be robust to  occlusions, we first learn to detect  some parts of the target object. Our key  idea is to then predict the  3D pose of  each part in  the form of the  2D projections of  a few control points.   The advantages  of this representation  is three-fold:  We can predict the  3D pose  of the  object even when  only one  part is  visible; when several parts are visible,  we can combine them easily to  compute a better pose of the object;  the 3D pose we  obtain is usually very accurate,  even when only few parts are visible.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Crivellaro_A_Novel_Representation_ICCV_2015_paper.pdf",
        "aff": "Computer Vision Laboratory, École Polytechnique Fédérale de Lausanne (EPFL), Switzerland; Institute for Computer Graphics and Vision, Graz University of Technology, Austria; Computer Vision Laboratory, École Polytechnique Fédérale de Lausanne (EPFL), Switzerland; Computer Vision Laboratory, École Polytechnique Fédérale de Lausanne (EPFL), Switzerland; Computer Vision Laboratory, École Polytechnique Fédérale de Lausanne (EPFL), Switzerland; Institute for Computer Graphics and Vision, Graz University of Technology, Austria",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4306230,
        "gs_citation": 135,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4200659001069467727&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff_domain": "epfl.ch;icg.tugraz.at;epfl.ch;epfl.ch;epfl.ch;icg.tugraz.at",
        "email": "epfl.ch;icg.tugraz.at;epfl.ch;epfl.ch;epfl.ch;icg.tugraz.at",
        "author_num": 6,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Crivellaro_A_Novel_Representation_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;0;0;0;1",
        "aff_unique_norm": "École Polytechnique Fédérale de Lausanne;Graz University of Technology",
        "aff_unique_dep": "Computer Vision Laboratory;Institute for Computer Graphics and Vision",
        "aff_unique_url": "https://www.epfl.ch;https://www.tugraz.at",
        "aff_unique_abbr": "EPFL;TU Graz",
        "aff_campus_unique_index": "0;1;0;0;0;1",
        "aff_campus_unique": "Lausanne;Graz",
        "aff_country_unique_index": "0;1;0;0;0;1",
        "aff_country_unique": "Switzerland;Austria",
        "bibtex": "@InProceedings{Crivellaro_2015_ICCV,\n    \n    author = {\n    Crivellaro,\n    Alberto and Rad,\n    Mahdi and Verdie,\n    Yannick and Yi,\n    Kwang Moo and Fua,\n    Pascal and Lepetit,\n    Vincent\n},\n    title = {\n    A Novel Representation of Parts for Accurate 3D Object Detection and Tracking in Monocular Images\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "5eb37bdf31",
        "title": "A Novel Sparsity Measure for Tensor Recovery",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Qian Zhao, Deyu Meng, Xu Kong, Qi Xie, Wenfei Cao, Yao Wang, Zongben Xu",
        "author": "Qian Zhao; Deyu Meng; Xu Kong; Qi Xie; Wenfei Cao; Yao Wang; Zongben Xu",
        "abstract": "In this paper, we propose a new sparsity regularizer for measuring the low-rank structure underneath a tensor. The proposed sparsity measure has a natural physical meaning which is intrinsically the size of the fundamental Kronecker basis to express the tensor. By embedding the sparsity measure into the tensor completion and tensor robust PCA frameworks, we formulate new models to enhance their capability in tensor recovery. Through introducing relaxation forms of the proposed sparsity measure, we also adopt the alternating direction method of multipliers (ADMM) for solving the proposed models. Experiments implemented on synthetic and multispectral image data sets substantiate the effectiveness of the proposed methods.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Zhao_A_Novel_Sparsity_ICCV_2015_paper.pdf",
        "aff": "School of Mathematics and Statistics, Xi’an Jiaotong University; School of Mathematics and Statistics, Xi’an Jiaotong University + Ministry of Education Key Lab of Intelligent Networks and Network Security, Xi’an Jiaotong University; School of Mathematical Sciences, Liaocheng University; School of Mathematics and Statistics, Xi’an Jiaotong University; School of Mathematics and Statistics, Xi’an Jiaotong University; School of Mathematics and Statistics, Xi’an Jiaotong University; School of Mathematics and Statistics, Xi’an Jiaotong University + Ministry of Education Key Lab of Intelligent Networks and Network Security, Xi’an Jiaotong University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2335280,
        "gs_citation": 51,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8877939362274230705&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 7,
        "aff_domain": "gmail.com;mail.xjtu.edu.cn;hotmail.com;stu.xjtu.edu.cn;gmail.com;gmail.com;mail.xjtu.edu.cn",
        "email": "gmail.com;mail.xjtu.edu.cn;hotmail.com;stu.xjtu.edu.cn;gmail.com;gmail.com;mail.xjtu.edu.cn",
        "author_num": 7,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Zhao_A_Novel_Sparsity_ICCV_2015_paper.html",
        "aff_unique_index": "0;0+0;1;0;0;0;0+0",
        "aff_unique_norm": "Xi'an Jiaotong University;Liaocheng University",
        "aff_unique_dep": "School of Mathematics and Statistics;School of Mathematical Sciences",
        "aff_unique_url": "http://en.xjtu.edu.cn/;http://www.lctu.edu.cn/",
        "aff_unique_abbr": "XJTU;",
        "aff_campus_unique_index": "0;0+0;0;0;0;0+0",
        "aff_campus_unique": "Xi'an;",
        "aff_country_unique_index": "0;0+0;0;0;0;0;0+0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Zhao_2015_ICCV,\n    \n    author = {\n    Zhao,\n    Qian and Meng,\n    Deyu and Kong,\n    Xu and Xie,\n    Qi and Cao,\n    Wenfei and Wang,\n    Yao and Xu,\n    Zongben\n},\n    title = {\n    A Novel Sparsity Measure for Tensor Recovery\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "936c5d703d",
        "title": "A Projection Free Method for Generalized Eigenvalue Problem With a Nonsmooth Regularizer",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Seong Jae Hwang, Maxwell D. Collins, Sathya N. Ravi, Vamsi K. Ithapu, Nagesh Adluru, Sterling C. Johnson, Vikas Singh",
        "author": "Seong Jae Hwang; Maxwell D. Collins; Sathya N. Ravi; Vamsi K. Ithapu; Nagesh Adluru; Sterling C. Johnson; Vikas Singh",
        "abstract": "Eigenvalue problems are ubiquitous in computer vision, covering a very broad spectrum of applications ranging from estimation problems in multi-view geometry to image segmentation. Few other linear algebra problems have a more mature set of numerical routines available and many computer vision libraries leverage such tools extensively. However, the ability to call the underlying solver only as a ``black box'' can often become restrictive. Many `human in the loop' settings in vision frequently exploit supervision from an expert, to the extent that the user can be considered a subroutine in the overall system. In other cases, there is additional domain knowledge, side or even partial information that one may want to incorporate within the formulation. In general, regularizing a (generalized) eigenvalue problem with such side information remains difficult. Motivated by these needs, this paper presents an optimization scheme to solve generalized eigenvalue problems (GEP) involving a (nonsmooth) regularizer. We start from an alternative formulation of GEP where the feasibility set of the model involves the Stiefel manifold. The core of this paper presents an end to end stochastic optimization scheme for the resultant problem. We show how this general algorithm enables improved statistical analysis of brain imaging data where the regularizer is derived from other `views' of the disease pathology, involving clinical measurements and other image-derived representations.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Hwang_A_Projection_Free_ICCV_2015_paper.pdf",
        "aff": "Dept. of Computer Sciences, University of Wisconsin - Madison; Dept. of Industrial and Systems Engineering, University of Wisconsin - Madison; Dept. of Industrial and Systems Engineering, University of Wisconsin - Madison; Dept. of Computer Sciences, University of Wisconsin - Madison; Waisman Center, Madison, WI; William S. Middleton V A Hospital, Madison, WI; Dept. of Computer Sciences, University of Wisconsin - Madison+Dept. of Biostatistics and Med. Informatics, University of Wisconsin - Madison",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1012291,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5938616152931401412&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "author_num": 7,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Hwang_A_Projection_Free_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0;1;2;0+0",
        "aff_unique_norm": "University of Wisconsin-Madison;Waisman Center;William S. Middleton Memorial Veterans Hospital",
        "aff_unique_dep": "Department of Computer Sciences;;",
        "aff_unique_url": "https://www.wisc.edu;;https://www.wisconsinva.gov/Madison/",
        "aff_unique_abbr": "UW-Madison;;",
        "aff_campus_unique_index": "0;0;0;0;0;0;0+0",
        "aff_campus_unique": "Madison",
        "aff_country_unique_index": "0;0;0;0;0;0;0+0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Hwang_2015_ICCV,\n    \n    author = {\n    Hwang,\n    Seong Jae and Collins,\n    Maxwell D. and Ravi,\n    Sathya N. and Ithapu,\n    Vamsi K. and Adluru,\n    Nagesh and Johnson,\n    Sterling C. and Singh,\n    Vikas\n},\n    title = {\n    A Projection Free Method for Generalized Eigenvalue Problem With a Nonsmooth Regularizer\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "ee7d027bae",
        "title": "A Randomized Ensemble Approach to Industrial CT Segmentation",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Hyojin Kim, Jayaraman Jayaraman J. Thiagarajan, Peer-Timo Bremer",
        "author": "Hyojin Kim; Jayaraman Jayaraman J. Thiagarajan; Peer-Timo Bremer",
        "abstract": "Tuning the models and parameters of common segmentation approaches is challenging especially in the presence of noise and artifacts. Ensemble-based techniques attempt to compensate by randomly varying models and/or parameters to create a diverse set of hypotheses, which are subsequently ranked to arrive at the best solution. However, these methods have been restricted to cases where the underlying models are well-established, e.g. natural images. In practice, it is difficult to determine a suitable base-model and the amount of randomization required. Furthermore, for multi-object scenes no single hypothesis may perform well for all objects, reducing the overall quality of the results. This paper presents a new ensemble-based segmentation framework for industrial CT images demonstrating that comparatively simple models and randomization strategies can significantly improve the result over existing techniques. Furthermore, we introduce a per-object based ranking, followed by a consensus inference that can outperform even the best case scenario of existing hypothesis ranking approaches. We demonstrate the effectiveness of our approach using a set of noise and artifact rich CT images from baggage security and show that it significantly outperforms existing solutions in this area.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Kim_A_Randomized_Ensemble_ICCV_2015_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2875374,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=188486124613621044&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Kim_A_Randomized_Ensemble_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Kim_2015_ICCV,\n    \n    author = {\n    Kim,\n    Hyojin and Thiagarajan,\n    Jayaraman Jayaraman J. and Bremer,\n    Peer-Timo\n},\n    title = {\n    A Randomized Ensemble Approach to Industrial CT Segmentation\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "d65ca72d61",
        "title": "A Self-Paced Multiple-Instance Learning Framework for Co-Saliency Detection",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Dingwen Zhang, Deyu Meng, Chao Li, Lu Jiang, Qian Zhao, Junwei Han",
        "author": "Dingwen Zhang; Deyu Meng; Chao Li; Lu Jiang; Qian Zhao; Junwei Han",
        "abstract": "As an interesting and emerging topic, co-saliency detection aims at simultaneously extracting common salient objects in a group of images. Traditional co-saliency detection approaches rely heavily on human knowledge for designing hand-crafted metrics to explore the intrinsic patterns underlying co-salient objects. Such strategies, however, always suffer from poor generalization capability to flexibly adapt various scenarios in real applications, especially due to their lack of insightful understanding of the biological mechanisms of human visual co-attention. To alleviate this problem, we propose a novel framework for this task, by naturally reformulating it as a multiple-instance learning (MIL) problem and further integrating it into a self-paced learning (SPL) regime. The proposed framework on one hand is capable of fitting insightful metric measurements and discovering common patterns under co-salient regions in a self-learning way by MIL, and on the other hand tends to promise the learning reliability and stability by simulating the human learning process through SPL. Experiments on benchmark datasets have demonstrated the effectiveness of the proposed framework as compared with the state-of-the-arts.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Zhang_A_Self-Paced_Multiple-Instance_ICCV_2015_paper.pdf",
        "aff": "School of Automation, Northwestern Polytechnical University; School of Mathematics and Statistics, Xi’an Jiaotong University; School of Automation, Northwestern Polytechnical University; School of Computer Science, Carnegie Mellon University; School of Mathematics and Statistics, Xi’an Jiaotong University; School of Automation, Northwestern Polytechnical University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1731150,
        "gs_citation": 155,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3910605580292771940&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "gmail.com;mail.xjtu.edu.cn;gmail.com;cs.cmu.edu;gmail.com;gmail.com",
        "email": "gmail.com;mail.xjtu.edu.cn;gmail.com;cs.cmu.edu;gmail.com;gmail.com",
        "author_num": 6,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Zhang_A_Self-Paced_Multiple-Instance_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;0;2;1;0",
        "aff_unique_norm": "Northwestern Polytechnical University;Xi'an Jiaotong University;Carnegie Mellon University",
        "aff_unique_dep": "School of Automation;School of Mathematics and Statistics;School of Computer Science",
        "aff_unique_url": "https://www.nwpu.edu.cn;http://en.xjtu.edu.cn/;https://www.cmu.edu",
        "aff_unique_abbr": "NWPU;XJTU;CMU",
        "aff_campus_unique_index": "1;2;1",
        "aff_campus_unique": ";Xi'an;Pittsburgh",
        "aff_country_unique_index": "0;0;0;1;0;0",
        "aff_country_unique": "China;United States",
        "bibtex": "@InProceedings{Zhang_2015_ICCV,\n    \n    author = {\n    Zhang,\n    Dingwen and Meng,\n    Deyu and Li,\n    Chao and Jiang,\n    Lu and Zhao,\n    Qian and Han,\n    Junwei\n},\n    title = {\n    A Self-Paced Multiple-Instance Learning Framework for Co-Saliency Detection\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "55023e7875",
        "title": "A Spatio-Temporal Appearance Representation for Viceo-Based Pedestrian Re-Identification",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Kan Liu, Bingpeng Ma, Wei Zhang, Rui Huang",
        "author": "Kan Liu; Bingpeng Ma; Wei Zhang; Rui Huang",
        "abstract": "Pedestrian re-identification is a difficult problem due to the large variations in a person's appearance caused by different poses and viewpoints, illumination changes, and occlusions. Spatial alignment is commonly used to address these issues by treating the appearance of different body parts independently. However, a body part can also appear differently during different phases of an action. In this paper we consider the temporal alignment problem, in addition to the spatial one, and propose a new approach that takes the video of a walking person as input and builds a spatio-temporal appearance representation for pedestrian re-identification. Particularly, given a video sequence we exploit the periodicity exhibited by a walking person to generate a spatio-temporal body-action model, which consists of a series of body-action units corresponding to certain action primitives of certain body parts. Fisher vectors are learned and extracted from individual body-action units and concatenated into the final representation of the walking person. Unlike previous spatio-temporal features that only take into account local dynamic appearance information, our representation aligns the spatio-temporal appearance of a pedestrian globally. Extensive experiments on public datasets show the effectiveness of our approach compared with the state of the art.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Liu_A_Spatio-Temporal_Appearance_ICCV_2015_paper.pdf",
        "aff": "School of Control Science and Engineering, Shandong University, China; School of Computer and Control Engineering, University of Chinese Academy of Sciences, China; School of Control Science and Engineering, Shandong University, China; NEC Laboratories, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 865326,
        "gs_citation": 287,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18214623512303321609&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "gmail.com;ucas.ac.cn;gmail.com;nec.cn",
        "email": "gmail.com;ucas.ac.cn;gmail.com;nec.cn",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Liu_A_Spatio-Temporal_Appearance_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;0;2",
        "aff_unique_norm": "Shandong University;University of Chinese Academy of Sciences;NEC Laboratories",
        "aff_unique_dep": "School of Control Science and Engineering;School of Computer and Control Engineering;",
        "aff_unique_url": "http://www.sdu.edu.cn;http://www.ucas.ac.cn;https://www.nec-labs.com",
        "aff_unique_abbr": "SDU;UCAS;NEC Labs",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Liu_2015_ICCV,\n    \n    author = {\n    Liu,\n    Kan and Ma,\n    Bingpeng and Zhang,\n    Wei and Huang,\n    Rui\n},\n    title = {\n    A Spatio-Temporal Appearance Representation for Viceo-Based Pedestrian Re-Identification\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "6cb819d8be",
        "title": "A Supervised Low-Rank Method for Learning Invariant Subspaces",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Farzad Siyahjani, Ranya Almohsen, Sinan Sabri, Gianfranco Doretto",
        "author": "Farzad Siyahjani; Ranya Almohsen; Sinan Sabri; Gianfranco Doretto",
        "abstract": "Sparse representation and low-rank matrix decomposition approaches have been successfully applied to several computer vision problems. They build a generative representation of the data, which often requires complex training as well as testing to be robust against data variations induced by nuisance factors. We introduce the invariant components, a discriminative representation invariant to nuisance factors, because it spans subspaces orthogonal to the space where nuisance factors are defined. This allows developing a framework based on geometry that ensures a uniform inter-class separation, and a very efficient and robust classification based on simple nearest neighbor. In addition, we show how the approach is equivalent to a local metric learning, where the local metrics (one for each class) are learned jointly, rather than independently, thus avoiding the risk of overfitting without the need for additional regularization. We evaluated the approach for face recognition with highly corrupted training and testing data, obtaining very promising results.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Siyahjani_A_Supervised_Low-Rank_ICCV_2015_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 869481,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9988671356851386529&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Siyahjani_A_Supervised_Low-Rank_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Siyahjani_2015_ICCV,\n    \n    author = {\n    Siyahjani,\n    Farzad and Almohsen,\n    Ranya and Sabri,\n    Sinan and Doretto,\n    Gianfranco\n},\n    title = {\n    A Supervised Low-Rank Method for Learning Invariant Subspaces\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "6a0ad6409f",
        "title": "A Unified Multiplicative Framework for Attribute Learning",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Kongming Liang, Hong Chang, Shiguang Shan, Xilin Chen",
        "author": "Kongming Liang; Hong Chang; Shiguang Shan; Xilin Chen",
        "abstract": "Attributes are mid-level semantic properties of objects. Recent research has shown that visual attributes can benefit many traditional learning problems in computer vision community. However, attribute learning is still a challenging problem as the attributes may not always be predictable directly from input images and the variation of visual attributes is sometimes large across categories. In this paper, we propose a unified multiplicative framework for attribute learning, which tackles the key problems. Specifically, images and category information are jointly projected into a shared feature space, where the latent factors are disentangled and multiplied for attribute prediction. The resulting attribute classifier is category-specific instead of being shared by all  categories. Moreover, our method can leverage auxiliary data to enhance the predictive ability of attribute classifiers, reducing the effort of instance-level attribute annotation to some extent. Experimental results show that our method achieves superior performance on both instance-level and category-level attribute prediction. For zero-shot learning based on attributes, our method significantly improves the state-of-the-art performance on AwA dataset and achieves comparable performance on CUB dataset.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Liang_A_Unified_Multiplicative_ICCV_2015_paper.pdf",
        "aff": "Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China + University of Chinese Academy of Sciences, Beijing 100049, China; Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China; Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China; Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 765561,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18180952310551190951&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "vipl.ict.ac.cn;vipl.ict.ac.cn;vipl.ict.ac.cn;vipl.ict.ac.cn",
        "email": "vipl.ict.ac.cn;vipl.ict.ac.cn;vipl.ict.ac.cn;vipl.ict.ac.cn",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Liang_A_Unified_Multiplicative_ICCV_2015_paper.html",
        "aff_unique_index": "0+1;0;0;0",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Computing Technology;",
        "aff_unique_url": "http://www.cas.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "CAS;UCAS",
        "aff_campus_unique_index": "0+0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Liang_2015_ICCV,\n    \n    author = {\n    Liang,\n    Kongming and Chang,\n    Hong and Shan,\n    Shiguang and Chen,\n    Xilin\n},\n    title = {\n    A Unified Multiplicative Framework for Attribute Learning\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "99a685d8bc",
        "title": "A Versatile Learning-Based 3D Temporal Tracker: Scalable, Robust, Online",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "David Joseph Tan, Federico Tombari, Slobodan Ilic, Nassir Navab",
        "author": "David Joseph Tan; Federico Tombari; Slobodan Ilic; Nassir Navab",
        "abstract": "This paper proposes a temporal tracking algorithm based on Random Forest that uses depth images to estimate and track the 3D pose of a rigid object in real-time. Compared to the state of the art aimed at the same goal, our algorithm holds important attributes such as high robustness against holes and occlusion, low computational cost of both learning and tracking stages, and low memory consumption. These are obtained (a) by a novel formulation of the learning strategy, based on a dense sampling of the camera viewpoints and learning independent trees from a single image for each camera view; as well as, (b) by an insightful occlusion handling strategy that enforces the forest to recognize the object's local and global structures. Due to these attributes, we report state-of-the-art tracking accuracy on benchmark datasets, and accomplish remarkable scalability with the number of targets, being able to simultaneously track the pose of over a hundred objects at 30 fps with an off-the-shelf CPU. In addition, the fast learning time enables us to extend our algorithm as a robust online tracker for model-free 3D objects under different viewpoints and appearance changes as demonstrated by the experiments.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Tan_A_Versatile_Learning-Based_ICCV_2015_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1079072,
        "gs_citation": 87,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8856739746354332184&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Tan_A_Versatile_Learning-Based_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Tan_2015_ICCV,\n    \n    author = {\n    Tan,\n    David Joseph and Tombari,\n    Federico and Ilic,\n    Slobodan and Navab,\n    Nassir\n},\n    title = {\n    A Versatile Learning-Based 3D Temporal Tracker: Scalable,\n    Robust,\n    Online\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "7731ccf43e",
        "title": "A Versatile Scene Model With Differentiable Visibility Applied to Generative Pose Estimation",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Helge Rhodin, Nadia Robertini, Christian Richardt, Hans-Peter Seidel, Christian Theobalt",
        "author": "Helge Rhodin; Nadia Robertini; Christian Richardt; Hans-Peter Seidel; Christian Theobalt",
        "abstract": "Generative reconstruction methods compute the 3D configuration (such as pose and/or geometry) of a shape by optimizing the overlap of the projected 3D shape model with images. Proper handling of occlusions is a big challenge, since the visibility function that indicates if a surface point is seen from a camera can often not be formulated in closed form, and is in general discrete and non-differentiable at occlusion boundaries. We present a new scene representation that enables an analytically differentiable closed-form formulation of surface visibility. In contrast to previous methods, this yields smooth, analytically differentiable, and efficient to optimize pose similarity energies with rigorous occlusion handling, fewer local minima, and experimentally verified improved convergence of numerical optimization. The underlying idea is a new image formation model that represents opaque objects by a translucent medium with a smooth Gaussian density distribution which turns visibility into a smooth phenomenon. We demonstrate the advantages of our versatile scene model in several generative pose estimation problems, namely marker-less multi-object pose estimation, marker-less human motion capture with few cameras, and image-based 3D geometry estimation.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Rhodin_A_Versatile_Scene_ICCV_2015_paper.pdf",
        "aff": "MPI Informatik; MPI Informatik+Intel Visual Computing Institute; MPI Informatik+Intel Visual Computing Institute; MPI Informatik; MPI Informatik",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1082677,
        "gs_citation": 113,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3296809043890101661&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Rhodin_A_Versatile_Scene_ICCV_2015_paper.html",
        "aff_unique_index": "0;0+1;0+1;0;0",
        "aff_unique_norm": "Max Planck Institute for Informatics;Intel Corporation",
        "aff_unique_dep": "Informatik;Visual Computing Institute",
        "aff_unique_url": "https://www.mpi-inf.mpg.de;https://www.intel.com",
        "aff_unique_abbr": "MPII;Intel",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+1;0+1;0;0",
        "aff_country_unique": "Germany;United States",
        "bibtex": "@InProceedings{Rhodin_2015_ICCV,\n    \n    author = {\n    Rhodin,\n    Helge and Robertini,\n    Nadia and Richardt,\n    Christian and Seidel,\n    Hans-Peter and Theobalt,\n    Christian\n},\n    title = {\n    A Versatile Scene Model With Differentiable Visibility Applied to Generative Pose Estimation\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "886fda8b7d",
        "title": "A Wavefront Marching Method for Solving the Eikonal Equation on Cartesian Grids",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Brais Cancela, Marcos Ortega, Manuel G. Penedo",
        "author": "Brais Cancela; Marcos Ortega; Manuel G. Penedo",
        "abstract": "This paper presents a new wavefront propagation method for dealing with the classic Eikonal equation. While classic Dijkstra-like graph-based techniques achieve the solution in O(M log M), they do not approximate the unique physically relevant solution very well. Fast Marching Methods (FMM) were created to efficiently solve the continuous problem. The proposed approximation tries to maintain the complexity, in order to make the algorithm useful in a wide range of contexts. The key idea behind our method is the creation of 'mini wave-fronts', which are combined to propagate the solution. Experimental results show the improvement in the accuracy with respect to the state of the art, while the average computational speed is maintained in O(M log M), similar to the FMM techniques.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Cancela_A_Wavefront_Marching_ICCV_2015_paper.pdf",
        "aff": "V ARPA Group, Universidade da Coru ˜na; V ARPA Group, Universidade da Coru ˜na; V ARPA Group, Universidade da Coru ˜na",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 567771,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7768886561293526341&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "udc.es;udc.es;udc.es",
        "email": "udc.es;udc.es;udc.es",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Cancela_A_Wavefront_Marching_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Universidade da Coruña",
        "aff_unique_dep": "V ARPA Group",
        "aff_unique_url": "https://www.udc.es",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Spain",
        "bibtex": "@InProceedings{Cancela_2015_ICCV,\n    \n    author = {\n    Cancela,\n    Brais and Ortega,\n    Marcos and Penedo,\n    Manuel G.\n},\n    title = {\n    A Wavefront Marching Method for Solving the Eikonal Equation on Cartesian Grids\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "0897b27f99",
        "title": "Accurate Camera Calibration Robust to Defocus Using a Smartphone",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Hyowon Ha, Yunsu Bok, Kyungdon Joo, Jiyoung Jung, In So Kweon",
        "author": "Hyowon Ha; Yunsu Bok; Kyungdon Joo; Jiyoung Jung; In So Kweon",
        "abstract": "We propose a novel camera calibration method for defocused images using a smartphone under the assumption that the defocus blur is modeled as a convolution of a sharp image with a Gaussian point spread function (PSF). In contrast to existing calibration approaches which require well-focused images, the proposed method achieves accurate camera calibration with severely defocused images. This robustness to defocus is due to the proposed set of unidirectional binary patterns, which simplifies 2D Gaussian deconvolution to a 1D Gaussian deconvolution problem with multiple observations. By capturing the set of patterns consecutively displayed on a smartphone, we formulate the feature extraction as a deconvolution problem to estimate feature point locations in sub-pixel accuracy and the blur kernel in each location. We also compensate the error in camera parameters due to refraction of the glass panel of the display device. We evaluate the performance of the proposed method on synthetic and real data. Even under severe defocus, our method shows accurate camera calibration result.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Ha_Accurate_Camera_Calibration_ICCV_2015_paper.pdf",
        "aff": "Robotics and Computer Vision Lab., KAIST; Robotics and Computer Vision Lab., KAIST; Robotics and Computer Vision Lab., KAIST; Robotics and Computer Vision Lab., KAIST; Robotics and Computer Vision Lab., KAIST",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3550027,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16414152578337144679&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "rcv.kaist.ac.kr;rcv.kaist.ac.kr;rcv.kaist.ac.kr;rcv.kaist.ac.kr;kaist.ac.kr",
        "email": "rcv.kaist.ac.kr;rcv.kaist.ac.kr;rcv.kaist.ac.kr;rcv.kaist.ac.kr;kaist.ac.kr",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Ha_Accurate_Camera_Calibration_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "KAIST",
        "aff_unique_dep": "Robotics and Computer Vision Lab.",
        "aff_unique_url": "https://www.kaist.ac.kr",
        "aff_unique_abbr": "KAIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "South Korea",
        "bibtex": "@InProceedings{Ha_2015_ICCV,\n    \n    author = {\n    Ha,\n    Hyowon and Bok,\n    Yunsu and Joo,\n    Kyungdon and Jung,\n    Jiyoung and Kweon,\n    In So\n},\n    title = {\n    Accurate Camera Calibration Robust to Defocus Using a Smartphone\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "53f371e1f3",
        "title": "Action Detection by Implicit Intentional Motion Clustering",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Wei Chen, Jason J. Corso",
        "author": "Wei Chen; Jason J. Corso",
        "abstract": "Explicitly using human detection and pose estimation has found limited success in action recognition problems.  This may be due to the complexity in the articulated motion human exhibit.  Yet, we know that action requires an actor and intention.  This paper hence seeks to understand the spatiotemporal properties of intentional movement and how to capture such intentional movement without relying on challenging human detection and tracking.  We conduct a quantitative analysis of intentional movement, and our findings motivate a new approach for implicit intentional movement extraction that is based on  spatiotemporal trajectory clustering by leveraging the properties of intentional movement.  The intentional movement clusters are then used as action proposals for detection.  Our results on three action detection benchmarks indicate the  relevance of focusing on intentional movement for action detection; our method significantly outperforms the state of the art on the challenging MSR-II multi-action video benchmark.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Chen_Action_Detection_by_ICCV_2015_paper.pdf",
        "aff": "CSE, SUNY at Buffalo; EECS, University of Michigan",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1539340,
        "gs_citation": 62,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2775856400813851002&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "buffalo.edu;eecs.umich.edu",
        "email": "buffalo.edu;eecs.umich.edu",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Chen_Action_Detection_by_ICCV_2015_paper.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "State University of New York at Buffalo;University of Michigan",
        "aff_unique_dep": "Department of Computer Science and Engineering;Electrical Engineering and Computer Science",
        "aff_unique_url": "https://www.buffalo.edu;https://www.umich.edu",
        "aff_unique_abbr": "SUNY Buffalo;UM",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Buffalo;Ann Arbor",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Chen_2015_ICCV,\n    \n    author = {\n    Chen,\n    Wei and Corso,\n    Jason J.\n},\n    title = {\n    Action Detection by Implicit Intentional Motion Clustering\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "da04c11754",
        "title": "Action Localization in Videos Through Context Walk",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Khurram Soomro, Haroon Idrees, Mubarak Shah",
        "author": "Khurram Soomro; Haroon Idrees; Mubarak Shah",
        "abstract": "This paper presents an efficient approach for localizing actions by learning contextual relations, in the form of relative locations between different video regions. We begin by over-segmenting the videos into supervoxels, which have the ability to preserve action boundaries and also reduce the complexity of the problem. Context relations are learned during training which capture displacements from all the supervoxels in a video to those belonging to foreground actions. Then, given a testing video, we select a supervoxel randomly and use the context information acquired during training to estimate the probability of each supervoxel belonging to the foreground action. The walk proceeds to a new supervoxel and the process is repeated for a few steps. This ``context walk'' generates a conditional distribution of an action over all the supervoxels. A Conditional Random Field is then used to find action proposals in the video, whose confidences are obtained using SVMs. We validated the proposed approach on several datasets and show that context in the form of relative displacements between supervoxels can be extremely useful for action localization. This also results in significantly fewer evaluations of the classifier, in sharp contrast to the alternate sliding window approaches.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Soomro_Action_Localization_in_ICCV_2015_paper.pdf",
        "aff": "Center for Research in Computer Vision (CRCV), University of Central Florida (UCF); Center for Research in Computer Vision (CRCV), University of Central Florida (UCF); Center for Research in Computer Vision (CRCV), University of Central Florida (UCF)",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1514035,
        "gs_citation": 85,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10334643267594996929&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "eecs.ucf.edu;eecs.ucf.edu;eecs.ucf.edu",
        "email": "eecs.ucf.edu;eecs.ucf.edu;eecs.ucf.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Soomro_Action_Localization_in_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Central Florida",
        "aff_unique_dep": "Center for Research in Computer Vision (CRCV)",
        "aff_unique_url": "https://www.ucf.edu",
        "aff_unique_abbr": "UCF",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Orlando",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Soomro_2015_ICCV,\n    \n    author = {\n    Soomro,\n    Khurram and Idrees,\n    Haroon and Shah,\n    Mubarak\n},\n    title = {\n    Action Localization in Videos Through Context Walk\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "4e99651267",
        "title": "Action Recognition by Hierarchical Mid-Level Action Elements",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Tian Lan, Yuke Zhu, Amir Roshan Zamir, Silvio Savarese",
        "author": "Tian Lan; Yuke Zhu; Amir Roshan Zamir; Silvio Savarese",
        "abstract": "Realistic videos of human actions exhibit rich spatiotemporal structures at multiple levels of granularity: an action can always be decomposed into multiple finer-grained elements in both space and time. To capture this intuition, we propose to represent videos by a hierarchy of mid-level action elements (MAEs), where each MAE corresponds to an action-related spatiotemporal segment in the video. We introduce an unsupervised method to generate this representation from videos. Our method is capable of distinguishing action-related segments from background segments and representing actions at multiple spatiotemporal resolutions. Given a set of spatiotemporal segments generated from the training data, we introduce a discriminative clustering algorithm that automatically discovers MAEs at multiple levels of granularity. We develop structured models that capture a rich set of spatial, temporal and hierarchical relations among the segments, where the action label and multiple levels of MAE labels are jointly inferred. The proposed model achieves state-of-the-art performance in multiple action recognition benchmarks. Moreover, we demonstrate the effectiveness of our model in real-world applications such as action recognition in large-scale untrimmed videos and action parsing.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Lan_Action_Recognition_by_ICCV_2015_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1603025,
        "gs_citation": 101,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11275703924675518568&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Lan_Action_Recognition_by_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Lan_2015_ICCV,\n    \n    author = {\n    Lan,\n    Tian and Zhu,\n    Yuke and Zamir,\n    Amir Roshan and Savarese,\n    Silvio\n},\n    title = {\n    Action Recognition by Hierarchical Mid-Level Action Elements\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "29b5441057",
        "title": "Actionness-Assisted Recognition of Actions",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Ye Luo, Loong-Fah Cheong, An Tran",
        "author": "Ye Luo; Loong-Fah Cheong; An Tran",
        "abstract": "We elicit from a fundamental definition of action low-level attributes that can reveal agency and intentionality. These descriptors are mainly trajectory-based, measuring sudden changes, temporal synchrony, and repetitiveness. The actionness map can be used to localize actions in a way that is generic across action and agent types. Furthermore, it also groups interacting regions into a useful unit of analysis, which is crucial for recognition of actions involving interactions. We then implement an actionness-driven pooling scheme to improve action recognition performance. Experimental results on three datasets show the advantages of our method on both action detection and action recognition comparing with other state-of-the-art methods.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Luo_Actionness-Assisted_Recognition_of_ICCV_2015_paper.pdf",
        "aff": "Department of Electrical & Computer Engineering, National University of Singapore; Department of Electrical & Computer Engineering, National University of Singapore; Department of Electrical & Computer Engineering, National University of Singapore",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 871728,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16207457214359779494&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "hotmail.com;nus.edu.sg;u.nus.edu.sg",
        "email": "hotmail.com;nus.edu.sg;u.nus.edu.sg",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Luo_Actionness-Assisted_Recognition_of_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "National University of Singapore",
        "aff_unique_dep": "Department of Electrical & Computer Engineering",
        "aff_unique_url": "https://www.nus.edu.sg",
        "aff_unique_abbr": "NUS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Singapore",
        "bibtex": "@InProceedings{Luo_2015_ICCV,\n    \n    author = {\n    Luo,\n    Ye and Cheong,\n    Loong-Fah and Tran,\n    An\n},\n    title = {\n    Actionness-Assisted Recognition of Actions\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "c9fc747622",
        "title": "Actions and Attributes From Wholes and Parts",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Georgia Gkioxari, Ross Girshick, Jitendra Malik",
        "author": "Georgia Gkioxari; Ross Girshick; Jitendra Malik",
        "abstract": "We investigate the importance of parts for the tasks of action and attribute classification. We develop a part-based approach by leveraging convolutional network features inspired by recent advances in computer vision. Our part detectors are a deep version of poselets and capture parts of the human body under a distinct set of poses. For the tasks of action and attribute classification, we train holistic convolutional neural networks and show that adding parts leads to top-performing results for both tasks. We observe that for deeper networks parts are less significant. In addition, we demonstrate the effectiveness of our approach when we replace an oracle person detector, as is the default in the current evaluation protocol for both tasks, with a state-of-the-art person detection system.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Gkioxari_Actions_and_Attributes_ICCV_2015_paper.pdf",
        "aff": "UC Berkeley; Microsoft Research; UC Berkeley",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1131609,
        "gs_citation": 195,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13723022999897850650&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "berkeley.edu;microsoft.com;berkeley.edu",
        "email": "berkeley.edu;microsoft.com;berkeley.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Gkioxari_Actions_and_Attributes_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of California, Berkeley;Microsoft Corporation",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.berkeley.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "UC Berkeley;MSR",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Gkioxari_2015_ICCV,\n    \n    author = {\n    Gkioxari,\n    Georgia and Girshick,\n    Ross and Malik,\n    Jitendra\n},\n    title = {\n    Actions and Attributes From Wholes and Parts\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "7ed38a04ca",
        "title": "Active Object Localization With Deep Reinforcement Learning",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Juan C. Caicedo, Svetlana Lazebnik",
        "author": "Juan C. Caicedo; Svetlana Lazebnik",
        "abstract": "We present an active detection model for localizing objects in scenes. The model is class-specific and allows an agent to focus attention on candidate regions for identifying the correct location of a target object. This agent learns to deform a bounding box using simple transformation actions, with the goal of determining the most specific location of target objects following top-down reasoning. The proposed localization agent is trained using deep reinforcement learning, and evaluated on the Pascal VOC 2007 dataset. We show that agents guided by the proposed model are able to localize a single instance of an object after analyzing only between 11 and 25 regions in an image, and obtain the best detection results among systems that do not use object proposals for object localization.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Caicedo_Active_Object_Localization_ICCV_2015_paper.pdf",
        "aff": ";",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 590,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16986174613012654953&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 20,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Caicedo_Active_Object_Localization_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Caicedo_2015_ICCV,\n    \n    author = {\n    Caicedo,\n    Juan C. and Lazebnik,\n    Svetlana\n},\n    title = {\n    Active Object Localization With Deep Reinforcement Learning\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "cdef574066",
        "title": "Active One-Shot Scan for Wide Depth Range Using a Light Field Projector Based on Coded Aperture",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Hiroshi Kawasaki, Satoshi Ono, Yuki Horita, Yuki Shiba, Ryo Furukawa, Shinsaku Hiura",
        "author": "Hiroshi Kawasaki; Satoshi Ono; Yuki Horita; Yuki Shiba; Ryo Furukawa; Shinsaku Hiura",
        "abstract": "The central projection model commonly used to model cameras as well as projectors, results in similar advantages and disadvantages in both types of system. Considering the case of active stereo systems using a projector and camera setup, a central projection model creates several problems; among them, narrow depth range and necessity of wide baseline are crucial. In the paper, we solve the problems by introducing a light field projector, which can project a depth-dependent pattern. The light field projector is realized by attaching a coded aperture with a high frequency mask in front of the lens of the video projector, which also projects a high frequency pattern. Because the light field projector cannot be approximated by a thin lens model and a precise calibration method is not established yet, an image-based approach is proposed to apply a stereo technique to the system. Although image-based techniques usually require a large database and often imply heavy computational costs, we propose a hierarchical approach and a feature-based search for solution. In the experiments, it is confirmed that our method can accurately recover the dense shape of curved and textured objects for a wide range of depths from a single captured image.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Kawasaki_Active_One-Shot_Scan_ICCV_2015_paper.pdf",
        "aff": "Kagoshima University; Kagoshima University; Kagoshima University; Kagoshima University; Hiroshima City University; Hiroshima City University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2994806,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1203765257587492672&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": "ibe.kagoshima-u.ac.jp;ibe.kagoshima-u.ac.jp; ; ;hiroshima-cu.ac.jp;hiroshima-cu.ac.jp",
        "email": "ibe.kagoshima-u.ac.jp;ibe.kagoshima-u.ac.jp; ; ;hiroshima-cu.ac.jp;hiroshima-cu.ac.jp",
        "author_num": 6,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Kawasaki_Active_One-Shot_Scan_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0;1;1",
        "aff_unique_norm": "Kagoshima University;Hiroshima City University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.kagu.ac.jp;https://www.hcu-hiroshima.ac.jp",
        "aff_unique_abbr": "KU;HCU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "Japan",
        "bibtex": "@InProceedings{Kawasaki_2015_ICCV,\n    \n    author = {\n    Kawasaki,\n    Hiroshi and Ono,\n    Satoshi and Horita,\n    Yuki and Shiba,\n    Yuki and Furukawa,\n    Ryo and Hiura,\n    Shinsaku\n},\n    title = {\n    Active One-Shot Scan for Wide Depth Range Using a Light Field Projector Based on Coded Aperture\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "85519dcdaa",
        "title": "Active Transfer Learning With Zero-Shot Priors: Reusing Past Datasets for Future Tasks",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Efstratios Gavves, Thomas Mensink, Tatiana Tommasi, Cees G. M. Snoek, Tinne Tuytelaars",
        "author": "Efstratios Gavves; Thomas Mensink; Tatiana Tommasi; Cees G. M. Snoek; Tinne Tuytelaars",
        "abstract": "How can we reuse existing knowledge, in the form of available  datasets, when solving a new and apparently unrelated target task from a set of unlabeled data?  In this work we make a first contribution to answer this question in the context of image classification.  We frame this quest as an active learning problem and use zero-shot  classifiers to guide the learning process by linking the new task to the the  existing classifiers. By revisiting the dual formulation of adaptive SVM, we reveal two basic conditions to choose greedily only the most relevant samples to be annotated.  On this basis we propose an effective active learning algorithm which learns the best possible target classification model with minimum human labeling effort.  Extensive experiments on two challenging datasets show the value of our approach compared to the state-of-the-art active learning methodologies, as well as its potential to reuse past datasets with minimal effort for future tasks.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Gavves_Active_Transfer_Learning_ICCV_2015_paper.pdf",
        "aff": "QUV A Lab, University of Amsterdam+ESAT-PSI, KU Leuven; University of Amsterdam; UNC Chapel Hill; QUV A Lab, University of Amsterdam; KU Leuven, ESAT-PSI, iMinds",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 995684,
        "gs_citation": 88,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10297030708131159099&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 22,
        "aff_domain": "; ; ; ; ",
        "email": "; ; ; ; ",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Gavves_Active_Transfer_Learning_ICCV_2015_paper.html",
        "aff_unique_index": "0+1;0;2;0;1",
        "aff_unique_norm": "University of Amsterdam;KU Leuven;University of North Carolina at Chapel Hill",
        "aff_unique_dep": "QUV A Lab;ESAT-PSI;",
        "aff_unique_url": "https://www.uva.nl;https://www.kuleuven.be;https://www.unc.edu",
        "aff_unique_abbr": "UvA;KU Leuven;UNC",
        "aff_campus_unique_index": ";1",
        "aff_campus_unique": ";Chapel Hill",
        "aff_country_unique_index": "0+1;0;2;0;1",
        "aff_country_unique": "Netherlands;Belgium;United States",
        "bibtex": "@InProceedings{Gavves_2015_ICCV,\n    \n    author = {\n    Gavves,\n    Efstratios and Mensink,\n    Thomas and Tommasi,\n    Tatiana and Snoek,\n    Cees G. M. and Tuytelaars,\n    Tinne\n},\n    title = {\n    Active Transfer Learning With Zero-Shot Priors: Reusing Past Datasets for Future Tasks\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "d6d00fd082",
        "title": "Activity Auto-Completion: Predicting Human Activities From Partial Videos",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Zhen Xu, Laiyun Qing, Jun Miao",
        "author": "Zhen Xu; Laiyun Qing; Jun Miao",
        "abstract": "In this paper, we propose an activity auto-completion (AAC) model for human activity prediction by formulating activity prediction as a query auto-completion (QAC) problem in information retrieval. First, we extract discriminative patches in frames of videos. A video is represented based on these patches and divided into a collection of segments, each of which is regarded as a character typed in the search box. Then a partially observed video is considered as an activity prefix, consisting of one or more characters. Finally, the missing observation of an activity is predicted as the activity candidates provided by the auto-completion model. The candidates are matched against the activity prefix on-the-fly and ranked by a learning-to-rank algorithm. We validate our method on UT-Interaction Set #1 and Set #2 [19]. The experimental results show that the proposed activity auto-completion model achieves promising performance.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Xu_Activity_Auto-Completion_Predicting_ICCV_2015_paper.pdf",
        "aff": "Key Laboratory of Big Data Mining and Knowledge Management, Chinese Academy of Sciences, University of Chinese Academy of Sciences, Beijing, China+Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, China; Key Laboratory of Big Data Mining and Knowledge Management, Chinese Academy of Sciences, University of Chinese Academy of Sciences, Beijing, China+Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, China; Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3416236,
        "gs_citation": 61,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6402079472574107021&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "mails.ucas.ac.cn;ucas.ac.cn;ict.ac.cn",
        "email": "mails.ucas.ac.cn;ucas.ac.cn;ict.ac.cn",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Xu_Activity_Auto-Completion_Predicting_ICCV_2015_paper.html",
        "aff_unique_index": "0+0;0+0;0",
        "aff_unique_norm": "Chinese Academy of Sciences",
        "aff_unique_dep": "Key Laboratory of Big Data Mining and Knowledge Management",
        "aff_unique_url": "http://www.cas.cn",
        "aff_unique_abbr": "CAS",
        "aff_campus_unique_index": "0+0;0+0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0+0;0+0;0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Xu_2015_ICCV,\n    \n    author = {\n    Xu,\n    Zhen and Qing,\n    Laiyun and Miao,\n    Jun\n},\n    title = {\n    Activity Auto-Completion: Predicting Human Activities From Partial Videos\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "1f2f701dd1",
        "title": "Adaptive Dither Voting for Robust Spatial Verification",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Xiaomeng Wu, Kunio Kashino",
        "author": "Xiaomeng Wu; Kunio Kashino",
        "abstract": "Hough voting in a geometric transformation space allows us to realize spatial verification, but remains sensitive to feature detection errors because of the inflexible quantization of single feature correspondences. To handle this problem, we propose a new method, called adaptive dither voting, for robust spatial verification. For each correspondence, instead of hard-mapping it to a single transformation, the method augments its description by using multiple dithered transformations that are deterministically generated by the other correspondences. The method reduces the probability of losing correspondences during transformation quantization, and provides high robustness as regards mismatches by imposing three geometric constraints on the dithering process. We also propose exploiting the non-uniformity of a Hough histogram as the spatial similarity to handle multiple matching surfaces. Extensive experiments conducted on four datasets show the superiority of our method. The method outperforms its state-of-the-art counterparts in both accuracy and scalability, especially when it comes to the retrieval of small, rotated objects.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Wu_Adaptive_Dither_Voting_ICCV_2015_paper.pdf",
        "aff": "Nippon Telegraph and Telephone Corporation; Nippon Telegraph and Telephone Corporation",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1136890,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6596751230405284924&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "lab.ntt.co.jp;lab.ntt.co.jp",
        "email": "lab.ntt.co.jp;lab.ntt.co.jp",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Wu_Adaptive_Dither_Voting_ICCV_2015_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Nippon Telegraph and Telephone Corporation",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ntt.co.jp",
        "aff_unique_abbr": "NTT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan",
        "bibtex": "@InProceedings{Wu_2015_ICCV,\n    \n    author = {\n    Wu,\n    Xiaomeng and Kashino,\n    Kunio\n},\n    title = {\n    Adaptive Dither Voting for Robust Spatial Verification\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "4a19a73837",
        "title": "Adaptive Exponential Smoothing for Online Filtering of Pixel Prediction Maps",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Kang Dang, Jiong Yang, Junsong Yuan",
        "author": "Kang Dang; Jiong Yang; Junsong Yuan",
        "abstract": "We propose an efficient online video filtering method, called adaptive exponential filtering (AES) to refine pixel prediction maps. Assuming each pixel is associated with a discriminative prediction score, the proposed AES applies exponentially decreasing weights over time to smooth the prediction score of each pixel, similar to classic exponential smoothing. However, instead of fixing the spatial pixel location to perform temporal filtering, we trace each pixel in the past frames by finding the optimal path that can bring the maximum exponential smoothing score, thus performing adaptive and non-linear filtering. Thanks to the pixel tracing, AES can better address object movements and avoid over-smoothing. To enable real-time filtering, we propose a linear-complexity dynamic programming scheme that can trace all pixels simultaneously. We apply the proposed filtering method to improve both saliency detection maps and scene parsing maps. The comparisons with average and exponential filtering, as well as state-of-the-art methods, validate that our AES can effectively refine the pixel prediction maps, without using the original video again.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Dang_Adaptive_Exponential_Smoothing_ICCV_2015_paper.pdf",
        "aff": "School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, 639798; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, 639798; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, 639798",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1619458,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17493012219890014798&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "e.ntu.edu.sg;e.ntu.edu.sg;ntu.edu.sg",
        "email": "e.ntu.edu.sg;e.ntu.edu.sg;ntu.edu.sg",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Dang_Adaptive_Exponential_Smoothing_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Nanyang Technological University",
        "aff_unique_dep": "School of Electrical and Electronic Engineering",
        "aff_unique_url": "https://www.ntu.edu.sg",
        "aff_unique_abbr": "NTU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Singapore",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Singapore",
        "bibtex": "@InProceedings{Dang_2015_ICCV,\n    \n    author = {\n    Dang,\n    Kang and Yang,\n    Jiong and Yuan,\n    Junsong\n},\n    title = {\n    Adaptive Exponential Smoothing for Online Filtering of Pixel Prediction Maps\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "1b24a943ec",
        "title": "Adaptive Hashing for Fast Similarity Search",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Fatih Cakir, Stan Sclaroff",
        "author": "Fatih Cakir; Stan Sclaroff",
        "abstract": "With the staggering growth in image and video datasets, algorithms that provide fast similarity search and compact storage are crucial. Hashing methods that map the data into Hamming space have shown promise; however, many of these methods employ a batch-learning strategy in which the computational cost and memory requirements may become intractable and infeasible with larger and larger datasets. To overcome these challenges, we propose an online learning algorithm based on stochastic gradient descent in which the hash functions are updated iteratively with streaming data. In experiments with three image retrieval benchmarks, our online algorithm attains retrieval accuracy that is comparable to competing state-of-the-art batch-learning solutions, while our formulation is orders of magnitude faster and being online it is adaptable to the variations of the data. Moreover, our formulation yields improved retrieval performance over a recently reported online hashing technique, Online Kernel Hashing.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Cakir_Adaptive_Hashing_for_ICCV_2015_paper.pdf",
        "aff": "Department of Computer Science, Boston University, Boston, MA; Department of Computer Science, Boston University, Boston, MA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1531969,
        "gs_citation": 97,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13736663925761949554&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "cs.bu.edu;cs.bu.edu",
        "email": "cs.bu.edu;cs.bu.edu",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Cakir_Adaptive_Hashing_for_ICCV_2015_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Boston University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.bu.edu",
        "aff_unique_abbr": "BU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Boston",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Cakir_2015_ICCV,\n    \n    author = {\n    Cakir,\n    Fatih and Sclaroff,\n    Stan\n},\n    title = {\n    Adaptive Hashing for Fast Similarity Search\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "e06496f6a1",
        "title": "Adaptive Spatial-Spectral Dictionary Learning for Hyperspectral Image Denoising",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Ying Fu, Antony Lam, Imari Sato, Yoichi Sato",
        "author": "Ying Fu; Antony Lam; Imari Sato; Yoichi Sato",
        "abstract": "Hyperspectral imaging is beneficial in a diverse range of applications from diagnostic medicine, to agriculture, to surveillance to name a few. However, hyperspectral images often times suffer from degradation due to the limited light, which introduces noise into the imaging process. In this paper, we propose an effective model for hyperspectral image (HSI) denoising that considers underlying characteristics of HSIs: sparsity across the spatial-spectral domain, high correlation across spectra, and non-local self-similarity over space. We first exploit high correlation across spectra and non-local self-similarity over space in the noisy HSI to learn an adaptive spatial-spectral dictionary. Then, we employ the local and non-local sparsity of the HSI under the learned spatial-spectral dictionary to design an HSI denoising model, which can be effectively solved by an iterative numerical algorithm with parameters that are adaptively adjusted for different clusters and different noise levels. Experimental results on HSI denoising show that the proposed method can provide substantial improvements over the current state-of-the-art HSI denoising methods in terms of both objective metric and subjective visual quality.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Fu_Adaptive_Spatial-Spectral_Dictionary_ICCV_2015_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 7349571,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3707016880243416537&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Fu_Adaptive_Spatial-Spectral_Dictionary_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Fu_2015_ICCV,\n    \n    author = {\n    Fu,\n    Ying and Lam,\n    Antony and Sato,\n    Imari and Sato,\n    Yoichi\n},\n    title = {\n    Adaptive Spatial-Spectral Dictionary Learning for Hyperspectral Image Denoising\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "36110f463b",
        "title": "Adaptively Unified Semi-Supervised Dictionary Learning With Active Points",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Xiaobo Wang, Xiaojie Guo, Stan Z. Li",
        "author": "Xiaobo Wang; Xiaojie Guo; Stan Z. Li",
        "abstract": "Semi-supervised dictionary learning aims to construct a dictionary by utilizing both labeled and unlabeled data. To enhance the discriminative capability of the learned dictionary, numerous discriminative terms have been proposed by evaluating either the prediction loss or the class separation criterion on the coding vectors of labeled data, but with rare consideration of the power of the coding vectors corresponding to unlabeled data. In this paper, we present a novel semi-supervised dictionary learning method, which uses the informative coding vectors of both labeled and unlabeled data, and adaptively emphasizes the high confidence coding vectors of unlabeled data to enhance the dictionary discriminative capability simultaneously. By doing so, we integrate the discrimination of dictionary, the induction of classifier to new testing data and the transduction of labels to unlabeled data into a unified framework. To solve the proposed problem, an effective iterative algorithm is designed. Experimental results on a series of benchmark databases show that our method outperforms other state-of-the-art dictionary learning methods in most cases.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Wang_Adaptively_Unified_Semi-Supervised_ICCV_2015_paper.pdf",
        "aff": "State Key Laboratory of Information Security, IIE, Chinese Academy of Sciences+Center for Biometrics and Security Research & National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences; State Key Laboratory of Information Security, IIE, Chinese Academy of Sciences; Center for Biometrics and Security Research & National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1099222,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18197937291401367296&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "nlpr.ia.ac.cn;gmail.com;nlpr.ia.ac.cn",
        "email": "nlpr.ia.ac.cn;gmail.com;nlpr.ia.ac.cn",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Wang_Adaptively_Unified_Semi-Supervised_ICCV_2015_paper.html",
        "aff_unique_index": "0+0;0;0",
        "aff_unique_norm": "Chinese Academy of Sciences",
        "aff_unique_dep": "State Key Laboratory of Information Security",
        "aff_unique_url": "http://www.ucas.ac.cn",
        "aff_unique_abbr": "CAS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Wang_2015_ICCV,\n    \n    author = {\n    Wang,\n    Xiaobo and Guo,\n    Xiaojie and Li,\n    Stan Z.\n},\n    title = {\n    Adaptively Unified Semi-Supervised Dictionary Learning With Active Points\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "c2a54ffe25",
        "title": "Additive Nearest Neighbor Feature Maps",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Zhenzhen Wang, Xiao-Tong Yuan, Qingshan Liu, Shuicheng Yan",
        "author": "Zhenzhen Wang; Xiao-Tong Yuan; Qingshan Liu; Shuicheng Yan",
        "abstract": "In this paper, we present a concise framework to approximately construct feature maps for nonlinear additive kernels such as the Intersection, Hellinger's, and Chi^2 kernels. The core idea is to construct for each individual feature a set of anchor points and assign to every query the feature map of its nearest neighbor or the weighted combination of those of its k-nearest neighbors in the anchors. The resultant feature maps can be compactly stored by a group of nearest neighbor (binary) indication vectors along with the anchor feature maps. The approximation error of such an anchored feature mapping approach is analyzed. We evaluate the performance of our approach on large-scale nonlinear support vector machines (SVMs) learning tasks in the context of visual object classification. Experimental results on several benchmark data sets show the superiority of our method over existing feature mapping methods in achieving reasonable trade-off between training time and testing accuracy.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Wang_Additive_Nearest_Neighbor_ICCV_2015_paper.pdf",
        "aff": "Nanjing University of Information Science and Technology, China+National University of Singapore, Singapore; Nanjing University of Information Science and Technology, China; Nanjing University of Information Science and Technology, China; National University of Singapore, Singapore",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 631358,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5022567660992341425&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "gmail.com;nuist.edu.cn;nuist.edu.cn;nus.edu.sg",
        "email": "gmail.com;nuist.edu.cn;nuist.edu.cn;nus.edu.sg",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Wang_Additive_Nearest_Neighbor_ICCV_2015_paper.html",
        "aff_unique_index": "0+1;0;0;1",
        "aff_unique_norm": "Nanjing University of Information Science and Technology;National University of Singapore",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.nuist.edu.cn;https://www.nus.edu.sg",
        "aff_unique_abbr": "NUIST;NUS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0;0;1",
        "aff_country_unique": "China;Singapore",
        "bibtex": "@InProceedings{Wang_2015_ICCV,\n    \n    author = {\n    Wang,\n    Zhenzhen and Yuan,\n    Xiao-Tong and Liu,\n    Qingshan and Yan,\n    Shuicheng\n},\n    title = {\n    Additive Nearest Neighbor Feature Maps\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "60e64c6326",
        "title": "Aggregating Local Deep Features for Image Retrieval",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Artem Babenko, Victor Lempitsky",
        "author": "Artem Babenko; Victor Lempitsky",
        "abstract": "Several recent works have shown that image descriptors produced by deep convolutional neural networks provide state-of-the-art performance for image classification and retrieval problems. It also has been shown that the activations from the convolutional layers can be interpreted as local features describing particular image regions. These local features can be aggregated using aggregating methods developed for local features (e.g. Fisher vectors), thus providing new powerful global descriptor.  In this paper we investigate possible ways to aggregate local deep features to produce compact descriptors for image retrieval. First, we show that deep features and traditional hand-engineered features have quite different distributions of pairwise similarities, hence existing aggregation methods have to be carefully re-evaluated. Such re-evaluation reveals that in contrast to shallow features, the simple aggregation method based on sum pooling provides the best performance for deep convolutional features. This method is efficient, has few parameters, and bears little risk of overfitting when e.g. learning the PCA matrix. In addition, we suggest a simple yet efficient query expansion scheme suitable for the proposed aggregation method. Overall, the new compact global descriptor improves the state-of-the-art on four common benchmarks considerably.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Babenko_Aggregating_Local_Deep_ICCV_2015_paper.pdf",
        "aff": "Yandex+Moscow Institute of Physics and Technology; Skolkovo Institute of Science and Technology (Skoltech)",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 756042,
        "gs_citation": 894,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2042933870643874763&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "phystech.edu;skoltech.ru",
        "email": "phystech.edu;skoltech.ru",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Babenko_Aggregating_Local_Deep_ICCV_2015_paper.html",
        "aff_unique_index": "0+1;2",
        "aff_unique_norm": "Yandex;Moscow Institute of Physics and Technology;Skolkovo Institute of Science and Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://yandex.com;https://www.mipt.ru/en;https://www.skoltech.ru",
        "aff_unique_abbr": "Yandex;MIPT;Skoltech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0",
        "aff_country_unique": "Russia",
        "bibtex": "@InProceedings{Babenko_2015_ICCV,\n    \n    author = {\n    Babenko,\n    Artem and Lempitsky,\n    Victor\n},\n    title = {\n    Aggregating Local Deep Features for Image Retrieval\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "6bb1598ffc",
        "title": "Airborne Three-Dimensional Cloud Tomography",
        "session": "computational photography and image enhancement",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "Aviad Levis, Yoav Y. Schechner, Amit Aides, Anthony B. Davis",
        "author": "Aviad Levis; Yoav Y. Schechner; Amit Aides; Anthony B. Davis",
        "abstract": "We seek to sense the three dimensional (3D) volumetric distribution of scatterers in a heterogenous medium. An important case study for such a medium is the atmosphere. Atmospheric contents and their role in Earth's radiation balance have significant uncertainties with regards to scattering components: aerosols and clouds. Clouds, made of water droplets, also lead to local effects as precipitation and shadows. Our sensing approach is computational tomography using passive multi-angular imagery. For light-matter interaction that accounts for multiple-scattering, we use the 3D radiative transfer equation as a forward model. Volumetric recovery by inverting this model suffers from a computational bottleneck on large scales, which include many unknowns. Steps taken make this tomography tractable, without approximating the scattering order or angle range.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Levis_Airborne_Three-Dimensional_Cloud_ICCV_2015_paper.pdf",
        "aff": "Dept. Electrical Eng., Technion - Israel Inst. Technology, Haifa, Israel; Dept. Electrical Eng., Technion - Israel Inst. Technology, Haifa, Israel; Dept. Electrical Eng., Technion - Israel Inst. Technology, Haifa, Israel; Jet Propulsion Laboratory, California Inst. of Technology, Pasadena, California",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2499183,
        "gs_citation": 93,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5267543508098015481&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "tx.technion.ac.il;ee.technion.ac.il;campus.technion.ac.il;jpl.nasa.gov",
        "email": "tx.technion.ac.il;ee.technion.ac.il;campus.technion.ac.il;jpl.nasa.gov",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Levis_Airborne_Three-Dimensional_Cloud_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Technion - Israel Institute of Technology;California Institute of Technology",
        "aff_unique_dep": "Department of Electrical Engineering;Jet Propulsion Laboratory",
        "aff_unique_url": "https://www.technion.ac.il;https://www.caltech.edu",
        "aff_unique_abbr": "Technion;Caltech",
        "aff_campus_unique_index": "0;0;0;1",
        "aff_campus_unique": "Haifa;Pasadena",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "Israel;United States",
        "bibtex": "@InProceedings{Levis_2015_ICCV,\n    \n    author = {\n    Levis,\n    Aviad and Schechner,\n    Yoav Y. and Aides,\n    Amit and Davis,\n    Anthony B.\n},\n    title = {\n    Airborne Three-Dimensional Cloud Tomography\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "463911c145",
        "title": "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books",
        "session": "vision and language",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, Sanja Fidler",
        "author": "Yukun Zhu; Ryan Kiros; Rich Zemel; Ruslan Salakhutdinov; Raquel Urtasun; Antonio Torralba; Sanja Fidler",
        "abstract": "Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story. This paper aims to align books to their movie releases in order to provide rich descriptive explanations for visual content that go semantically far beyond the captions available in the current datasets. To align movies and books we propose a neural sentence embedding that is trained in an unsupervised way from a large corpus of books, as well as a video-text neural embedding for computing similarities between movie clips and sentences in the book. We propose a context-aware CNN to combine information from multiple sources. We demonstrate good quantitative performance for movie/book alignment and show several qualitative examples that showcase the diversity of tasks our model can be used for.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Zhu_Aligning_Books_and_ICCV_2015_paper.pdf",
        "aff": "University of Toronto; University of Toronto; University of Toronto; University of Toronto; University of Toronto; Massachusetts Institute of Technology; University of Toronto",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2270854,
        "gs_citation": 3512,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4414474086145090761&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff_domain": "cs.toronto.edu;cs.toronto.edu;cs.toronto.edu;cs.toronto.edu;cs.toronto.edu;csail.mit.edu;cs.toronto.edu",
        "email": "cs.toronto.edu;cs.toronto.edu;cs.toronto.edu;cs.toronto.edu;cs.toronto.edu;csail.mit.edu;cs.toronto.edu",
        "author_num": 7,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Zhu_Aligning_Books_and_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0;0;1;0",
        "aff_unique_norm": "University of Toronto;Massachusetts Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.utoronto.ca;https://web.mit.edu",
        "aff_unique_abbr": "U of T;MIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;1;0",
        "aff_country_unique": "Canada;United States",
        "bibtex": "@InProceedings{Zhu_2015_ICCV,\n    \n    author = {\n    Zhu,\n    Yukun and Kiros,\n    Ryan and Zemel,\n    Rich and Salakhutdinov,\n    Ruslan and Urtasun,\n    Raquel and Torralba,\n    Antonio and Fidler,\n    Sanja\n},\n    title = {\n    Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "65d8721a50",
        "title": "Alternating Co-Quantization for Cross-Modal Hashing",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Go Irie, Hiroyuki Arai, Yukinobu Taniguchi",
        "author": "Go Irie; Hiroyuki Arai; Yukinobu Taniguchi",
        "abstract": "This paper addresses the problem of unsupervised learning of binary hash codes for efficient cross-modal retrieval. Many unimodal hashing studies have proven that both similarity preservation of data and maintenance of quantization quality are essential for improving retrieval performance with binary hash codes. However, most existing cross-modal hashing methods mainly have focused on the former, and the latter still remains almost untouched. We propose a method to minimize the binary quantization errors, which is tailored to cross-modal hashing. Our approach, named Alternating Co-Quantization (ACQ), alternately seeks binary quantizers for each modality space with the help of connections to other modality data so that they give minimal quantization errors while preserving data similarities. ACQ can be coupled with various existing cross-modal dimension reduction methods such as Canonical Correlation Analysis (CCA) and substantially boosts their retrieval performance in the Hamming space. Extensive experiments demonstrate that ACQ can outperform several state-of-the-art methods, even when it is combined with simple CCA.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Irie_Alternating_Co-Quantization_for_ICCV_2015_paper.pdf",
        "aff": "NTT Corporation; NTT Corporation; NTT Corporation",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1158114,
        "gs_citation": 93,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15217211410044192100&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "lab.ntt.co.jp;lab.ntt.co.jp;lab.ntt.co.jp",
        "email": "lab.ntt.co.jp;lab.ntt.co.jp;lab.ntt.co.jp",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Irie_Alternating_Co-Quantization_for_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "NTT Corporation",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ntt.co.jp",
        "aff_unique_abbr": "NTT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Japan",
        "bibtex": "@InProceedings{Irie_2015_ICCV,\n    \n    author = {\n    Irie,\n    Go and Arai,\n    Hiroyuki and Taniguchi,\n    Yukinobu\n},\n    title = {\n    Alternating Co-Quantization for Cross-Modal Hashing\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "3bf92f9d1c",
        "title": "Amodal Completion and Size Constancy in Natural Scenes",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Abhishek Kar, Shubham Tulsiani, Joao Carreira, Jitendra Malik",
        "author": "Abhishek Kar; Shubham Tulsiani; Joao Carreira; Jitendra Malik",
        "abstract": "We consider the problem of enriching current object detection systems with veridical object sizes and relative depth estimates from a single image. There are several technical challenges to this, such as occlusions, lack of calibration data and the scale ambiguity between object size and distance. These have not been addressed in full generality in previous work. Here we propose to tackle these issues by building upon advances in object recognition and using recently created large-scale datasets. We first introduce the task of amodal bounding box completion, which aims to infer the the full extent of the object instances in the image. We then propose a probabilistic framework for learning category-specific object size distributions from available annotations and leverage these in conjunction with amodal completions to infer veridical sizes of objects in novel images. Finally, we introduce a focal length prediction approach that exploits scene recognition to overcome inherent scale ambiguities and demonstrate qualitative results on challenging real-world scenes.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Kar_Amodal_Completion_and_ICCV_2015_paper.pdf",
        "aff": "University of California, Berkeley - Berkeley, CA 94720; University of California, Berkeley - Berkeley, CA 94720; University of California, Berkeley - Berkeley, CA 94720; University of California, Berkeley - Berkeley, CA 94720",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2293415,
        "gs_citation": 65,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12086473224040374131&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu",
        "email": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Kar_Amodal_Completion_and_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Kar_2015_ICCV,\n    \n    author = {\n    Kar,\n    Abhishek and Tulsiani,\n    Shubham and Carreira,\n    Joao and Malik,\n    Jitendra\n},\n    title = {\n    Amodal Completion and Size Constancy in Natural Scenes\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "4344237d65",
        "title": "An Accurate Iris Segmentation Framework Under Relaxed Imaging Constraints Using Total Variation Model",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Zijing Zhao, Kumar Ajay",
        "author": "Zijing Zhao; Kumar Ajay",
        "abstract": "This paper proposes a novel and more accurate iris segmentation framework to automatically segment iris region from the face images acquired with relaxed imaging under visible or near-infrared illumination, which provides strong feasibility for applications in surveillance, forensics and the search for missing children, etc. The proposed framework is built on a novel total-variation based formulation which uses l1 norm regularization to robustly suppress noisy texture pixels for the accurate iris localization. A series of novel and robust post processing operations are introduced to more accurately localize the limbic boundaries. Our experimental results on three publicly available databases, i.e., FRGC, UBIRIS.v2 and CASIA.v4-distance, achieve significant performance improvement in terms of iris segmentation accuracy over the state-of-the-art approaches in the literature. Besides, we have shown that using iris masks generated from the proposed approach helps to improve iris recognition performance as well. Unlike prior work, all the implementations in this paper are made publicly available to further advance research and applications in biometrics at-d-distance.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Zhao_An_Accurate_Iris_ICCV_2015_paper.pdf",
        "aff": "Department of Computing, The Hong Kong Polytechnic University; Department of Computing, The Hong Kong Polytechnic University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1246192,
        "gs_citation": 168,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10926494127365886117&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "connect.polyu.hk;polyu.edu.hk",
        "email": "connect.polyu.hk;polyu.edu.hk",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Zhao_An_Accurate_Iris_ICCV_2015_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "The Hong Kong Polytechnic University",
        "aff_unique_dep": "Department of Computing",
        "aff_unique_url": "https://www.polyu.edu.hk",
        "aff_unique_abbr": "PolyU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Zhao_2015_ICCV,\n    \n    author = {\n    Zhao,\n    Zijing and Ajay,\n    Kumar\n},\n    title = {\n    An Accurate Iris Segmentation Framework Under Relaxed Imaging Constraints Using Total Variation Model\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "7d3bfb2100",
        "title": "An Adaptive Data Representation for Robust Point-Set Registration and Merging",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Dylan Campbell, Lars Petersson",
        "author": "Dylan Campbell; Lars Petersson",
        "abstract": "This paper presents a framework for rigid point-set registration and merging using a robust continuous data representation. Our point-set representation is constructed by training a one-class support vector machine with a Gaussian radial basis function kernel and subsequently approximating the output function with a Gaussian mixture model. We leverage the representation's sparse parametrisation and robustness to noise, outliers and occlusions in an efficient registration algorithm that minimises the L2 distance between our support vector-parametrised Gaussian mixtures. In contrast, existing techniques, such as Iterative Closest Point and Gaussian mixture approaches, manifest a narrower region of convergence and are less robust to occlusions and missing data, as demonstrated in the evaluation on a range of 2D and 3D datasets. Finally, we present a novel algorithm, GMMerge, that parsimoniously and equitably merges aligned mixture models, allowing the framework to be used for reconstruction and mapping.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Campbell_An_Adaptive_Data_ICCV_2015_paper.pdf",
        "aff": "Australian National University; National ICT Australia (NICTA)",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 6801263,
        "gs_citation": 96,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2047380503150326690&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "nicta.com.au;nicta.com.au",
        "email": "nicta.com.au;nicta.com.au",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Campbell_An_Adaptive_Data_ICCV_2015_paper.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Australian National University;National ICT Australia",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.anu.edu.au;https://www.nicta.com.au",
        "aff_unique_abbr": "ANU;NICTA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Australia",
        "bibtex": "@InProceedings{Campbell_2015_ICCV,\n    \n    author = {\n    Campbell,\n    Dylan and Petersson,\n    Lars\n},\n    title = {\n    An Adaptive Data Representation for Robust Point-Set Registration and Merging\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "350fc82218",
        "title": "An Efficient Minimal Solution for Multi-Camera Motion",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Jonathan Ventura, Clemens Arth, Vincent Lepetit",
        "author": "Jonathan Ventura; Clemens Arth; Vincent Lepetit",
        "abstract": "We propose an efficient method for estimating the motion of a multi-camera rig from a minimal set of feature correspondences.  Existing methods for solving the multi-camera relative pose problem require extra correspondences, are slow to compute, and/or produce a multitude of solutions.  Our solution uses a first-order approximation to relative pose in order to simplify the problem and produce an accurate estimate quickly.  The solver is applicable to sequential multi-camera motion estimation and is fast enough for real-time implementation in a random sampling framework.  Our experiments show that our approach is both stable and efficient on challenging test sequences.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Ventura_An_Efficient_Minimal_ICCV_2015_paper.pdf",
        "aff": "Department of Computer Science, University of Colorado Colorado Springs; Institute for Computer Graphics and Vision, Graz University of Technology; Institute for Computer Graphics and Vision, Graz University of Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1621466,
        "gs_citation": 59,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=921885596464257914&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff_domain": "uccs.edu;icg.tugraz.at;icg.tugraz.at",
        "email": "uccs.edu;icg.tugraz.at;icg.tugraz.at",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Ventura_An_Efficient_Minimal_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of Colorado Colorado Springs;Graz University of Technology",
        "aff_unique_dep": "Department of Computer Science;Institute for Computer Graphics and Vision",
        "aff_unique_url": "https://www.uccs.edu;https://www.tugraz.at",
        "aff_unique_abbr": "UCCS;TU Graz",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Colorado Springs;Graz",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "United States;Austria",
        "bibtex": "@InProceedings{Ventura_2015_ICCV,\n    \n    author = {\n    Ventura,\n    Jonathan and Arth,\n    Clemens and Lepetit,\n    Vincent\n},\n    title = {\n    An Efficient Minimal Solution for Multi-Camera Motion\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "638d583b51",
        "title": "An Efficient Statistical Method for Image Noise Level Estimation",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Guangyong Chen, Fengyuan Zhu, Pheng Ann Heng",
        "author": "Guangyong Chen; Fengyuan Zhu; Pheng Ann Heng",
        "abstract": "In this paper, we address the problem of estimating noise level from a single image contaminated by additive zero-mean Gaussian noise. We first provide rigorous analysis on the statistical relationship between the noise variance and the eigenvalues of the covariance matrix of patches within an image, which shows that many state-of-the-art noise estimation methods underestimate the noise level of an image. To this end, we derive a new nonparametric algorithm for efficient noise level estimation based on the observation that patches decomposed from a clean image often lie around a low-dimensional subspace. The performance of our method has been guaranteed both theoretically and empirically. Specifically, our method outperforms existing state-of-the-art algorithms on estimating noise level with the least executing time in our experiments. We further demonstrate that the denoising algorithm BM3D algorithm achieves optimal performance using noise variance estimated by our algorithm.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Chen_An_Efficient_Statistical_ICCV_2015_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3147178,
        "gs_citation": 318,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6733850281693479821&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Chen_An_Efficient_Statistical_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Chen_2015_ICCV,\n    \n    author = {\n    Chen,\n    Guangyong and Zhu,\n    Fengyuan and Heng,\n    Pheng Ann\n},\n    title = {\n    An Efficient Statistical Method for Image Noise Level Estimation\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "b9388af977",
        "title": "An Exploration of Parameter Redundancy in Deep Networks With Circulant Projections",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Yu Cheng, Felix X. Yu, Rogerio S. Feris, Sanjiv Kumar, Alok Choudhary, Shi-Fu Chang",
        "author": "Yu Cheng; Felix X. Yu; Rogerio S. Feris; Sanjiv Kumar; Alok Choudhary; Shi-Fu Chang",
        "abstract": "We explore the redundancy of parameters in deep neural networks by replacing the conventional linear projection in fully-connected layers with the circulant projection. The circulant structure substantially reduces memory footprint and enables the use of the Fast Fourier Transform to speed up the computation. Considering a fully-connected neural network layer with d input nodes, and d output nodes, this method improves the time complexity from O(d^2) to O(dlogd) and space complexity from O(d^2) to O(d). The space savings are particularly important for modern deep convolutional neural network architectures, where fully-connected layers typically contain more than 90% of the network parameters. We further show that the gradient computation and optimization of the circulant projections can be performed very efficiently. Our experiments on three standard datasets show that the proposed approach achieves this significant gain in storage and efficiency with minimal increase in error rate compared to neural networks with unstructured projections.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Cheng_An_Exploration_of_ICCV_2015_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 872724,
        "gs_citation": 407,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=700401025169084360&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 22,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Cheng_An_Exploration_of_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Cheng_2015_ICCV,\n    \n    author = {\n    Cheng,\n    Yu and Yu,\n    Felix X. and Feris,\n    Rogerio S. and Kumar,\n    Sanjiv and Choudhary,\n    Alok and Chang,\n    Shi-Fu\n},\n    title = {\n    An Exploration of Parameter Redundancy in Deep Networks With Circulant Projections\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "230724b808",
        "title": "An MRF-Poselets Model for Detecting Highly Articulated Humans",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Duc Thanh Nguyen, Minh-Khoi Tran, Sai-Kit Yeung",
        "author": "Duc Thanh Nguyen; Minh-Khoi Tran; Sai-Kit Yeung",
        "abstract": "Detecting highly articulated objects such as humans is a challenging problem. This paper proposes a novel part-based model built upon poselets, a notion of parts, and Markov Random Field (MRF) for modelling the human body structure under the variation of human poses and viewpoints. The problem of human detection is then formulated as maximum a posteriori (MAP) estimation in the MRF model. Variational mean field method, a robust statistical inference, is adopted to approximate the MAP estimation. The proposed method was evaluated and compared with existing methods on different test sets including H3D and PASCAL VOC 2007-2009. Experimental results have favourbly shown the robustness of the proposed method in comparison to the state-of-the-art.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Nguyen_An_MRF-Poselets_Model_ICCV_2015_paper.pdf",
        "aff": "Singapore University of Technology and Design; Singapore University of Technology and Design; Singapore University of Technology and Design",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1913370,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14907233215388733717&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "sutd.edu.sg;mymail.sutd.edu.sg;sutd.edu.sg",
        "email": "sutd.edu.sg;mymail.sutd.edu.sg;sutd.edu.sg",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Nguyen_An_MRF-Poselets_Model_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Singapore University of Technology and Design",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.sutd.edu.sg",
        "aff_unique_abbr": "SUTD",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Singapore",
        "bibtex": "@InProceedings{Nguyen_2015_ICCV,\n    \n    author = {\n    Nguyen,\n    Duc Thanh and Tran,\n    Minh-Khoi and Yeung,\n    Sai-Kit\n},\n    title = {\n    An MRF-Poselets Model for Detecting Highly Articulated Humans\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "2011d34a3f",
        "title": "An NMF Perspective on Binary Hashing",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Lopamudra Mukherjee, Sathya N. Ravi, Vamsi K. Ithapu, Tyler Holmes, Vikas Singh",
        "author": "Lopamudra Mukherjee; Sathya N. Ravi; Vamsi K. Ithapu; Tyler Holmes; Vikas Singh",
        "abstract": "The pervasiveness of massive data repositories has led to much interest in efficient methods for indexing, search, and retrieval. For image data, a rapidly developing body of work for these applications shows impressive performance with methods that broadly fall under the umbrella term of Binary Hashing. Given a distance matrix, a binary hashing algorithm solves for a binary code for the given set of examples, whose Hamming distance nicely approximates the original distances. The formulation is non-convex-- so existing solutions adopt spectral relaxations or perform coordinate descent (or quantization) on a surrogate objective that is numerically more tractable. In this paper, we first derive an Augmented Lagrangian approach to optimize the standard binary Hashing objective (i.e.,maintain fidelity with a given distance matrix). With appropriate step sizes, we find that this scheme already yields results that match or substantially outperform state of the art methods on most benchmarks used in the literature. Then, to allow the model to scale to large datasets, we obtain an interesting reformulation of the binary hashing objective as a non negative matrix factorization. Later, this leads to a simple multiplicative updates algorithm -- whose parallelization properties are exploited to obtain a fast GPU based implementation. We give a probabilistic analysis of our initialization scheme and present a range of experiments to show that the method is simple to implement and competes favorably with available methods (both for optimization and generalization).",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Mukherjee_An_NMF_Perspective_ICCV_2015_paper.pdf",
        "aff": "University of Wisconsin-Whitewater; University of Wisconsin-Madison; University of Wisconsin-Madison; University of Wisconsin-Whitewater; University of Wisconsin-Madison",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 746715,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7280815813431666033&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Mukherjee_An_NMF_Perspective_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;1;0;1",
        "aff_unique_norm": "University of Wisconsin-Whitewater;University of Wisconsin-Madison",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uww.edu;https://www.wisc.edu",
        "aff_unique_abbr": "UW-Whitewater;UW-Madison",
        "aff_campus_unique_index": "0;1;1;0;1",
        "aff_campus_unique": "Whitewater;Madison",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Mukherjee_2015_ICCV,\n    \n    author = {\n    Mukherjee,\n    Lopamudra and Ravi,\n    Sathya N. and Ithapu,\n    Vamsi K. and Holmes,\n    Tyler and Singh,\n    Vikas\n},\n    title = {\n    An NMF Perspective on Binary Hashing\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "30e790a4fb",
        "title": "As-Rigid-As-Possible Volumetric Shape-From-Template",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Shaifali Parashar, Daniel Pizarro, Adrien Bartoli, Toby Collins",
        "author": "Shaifali Parashar; Daniel Pizarro; Adrien Bartoli; Toby Collins",
        "abstract": "The objective of Shape-from-Template (SfT) is to infer an object's shape from a single image and a 3D object tem- plate. Existing methods are called thin-shell SfT as they represent the object by its outer surface. This may be an open surface for thin objects such as a piece of paper or a closed surface for thicker objects such as a ball. We pro- pose volumetric SfT, which specifically handles objects of the latter kind. Volumetric SfT uses the object's full volume to express the deformation constraints and reconstructs the object's surface and interior deformation. This is a chal- lenging problem because for opaque objects, only a part of the outer surface is visible in the image. Inspired by mesh- editing techniques, we use an As-Rigid-As-Possible (ARAP) deformation model that softly imposes local rigidity. We formalise ARAP isometric SfT as a constrained variational optimisation problem which we solve using iterative opti- misation. We present strategies to find an initial solution based on thin-shell SfT and volume propagation. Experi- ments with synthetic and real data show that our method has a typical maximum relative error of 5% in reconstruct- ing the deformation of an entire object, including its back and interior for which no visual data is available.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Parashar_As-Rigid-As-Possible_Volumetric_Shape-From-Template_ICCV_2015_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1153252,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9500712346215951324&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Parashar_As-Rigid-As-Possible_Volumetric_Shape-From-Template_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Parashar_2015_ICCV,\n    \n    author = {\n    Parashar,\n    Shaifali and Pizarro,\n    Daniel and Bartoli,\n    Adrien and Collins,\n    Toby\n},\n    title = {\n    As-Rigid-As-Possible Volumetric Shape-From-Template\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "26729b6c2a",
        "title": "Ask Your Neurons: A Neural-Based Approach to Answering Questions About Images",
        "session": "vision and language",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "Mateusz Malinowski, Marcus Rohrbach, Mario Fritz",
        "author": "Mateusz Malinowski; Marcus Rohrbach; Mario Fritz",
        "abstract": "We address a question answering task on real-world images that is set up as a Visual Turing Test. By combining latest advances in image representation and natural language processing, we propose Neural-Image-QA, an end-to-end formulation to this problem for which all parts are trained jointly. In contrast to previous efforts, we are facing a multi-modal problem where the language output (answer) is conditioned on visual and natural language input (image and question). Our approach Neural-Image-QA doubles the performance of the previous best approach on this problem. We provide additional insights into the problem by analyzing how much information is contained only in the language part for which we provide a new human baseline. To study human consensus, which is related to the ambiguities inherent in this challenging task, we propose two novel metrics and collect additional answers which extends the original DAQUAR dataset to DAQUAR-Consensus.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Malinowski_Ask_Your_Neurons_ICCV_2015_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 778029,
        "gs_citation": 816,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6398699668058993420&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Malinowski_Ask_Your_Neurons_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Malinowski_2015_ICCV,\n    \n    author = {\n    Malinowski,\n    Mateusz and Rohrbach,\n    Marcus and Fritz,\n    Mario\n},\n    title = {\n    Ask Your Neurons: A Neural-Based Approach to Answering Questions About Images\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "4cbfad9723",
        "title": "AttentionNet: Aggregating Weak Directions for Accurate Object Detection",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Donggeun Yoo, Sunggyun Park, Joon-Young Lee, Anthony S. Paek, In So Kweon",
        "author": "Donggeun Yoo; Sunggyun Park; Joon-Young Lee; Anthony S. Paek; In So Kweon",
        "abstract": "We present a novel detection method using a deep convolutional neural network (CNN), named AttentionNet. We cast an object detection problem as an iterative classification problem, which is the most suitable form of a CNN. AttentionNet provides quantized weak directions pointing a target object and the ensemble of iterative predictions from AttentionNet converges to an accurate object boundary box. Since AttentionNet is a unified network for object detection, it detects objects without any separated models from the  object proposal to the post bounding-box regression. We evaluate AttentionNet by a human detection task and achieve the state-of-the-art performance of 65% (AP) on PASCAL VOC 2007/2012 with an 8-layered architecture only.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Yoo_AttentionNet_Aggregating_Weak_ICCV_2015_paper.pdf",
        "aff": "KAIST; KAIST; KAIST+Adobe Research; Lunit Inc.; KAIST",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1725265,
        "gs_citation": 236,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7945502788875583930&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": "rcv.kaist.ac.kr;kaist.ac.kr;rcv.kaist.ac.kr;lunit.io;kaist.ac.kr",
        "email": "rcv.kaist.ac.kr;kaist.ac.kr;rcv.kaist.ac.kr;lunit.io;kaist.ac.kr",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Yoo_AttentionNet_Aggregating_Weak_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0+1;2;0",
        "aff_unique_norm": "Korea Advanced Institute of Science and Technology;Adobe;Lunit Inc.",
        "aff_unique_dep": ";Adobe Research;",
        "aff_unique_url": "https://www.kaist.ac.kr;https://research.adobe.com;https://www.lunit.io",
        "aff_unique_abbr": "KAIST;Adobe;Lunit",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+1;0;0",
        "aff_country_unique": "South Korea;United States",
        "bibtex": "@InProceedings{Yoo_2015_ICCV,\n    \n    author = {\n    Yoo,\n    Donggeun and Park,\n    Sunggyun and Lee,\n    Joon-Young and Paek,\n    Anthony S. and Kweon,\n    In So\n},\n    title = {\n    AttentionNet: Aggregating Weak Directions for Accurate Object Detection\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "7b7f8864f1",
        "title": "Attribute-Graph: A Graph Based Approach to Image Ranking",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Nikita Prabhu, R. Venkatesh Babu",
        "author": "Nikita Prabhu; R. Venkatesh Babu",
        "abstract": "We propose a novel image representation, termed Attribute-Graph, to rank images by their semantic similarity to a given query image. An Attribute-Graph is an undirected fully connected graph, incorporating both local and global image characteristics. The graph nodes characterise objects as well as the overall scene context using mid-level semantic attributes, while the edges capture the object topology. We demonstrate the effectiveness of Attribute-Graphs by applying them to the problem of image ranking. We benchmark the performance of our algorithm on the 'rPascal' and 'rImageNet' datasets, which we have created in order to evaluate the ranking performance on complex queries containing multiple objects. Our experimental evaluation shows that modelling images as Attribute-Graphs results in improved ranking performance over existing techniques.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Prabhu_Attribute-Graph_A_Graph_ICCV_2015_paper.pdf",
        "aff": "Video Analytics Lab, SERC, Indian Institute of Science, Bangalore, India; Video Analytics Lab, SERC, Indian Institute of Science, Bangalore, India",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1992468,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15150230976296845172&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "ssl.serc.iisc.in;serc.iisc.in",
        "email": "ssl.serc.iisc.in;serc.iisc.in",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Prabhu_Attribute-Graph_A_Graph_ICCV_2015_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Indian Institute of Science",
        "aff_unique_dep": "Video Analytics Lab, SERC",
        "aff_unique_url": "https://www.iisc.ac.in",
        "aff_unique_abbr": "IISc",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Bangalore",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "India",
        "bibtex": "@InProceedings{Prabhu_2015_ICCV,\n    \n    author = {\n    Prabhu,\n    Nikita and Babu,\n    R. Venkatesh\n},\n    title = {\n    Attribute-Graph: A Graph Based Approach to Image Ranking\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "d87779fd41",
        "title": "Attributed Grammars for Joint Estimation of Human Attributes, Part and Pose",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Seyoung Park, Song-Chun Zhu",
        "author": "Seyoung Park; Song-Chun Zhu",
        "abstract": "In this paper, we are interested in developing compositional models to explicit representing  pose, parts and attributes and tackling the tasks of attribute recognition, pose estimation and part localization jointly. This is different from the recent trend of using CNN-based approaches for training and testing on these tasks separately with a large amount of data. Conventional attribute models typically use a large number of region-based attribute classifiers on parts of pre-trained pose estimator without explicitly detecting the object or its parts, or considering the correlations between attributes. In contrast, our approach jointly represents both the object parts and their semantic attributes within a unified compositional hierarchy. We apply our attributed grammar model to the task of human parsing by simultaneously performing part localization and attribute recognition. We show our modeling helps performance improvements on pose-estimation task and also outperforms on other existing methods on attribute prediction task.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Park_Attributed_Grammars_for_ICCV_2015_paper.pdf",
        "aff": "Center for Vision, Cognition, Learning and Autonomy, Department of Computer Science and Statistics, UCLA; Center for Vision, Cognition, Learning and Autonomy, Department of Computer Science and Statistics, UCLA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2278330,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17316165098864110921&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "cs.ucla.edu;stat.ucla.edu",
        "email": "cs.ucla.edu;stat.ucla.edu",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Park_Attributed_Grammars_for_ICCV_2015_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Los Angeles",
        "aff_unique_dep": "Department of Computer Science and Statistics",
        "aff_unique_url": "https://www.ucla.edu",
        "aff_unique_abbr": "UCLA",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Park_2015_ICCV,\n    \n    author = {\n    Park,\n    Seyoung and Zhu,\n    Song-Chun\n},\n    title = {\n    Attributed Grammars for Joint Estimation of Human Attributes,\n    Part and Pose\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "7c6e8eb68a",
        "title": "Augmenting Strong Supervision Using Web Data for Fine-Grained Categorization",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Zhe Xu, Shaoli Huang, Ya Zhang, Dacheng Tao",
        "author": "Zhe Xu; Shaoli Huang; Ya Zhang; Dacheng Tao",
        "abstract": "We propose a new method for fine-grained object recognition that employs part-level annotations and deep convolutional neural networks (CNNs) in a unified framework. Although both schemes have been widely used to boost recognition performance, due to the difficulty in acquiring detailed part annotations, strongly supervised fine-grained datasets are usually too small to keep pace with the rapid evolution of CNN architectures. In this paper, we solve this problem by exploiting inexhaustible web data. The proposed method improves classification accuracy in two ways: more discriminative CNN feature representations are generated using a training set augmented by collecting a large number of part patches from weakly supervised web images; and more robust object classifiers are learned using a multi-instance learning algorithm jointly on the strong and weak datasets. Despite its simplicity, the proposed method delivers a remarkable performance improvement on the CUB200-2011 dataset compared to baseline part-based R-CNN methods, and achieves the highest accuracy on this dataset even in the absence of test image annotations.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Xu_Augmenting_Strong_Supervision_ICCV_2015_paper.pdf",
        "aff": "Cooperative Medianet Innovation Center and the Shanghai Key Laboratory of Multimedia Processing and Transmissions, Shanghai Jiao Tong University, Shanghai, 200240, China+Centre for Quantum Computation and Intelligent Systems, Faculty of Engineering and Information Technology, University of Technology, Sydney, Ultimo, NSW 2007, Australia; Centre for Quantum Computation and Intelligent Systems, Faculty of Engineering and Information Technology, University of Technology, Sydney, Ultimo, NSW 2007, Australia; Cooperative Medianet Innovation Center and the Shanghai Key Laboratory of Multimedia Processing and Transmissions, Shanghai Jiao Tong University, Shanghai, 200240, China; Centre for Quantum Computation and Intelligent Systems, Faculty of Engineering and Information Technology, University of Technology, Sydney, Ultimo, NSW 2007, Australia",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1624630,
        "gs_citation": 101,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7235554614207980637&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 6,
        "aff_domain": "sjtu.edu.cn;student.uts.edu.au;sjtu.edu.cn;uts.edu.au",
        "email": "sjtu.edu.cn;student.uts.edu.au;sjtu.edu.cn;uts.edu.au",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Xu_Augmenting_Strong_Supervision_ICCV_2015_paper.html",
        "aff_unique_index": "0+1;1;0;1",
        "aff_unique_norm": "Shanghai Jiao Tong University;University of Technology, Sydney",
        "aff_unique_dep": "Cooperative Medianet Innovation Center and Shanghai Key Laboratory of Multimedia Processing and Transmissions;Faculty of Engineering and Information Technology",
        "aff_unique_url": "https://www.sjtu.edu.cn;https://www.uts.edu.au",
        "aff_unique_abbr": "SJTU;UTS",
        "aff_campus_unique_index": "0+1;1;0;1",
        "aff_campus_unique": "Shanghai;Ultimo",
        "aff_country_unique_index": "0+1;1;0;1",
        "aff_country_unique": "China;Australia",
        "bibtex": "@InProceedings{Xu_2015_ICCV,\n    \n    author = {\n    Xu,\n    Zhe and Huang,\n    Shaoli and Zhang,\n    Ya and Tao,\n    Dacheng\n},\n    title = {\n    Augmenting Strong Supervision Using Web Data for Fine-Grained Categorization\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "218492b613",
        "title": "Automated Facial Trait Judgment and Election Outcome Prediction: Social Dimensions of Face",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Jungseock Joo, Francis F. Steen, Song-Chun Zhu",
        "author": "Jungseock Joo; Francis F. Steen; Song-Chun Zhu",
        "abstract": "The human face is a primary medium of human communication and a prominent source of information used to infer various attributes. In this paper, we study a fully automated system that can infer the perceived traits of a person from his face -- social dimensions, such as \"intelligence,\" \"honesty,\" and \"competence\" -- and how those traits can be used to predict the outcomes of real-world social events that involve long-term commitments, such as political elections, job hires, and marriage engagements. To this end, we propose a hierarchical model for enduring traits inferred from faces, incorporating high-level perceptions and intermediate-level attributes.  We show that our trained model can successfully classify the outcomes of two important political events, only using the photographs of politicians' faces. Firstly, it classifies the winners of a series of recent U.S. elections with the accuracy of 67.9% (Governors) and 65.5% (Senators). We also reveal that the different political offices require different types of preferred traits. Secondly, our model can categorize the political party affiliations of politicians, i.e., Democrats vs. Republicans, with the accuracy of 62.6% (male) and 60.1% (female). To the best of our knowledge, our paper is the first to use automated visual trait analysis to predict the outcomes of real-world social events. This approach is more scalable and objective than the prior behavioral studies, and opens for a range of new applications.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Joo_Automated_Facial_Trait_ICCV_2015_paper.pdf",
        "aff": "Departments of Computer Science and Statistics, UCLA + Facebook; Department of Communication Studies, UCLA; Departments of Computer Science and Statistics, UCLA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1630182,
        "gs_citation": 92,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4963447702129373656&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 9,
        "aff_domain": "gmail.com; ; ",
        "email": "gmail.com; ; ",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Joo_Automated_Facial_Trait_ICCV_2015_paper.html",
        "aff_unique_index": "0+1;0;0",
        "aff_unique_norm": "University of California, Los Angeles;Facebook, Inc.",
        "aff_unique_dep": "Departments of Computer Science and Statistics;",
        "aff_unique_url": "https://www.ucla.edu;https://www.facebook.com",
        "aff_unique_abbr": "UCLA;FB",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Joo_2015_ICCV,\n    \n    author = {\n    Joo,\n    Jungseock and Steen,\n    Francis F. and Zhu,\n    Song-Chun\n},\n    title = {\n    Automated Facial Trait Judgment and Election Outcome Prediction: Social Dimensions of Face\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "b335d14e04",
        "title": "Automatic Concept Discovery From Parallel Text and Visual Corpora",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Chen Sun, Chuang Gan, Ram Nevatia",
        "author": "Chen Sun; Chuang Gan; Ram Nevatia",
        "abstract": "Humans connect language and vision to perceive the world. How to build a similar connection for computers? One possible way is via visual concepts, which are text terms that relate to visually discriminative entities. We propose an automatic visual concept discovery algorithm using parallel text and visual corpora; it filters text terms based on the visual discriminative power of the associated images, and groups them into concepts using visual and semantic similarities. We illustrate the applications of the discovered concepts using bidirectional image and sentence retrieval task and image tagging task, and show that the discovered concepts not only outperform several large sets of manually selected concepts significantly, but also achieves the state-of-the-art performance in the retrieval task.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Sun_Automatic_Concept_Discovery_ICCV_2015_paper.pdf",
        "aff": "Univ. of Southern California; Tsinghua University + Univ. of Southern California; Univ. of Southern California",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1057387,
        "gs_citation": 144,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11821025609799551977&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "usc.edu;gmail.com;usc.edu",
        "email": "usc.edu;gmail.com;usc.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Sun_Automatic_Concept_Discovery_ICCV_2015_paper.html",
        "aff_unique_index": "0;1+0;0",
        "aff_unique_norm": "University of Southern California;Tsinghua University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.usc.edu;https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "USC;THU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0;1+0;0",
        "aff_country_unique": "United States;China",
        "bibtex": "@InProceedings{Sun_2015_ICCV,\n    \n    author = {\n    Sun,\n    Chen and Gan,\n    Chuang and Nevatia,\n    Ram\n},\n    title = {\n    Automatic Concept Discovery From Parallel Text and Visual Corpora\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "4e1b30f831",
        "title": "Automatic Thumbnail Generation Based on Visual Representativeness and Foreground Recognizability",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Jingwei Huang, Huarong Chen, Bin Wang, Stephen Lin",
        "author": "Jingwei Huang; Huarong Chen; Bin Wang; Stephen Lin",
        "abstract": "We present an automatic thumbnail generation technique based on two essential considerations: how well they visually represent the original photograph, and how well the foreground can be recognized after the cropping and downsizing steps of thumbnailing. These factors, while important for the image indexing purpose of thumbnails, have largely been ignored in previous methods, which instead are designed to highlight salient content while disregarding the effects of downsizing. We propose a set of image features for modeling these two considerations of thumbnails, and learn how to balance their relative effects on thumbnail generation through training on image pairs composed of photographs and their corresponding thumbnails created by an expert photographer. Experiments show the effectiveness of this approach on a variety of images, as well as its advantages over related techniques.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Huang_Automatic_Thumbnail_Generation_ICCV_2015_paper.pdf",
        "aff": "School of Software, Tsinghua University + Tsinghua National Laboratory for Information Science and Technology; School of Software, Tsinghua University + Tsinghua National Laboratory for Information Science and Technology; School of Software, Tsinghua University + Tsinghua National Laboratory for Information Science and Technology; Microsoft Research",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1122557,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9445605157825546944&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "tsinghua.edu.cn;tsinghua.edu.cn;tsinghua.edu.cn;microsoft.com",
        "email": "tsinghua.edu.cn;tsinghua.edu.cn;tsinghua.edu.cn;microsoft.com",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Huang_Automatic_Thumbnail_Generation_ICCV_2015_paper.html",
        "aff_unique_index": "0+0;0+0;0+0;1",
        "aff_unique_norm": "Tsinghua University;Microsoft Corporation",
        "aff_unique_dep": "School of Software;Microsoft Research",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "THU;MSR",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;1",
        "aff_country_unique": "China;United States",
        "bibtex": "@InProceedings{Huang_2015_ICCV,\n    \n    author = {\n    Huang,\n    Jingwei and Chen,\n    Huarong and Wang,\n    Bin and Lin,\n    Stephen\n},\n    title = {\n    Automatic Thumbnail Generation Based on Visual Representativeness and Foreground Recognizability\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "4512d10cb6",
        "title": "Bayesian Model Adaptation for Crowd Counts",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Bo Liu, Nuno Vasconcelos",
        "author": "Bo Liu; Nuno Vasconcelos",
        "abstract": "The problem of transfer learning is considered in the domain of crowd counting. A solution based on Bayesian model adaptation of Gaussian processes is proposed. This is shown to produce intuitive model updates, which are tractable, and lead to an adapted model (predictive distribution) that accounts for all information in both training and adaptation data. The new adaptation procedure achieves significant gains over previous approaches, based on multi-task learning, while requiring much less computation to deploy. This makes it particularly suited for the problem of expanding the capacity of crowd counting camera networks. A large video dataset for the evaluation of adaptation approaches to crowd counting is also introduced. This contains a number of adaptation tasks, involving information transfer across video collected by 1) a single camera under different scene conditions (different times of the day) and 2) video collected from different cameras. Evaluation of the proposed model adaptation procedure in this dataset shows good performance in realistic operating conditions.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Liu_Bayesian_Model_Adaptation_ICCV_2015_paper.pdf",
        "aff": ";",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 78,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4764736508422649599&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Liu_Bayesian_Model_Adaptation_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Liu_2015_ICCV,\n    \n    author = {\n    Liu,\n    Bo and Vasconcelos,\n    Nuno\n},\n    title = {\n    Bayesian Model Adaptation for Crowd Counts\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "6a5abd4c8a",
        "title": "Bayesian Non-Parametric Inference for Manifold Based MoCap Representation",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Fabrizio Natola, Valsamis Ntouskos, Marta Sanzari, Fiora Pirri",
        "author": "Fabrizio Natola; Valsamis Ntouskos; Marta Sanzari; Fiora Pirri",
        "abstract": "We propose a novel approach to human action recognition, with motion capture data (MoCap), based on  grouping sub-body parts. By representing configurations of actions as manifolds, joint positions are mapped on a subspace via principal geodesic analysis. The reduced space is still highly informative and allows for classification based on a non-parametric Bayesian approach,  generating behaviors for each sub-body part. Having partitioned the set of joints, poses relative to a sub-body part are exchangeable, given a specified prior and can elicit, in principle, infinite behaviors. The generation of these behaviors is specified by a Dirichlet process mixture. We show with several experiments that the recognition gives very promising results, outperforming methods requiring temporal alignment.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Natola_Bayesian_Non-Parametric_Inference_ICCV_2015_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17806393521201338963&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Natola_Bayesian_Non-Parametric_Inference_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Natola_2015_ICCV,\n    \n    author = {\n    Natola,\n    Fabrizio and Ntouskos,\n    Valsamis and Sanzari,\n    Marta and Pirri,\n    Fiora\n},\n    title = {\n    Bayesian Non-Parametric Inference for Manifold Based MoCap Representation\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "e9f0adb78f",
        "title": "Beyond Covariance: Feature Representation With Nonlinear Kernel Matrices",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Lei Wang, Jianjia Zhang, Luping Zhou, Chang Tang, Wanqing Li",
        "author": "Lei Wang; Jianjia Zhang; Luping Zhou; Chang Tang; Wanqing Li",
        "abstract": "Covariance matrix has recently received increasing attention in computer vision by leveraging Riemannian geometry of symmetric positive-definite (SPD) matrices. Originally proposed as a region descriptor, it has now been used as a generic representation in various recognition tasks. However, covariance matrix has shortcomings such as being prone to be singular, limited capability in modeling complicated feature relationship, and having a fixed form of representation. This paper argues that more appropriate SPD-matrix-based representations shall be explored to achieve better recognition. It proposes an open framework to use the kernel matrix over feature dimensions as a generic representation and discusses its properties and advantages. The proposed framework significantly elevates covariance representation to the unlimited opportunities provided by this new representation. Experimental study shows that this representation consistently outperforms its covariance counterpart on various visual recognition tasks. In particular, it achieves significant improvement on skeleton-based human action recognition, demonstrating the state-of-the-art performance over both the covariance and the existing non-covariance representations.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Wang_Beyond_Covariance_Feature_ICCV_2015_paper.pdf",
        "aff": "School of Computing and Information Technology, University of Wollongong, Australia; School of Computing and Information Technology, University of Wollongong, Australia; School of Computing and Information Technology, University of Wollongong, Australia; School of Computing and Information Technology, University of Wollongong, Australia+School of Electronic Information Engineering, Tianjin University, China; School of Computing and Information Technology, University of Wollongong, Australia",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 463739,
        "gs_citation": 114,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7832806316280610569&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Wang_Beyond_Covariance_Feature_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0+1;0",
        "aff_unique_norm": "University of Wollongong;Tianjin University",
        "aff_unique_dep": "School of Computing and Information Technology;School of Electronic Information Engineering",
        "aff_unique_url": "https://www.uow.edu.au;http://www.tju.edu.cn",
        "aff_unique_abbr": "UOW;Tianjin University",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0+1;0",
        "aff_country_unique": "Australia;China",
        "bibtex": "@InProceedings{Wang_2015_ICCV,\n    \n    author = {\n    Wang,\n    Lei and Zhang,\n    Jianjia and Zhou,\n    Luping and Tang,\n    Chang and Li,\n    Wanqing\n},\n    title = {\n    Beyond Covariance: Feature Representation With Nonlinear Kernel Matrices\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "3b69100970",
        "title": "Beyond Gauss: Image-Set Matching on the Riemannian Manifold of PDFs",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Mehrtash Harandi, Mathieu Salzmann, Mahsa Baktashmotlagh",
        "author": "Mehrtash Harandi; Mathieu Salzmann; Mahsa Baktashmotlagh",
        "abstract": "State-of-the-art image-set matching techniques typically implicitly model each image-set with a Gaussian distribution. Here, we propose to go beyond these representations and model image-sets as probability distribution functions (PDFs) using kernel density estimators. To compare and match image-sets, we exploit Csiszar f-divergences, which bear strong connections to the geodesic distance defined on the space of PDFs, i.e., the statistical manifold. Furthermore, we introduce valid positive definite kernels on the statistical manifolds, which let us make use of more powerful classification schemes to match image-sets.  Finally, we introduce a supervised dimensionality reduction technique that learns a latent space where f-divergences reflect the class labels of the data. Our experiments on diverse problems, such as video-based face recognition and dynamic texture classification, evidence the benefits of our approach over the state-of-the-art image-set matching methods.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Harandi_Beyond_Gauss_Image-Set_ICCV_2015_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 568722,
        "gs_citation": 63,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10640623244180261200&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Harandi_Beyond_Gauss_Image-Set_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Harandi_2015_ICCV,\n    \n    author = {\n    Harandi,\n    Mehrtash and Salzmann,\n    Mathieu and Baktashmotlagh,\n    Mahsa\n},\n    title = {\n    Beyond Gauss: Image-Set Matching on the Riemannian Manifold of PDFs\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "2e2d945d0f",
        "title": "Beyond Tree Structure Models: A New Occlusion Aware Graphical Model for Human Pose Estimation",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Lianrui Fu, Junge Zhang, Kaiqi Huang",
        "author": "Lianrui Fu; Junge Zhang; Kaiqi Huang",
        "abstract": "Occlusion is a main challenge for human pose estimation, which is largely ignored in popular tree structure models. The tree structure model is simple and convenient for exact inference, but short in modeling the occlusion coherence especially in the case of self-occlusion. We propose an occlusion aware graphical model which is able to model both self-occlusion and occlusion by the other objects simultaneously. The proposed model structure can encode the interactions between human body parts and objects, and hence enable it to learn occlusion coherence from data discriminatively. We evaluate our model on several public benchmarks for human pose estimation including challenging subsets featuring significant occlusion. The experimental results show that our method obtains comparable accuracy with the state-of-the-arts, and achieves promising performance in 2D human pose estimation with occlusion.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Fu_Beyond_Tree_Structure_ICCV_2015_paper.pdf",
        "aff": "Center for Research on Intelligent Perception and Computing+National Laboratory of Pattern Recognition+Institute of Automation, Chinese Academy of Sciences; Center for Research on Intelligent Perception and Computing+National Laboratory of Pattern Recognition+Institute of Automation, Chinese Academy of Sciences; Center for Research on Intelligent Perception and Computing+National Laboratory of Pattern Recognition+CAS Center for Excellence in Brain Science and Intelligence Technology+Institute of Automation, Chinese Academy of Sciences",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1563497,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10954130585282259649&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "email": "nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Fu_Beyond_Tree_Structure_ICCV_2015_paper.html",
        "aff_unique_index": "0+1+2;0+1+2;0+1+2+2",
        "aff_unique_norm": "Center for Research on Intelligent Perception and Computing;National Laboratory of Pattern Recognition;Chinese Academy of Sciences",
        "aff_unique_dep": ";;Institute of Automation",
        "aff_unique_url": ";http://www.nlpr.ia.ac.cn/en.html;http://www.ia.cas.cn",
        "aff_unique_abbr": ";NLPR;CAS",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1+1;1+1;1+1+1",
        "aff_country_unique": ";China",
        "bibtex": "@InProceedings{Fu_2015_ICCV,\n    \n    author = {\n    Fu,\n    Lianrui and Zhang,\n    Junge and Huang,\n    Kaiqi\n},\n    title = {\n    Beyond Tree Structure Models: A New Occlusion Aware Graphical Model for Human Pose Estimation\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "f6591994ed",
        "title": "Beyond White: Ground Truth Colors for Color Constancy Correction",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Dongliang Cheng, Brian Price, Scott Cohen, Michael S. Brown",
        "author": "Dongliang Cheng; Brian Price; Scott Cohen; Michael S. Brown",
        "abstract": "A limitation in color constancy research is the inability to establish ground truth colors for evaluating corrected images. Many existing datasets contain images of scenes with a color chart included; however, only the chart's neutral colors (grayscale patches) are used to provide the ground truth for illumination estimation and correction.  This is because the corrected neutral colors are known to lie along the achromatic line in the camera's color space (i.e. R=G=B) ; the correct RGB values of the other color patches are not known.  As a result, most methods estimate a 3*3 diagonal matrix that ensures only the neutral colors are correct.  In this paper, we describe how to overcome this limitation.   Specifically, we show that under certain illuminations, a diagonal 3*3 matrix is capable of correcting not only neutral colors, but all the colors in a scene.   This finding allows us to find the ground truth RGB values for the color chart in the camera's color space. We show how to use this information to correct all the images in existing datasets to have correct colors.  Working from these new color corrected datasets, we describe how to modify existing color constancy algorithms to perform better image correction.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Cheng_Beyond_White_Ground_ICCV_2015_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 64,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13350298019007197298&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Cheng_Beyond_White_Ground_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Cheng_2015_ICCV,\n    \n    author = {\n    Cheng,\n    Dongliang and Price,\n    Brian and Cohen,\n    Scott and Brown,\n    Michael S.\n},\n    title = {\n    Beyond White: Ground Truth Colors for Color Constancy Correction\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "ea6cb526bb",
        "title": "Bi-Shifting Auto-Encoder for Unsupervised Domain Adaptation",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Meina Kan, Shiguang Shan, Xilin Chen",
        "author": "Meina Kan; Shiguang Shan; Xilin Chen",
        "abstract": "In many real-world applications, the domain of model learning (referred as source domain) is usually inconsistent with or even different from the domain of testing (referred as target domain), which makes the learnt model degenerate in target domain, i.e., the test domain. To alleviate the discrepancy between source and target domains, we propose a domain adaptation method, named as Bi-shifting Auto-Encoder network (BAE). The proposed BAE attempts to shift source domain samples to target domain, and also shift the target domain samples to source domain. The non-linear transformation of BAE ensures the feasibility of shifting between domains, and the distribution consistency between the shifted domain and the desirable domain is constrained by sparse reconstruction between them. As a result, the shifted source domain is supervised and follows similar distribution as target domain. Therefore, any supervised method can be applied on the shifted source domain to train a classifier for classification in target domain. The proposed method is evaluated on three domain adaptation scenarios of face recognition, i.e., domain adaptation across view angle, ethnicity, and imaging sensor, and the promising results demonstrate that our proposed BAE can shift samples between domains and thus effectively deal with the domain discrepancy.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Kan_Bi-Shifting_Auto-Encoder_for_ICCV_2015_paper.pdf",
        "aff": "Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China; Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China + CAS Center for Excellence in Brain Science and Intelligence Technology; Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1265886,
        "gs_citation": 75,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10013963346010907169&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "ict.ac.cn;ict.ac.cn;ict.ac.cn",
        "email": "ict.ac.cn;ict.ac.cn;ict.ac.cn",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Kan_Bi-Shifting_Auto-Encoder_for_ICCV_2015_paper.html",
        "aff_unique_index": "0;0+0;0",
        "aff_unique_norm": "Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Computing Technology",
        "aff_unique_url": "http://www.cas.ac.cn",
        "aff_unique_abbr": "CAS",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0;0+0;0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Kan_2015_ICCV,\n    \n    author = {\n    Kan,\n    Meina and Shan,\n    Shiguang and Chen,\n    Xilin\n},\n    title = {\n    Bi-Shifting Auto-Encoder for Unsupervised Domain Adaptation\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "e1c18e3b26",
        "title": "Bilinear CNN Models for Fine-Grained Visual Recognition",
        "session": "learning representations and attributes",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "Tsung-Yu Lin, Aruni RoyChowdhury, Subhransu Maji",
        "author": "Tsung-Yu Lin; Aruni RoyChowdhury; Subhransu Maji",
        "abstract": "We propose bilinear models, a recognition architecture that consists of two feature extractors whose outputs are multiplied using outer product at each location of the image and pooled to obtain an image descriptor. This architecture can model local pairwise feature interactions in a translationally invariant manner which is particularly useful for fine-grained categorization. It also generalizes various orderless texture descriptors such as the Fisher vector, VLAD and O2P. We present experiments with bilinear models where the feature extractors are based on convolutional neural networks. The bilinear form simplifies gradient computation and allows end-to-end training of both networks using image labels only. Using networks initialized from the ImageNet dataset followed by domain specific fine-tuning we obtain 84.1% accuracy of the CUB-200-2011 dataset requiring only category labels at training time. We present experiments and visualizations that analyze the effects of fine-tuning and the choice two networks on the speed and accuracy of the models. Results show that the architecture compares favorably to the existing state of the art on a number of fine-grained datasets while being substantially simpler and easier to train. Moreover, our most accurate model is fairly efficient running at 8 frames/sec on a NVIDIA Tesla K40 GPU. The source code for the complete system will be made available at http://vis-www.cs.umass.edu/bcnn",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Lin_Bilinear_CNN_Models_ICCV_2015_paper.pdf",
        "aff": "University of Massachusetts, Amherst; University of Massachusetts, Amherst; University of Massachusetts, Amherst",
        "project": "http://vis-www.cs.umass.edu/bcnn",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 954536,
        "gs_citation": 2954,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13864042896681539790&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff_domain": "cs.umass.edu;cs.umass.edu;cs.umass.edu",
        "email": "cs.umass.edu;cs.umass.edu;cs.umass.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Lin_Bilinear_CNN_Models_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Massachusetts Amherst",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.umass.edu",
        "aff_unique_abbr": "UMass Amherst",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Amherst",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Lin_2015_ICCV,\n    \n    author = {\n    Lin,\n    Tsung-Yu and RoyChowdhury,\n    Aruni and Maji,\n    Subhransu\n},\n    title = {\n    Bilinear CNN Models for Fine-Grained Visual Recognition\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "c2aa7bc4a5",
        "title": "Blur-Aware Disparity Estimation From Defocus Stereo Images",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Ching-Hui Chen, Hui Zhou, Timo Ahonen",
        "author": "Ching-Hui Chen; Hui Zhou; Timo Ahonen",
        "abstract": "Defocus blur usually causes performance degradation in establishing the visual correspondence between stereo images. We propose a blur-aware disparity estimation method that is robust to the mismatch of focus in stereo images. The relative blur resulting from the mismatch of focus between stereo images is approximated as the difference of the square diameters of the blur kernels. Based on the defocus and stereo model, we propose the relative blur versus disparity (RBD) model that characterizes the relative blur as a second-order polynomial function of disparity. Our method alternates between RBD model update and disparity update in each iteration. The RBD model in return refines the disparity estimation by updating the matching cost and aggregation weight to compensate the mismatch of focus. Experiments using both synthesized and real datasets demonstrate the effectiveness of our proposed algorithm.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Chen_Blur-Aware_Disparity_Estimation_ICCV_2015_paper.pdf",
        "aff": "Department of Electrical and Computer Engineering, University of Maryland, College Park, MD, USA; Nokia Technologies, Sunnyvale, CA, USA; Nokia Technologies, Sunnyvale, CA, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2205387,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10945224174414696804&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "umiacs.umd.edu;nokia.com;nokia.com",
        "email": "umiacs.umd.edu;nokia.com;nokia.com",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Chen_Blur-Aware_Disparity_Estimation_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of Maryland, College Park;Nokia Technologies",
        "aff_unique_dep": "Department of Electrical and Computer Engineering;",
        "aff_unique_url": "https://www.umd.edu;https://www.nokia.com",
        "aff_unique_abbr": "UMD;Nokia",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "College Park;Sunnyvale",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Chen_2015_ICCV,\n    \n    author = {\n    Chen,\n    Ching-Hui and Zhou,\n    Hui and Ahonen,\n    Timo\n},\n    title = {\n    Blur-Aware Disparity Estimation From Defocus Stereo Images\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "4dd6706252",
        "title": "BodyPrint: Pose Invariant 3D Shape Matching of Human Bodies",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Jiangping Wang, Kai Ma, Vivek Kumar Singh, Thomas Huang, Terrence Chen",
        "author": "Jiangping Wang; Kai Ma; Vivek Kumar Singh; Thomas Huang; Terrence Chen",
        "abstract": "3D human body shape matching has large potential on many real world applications, especially with the recent advances in the 3D range sensing technology. We address this problem by proposing a novel holistic human body shape descriptor called BodyPrint. To compute the bodyprint for a given body scan, we fit a deformable human body mesh and project the mesh parameters to a low-dimensional subspace which improves discriminability across different persons. Experiments are carried out on three real-world human body datasets to demonstrate that BodyPrint is robust to pose variation as well as missing information and sensor noise. It improves the matching accuracy significantly compared to conventional 3D shape matching techniques using local features. To facilitate practical applications where the shape database may grow over time, we also extend our learning framework to handle online updates.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Wang_BodyPrint_Pose_Invariant_ICCV_2015_paper.pdf",
        "aff": "Beckman Institute, University of Illinois at Urbana-Champaign, USA+Medical Imaging Technologies, Siemens Healthcare, Princeton, NJ, USA; Medical Imaging Technologies, Siemens Healthcare, Princeton, NJ, USA; Medical Imaging Technologies, Siemens Healthcare, Princeton, NJ, USA; Beckman Institute, University of Illinois at Urbana-Champaign, USA; Medical Imaging Technologies, Siemens Healthcare, Princeton, NJ, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 881861,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4445717959659193836&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "ifp.uiuc.edu;siemens.com;siemens.com;ifp.uiuc.edu;siemens.com",
        "email": "ifp.uiuc.edu;siemens.com;siemens.com;ifp.uiuc.edu;siemens.com",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Wang_BodyPrint_Pose_Invariant_ICCV_2015_paper.html",
        "aff_unique_index": "0+1;1;1;0;1",
        "aff_unique_norm": "University of Illinois at Urbana-Champaign;Siemens Healthcare",
        "aff_unique_dep": "Beckman Institute;Medical Imaging Technologies",
        "aff_unique_url": "https://www.illinois.edu;https://www.siemens-healthineers.com",
        "aff_unique_abbr": "UIUC;Siemens",
        "aff_campus_unique_index": "0+1;1;1;0;1",
        "aff_campus_unique": "Urbana-Champaign;Princeton",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Wang_2015_ICCV,\n    \n    author = {\n    Wang,\n    Jiangping and Ma,\n    Kai and Singh,\n    Vivek Kumar and Huang,\n    Thomas and Chen,\n    Terrence\n},\n    title = {\n    BodyPrint: Pose Invariant 3D Shape Matching of Human Bodies\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "ef583113a1",
        "title": "Boosting Object Proposals: From Pascal to COCO",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Jordi Pont-Tuset, Luc Van Gool",
        "author": "Jordi Pont-Tuset; Luc Van Gool",
        "abstract": "Computer vision in general, and object proposals in particular, are nowadays strongly influenced by the databases on which researchers evaluate the performance of their algorithms. This paper studies the transition from the Pascal Visual Object Challenge dataset, which has been the  benchmark of reference for the last years, to the updated, bigger, and more challenging Microsoft Common Objects in Context. We first review and deeply analyze the new challenges, and opportunities, that this database presents. We then survey the current state of the art in object proposals and evaluate it focusing on how it generalizes to the new dataset. In sight of these results, we propose various lines of research to take advantage of the new benchmark and improve the techniques. We explore one of these lines, which leads to an improvement over the state of the art of +5.2%.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Pont-Tuset_Boosting_Object_Proposals_ICCV_2015_paper.pdf",
        "aff": "Computer Vision Lab, ETH Zürich, Switzerland; Computer Vision Lab, ETH Zürich, Switzerland",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2065086,
        "gs_citation": 65,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9037455007070554951&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 12,
        "aff_domain": "vision.ee.ethz.ch;vision.ee.ethz.ch",
        "email": "vision.ee.ethz.ch;vision.ee.ethz.ch",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Pont-Tuset_Boosting_Object_Proposals_ICCV_2015_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "ETH Zürich",
        "aff_unique_dep": "Computer Vision Lab",
        "aff_unique_url": "https://www.ethz.ch",
        "aff_unique_abbr": "ETHZ",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Zürich",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Switzerland",
        "bibtex": "@InProceedings{Pont-Tuset_2015_ICCV,\n    \n    author = {\n    Pont-Tuset,\n    Jordi and Van Gool,\n    Luc\n},\n    title = {\n    Boosting Object Proposals: From Pascal to COCO\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "13eaac0fdf",
        "title": "Box Aggregation for Proposal Decimation: Last Mile of Object Detection",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Shu Liu, Cewu Lu, Jiaya Jia",
        "author": "Shu Liu; Cewu Lu; Jiaya Jia",
        "abstract": "Regions-with-convolutional-neural-network (RCNN) is now a commonly employed object detection pipeline. Its main steps, i.e., proposal generation and convolutional neural network (CNN) feature extraction, have been intensively investigated. We focus on the last step of the system to aggregate thousands of scored box proposals into final object prediction, which we call proposal decimation. We show this step can be enhanced with a very simple box aggregation function by considering statistical properties of proposals with respect to ground truth objects. Our method is with extremely light-weight computation, while it yields an improvement of 3.7% in mAP on PASCAL VOC 2007 test. We explain why it works using some statistics in this paper.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Liu_Box_Aggregation_for_ICCV_2015_paper.pdf",
        "aff": "The Chinese University of Hong Kong; Stanford University + Shanghai Jiao Tong University; The Chinese University of Hong Kong",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1514930,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17122745394026216259&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "cse.cuhk.edu.hk;cs.sjtu.edu.cn;cse.cuhk.edu.hk",
        "email": "cse.cuhk.edu.hk;cs.sjtu.edu.cn;cse.cuhk.edu.hk",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Liu_Box_Aggregation_for_ICCV_2015_paper.html",
        "aff_unique_index": "0;1+2;0",
        "aff_unique_norm": "The Chinese University of Hong Kong;Stanford University;Shanghai Jiao Tong University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.cuhk.edu.hk;https://www.stanford.edu;https://www.sjtu.edu.cn",
        "aff_unique_abbr": "CUHK;Stanford;SJTU",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Hong Kong SAR;Stanford;",
        "aff_country_unique_index": "0;1+0;0",
        "aff_country_unique": "China;United States",
        "bibtex": "@InProceedings{Liu_2015_ICCV,\n    \n    author = {\n    Liu,\n    Shu and Lu,\n    Cewu and Jia,\n    Jiaya\n},\n    title = {\n    Box Aggregation for Proposal Decimation: Last Mile of Object Detection\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "9a0ea00c5e",
        "title": "BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Jifeng Dai, Kaiming He, Jian Sun",
        "author": "Jifeng Dai; Kaiming He; Jian Sun",
        "abstract": "Recent leading approaches to semantic segmentation rely on deep convolutional networks trained with human-annotated, pixel-level segmentation masks. Such pixel-accurate supervision demands expensive labeling effort and limits the performance of deep networks that usually benefit from more training data. In this paper, we propose a method that achieves competitive accuracy but only requires easily obtained bounding box annotations. The basic idea is to iterate between automatically generating region proposals and training convolutional networks. These two steps gradually recover segmentation masks for improving the networks, and vise versa. Our method, called \"BoxSup\", produces competitive results (e.g., 62.0% mAP for validation) supervised by boxes only, on par with strong baselines (e.g., 63.8% mAP) fully supervised by masks under the same setting. By leveraging a large amount of bounding boxes, BoxSup further yields state-of-the-art results on PASCAL VOC 2012 and PASCAL-CONTEXT.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Dai_BoxSup_Exploiting_Bounding_ICCV_2015_paper.pdf",
        "aff": "Microsoft Research; Microsoft Research; Microsoft Research",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3353121,
        "gs_citation": 1368,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10583411756105923851&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "microsoft.com;microsoft.com;microsoft.com",
        "email": "microsoft.com;microsoft.com;microsoft.com",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Dai_BoxSup_Exploiting_Bounding_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Microsoft Corporation",
        "aff_unique_dep": "Microsoft Research",
        "aff_unique_url": "https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Dai_2015_ICCV,\n    \n    author = {\n    Dai,\n    Jifeng and He,\n    Kaiming and Sun,\n    Jian\n},\n    title = {\n    BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "a0ff115679",
        "title": "BubbLeNet: Foveated Imaging for Visual Discovery",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Kevin Matzen, Noah Snavely",
        "author": "Kevin Matzen; Noah Snavely",
        "abstract": "We propose a new method for turning an Internet-scale corpus of categorized images into a small set of human-interpretable discriminative visual elements using powerful tools based on deep learning. A key challenge with deep learning methods is generating human-interpretable models. To address this, we propose a new technique that uses bubble images -- images where most of the content has been obscured -- to identify spatially localized, discriminative content in each image. By modifying the model training procedure to use both the source imagery and these bubble images, we can arrive at final models which retain much of the original classification performance, but are much more amenable to identifying interpretable visual elements. We apply our algorithm to a wide variety of datasets, including two new Internet-scale datasets of people and places, and show applications to visual mining and discovery. Our method is simple, scalable, and produces visual elements that are highly representative compared to prior work.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Matzen_BubbLeNet_Foveated_Imaging_ICCV_2015_paper.pdf",
        "aff": ";",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3313421,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9044951784196306112&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Matzen_BubbLeNet_Foveated_Imaging_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Matzen_2015_ICCV,\n    \n    author = {\n    Matzen,\n    Kevin and Snavely,\n    Noah\n},\n    title = {\n    BubbLeNet: Foveated Imaging for Visual Discovery\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "5820963c1c",
        "title": "Building Dynamic Cloud Maps From the Ground Up",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Calvin Murdock, Nathan Jacobs, Robert Pless",
        "author": "Calvin Murdock; Nathan Jacobs; Robert Pless",
        "abstract": "Satellite imagery of cloud cover is extremely important for understanding and predicting weather. We demonstrate how this imagery can be constructed \"from the ground up\" without requiring expensive geo-stationary satellites. This is accomplished through a novel approach to approximate continental-scale cloud maps using only ground-level imagery from publicly-available webcams. We collected a year's worth of satellite data and simultaneously-captured, geo-located outdoor webcam images from 4388 sparsely distributed cameras across the continental USA. The satellite data is used to train a dynamic model of cloud motion alongside 4388 regression models (one for each camera) to relate ground-level webcam data to the satellite data at the camera's location. This novel application of large-scale computer vision to meteorology and remote sensing is enabled by a smoothed, hierarchically-regularized dynamic texture model whose system dynamics are driven to remain consistent with measurements from the geo-located webcams. We show that our hierarchical model is better able to incorporate sparse webcam measurements resulting in more accurate cloud maps in comparison to a standard dynamic textures implementation. Finally, we demonstrate that our model can be successfully applied to other natural image sequences from the DynTex database, suggesting a broader applicability of our method.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Murdock_Building_Dynamic_Cloud_ICCV_2015_paper.pdf",
        "aff": "Machine Learning Department, Carnegie Mellon University; Department of Computer Science, University of Kentucky; Department of Computer Science and Engineering, Washington University in St. Louis",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1552971,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7095435225181523805&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "cs.cmu.edu;cs.uky.edu;cse.wustl.edu",
        "email": "cs.cmu.edu;cs.uky.edu;cse.wustl.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Murdock_Building_Dynamic_Cloud_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Carnegie Mellon University;University of Kentucky;Washington University in St. Louis",
        "aff_unique_dep": "Machine Learning Department;Department of Computer Science;Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.cmu.edu;https://www.uky.edu;https://wustl.edu",
        "aff_unique_abbr": "CMU;UK;WashU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";St. Louis",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Murdock_2015_ICCV,\n    \n    author = {\n    Murdock,\n    Calvin and Jacobs,\n    Nathan and Pless,\n    Robert\n},\n    title = {\n    Building Dynamic Cloud Maps From the Ground Up\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "a1228b7685",
        "title": "COUNT Forest: CO-Voting Uncertain Number of Targets Using Random Forest for Crowd Density Estimation",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Viet-Quoc Pham, Tatsuo Kozakaya, Osamu Yamaguchi, Ryuzo Okada",
        "author": "Viet-Quoc Pham; Tatsuo Kozakaya; Osamu Yamaguchi; Ryuzo Okada",
        "abstract": "This paper presents a patch-based approach for crowd density estimation in public scenes. We formulate the problem of estimating density in a structured learning framework applied to random decision forests. Our approach learns the mapping between patch features and relative locations of all objects inside each patch, which contribute to generate the patch density map through Gaussian kernel density estimation. We build the forest in a coarse-to-fine manner with two split node layers, and further propose a crowdedness prior and an effective forest reduction method to improve the estimation accuracy and speed. Moreover, we introduce a semi-automatic training method to learn the estimator for a specific scene. We achieved state-of-the-art results on the public Mall dataset and UCSD dataset, and also proposed two potential applications in traffic counts and scene understanding with promising results.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Pham_COUNT_Forest_CO-Voting_ICCV_2015_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2785973,
        "gs_citation": 440,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14924596455946214704&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Pham_COUNT_Forest_CO-Voting_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Pham_2015_ICCV,\n    \n    author = {\n    Pham,\n    Viet-Quoc and Kozakaya,\n    Tatsuo and Yamaguchi,\n    Osamu and Okada,\n    Ryuzo\n},\n    title = {\n    COUNT Forest: CO-Voting Uncertain Number of Targets Using Random Forest for Crowd Density Estimation\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "ec55a509dd",
        "title": "CV-HAZOP: Introducing Test Data Validation for Computer Vision",
        "session": "registration, alignment and stereo",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "Oliver Zendel, Markus Murschitz, Martin Humenberger, Wolfgang Herzner",
        "author": "Oliver Zendel; Markus Murschitz; Martin Humenberger; Wolfgang Herzner",
        "abstract": "Test data plays an important role in computer vision (CV) but is plagued by two questions: Which situations should be covered by the test data and have we tested enough to reach a conclusion? In this paper we propose a new solution answering these questions using a standard procedure devised by the safety community to validate complex systems: The Hazard and Operability Analysis (HAZOP). It is designed to systematically search and identify difficult, performance-decreasing situations and aspects. We introduce a generic CV model that creates the basis for the hazard analysis and, for the first time, apply an extensive HAZOP to the CV domain. The result is a publicly available checklist with more than 900 identified individual hazards. This checklist can be used to evaluate existing test datasets by quantifying the amount of covered hazards. We evaluate our approach by first analyzing and annotating the popular stereo vision test datasets Middlebury and KITTI. Second, we compare the performance of six popular stereo matching algorithms at the identified hazards from our checklist with their average performance and show, as expected, a clear negative influence of the hazards. The presented approach is a useful tool to evaluate and improve test datasets and creates a common basis for future dataset designs.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Zendel_CV-HAZOP_Introducing_Test_ICCV_2015_paper.pdf",
        "aff": "AIT Austrian Institute of Technology; AIT Austrian Institute of Technology; AIT Austrian Institute of Technology; AIT Austrian Institute of Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1329906,
        "gs_citation": 57,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11515885366775540597&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "ait.ac.at;ait.ac.at;ait.ac.at;ait.ac.at",
        "email": "ait.ac.at;ait.ac.at;ait.ac.at;ait.ac.at",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Zendel_CV-HAZOP_Introducing_Test_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Austrian Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ait.ac.at",
        "aff_unique_abbr": "AIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Austria",
        "bibtex": "@InProceedings{Zendel_2015_ICCV,\n    \n    author = {\n    Zendel,\n    Oliver and Murschitz,\n    Markus and Humenberger,\n    Martin and Herzner,\n    Wolfgang\n},\n    title = {\n    CV-HAZOP: Introducing Test Data Validation for Computer Vision\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "6cba74c4e7",
        "title": "Camera Pose Voting for Large-Scale Image-Based Localization",
        "session": "3d representations for recognition and localization",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "Bernhard Zeisl, Torsten Sattler, Marc Pollefeys",
        "author": "Bernhard Zeisl; Torsten Sattler; Marc Pollefeys",
        "abstract": "Image-based localization approaches aim to determine the camera pose from which an image was taken. Finding correct 2D-3D correspondences between query image features and 3D points in the scene model becomes harder as the size of the model increases. Current state-of-the-art methods therefore combine elaborate matching schemes with camera pose estimation techniques that are able to handle large fractions of wrong matches. In this work we study the benefits and limitations of spatial verification compared to appearance-based filtering. We propose a voting-based pose estimation strategy that exhibits O(n) complexity in the number of matches and thus facilitates to consider much more matches than previous approaches - whose complexity grows at least quadratically. This new outlier rejection formulation enables us to evaluate pose estimation for 1-to-many matches and to surpass the state-of-the-art. At the same time, we show that using more matches does not automatically lead to a better performance.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Zeisl_Camera_Pose_Voting_ICCV_2015_paper.pdf",
        "aff": "Department of Computer Science, ETH Zurich, Switzerland; Department of Computer Science, ETH Zurich, Switzerland; Department of Computer Science, ETH Zurich, Switzerland",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1522295,
        "gs_citation": 219,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5871776788867280644&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff_domain": "inf.ethz.ch;inf.ethz.ch;inf.ethz.ch",
        "email": "inf.ethz.ch;inf.ethz.ch;inf.ethz.ch",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Zeisl_Camera_Pose_Voting_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "ETH Zurich",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ethz.ch",
        "aff_unique_abbr": "ETHZ",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Switzerland",
        "bibtex": "@InProceedings{Zeisl_2015_ICCV,\n    \n    author = {\n    Zeisl,\n    Bernhard and Sattler,\n    Torsten and Pollefeys,\n    Marc\n},\n    title = {\n    Camera Pose Voting for Large-Scale Image-Based Localization\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "3eae6e5c4f",
        "title": "Car That Knows Before You Do: Anticipating Maneuvers via Learning Temporal Driving Models",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Ashesh Jain, Hema S. Koppula, Bharad Raghavan, Shane Soh, Ashutosh Saxena",
        "author": "Ashesh Jain; Hema S. Koppula; Bharad Raghavan; Shane Soh; Ashutosh Saxena",
        "abstract": "Advanced Driver Assistance Systems (ADAS) have made driving safer over the last decade. They prepare vehicles for  unsafe road conditions and alert drivers if they perform a dangerous maneuver.  However, many accidents are unavoidable because by the time drivers are alerted, it is already too late.  Anticipating maneuvers beforehand can alert drivers before they perform the  maneuver and also give ADAS more time to avoid or prepare for the danger.  In this work we anticipate driving maneuvers a few seconds before they occur. For this purpose we equip a car with cameras and a computing device to capture the driving context from both inside and outside of the car. We propose an Autoregressive Input-Output HMM to model the contextual information alongwith the maneuvers. We evaluate our approach on a  diverse data set with 1180 miles of natural freeway and city driving and show that we can anticipate  maneuvers 3.5 seconds before they occur with over 80% F1-score in real-time.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Jain_Car_That_Knows_ICCV_2015_paper.pdf",
        "aff": "Stanford University1 + Cornell University2; Stanford University1 + Cornell University2; Stanford University1; Stanford University1; Cornell University2 + Brain Of Things Inc.3",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1354515,
        "gs_citation": 350,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5337605542584768236&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 20,
        "aff_domain": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Jain_Car_That_Knows_ICCV_2015_paper.html",
        "aff_unique_index": "0+1;0+1;0;0;1+2",
        "aff_unique_norm": "Stanford University;Cornell University;Brain Of Things",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.stanford.edu;https://www.cornell.edu;https://www.brainofthings.com",
        "aff_unique_abbr": "Stanford;Cornell;BOT",
        "aff_campus_unique_index": "0;0;0;0;",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0+0;0+0;0;0;0+0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Jain_2015_ICCV,\n    \n    author = {\n    Jain,\n    Ashesh and Koppula,\n    Hema S. and Raghavan,\n    Bharad and Soh,\n    Shane and Saxena,\n    Ashutosh\n},\n    title = {\n    Car That Knows Before You Do: Anticipating Maneuvers via Learning Temporal Driving Models\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "92ee557609",
        "title": "Cascaded Sparse Spatial Bins for Efficient and Effective Generic Object Detection",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "David Novotny, Jiří Matas",
        "author": "David Novotny; Jiri Matas",
        "abstract": "A novel efficient method for extraction of object proposals is introduced. Its \"objectness\" function exploits deep spatial pyramid features, a novel fast-to-compute HoG-based edge statistic  and the EdgeBoxes score. The efficiency is achieved by the use of spatial bins in a novel combination with sparsity-inducing group normalized SVM.  State-of-the-art recall performance is achieved on Pascal VOC07, significantly outperforming methods with comparable speed. Interestingly, when only 100 proposals per image are considered the method attains 78 % recall on VOC07.  The method improves mAP of the RCNN class-specific detector, increasing it by 10 points when only 50 proposals are used in each image. The system trained on twenty classes performs well on the two hundred class ILSVRC2013 set confirming generalization capability.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Novotny_Cascaded_Sparse_Spatial_ICCV_2015_paper.pdf",
        "aff": "Visual Geometry Group, University of Oxford; Center for Machine Perception, Czech Technical University in Prague",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2638549,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14106023117321680927&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff_domain": "robots.ox.ac.uk;cmp.felk.cvut.cz",
        "email": "robots.ox.ac.uk;cmp.felk.cvut.cz",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Novotny_Cascaded_Sparse_Spatial_ICCV_2015_paper.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Oxford;Czech Technical University in Prague",
        "aff_unique_dep": "Visual Geometry Group;Center for Machine Perception",
        "aff_unique_url": "https://www.ox.ac.uk;https://www.cvut.cz",
        "aff_unique_abbr": "Oxford;CTU",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Oxford;Prague",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United Kingdom;Czech Republic",
        "bibtex": "@InProceedings{Novotny_2015_ICCV,\n    \n    author = {\n    Novotny,\n    David and Matas,\n    Jiri\n},\n    title = {\n    Cascaded Sparse Spatial Bins for Efficient and Effective Generic Object Detection\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "c56eb51642",
        "title": "Category-Blind Human Action Recognition: A Practical Recognition System",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Wenbo Li, Longyin Wen, Mooi Choo Chuah, Siwei Lyu",
        "author": "Wenbo Li; Longyin Wen; Mooi Choo Chuah; Siwei Lyu",
        "abstract": "Existing human action recognition systems for 3D sequences obtained from the depth camera are designed to cope with only one action category, either single-person action or two-person interaction, and are difficult to be extended to scenarios where both action categories co-exist. In this paper, we propose the category-blind human recognition method (CHARM) which can recognize a human action without making assumptions of the action category. In our CHARM approach, we represent a human action (either a single-person action or a two-person interaction) class using a co-occurrence of motion primitives. Subsequently, we classify an action instance based on matching its motion primitive co-occurrence patterns to each class representation. The matching task is formulated as maximum clique problems. We conduct extensive evaluations of CHARM using three datasets for single-person actions, two-person interactions, and their mixtures. Experimental results show that CHARM performs favorably when compared with several state-of-the-art single-person action and two-person interaction based methods without making explicit assumptions of action category.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Li_Category-Blind_Human_Action_ICCV_2015_paper.pdf",
        "aff": "Department of Computer Science and Engineering, Lehigh University; Department of Computer Science, University at Albany, SUNY; Department of Computer Science and Engineering, Lehigh University; Department of Computer Science, University at Albany, SUNY",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1615421,
        "gs_citation": 94,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18086248297793550608&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "lehigh.edu;albany.edu;lehigh.edu;albany.edu",
        "email": "lehigh.edu;albany.edu;lehigh.edu;albany.edu",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Li_Category-Blind_Human_Action_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "Lehigh University;University at Albany, SUNY",
        "aff_unique_dep": "Department of Computer Science and Engineering;Department of Computer Science",
        "aff_unique_url": "https://www.lehigh.edu;https://www.albany.edu",
        "aff_unique_abbr": "Lehigh;UAlbany",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Albany",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Li_2015_ICCV,\n    \n    author = {\n    Li,\n    Wenbo and Wen,\n    Longyin and Chuah,\n    Mooi Choo and Lyu,\n    Siwei\n},\n    title = {\n    Category-Blind Human Action Recognition: A Practical Recognition System\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "31b1ec0045",
        "title": "Class-Specific Image Deblurring",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Saeed Anwar, Cong Phuoc Huynh, Fatih Porikli",
        "author": "Saeed Anwar; Cong Phuoc Huynh; Fatih Porikli",
        "abstract": "In image deblurring, a fundamental problem is that the blur kernel suppresses a number of spatial frequencies that are difficult to recover reliably. In this paper, we explore the potential of a class-specific image prior for recovering spatial frequencies attenuated by the blurring process. Specifically, we devise a prior based on the class-specific subspace of image intensity responses to band-pass filters. We learn that the aggregation of these subspaces across all frequency bands serves as a good class-specific prior for the restoration of frequencies that cannot be recovered with generic image priors. In an extensive validation, our method, equipped with the above prior, yields greater image quality than many state-of-the-art methods by up to 5 dB in terms of image PSNR, across various image categories including portraits, cars, cats, pedestrians and household objects.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Anwar_Class-Specific_Image_Deblurring_ICCV_2015_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1190361,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=229799228351701065&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Anwar_Class-Specific_Image_Deblurring_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Anwar_2015_ICCV,\n    \n    author = {\n    Anwar,\n    Saeed and Huynh,\n    Cong Phuoc and Porikli,\n    Fatih\n},\n    title = {\n    Class-Specific Image Deblurring\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "a5a09a92af",
        "title": "Classical Scaling Revisited",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Gil Shamai, Yonathan Aflalo, Michael Zibulevsky, Ron Kimmel",
        "author": "Gil Shamai; Yonathan Aflalo; Michael Zibulevsky; Ron Kimmel",
        "abstract": "Multidimensional-scaling (MDS) is an information analysis tool. It involves the evaluation of distances between data points, which is a quadratic space-time problem. Then, MDS procedures find an embedding of the points in a low dimensional Euclidean (flat) domain, optimizing for the similarity of inter-points distances. We present an efficient solver for Classical Scaling (a specific MDS model) by extending the distances measured from a subset of the points to the rest, while exploiting the smoothness property of the distance functions. The smoothness is measured by the L2 norm of the Laplace-Beltrami operator applied to the unknown distance function. The Laplace Beltrami reflects the local differential relations between points, and can be computed in linear time. Classical-scaling is thereby reformulated into a quasi-linear space-time complexities procedure.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Shamai_Classical_Scaling_Revisited_ICCV_2015_paper.pdf",
        "aff": "Computer Science Department, Technion, Israel Institute of Technology, Haifa 32000; Computer Science Department, Technion, Israel Institute of Technology, Haifa 32000; Computer Science Department, Technion, Israel Institute of Technology, Haifa 32000; Computer Science Department, Technion, Israel Institute of Technology, Haifa 32000",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1136244,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5617281122994856188&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";cs.technion.ac.il; ; ",
        "email": ";cs.technion.ac.il; ; ",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Shamai_Classical_Scaling_Revisited_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Technion - Israel Institute of Technology",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.technion.ac.il",
        "aff_unique_abbr": "Technion",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Haifa",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Israel",
        "bibtex": "@InProceedings{Shamai_2015_ICCV,\n    \n    author = {\n    Shamai,\n    Gil and Aflalo,\n    Yonathan and Zibulevsky,\n    Michael and Kimmel,\n    Ron\n},\n    title = {\n    Classical Scaling Revisited\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "4e883eb36f",
        "title": "Cluster-Based Point Set Saliency",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Flora Ponjou Tasse, Jiri Kosinka, Neil Dodgson",
        "author": "Flora Ponjou Tasse; Jiri Kosinka; Neil Dodgson",
        "abstract": "We propose a cluster-based approach to point set saliency detection, a challenge since point sets lack topological information. A point set is first decomposed into small clusters, using fuzzy clustering. We evaluate cluster uniqueness and spatial distribution of each cluster and combine these values into a cluster saliency function. Finally, the probabilities of points belonging to each cluster are used to assign a saliency to each point. Our approach detects fine-scale salient features and uninteresting regions consistently have lower saliency values. We evaluate the proposed saliency model  by testing our saliency-based keypoint detection against a 3D interest point detection benchmark. The evaluation shows that our method achieves a good balance between false positive and false negative error rates, without using  any topological information.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Tasse_Cluster-Based_Point_Set_ICCV_2015_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1540847,
        "gs_citation": 70,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9080566092409730165&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Tasse_Cluster-Based_Point_Set_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Tasse_2015_ICCV,\n    \n    author = {\n    Tasse,\n    Flora Ponjou and Kosinka,\n    Jiri and Dodgson,\n    Neil\n},\n    title = {\n    Cluster-Based Point Set Saliency\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "4fa5ea67e2",
        "title": "Co-Interest Person Detection From Multiple Wearable Camera Videos",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Yuewei Lin, Kareem Abdelfatah, Youjie Zhou, Xiaochuan Fan, Hongkai Yu, Hui Qian, Song Wang",
        "author": "Yuewei Lin; Kareem Abdelfatah; Youjie Zhou; Xiaochuan Fan; Hongkai Yu; Hui Qian; Song Wang",
        "abstract": "Wearable cameras, such as Google Glass and Go Pro, enable video data collection over larger areas and from different views. In this paper, we tackle a new problem of locating the co-interest person (CIP), i.e., the one who draws attention from most camera wearers, from temporally synchronized videos taken by multiple wearable cameras. Our basic idea is to exploit the motion patterns of people and use them to correlate the persons across different videos, instead of performing appearance-based matching as in traditional video co-segmentation/localization. This way, we can identify CIP even if a group of people with similar appearance are present in the view. More specifically, we detect a set of persons on each frame as the candidates of the CIP and then build a Conditional Random Field (CRF) model to select the one with consistent motion patterns in different videos and high spacial-temporal consistency in each video. We collect three sets of wearable-camera videos for testing the proposed algorithm. All the involved people have similar appearances in the collected videos and the experiments demonstrate the effectiveness of the proposed algorithm.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Lin_Co-Interest_Person_Detection_ICCV_2015_paper.pdf",
        "aff": "University of South Carolina; University of South Carolina; University of South Carolina; University of South Carolina; University of South Carolina; Zhejiang University; University of South Carolina",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 5544074,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16958116331962130144&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "gmail.com;fayoum.edu.eg;email.sc.edu;email.sc.edu;email.sc.edu;zju.edu.cn;cec.sc.edu",
        "email": "gmail.com;fayoum.edu.eg;email.sc.edu;email.sc.edu;email.sc.edu;zju.edu.cn;cec.sc.edu",
        "author_num": 7,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Lin_Co-Interest_Person_Detection_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0;0;1;0",
        "aff_unique_norm": "University of South Carolina;Zhejiang University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.sc.edu;https://www.zju.edu.cn",
        "aff_unique_abbr": "USC;ZJU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;1;0",
        "aff_country_unique": "United States;China",
        "bibtex": "@InProceedings{Lin_2015_ICCV,\n    \n    author = {\n    Lin,\n    Yuewei and Abdelfatah,\n    Kareem and Zhou,\n    Youjie and Fan,\n    Xiaochuan and Yu,\n    Hongkai and Qian,\n    Hui and Wang,\n    Song\n},\n    title = {\n    Co-Interest Person Detection From Multiple Wearable Camera Videos\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "7e20985a23",
        "title": "Common Subspace for Model and Similarity: Phrase Learning for Caption Generation From Images",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Yoshitaka Ushiku, Masataka Yamaguchi, Yusuke Mukuta, Tatsuya Harada",
        "author": "Yoshitaka Ushiku; Masataka Yamaguchi; Yusuke Mukuta; Tatsuya Harada",
        "abstract": "Generating captions to describe images is a fundamental problem that combines computer vision and natural language processing. Recent works focus on descriptive phrases, such as \"a white dog\" to explain the visual composites of an input image. The phrases can not only express objects, attributes, events, and their relations but can also reduce visual complexity. A caption for an input image can be generated by connecting estimated phrases using a grammar model. However, because phrases are combinations of various words, the number of phrases is much larger than the number of single words. Consequently, the accuracy of phrase estimation suffers from too few training samples per phrase.  In this paper, we propose a novel phrase-learning method: Common Subspace for Model and Similarity (CoSMoS). In order to overcome the shortage of training samples, CoSMoS obtains a subspace in which (a) all feature vectors associated with the same phrase are mapped as mutually close, (b) classifiers for each phrase are learned, and (c) training samples are shared among co-occurring phrases. Experimental results demonstrate that our system is more accurate than those in earlier work and that the accuracy increases when the dataset from the web increases.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Ushiku_Common_Subspace_for_ICCV_2015_paper.pdf",
        "aff": "The University of Tokyo; The University of Tokyo; The University of Tokyo; The University of Tokyo",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1903404,
        "gs_citation": 81,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14111204694971426004&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "mi.t.u-tokyo.ac.jp;mi.t.u-tokyo.ac.jp;mi.t.u-tokyo.ac.jp;mi.t.u-tokyo.ac.jp",
        "email": "mi.t.u-tokyo.ac.jp;mi.t.u-tokyo.ac.jp;mi.t.u-tokyo.ac.jp;mi.t.u-tokyo.ac.jp",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Ushiku_Common_Subspace_for_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Tokyo",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.u-tokyo.ac.jp",
        "aff_unique_abbr": "UTokyo",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Japan",
        "bibtex": "@InProceedings{Ushiku_2015_ICCV,\n    \n    author = {\n    Ushiku,\n    Yoshitaka and Yamaguchi,\n    Masataka and Mukuta,\n    Yusuke and Harada,\n    Tatsuya\n},\n    title = {\n    Common Subspace for Model and Similarity: Phrase Learning for Caption Generation From Images\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "9c8c351572",
        "title": "Complementary Sets of Shutter Sequences for Motion Deblurring",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Hae-Gon Jeon, Joon-Young Lee, Yudeog Han, Seon Joo Kim, In So Kweon",
        "author": "Hae-Gon Jeon; Joon-Young Lee; Yudeog Han; Seon Joo Kim; In So Kweon",
        "abstract": "In this paper, we present a novel multi-image motion deblurring method utilizing the coded exposure technique. The key idea of our work is to capture video frames with a set of complementary fluttering patterns to preserve spatial frequency details. We introduce an algorithm for generating a complementary set of binary sequences based on the modern communication theory and implement the coded exposure video system with an off-the-shelf machine vision camera. The effectiveness of our method is demonstrated on various challenging examples with quantitative and qualitative comparisons to other computational image capturing methods used for image deblurring.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Jeon_Complementary_Sets_of_ICCV_2015_paper.pdf",
        "aff": "Robotics and Computer Vision Lab., KAIST; Robotics and Computer Vision Lab., KAIST; Agency for Defense Development; Yonsei University; Robotics and Computer Vision Lab., KAIST",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 5574819,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11410918298499448408&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "rcv.kaist.ac.kr;rcv.kaist.ac.kr;add.re.kr;yonsei.ac.kr;kaist.ac.kr",
        "email": "rcv.kaist.ac.kr;rcv.kaist.ac.kr;add.re.kr;yonsei.ac.kr;kaist.ac.kr",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Jeon_Complementary_Sets_of_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;1;2;0",
        "aff_unique_norm": "KAIST;Agency for Defense Development;Yonsei University",
        "aff_unique_dep": "Robotics and Computer Vision Lab.;;",
        "aff_unique_url": "https://www.kaist.ac.kr;https://www.add.re.kr;https://www.yonsei.ac.kr",
        "aff_unique_abbr": "KAIST;ADD;Yonsei",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "South Korea",
        "bibtex": "@InProceedings{Jeon_2015_ICCV,\n    \n    author = {\n    Jeon,\n    Hae-Gon and Lee,\n    Joon-Young and Han,\n    Yudeog and Kim,\n    Seon Joo and Kweon,\n    In So\n},\n    title = {\n    Complementary Sets of Shutter Sequences for Motion Deblurring\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "1d06d0b637",
        "title": "Component-Wise Modeling of Articulated Objects",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Valsamis Ntouskos, Marta Sanzari, Bruno Cafaro, Federico Nardi, Fabrizio Natola, Fiora Pirri, Manuel Ruiz",
        "author": "Valsamis Ntouskos; Marta Sanzari; Bruno Cafaro; Federico Nardi; Fabrizio Natola; Fiora Pirri; Manuel Ruiz",
        "abstract": "We introduce a novel framework for modeling articulated objects based on the aspects of their components. By decomposing the object into components, we divide the problem in smaller modeling tasks. After obtaining 3D models for each component aspect by employing a shape deformation paradigm, we merge them together, forming the object components. The final model is obtained by assembling the components using an optimization scheme which fits the respective 3D models to the corresponding apparent contours in a reference pose.  The results suggest that our approach can produce realistic 3D models of articulated objects in reasonable time.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Ntouskos_Component-Wise_Modeling_of_ICCV_2015_paper.pdf",
        "aff": ";;;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2851271,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14253282286165708931&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "author_num": 7,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Ntouskos_Component-Wise_Modeling_of_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Ntouskos_2015_ICCV,\n    \n    author = {\n    Ntouskos,\n    Valsamis and Sanzari,\n    Marta and Cafaro,\n    Bruno and Nardi,\n    Federico and Natola,\n    Fabrizio and Pirri,\n    Fiora and Ruiz,\n    Manuel\n},\n    title = {\n    Component-Wise Modeling of Articulated Objects\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "c45195f881",
        "title": "Compositional Hierarchical Representation of Shape Manifolds for Classification of Non-Manifold Shapes",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Mete Ozay, Umit Rusen Aktas, Jeremy L. Wyatt, Aleš Leonardis",
        "author": "Mete Ozay; Umit Rusen Aktas; Jeremy L. Wyatt; Ales Leonardis",
        "abstract": "We address the problem of statistical learning of shape models which are invariant to translation, rotation and scale in compositional hierarchies when data spaces of measurements and shape spaces are not topological manifolds. In practice, this problem is observed while modeling shapes having multiple disconnected components, e.g. partially occluded shapes in cluttered scenes. We resolve the aforementioned problem by first reformulating the relationship between data and shape spaces considering the interaction between Receptive Fields (RFs) and Shape Manifolds (SMs) in a compositional hierarchical shape vocabulary. Then, we suggest a method to model the topological structure of the SMs for statistical learning of the geometric transformations of the shapes that are defined by group actions on the SMs. For this purpose, we design a disjoint union topology using an indexing mechanism for the formation of shape models on SMs in the vocabulary, recursively. We represent the topological relationship between shape components using graphs, which are aggregated to construct a hierarchical graph structure for the shape vocabulary. To this end, we introduce a framework to implement the indexing mechanisms for the employment of the vocabulary for structural shape classification. The proposed approach is used to construct invariant shape representations. Results on benchmark shape classification outperform state-of-the-art methods.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Ozay_Compositional_Hierarchical_Representation_ICCV_2015_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 839983,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3136438653559375339&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Ozay_Compositional_Hierarchical_Representation_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Ozay_2015_ICCV,\n    \n    author = {\n    Ozay,\n    Mete and Aktas,\n    Umit Rusen and Wyatt,\n    Jeremy L. and Leonardis,\n    Ales\n},\n    title = {\n    Compositional Hierarchical Representation of Shape Manifolds for Classification of Non-Manifold Shapes\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "c7bd5c31b8",
        "title": "Compression Artifacts Reduction by a Deep Convolutional Network",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Chao Dong, Yubin Deng, Chen Change Loy, Xiaoou Tang",
        "author": "Chao Dong; Yubin Deng; Chen Change Loy; Xiaoou Tang",
        "abstract": "Lossy compression introduces complex compression artifacts, particularly the blocking artifacts, ringing effects and blurring. Existing algorithms either focus on removing blocking artifacts and produce blurred output, or restores sharpened images that are accompanied with ringing effects. Inspired by the deep convolutional networks (DCN) on super-resolution, we formulate a compact and efficient network for seamless attenuation of different compression artifacts. We also demonstrate that a deeper model can be effectively trained with the features learned in a shallow network. Following a similar \"easy to hard\" idea, we systematically investigate several practical transfer settings and show the effectiveness of transfer learning in low level vision problems. Our method shows superior performance than the state-of-the-arts both on the benchmark datasets and the real-world use cases (i.e. Twitter).",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Dong_Compression_Artifacts_Reduction_ICCV_2015_paper.pdf",
        "aff": "Department of Information Engineering, The Chinese University of Hong Kong; Department of Information Engineering, The Chinese University of Hong Kong; Department of Information Engineering, The Chinese University of Hong Kong; Department of Information Engineering, The Chinese University of Hong Kong",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1250771,
        "gs_citation": 1033,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2364220051429841371&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 13,
        "aff_domain": "ie.cuhk.edu.com;ie.cuhk.edu.com;ie.cuhk.edu.com;ie.cuhk.edu.com",
        "email": "ie.cuhk.edu.com;ie.cuhk.edu.com;ie.cuhk.edu.com;ie.cuhk.edu.com",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Dong_Compression_Artifacts_Reduction_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "The Chinese University of Hong Kong",
        "aff_unique_dep": "Department of Information Engineering",
        "aff_unique_url": "https://www.cuhk.edu.hk",
        "aff_unique_abbr": "CUHK",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Dong_2015_ICCV,\n    \n    author = {\n    Dong,\n    Chao and Deng,\n    Yubin and Loy,\n    Chen Change and Tang,\n    Xiaoou\n},\n    title = {\n    Compression Artifacts Reduction by a Deep Convolutional Network\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "eb2a52c6e3",
        "title": "Conditional Convolutional Neural Network for Modality-Aware Face Recognition",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Chao Xiong, Xiaowei Zhao, Danhang Tang, Karlekar Jayashree, Shuicheng Yan, Tae-Kyun Kim",
        "author": "Chao Xiong; Xiaowei Zhao; Danhang Tang; Karlekar Jayashree; Shuicheng Yan; Tae-Kyun Kim",
        "abstract": "Faces in the wild are usually captured with various poses, illuminations and occlusions, and thus inherently multimodally distributed in many tasks. We propose a conditional Convolutional Neural Network, named as c-CNN, to handle multimodal face recognition. Different from traditional CNN that adopts fixed convolution kernels, samples in c-CNN are processed with dynamically activated sets of kernels. In particular, convolution kernels within each layer are only sparsely activated when a sample is passed through the network. For a given sample, the activations of  convolution kernels in a certain layer are conditioned on its present intermediate representation and the activation status in the lower layers. The activated kernels across layers define the sample-specific adaptive routes that reveal the distribution of underlying modalities. Consequently, the proposed framework does not rely on any prior knowledge of modalities in contrast with most existing methods. To substantiate the generic framework, we introduce a special case of c-CNN via incorporating the conditional routing of the decision tree, which is evaluated with two problems of multimodality - multi-view face identification and occluded face verification. Extensive experiments demonstrate consistent improvements over the counterparts unaware of modalities.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Xiong_Conditional_Convolutional_Neural_ICCV_2015_paper.pdf",
        "aff": "Department of Electrical and Electronic Engineering, Imperial College London; Department of Electrical and Electronic Engineering, Imperial College London; Department of Electrical and Electronic Engineering, Imperial College London; Panasonic R&D Center Singapore; Department of Electrical and Computer Engineering, National University of Singapore; Department of Electrical and Electronic Engineering, Imperial College London",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 885894,
        "gs_citation": 111,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10363004042890465294&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "imperial.ac.uk;imperial.ac.uk;imperial.ac.uk;sg.panasonic.com;nus.edu.sg;imperial.ac.uk",
        "email": "imperial.ac.uk;imperial.ac.uk;imperial.ac.uk;sg.panasonic.com;nus.edu.sg;imperial.ac.uk",
        "author_num": 6,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Xiong_Conditional_Convolutional_Neural_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;1;2;0",
        "aff_unique_norm": "Imperial College London;Panasonic;National University of Singapore",
        "aff_unique_dep": "Department of Electrical and Electronic Engineering;R&D Center;Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.imperial.ac.uk;https://www.panasonic.com.sg;https://www.nus.edu.sg",
        "aff_unique_abbr": "Imperial;Panasonic;NUS",
        "aff_campus_unique_index": "0;0;0;1;0",
        "aff_campus_unique": "London;Singapore;",
        "aff_country_unique_index": "0;0;0;1;1;0",
        "aff_country_unique": "United Kingdom;Singapore",
        "bibtex": "@InProceedings{Xiong_2015_ICCV,\n    \n    author = {\n    Xiong,\n    Chao and Zhao,\n    Xiaowei and Tang,\n    Danhang and Jayashree,\n    Karlekar and Yan,\n    Shuicheng and Kim,\n    Tae-Kyun\n},\n    title = {\n    Conditional Convolutional Neural Network for Modality-Aware Face Recognition\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "569e107583",
        "title": "Conditional High-Order Boltzmann Machine: A Supervised Learning Model for Relation Learning",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Yan Huang, Wei Wang, Liang Wang",
        "author": "Yan Huang; Wei Wang; Liang Wang",
        "abstract": "Relation learning is a fundamental operation in many computer vision tasks. Recently, high-order Boltzmann machine and its variants have exhibited the great power of modelling various data relation. However, most of them are unsupervised learning models which are not very discriminative and thus cannot server as a standalone solution to relation learning tasks. In this paper, we explore supervised learning algorithms and propose a new model named Conditional High-order Boltzmann Machine (CHBM), which can be directly used as a bilinear classifier to assign similarity scores for pairwise images. Then, to better deal with complex data relation, we propose a gated version of CHBM which untangles factors of variation by exploiting a set of latent variables to gate classification. We perform four-order tensor factorization for parameter reduction, and present two efficient supervised learning algorithms from the perspectives of being generative and discriminative, respectively. The experimental results of image transformation visualization, binary-way classification and face verification demonstrate that, by performing supervised learning, our models can greatly improve the performance.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Huang_Conditional_High-Order_Boltzmann_ICCV_2015_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11054125964341364638&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Huang_Conditional_High-Order_Boltzmann_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Huang_2015_ICCV,\n    \n    author = {\n    Huang,\n    Yan and Wang,\n    Wei and Wang,\n    Liang\n},\n    title = {\n    Conditional High-Order Boltzmann Machine: A Supervised Learning Model for Relation Learning\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "e6db6c4606",
        "title": "Conditional Random Fields as Recurrent Neural Networks",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du, Chang Huang, Philip H. S. Torr",
        "author": "Shuai Zheng; Sadeep Jayasumana; Bernardino Romera-Paredes; Vibhav Vineet; Zhizhong Su; Dalong Du; Chang Huang; Philip H. S. Torr",
        "abstract": "Pixel-level labelling tasks, such as semantic segmentation, play a central role in image understanding. Recent approaches have attempted to harness the capabilities of deep learning techniques for image recognition to tackle pixel-level labelling tasks. One central issue in this methodology is the limited capacity of deep learning techniques to delineate visual objects. To solve this problem, we introduce a new form of convolutional neural network that combines the strengths of Convolutional Neural Networks (CNNs) and Conditional Random Fields (CRFs)-based probabilistic graphical modelling. To this end, we formulate Conditional Random Fields with Gaussian pairwise potentials and mean-field approximate inference as Recurrent Neural Networks. This network, called CRF-RNN, is then plugged in as a part of a CNN to obtain a deep network that has desirable properties of both CNNs and CRFs. Importantly, our system fully integrates CRF modelling with CNNs, making it possible to train the whole deep network end-to-end with the usual back-propagation algorithm, avoiding offline post-processing methods for object delineation. We apply the proposed method to the problem of semantic image segmentation, obtaining top results on the challenging Pascal VOC 2012 segmentation benchmark.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Zheng_Conditional_Random_Fields_ICCV_2015_paper.pdf",
        "aff": "Torr-Vision Group, University of Oxford; Torr-Vision Group, University of Oxford; Torr-Vision Group, University of Oxford; Torr-Vision Group, University of Oxford+Stanford University; Baidu Research; Baidu Research; Baidu Research; Torr-Vision Group, University of Oxford",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1034714,
        "gs_citation": 3378,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4680896688857314530&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 21,
        "aff_domain": "eng.ox.ac.uk;eng.ox.ac.uk;eng.ox.ac.uk;stanford.edu;baidu.com;baidu.com;baidu.com;eng.ox.ac.uk",
        "email": "eng.ox.ac.uk;eng.ox.ac.uk;eng.ox.ac.uk;stanford.edu;baidu.com;baidu.com;baidu.com;eng.ox.ac.uk",
        "author_num": 8,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Zheng_Conditional_Random_Fields_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0+1;2;2;2;0",
        "aff_unique_norm": "University of Oxford;Stanford University;Baidu",
        "aff_unique_dep": "Torr-Vision Group;;Baidu Research",
        "aff_unique_url": "https://www.ox.ac.uk;https://www.stanford.edu;https://research.baidu.com",
        "aff_unique_abbr": "Oxford;Stanford;Baidu",
        "aff_campus_unique_index": "0;0;0;0+1;0",
        "aff_campus_unique": "Oxford;Stanford;",
        "aff_country_unique_index": "0;0;0;0+1;2;2;2;0",
        "aff_country_unique": "United Kingdom;United States;China",
        "bibtex": "@InProceedings{Zheng_2015_ICCV,\n    \n    author = {\n    Zheng,\n    Shuai and Jayasumana,\n    Sadeep and Romera-Paredes,\n    Bernardino and Vineet,\n    Vibhav and Su,\n    Zhizhong and Du,\n    Dalong and Huang,\n    Chang and Torr,\n    Philip H. S.\n},\n    title = {\n    Conditional Random Fields as Recurrent Neural Networks\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "8a07600677",
        "title": "Conditioned Regression Models for Non-Blind Single Image Super-Resolution",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Gernot Riegler, Samuel Schulter, Matthias Rüther, Horst Bischof",
        "author": "Gernot Riegler; Samuel Schulter; Matthias Ruther; Horst Bischof",
        "abstract": "Single image super-resolution is an important task in the field of computer vision and finds many practical applications.  Current state-of-the-art methods typically rely on machine learning algorithms to infer a mapping from low- to high-resolution images.  These methods use a single fixed blur kernel during training and, consequently, assume the exact same kernel underlying the image formation process for all test images.  However, this setting is not realistic for practical applications, because the blur is typically different for each test image.  In this paper, we loosen this restrictive constraint and propose conditioned regression models (including convolutional neural networks and random forests) that can effectively exploit the additional kernel information during both, training and inference.  This allows for training a single model, while previous methods need to be re-trained for every blur kernel individually to achieve good results, which we demonstrate in our evaluations.  We also empirically show that the proposed conditioned regression models (i) can effectively handle scenarios where the blur kernel is different for each image and (ii) outperform related approaches trained for only a single kernel.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Riegler_Conditioned_Regression_Models_ICCV_2015_paper.pdf",
        "aff": "Institute for Computer Graphics and Vision, Graz University of Technology; Institute for Computer Graphics and Vision, Graz University of Technology; Institute for Computer Graphics and Vision, Graz University of Technology; Institute for Computer Graphics and Vision, Graz University of Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1337058,
        "gs_citation": 121,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15578222860083687019&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "icg.tugraz.at;icg.tugraz.at;icg.tugraz.at;icg.tugraz.at",
        "email": "icg.tugraz.at;icg.tugraz.at;icg.tugraz.at;icg.tugraz.at",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Riegler_Conditioned_Regression_Models_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Graz University of Technology",
        "aff_unique_dep": "Institute for Computer Graphics and Vision",
        "aff_unique_url": "https://www.tugraz.at",
        "aff_unique_abbr": "TU Graz",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Graz",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Austria",
        "bibtex": "@InProceedings{Riegler_2015_ICCV,\n    \n    author = {\n    Riegler,\n    Gernot and Schulter,\n    Samuel and Ruther,\n    Matthias and Bischof,\n    Horst\n},\n    title = {\n    Conditioned Regression Models for Non-Blind Single Image Super-Resolution\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "6107000882",
        "title": "Confidence Preserving Machine for Facial Action Unit Detection",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Jiabei Zeng, Wen-Sheng Chu, Fernando De la Torre, Jeffrey F. Cohn, Zhang Xiong",
        "author": "Jiabei Zeng; Wen-Sheng Chu; Fernando De la Torre; Jeffrey F. Cohn; Zhang Xiong",
        "abstract": "Varied sources of error contribute to the challenge of facial action unit detection. Previous approaches address specific and known sources. However, many sources are unknown. To address the ubiquity of error, we propose a Confident Preserving Machine (CPM) that follows an easy-to-hard classification strategy. During training, CPM learns two confident classifiers. A confident positive classifier separates easily identified positive samples from all else; a confident negative classifier does same for negative samples. During testing, CPM then learns a person-specific classifier using ``virtual labels'' provided by confident classifiers. This step is achieved using a quasi-semi-supervised (QSS) approach. Hard samples are typically close to the decision boundary, and the QSS approach disambiguates them using spatio-temporal constraints. To evaluate CPM, we compared it with a baseline single-margin classifier and state-of-the-art semi-supervised learning, transfer learning, and boosting methods in three datasets of spontaneous facial behavior. With few exceptions, CPM outperformed baseline and state-of-the art methods.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Zeng_Confidence_Preserving_Machine_ICCV_2015_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2760732,
        "gs_citation": 101,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15764890414375520327&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Zeng_Confidence_Preserving_Machine_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Zeng_2015_ICCV,\n    \n    author = {\n    Zeng,\n    Jiabei and Chu,\n    Wen-Sheng and De la Torre,\n    Fernando and Cohn,\n    Jeffrey F. and Xiong,\n    Zhang\n},\n    title = {\n    Confidence Preserving Machine for Facial Action Unit Detection\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "795c3533fb",
        "title": "Conformal and Low-Rank Sparse Representation for Image Restoration",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Jianwei Li, Xiaowu Chen, Dongqing Zou, Bo Gao, Wei Teng",
        "author": "Jianwei Li; Xiaowu Chen; Dongqing Zou; Bo Gao; Wei Teng",
        "abstract": "Obtaining an appropriate dictionary is the key point when sparse representation is applied to computer vision or image processing problems such as image restoration. It is expected that preserving data structure during sparse coding and dictionary learning can enhance the recovery performance. However, many existing dictionary learning methods handle training samples individually, while missing relationships between samples, which result in dictionaries with redundant atoms but poor representation ability. In this paper, we propose a novel sparse representation approach called conformal and low-rank sparse representation (CLRSR) for image restoration problems. To achieve a more compact and representative dictionary, conformal property is introduced by preserving the angles of local geometry formed by neighboring samples in the feature space. Furthermore, imposing low-rank constraint on the coefficient matrix can lead more faithful subspaces and capture the global structure of data. We apply our CLRSR model to several image restoration tasks to demonstrate the effectiveness.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Li_Conformal_and_Low-Rank_ICCV_2015_paper.pdf",
        "aff": "State Key Laboratory of Virtual Reality Technology and Systems; State Key Laboratory of Virtual Reality Technology and Systems; State Key Laboratory of Virtual Reality Technology and Systems; State Key Laboratory of Virtual Reality Technology and Systems; State Key Laboratory of Virtual Reality Technology and Systems",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 928345,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10872076689916369727&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;buaa.edu.cn",
        "email": "buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;buaa.edu.cn",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Li_Conformal_and_Low-Rank_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "State Key Laboratory of Virtual Reality Technology and Systems",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Li_2015_ICCV,\n    \n    author = {\n    Li,\n    Jianwei and Chen,\n    Xiaowu and Zou,\n    Dongqing and Gao,\n    Bo and Teng,\n    Wei\n},\n    title = {\n    Conformal and Low-Rank Sparse Representation for Image Restoration\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "cd6f29c1ef",
        "title": "Constrained Convolutional Neural Networks for Weakly Supervised Segmentation",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Deepak Pathak, Philipp Krähenbühl, Trevor Darrell",
        "author": "Deepak Pathak; Philipp Krahenbuhl; Trevor Darrell",
        "abstract": "We present an approach to learn a dense pixel-wise labeling from image-level tags. Each image-level tag imposes constraints on the output labeling of a Convolutional Neural Network (CNN) classifier. We propose Constrained CNN (CCNN), a method which uses a novel loss function to optimize for any set of linear constraints on the output space (i.e. predicted label distribution) of a CNN. Our loss formulation is easy to optimize and can be incorporated directly into standard stochastic gradient descent optimization. The key idea is to phrase the training objective as a biconvex optimization for linear models, which we then relax to nonlinear deep networks. Extensive experiments demonstrate the generality of our new learning framework. The constrained loss yields state-of-the-art results on weakly supervised semantic image segmentation. We further demonstrate that adding slightly more supervision can greatly improve the performance of the learning algorithm.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Pathak_Constrained_Convolutional_Neural_ICCV_2015_paper.pdf",
        "aff": "University of California, Berkeley; University of California, Berkeley; University of California, Berkeley",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 728901,
        "gs_citation": 791,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18113115400192563138&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "cs.berkeley.edu;cs.berkeley.edu;cs.berkeley.edu",
        "email": "cs.berkeley.edu;cs.berkeley.edu;cs.berkeley.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Pathak_Constrained_Convolutional_Neural_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Pathak_2015_ICCV,\n    \n    author = {\n    Pathak,\n    Deepak and Krahenbuhl,\n    Philipp and Darrell,\n    Trevor\n},\n    title = {\n    Constrained Convolutional Neural Networks for Weakly Supervised Segmentation\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "cce32f6e79",
        "title": "Context Aware Active Learning of Activity Recognition Models",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Mahmudul Hasan, Amit K. Roy-Chowdhury",
        "author": "Mahmudul Hasan; Amit K. Roy-Chowdhury",
        "abstract": "Activity recognition in video has recently benefited from the use of the context e.g., inter-relationships among the activities and objects. However, these approaches require data to be labeled and entirely available at the outset. In contrast, we formulate a continuous learning framework for context aware activity recognition from unlabeled video data which has two distinct advantages over most existing methods. First,  we propose a novel active learning technique which not only exploits the informativeness of the individual activity instances but also utilizes their contextual information during the query selection process; this leads to significant reduction in expensive manual annotation effort. Second, the learned models can be adapted online as more data is available. We formulate a conditional random field (CRF) model that encodes the context and devise an information theoretic approach that utilizes entropy and mutual information of the nodes to compute the set of most informative query instances, which need to be labeled by a human. These labels are combined with graphical inference techniques for incrementally updating the model as new videos come in. Experiments on four challenging datasets demonstrate that our framework achieves superior performance with significantly less amount of manual labeling.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Hasan_Context_Aware_Active_ICCV_2015_paper.pdf",
        "aff": "Dept. of Computer Science and Engineering, University of California, Riverside; Dept. of Electrical and Computer Engineering, University of California, Riverside",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1206733,
        "gs_citation": 122,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6037701640155273707&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "ucr.edu;ee.ucr.edu",
        "email": "ucr.edu;ee.ucr.edu",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Hasan_Context_Aware_Active_ICCV_2015_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Riverside",
        "aff_unique_dep": "Dept. of Computer Science and Engineering",
        "aff_unique_url": "https://www.ucr.edu",
        "aff_unique_abbr": "UCR",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Riverside",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Hasan_2015_ICCV,\n    \n    author = {\n    Hasan,\n    Mahmudul and Roy-Chowdhury,\n    Amit K.\n},\n    title = {\n    Context Aware Active Learning of Activity Recognition Models\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "75f303cc8b",
        "title": "Context-Aware CNNs for Person Head Detection",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Tuan-Hung Vu, Anton Osokin, Ivan Laptev",
        "author": "Tuan-Hung Vu; Anton Osokin; Ivan Laptev",
        "abstract": "Person detection is a key problem for many computer vision tasks. While face detection has reached maturity, detecting people under full variation of camera view-points, human poses, lighting conditions and occlusions is still a difficult challenge. In this work we focus on detecting human heads in natural scenes. Starting from the recent R-CNN object detector, we extend it in two ways. First, we leverage person-scene relations and propose a global CNN model trained to predict positions and scales of heads directly from the full image. Second, we explicitly model pairwise relations among the objects via energy-based model where the potentials are computed with a CNN framework. Our full combined model complements R-CNN with contextual cues derived from the scene. To train and test our model, we introduce a large dataset with 369,846 human heads annotated in 224,740 movie frames. We evaluate our method and demonstrate improvements of person head detection compared to several recent baselines on three datasets. We also show improvements of the detection speed provided by our model.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Vu_Context-Aware_CNNs_for_ICCV_2015_paper.pdf",
        "aff": "WILLOW project-team, D´epartment d’Informatique de l’Ecole Normale Sup´erieure, ENS/INRIA/CNRS UMR 8548, Paris, France; SIERRA project-team, D´epartment d’Informatique de l’Ecole Normale Sup´erieure, ENS/INRIA/CNRS UMR 8548, Paris, France; WILLOW project-team, D´epartment d’Informatique de l’Ecole Normale Sup´erieure, ENS/INRIA/CNRS UMR 8548, Paris, France",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1633371,
        "gs_citation": 205,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4805087293645598420&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff_domain": "; ; ",
        "email": "; ; ",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Vu_Context-Aware_CNNs_for_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Ecole Normale Superieure",
        "aff_unique_dep": "Department of Informatics",
        "aff_unique_url": "https://www.ens.fr",
        "aff_unique_abbr": "ENS",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Paris",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "France",
        "bibtex": "@InProceedings{Vu_2015_ICCV,\n    \n    author = {\n    Vu,\n    Tuan-Hung and Osokin,\n    Anton and Laptev,\n    Ivan\n},\n    title = {\n    Context-Aware CNNs for Person Head Detection\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "9ec1fd2fbb",
        "title": "Context-Guided Diffusion for Label Propagation on Graphs",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Kwang In Kim, James Tompkin, Hanspeter Pfister, Christian Theobalt",
        "author": "Kwang In Kim; James Tompkin; Hanspeter Pfister; Christian Theobalt",
        "abstract": "Existing approaches for diffusion on graphs, e.g., for label propagation, are mainly focused on isotropic diffusion, which is induced by the commonly-used graph Laplacian regularizer. Inspired by the success of diffusivity tensors for anisotropic diffusion in image processing, we presents anisotropic diffusion on graphs and the corresponding label propagation algorithm. We develop positive definite diffusivity operators on the vector bundles of Riemannian manifolds, and discretize them to diffusivity operators on graphs. This enables us to easily define new robust diffusivity operators which significantly improve semi-supervised learning performance over existing diffusion algorithms.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Kim_Context-Guided_Diffusion_for_ICCV_2015_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 888709,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2914818871689405228&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Kim_Context-Guided_Diffusion_for_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Kim_2015_ICCV,\n    \n    author = {\n    Kim,\n    Kwang In and Tompkin,\n    James and Pfister,\n    Hanspeter and Theobalt,\n    Christian\n},\n    title = {\n    Context-Guided Diffusion for Label Propagation on Graphs\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "62ef2bcc4b",
        "title": "Contextual Action Recognition With R*CNN",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Georgia Gkioxari, Ross Girshick, Jitendra Malik",
        "author": "Georgia Gkioxari; Ross Girshick; Jitendra Malik",
        "abstract": "There are multiple cues in an image which reveal what action a person is performing. For example, a jogger has a pose that is characteristic for jogging, but the scene (e.g. road, trail) and the presence of other joggers can be an additional source of information. In this work, we exploit the simple observation that actions are accompanied by contextual cues to build a strong action recognition system. We adapt RCNN to use more than one region for classification while still maintaining the ability to localize the action. We call our system R*CNN. The action-specific models and the feature maps are trained jointly, allowing for action specific representations to emerge. R*CNN achieves 90.2% mean AP on the PASAL VOC Action dataset, outperforming all other approaches in the field by a significant margin. Last, we show that R*CNN is not limited to action recognition. In particular, R*CNN can also be used to tackle fine-grained tasks such as attribute classification. We validate this claim by reporting state-of-the-art performance on the Berkeley Attributes of People dataset.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Gkioxari_Contextual_Action_Recognition_ICCV_2015_paper.pdf",
        "aff": "UC Berkeley; Microsoft Research; UC Berkeley",
        "project": "",
        "github": "https://github.com/gkioxari/RstarCNN",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1331511,
        "gs_citation": 540,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2293931322701475482&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": "berkeley.edu;microsoft.com;berkeley.edu",
        "email": "berkeley.edu;microsoft.com;berkeley.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Gkioxari_Contextual_Action_Recognition_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of California, Berkeley;Microsoft Corporation",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.berkeley.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "UC Berkeley;MSR",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Gkioxari_2015_ICCV,\n    \n    author = {\n    Gkioxari,\n    Georgia and Girshick,\n    Ross and Malik,\n    Jitendra\n},\n    title = {\n    Contextual Action Recognition With R*CNN\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "5bfe422faa",
        "title": "Continuous Pose Estimation With a Spatial Ensemble of Fisher Regressors",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Michele Fenzi, Laura Leal-Taixé, Jörn Ostermann, Tinne Tuytelaars",
        "author": "Michele Fenzi; Laura Leal-Taixe; Jorn Ostermann; Tinne Tuytelaars",
        "abstract": "In this paper, we treat the problem of continuous pose estimation for object categories as a regression problem on the basis of only 2D training information. While regression is a natural framework for continuous problems, regression methods so far achieved inferior results with respect to 3D-based and 2D-based classification-and-refinement approaches. This may be attributed to their weakness to high intra-class variability as well as to noisy matching procedures and lack of geometrical constraints.  We propose to apply regression to Fisher-encoded vectors computed from large cells by learning an array of Fisher regressors. Fisher encoding makes our algorithm flexible to variations in class appearance, while the array structure permits to indirectly introduce spatial context information in the approach. We formulate our problem as a MAP inference problem, where the likelihood function is composed of a generative term based on the prediction error generated by the ensemble of Fisher regressors as well as a discriminative term based on SVM classifiers.  We test our algorithm on three publicly available datasets that envisage several difficulties, such as high intra-class variability, truncations, occlusions, and motion blur, obtaining state-of-the-art results.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Fenzi_Continuous_Pose_Estimation_ICCV_2015_paper.pdf",
        "aff": "Institut für Informationsverarbeitung (TNT), Leibniz Universität Hannover; Institute of Geodesy and Photogrammetry, ETH Zurich; Institut für Informationsverarbeitung (TNT), Leibniz Universität Hannover; KU Leuven, ESAT - PSI, iMinds",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 765331,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2285533374396119513&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "tnt.uni-hannover.de;geod.baug.ethz.ch;tnt.uni-hannover.de;esat.kuleuven.be",
        "email": "tnt.uni-hannover.de;geod.baug.ethz.ch;tnt.uni-hannover.de;esat.kuleuven.be",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Fenzi_Continuous_Pose_Estimation_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;0;2",
        "aff_unique_norm": "Leibniz Universität Hannover;ETH Zurich;KU Leuven",
        "aff_unique_dep": "Institut für Informationsverarbeitung (TNT);Institute of Geodesy and Photogrammetry;ESAT - PSI",
        "aff_unique_url": "https://www.uni-hannover.de;https://www.ethz.ch;https://www.kuleuven.be",
        "aff_unique_abbr": "LUH;ETHZ;KU Leuven",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;2",
        "aff_country_unique": "Germany;Switzerland;Belgium",
        "bibtex": "@InProceedings{Fenzi_2015_ICCV,\n    \n    author = {\n    Fenzi,\n    Michele and Leal-Taixe,\n    Laura and Ostermann,\n    Jorn and Tuytelaars,\n    Tinne\n},\n    title = {\n    Continuous Pose Estimation With a Spatial Ensemble of Fisher Regressors\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "395e74db9a",
        "title": "Contour Box: Rejecting Object Proposals Without Explicit Closed Contours",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Cewu Lu, Shu Liu, Jiaya Jia, Chi-Keung Tang",
        "author": "Cewu Lu; Shu Liu; Jiaya Jia; Chi-Keung Tang",
        "abstract": "Closed contour is an important objectness indicator. We propose a new measure subject to the completeness and tightness constraints, where the optimized closed contour should be tightly bounded within an object proposal. The closed contour measure is defined using closed path integral, and we solve the optimization problem efficiently in polar coordinate system with a global optimum guaranteed. Extensive experiments show that our method can reject a large number of false proposals, and achieve over 6% improvement in object recall at the challenging overlap threshold 0.8 on the VOC 2007 test dataset.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Lu_Contour_Box_Rejecting_ICCV_2015_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1834469,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3956097735383063398&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Lu_Contour_Box_Rejecting_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Lu_2015_ICCV,\n    \n    author = {\n    Lu,\n    Cewu and Liu,\n    Shu and Jia,\n    Jiaya and Tang,\n    Chi-Keung\n},\n    title = {\n    Contour Box: Rejecting Object Proposals Without Explicit Closed Contours\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "6a37d9da4d",
        "title": "Contour Detection and Characterization for Asynchronous Event Sensors",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Francisco Barranco, Ching L. Teo, Cornelia Fermüller, Yiannis Aloimonos",
        "author": "Francisco Barranco; Ching L. Teo; Cornelia Fermuller; Yiannis Aloimonos",
        "abstract": "The bio-inspired, asynchronous event-based dynamic vision sensor records temporal changes in the luminance of the scene at high temporal resolution. Since events are only triggered at significant luminance changes, most events occur at the boundary of objects and their parts. The detection of these contours is an essential step for further interpretation of the scene. This paper presents an approach to learn the location of contours and their border ownership using Structured Random Forests on event-based features that encode motion, timing, texture, and spatial orientations. The classifier integrates elegantly information over time by utilizing the classification results previously computed. Finally, the contour detection and boundary assignment are demonstrated in a layer-segmentation of the scene. Experimental results demonstrate good performance in boundary detection and segmentation.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Barranco_Contour_Detection_and_ICCV_2015_paper.pdf",
        "aff": "Computer Vision Lab, University of Maryland (USA)+CITIC, University of Granada (Spain); Computer Vision Lab, University of Maryland (USA); Computer Vision Lab, University of Maryland (USA); Computer Vision Lab, University of Maryland (USA)",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3790547,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5553184782955918473&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "umiacs.umd.edu;umiacs.umd.edu;umiacs.umd.edu;umiacs.umd.edu",
        "email": "umiacs.umd.edu;umiacs.umd.edu;umiacs.umd.edu;umiacs.umd.edu",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Barranco_Contour_Detection_and_ICCV_2015_paper.html",
        "aff_unique_index": "0+1;0;0;0",
        "aff_unique_norm": "University of Maryland;University of Granada",
        "aff_unique_dep": "Computer Vision Lab;CITIC",
        "aff_unique_url": "https://www.umd.edu;https://www.ugr.es",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0;0;0",
        "aff_country_unique": "United States;Spain",
        "bibtex": "@InProceedings{Barranco_2015_ICCV,\n    \n    author = {\n    Barranco,\n    Francisco and Teo,\n    Ching L. and Fermuller,\n    Cornelia and Aloimonos,\n    Yiannis\n},\n    title = {\n    Contour Detection and Characterization for Asynchronous Event Sensors\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "bc70b8c790",
        "title": "Contour Flow: Middle-Level Motion Estimation by Combining Motion Segmentation and Contour Alignment",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Huijun Di, Qingxuan Shi, Feng Lv, Ming Qin, Yao Lu",
        "author": "Huijun Di; Qingxuan Shi; Feng Lv; Ming Qin; Yao Lu",
        "abstract": "Our goal is to estimate contour flow (the contour pairs with consistent point correspondence) from inconsistent contours extracted independently in two video frames. We formulate the contour flow estimation locally as a motion segmentation problem where motion patterns grouped from optical flow field are exploited for local correspondence measurement. To solve local ambiguities, contour flow estimation is further formulated globally as a contour alignment problem. We propose a novel two-staged strategy to obtain global consistent point correspondence under various contour transitions such as splitting, merging and branching. The goal of the first stage is to obtain possible accurate contour-to-contour alignments, and the second stage aims to make a consistent fusion of many partial alignments. Such a strategy can properly balance the accuracy and the consistency, which enables a middle-level motion representation to be constructed by just concatenating frame-by-frame contour flow estimation. Experiments prove the effectiveness of our method.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Di_Contour_Flow_Middle-Level_ICCV_2015_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1666663,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16012942782435118269&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Di_Contour_Flow_Middle-Level_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Di_2015_ICCV,\n    \n    author = {\n    Di,\n    Huijun and Shi,\n    Qingxuan and Lv,\n    Feng and Qin,\n    Ming and Lu,\n    Yao\n},\n    title = {\n    Contour Flow: Middle-Level Motion Estimation by Combining Motion Segmentation and Contour Alignment\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "a71bf885d3",
        "title": "Contour Guided Hierarchical Model for Shape Matching",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Yuanqi Su, Yuehu Liu, Bonan Cuan, Nanning Zheng",
        "author": "Yuanqi Su; Yuehu Liu; Bonan Cuan; Nanning Zheng",
        "abstract": "For its simplicity and effectiveness, star model is popular in shape matching. However, it suffers from the loose geometric connections among parts. In the paper, we present a novel algorithm that reconsiders these connections and reduces the global matching to a set of interrelated local matching. For the purpose, we divide the shape template into overlapped parts and model the matching through a part-based layered structure that uses the latent variable to constrain parts' deformation.  As for inference, each part is used for localizing candidates by the partial matching. Thanks to the contour fragments, the partial matching can be solved via modified dynamic programming. The overlapped regions among parts of the template are then explored to make the candidates of parts meet at their shared points. The process is fulfilled via a refined procedure based on iterative dynamic programming. Results on ETHZ shape and Inria Horse datasets demonstrate the benefits of the proposed algorithm.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Su_Contour_Guided_Hierarchical_ICCV_2015_paper.pdf",
        "aff": "Xi’an Jiaotong University, Xi’an, Shaanxi Province, China, 710049; Xi’an Jiaotong University, Xi’an, Shaanxi Province, China, 710049; Xi’an Jiaotong University, Xi’an, Shaanxi Province, China, 710049; Xi’an Jiaotong University, Xi’an, Shaanxi Province, China, 710049",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1375424,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10375997241890466903&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "mail.xjtu.edu.cn;mail.xjtu.edu.cn;gmail.com;mail.xjtu.edu.cn",
        "email": "mail.xjtu.edu.cn;mail.xjtu.edu.cn;gmail.com;mail.xjtu.edu.cn",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Su_Contour_Guided_Hierarchical_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Xi'an Jiaotong University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.xjtu.edu.cn",
        "aff_unique_abbr": "XJTU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Xi'an",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Su_2015_ICCV,\n    \n    author = {\n    Su,\n    Yuanqi and Liu,\n    Yuehu and Cuan,\n    Bonan and Zheng,\n    Nanning\n},\n    title = {\n    Contour Guided Hierarchical Model for Shape Matching\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "5e2eff1a0f",
        "title": "Contractive Rectifier Networks for Nonlinear Maximum Margin Classification",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Senjian An, Munawar Hayat, Salman H. Khan, Mohammed Bennamoun, Farid Boussaid, Ferdous Sohel",
        "author": "Senjian An; Munawar Hayat; Salman H. Khan; Mohammed Bennamoun; Farid Boussaid; Ferdous Sohel",
        "abstract": "To find the optimal nonlinear separating boundary with maximum margin in the input data space, this paper proposes Contractive Rectifier Networks (CRNs), wherein the hidden-layer transformations are restricted to be contraction mappings. The contractive constraints ensure that the achieved separating margin in the input space is larger than or equal to the separating margin in the output layer. The training of the proposed CRNs is formulated as a linear support vector machine (SVM) in the output layer, combined with two or more contractive hidden layers. Effective algorithms have been proposed to address the optimization challenges arising from contraction constraints. Experimental results on MNIST, CIFAR-10, CIFAR-100 and MIT-67 datasets demonstrate that the proposed contractive rectifier networks consistently outperform their conventional unconstrained rectifier network counterparts.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/An_Contractive_Rectifier_Networks_ICCV_2015_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 490687,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=855022379098388236&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/An_Contractive_Rectifier_Networks_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{An_2015_ICCV,\n    \n    author = {\n    An,\n    Senjian and Hayat,\n    Munawar and Khan,\n    Salman H. and Bennamoun,\n    Mohammed and Boussaid,\n    Farid and Sohel,\n    Ferdous\n},\n    title = {\n    Contractive Rectifier Networks for Nonlinear Maximum Margin Classification\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "fb68f7c39c",
        "title": "Convex Optimization With Abstract Linear Operators",
        "session": "plenary session",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Steven Diamond, Stephen Boyd",
        "author": "Steven Diamond; Stephen Boyd",
        "abstract": "We introduce a convex optimization modeling framework that transforms a convex optimization problem expressed in a form natural and convenient for the user into an equivalent cone program in a way that preserves fast linear transforms in the original problem. By representing linear functions in the transformation process not as matrices, but as graphs that encode composition of abstract linear operators, we arrive at a matrix-free cone program, i.e., one whose data matrix is represented by an abstract linear operator and its adjoint. This cone program can then be solved by a matrix-free cone solver. By combining the matrix-free modeling framework and cone solver, we obtain a general method for efficiently solving convex optimization problems involving fast linear transforms.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Diamond_Convex_Optimization_With_ICCV_2015_paper.pdf",
        "aff": "Dept. of Computer Science and Electrical Engineering, Stanford University; Dept. of Computer Science and Electrical Engineering, Stanford University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 524005,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14825983002578096112&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "stanford.edu;stanford.edu",
        "email": "stanford.edu;stanford.edu",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Diamond_Convex_Optimization_With_ICCV_2015_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Dept. of Computer Science and Electrical Engineering",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Diamond_2015_ICCV,\n    \n    author = {\n    Diamond,\n    Steven and Boyd,\n    Stephen\n},\n    title = {\n    Convex Optimization With Abstract Linear Operators\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "cba37f2d28",
        "title": "Convolutional Channel Features",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Bin Yang, Junjie Yan, Zhen Lei, Stan Z. Li",
        "author": "Bin Yang; Junjie Yan; Zhen Lei; Stan Z. Li",
        "abstract": "Deep learning methods are powerful tools but often suffer from expensive computation and limited flexibility. An alternative is to combine light-weight models with deep representations. As successful cases exist in several visual problems, a unified framework is absent. In this paper, we revisit two widely used approaches in computer vision, namely filtered channel features and Convolutional Neural Networks (CNN), and absorb merits from both by proposing an integrated method called Convolutional Channel Features (CCF). CCF transfers low-level features from pre-trained CNN models to feed the boosting forest model. With the combination of CNN features and boosting forest, CCF benefits from the richer capacity in feature representation compared with channel features, as well as lower cost in computation and storage compared with end-to-end CNN methods. We show that CCF serves as a good way of tailoring pre-trained CNN models to diverse tasks without fine-tuning the whole network to each task by achieving state-of-the-art performances in pedestrian detection, face detection, edge detection and object proposal generation.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Yang_Convolutional_Channel_Features_ICCV_2015_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 927166,
        "gs_citation": 403,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15348182900133754024&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Yang_Convolutional_Channel_Features_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Yang_2015_ICCV,\n    \n    author = {\n    Yang,\n    Bin and Yan,\n    Junjie and Lei,\n    Zhen and Li,\n    Stan Z.\n},\n    title = {\n    Convolutional Channel Features\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "25e11ddfbb",
        "title": "Convolutional Color Constancy",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Jonathan T. Barron",
        "author": "Jonathan T. Barron",
        "abstract": "Color constancy is the problem of inferring the color of the light that illuminated a scene, usually so that the illumination color can be removed. Because this problem is underconstrained, it is often solved by modeling the statistical regularities of the colors of natural objects and illumination. In contrast, in this paper we reformulate the problem of color constancy as a 2D spatial localization task in a log-chrominance space, thereby allowing us to apply techniques from object detection and structured prediction to the color constancy problem. By directly learning how to discriminate between correctly white-balanced images and poorly white-balanced images, our model is able to improve performance on standard benchmarks by nearly 40%.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Barron_Convolutional_Color_Constancy_ICCV_2015_paper.pdf",
        "aff": "",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 766208,
        "gs_citation": 277,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3751446278641517737&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff_domain": "google.com",
        "email": "google.com",
        "author_num": 1,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Barron_Convolutional_Color_Constancy_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Barron_2015_ICCV,\n    \n    author = {\n    Barron,\n    Jonathan T.\n},\n    title = {\n    Convolutional Color Constancy\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "6b8dab419a",
        "title": "Convolutional Sparse Coding for Image Super-Resolution",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Shuhang Gu, Wangmeng Zuo, Qi Xie, Deyu Meng, Xiangchu Feng, Lei Zhang",
        "author": "Shuhang Gu; Wangmeng Zuo; Qi Xie; Deyu Meng; Xiangchu Feng; Lei Zhang",
        "abstract": "Sparse coding (SC) plays an important role in versatile computer vision applications such as image super-resolution (SR). Most of the previous SC based SR methods partition the image into overlapped patches, and process each patch separately. These methods, however, ignore the consistency of pixels in overlapped patches, which is a strong constraint for image reconstruction. In this paper, we propose a convolutional sparse coding (CSC) based SR (CSC-SR) method to address the consistency issue. Our CSC-SR involves three groups of parameters to be learned: (i) a set of filters to decompose the low resolution (LR) image into LR sparse feature maps; (ii) a  mapping function to predict the high resolution (HR) feature maps from the LR ones; and (iii) a set of filters to reconstruct the HR images from the predicted HR feature maps via simple convolution operations. By working directly on the whole image, the proposed CSC-SR algorithm does not need to divide the image into overlapped patches, and can exploit the image global correlation to produce more robust reconstruction of image local structures. Experimental results clearly validate the advantages of CSC over patch based SC in SR application. Compared with state-of-the-art SR methods, the proposed CSC-SR method achieves highly competitive PSNR results, while demonstrating better edge and texture preservation performance.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Gu_Convolutional_Sparse_Coding_ICCV_2015_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1249926,
        "gs_citation": 448,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18252130550804750214&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Gu_Convolutional_Sparse_Coding_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Gu_2015_ICCV,\n    \n    author = {\n    Gu,\n    Shuhang and Zuo,\n    Wangmeng and Xie,\n    Qi and Meng,\n    Deyu and Feng,\n    Xiangchu and Zhang,\n    Lei\n},\n    title = {\n    Convolutional Sparse Coding for Image Super-Resolution\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "2edbfbb8ba",
        "title": "Cross-Domain Image Retrieval With a Dual Attribute-Aware Ranking Network",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Junshi Huang, Rogerio S. Feris, Qiang Chen, Shuicheng Yan",
        "author": "Junshi Huang; Rogerio S. Feris; Qiang Chen; Shuicheng Yan",
        "abstract": "We address the problem of cross-domain image retrieval, considering the following practical application: given a user photo depicting a clothing image, our goal is to retrieve the same or attribute-similar clothing items from online shopping stores. This is a challenging problem due to the large discrepancy between online shopping images, usually taken in ideal lighting/pose/background conditions, and user photos captured in uncontrolled conditions. To address this problem, we propose a Dual Attribute-aware Ranking Network (DARN) for retrieval feature learning. More specifically, DARN consists of two sub-networks, one for each domain, whose retrieval feature representations are driven by semantic attribute learning. We show that this attribute-guided learning is a key factor for retrieval accuracy improvement. In addition, to further align with the nature of the retrieval problem, we impose a triplet visual similarity constraint for learning to rank across the two subnetworks. Another contribution of our work is a large-scale dataset which makes the network learning feasible. We exploit customer review websites to crawl a large set of online shopping images and corresponding offline user photos with fine-grained clothing attributes, i.e., around 450,000 online shopping images and about 90,000 exact offline counterpart images of those online ones. All these images are collected from real-world consumer websites reflecting the diversity of the data modality, which makes this dataset unique and rare in the academic community. We extensively evaluate the retrieval performance of networks in different configurations. The top-20 retrieval accuracy is doubled when using the proposed DARN other than the current popular solution using pre-trained CNN features only (0.570 vs. 0.268).",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Huang_Cross-Domain_Image_Retrieval_ICCV_2015_paper.pdf",
        "aff": "National University of Singapore; IBM T.J. Watson Research Center; IBM Research, Australia; National University of Singapore",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 744492,
        "gs_citation": 541,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16812097667139377249&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff_domain": "nus.edu.sg;nus.edu.sg;us.ibm.com;au1.ibm.com",
        "email": "nus.edu.sg;nus.edu.sg;us.ibm.com;au1.ibm.com",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Huang_Cross-Domain_Image_Retrieval_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "National University of Singapore;IBM;IBM Research",
        "aff_unique_dep": ";Research Center;",
        "aff_unique_url": "https://www.nus.edu.sg;https://www.ibm.com/research/watson;https://www.ibm.com/research",
        "aff_unique_abbr": "NUS;IBM;IBM",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";T.J. Watson",
        "aff_country_unique_index": "0;1;2;0",
        "aff_country_unique": "Singapore;United States;Australia",
        "bibtex": "@InProceedings{Huang_2015_ICCV,\n    \n    author = {\n    Huang,\n    Junshi and Feris,\n    Rogerio S. and Chen,\n    Qiang and Yan,\n    Shuicheng\n},\n    title = {\n    Cross-Domain Image Retrieval With a Dual Attribute-Aware Ranking Network\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "87b975018e",
        "title": "Cutting Edge: Soft Correspondences in Multimodal Scene Parsing",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Sarah Taghavi Namin, Mohammad Najafi, Mathieu Salzmann, Lars Petersson",
        "author": "Sarah Taghavi Namin; Mohammad Najafi; Mathieu Salzmann; Lars Petersson",
        "abstract": "Exploiting multiple modalities for semantic scene parsing has been shown to improve accuracy over the single modality scenario. Existing methods, however, assume that corresponding regions in two modalities have the same label. In this paper, we address the problem of data misalignment and label inconsistencies, e.g., due to moving objects, in semantic labeling, which violate the assumption of existing techniques. To this end, we formulate multimodal semantic labeling as inference in a CRF, and introduce latent nodes to explicitly model inconsistencies between two domains. These latent nodes allow us not only to leverage information from both domains to improve their labeling, but also to cut the edges between inconsistent regions. To eliminate the need for hand tuning the parameters of our model, we propose to learn intra-domain and inter-domain potential functions from training data. We demonstrate the benefits of our approach on two publicly available datasets containing 2D imagery and 3D point clouds. Thanks to our latent nodes and our learning strategy, our method outperforms the state-of-the-art in both cases.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Namin_Cutting_Edge_Soft_ICCV_2015_paper.pdf",
        "aff": "Australian National University (ANU) + NICTA; NICTA + CVLab, EPFL, Switzerland; CVLab, EPFL, Switzerland; Australian National University (ANU) + NICTA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1653803,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16135244527376167499&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "nicta.com.au;nicta.com.au;epfl.ch;nicta.com.au",
        "email": "nicta.com.au;nicta.com.au;epfl.ch;nicta.com.au",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Namin_Cutting_Edge_Soft_ICCV_2015_paper.html",
        "aff_unique_index": "0+1;1+2;2;0+1",
        "aff_unique_norm": "Australian National University;National Information and Communications Technology Australia;École Polytechnique Fédérale de Lausanne",
        "aff_unique_dep": ";;CVLab",
        "aff_unique_url": "https://www.anu.edu.au;https://www.nicta.com.au;https://cvlab.epfl.ch",
        "aff_unique_abbr": "ANU;NICTA;EPFL",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+1;1;0+0",
        "aff_country_unique": "Australia;Switzerland",
        "bibtex": "@InProceedings{Namin_2015_ICCV,\n    \n    author = {\n    Namin,\n    Sarah Taghavi and Najafi,\n    Mohammad and Salzmann,\n    Mathieu and Petersson,\n    Lars\n},\n    title = {\n    Cutting Edge: Soft Correspondences in Multimodal Scene Parsing\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "8e58f1be17",
        "title": "Deep Colorization",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Zezhou Cheng, Qingxiong Yang, Bin Sheng",
        "author": "Zezhou Cheng; Qingxiong Yang; Bin Sheng",
        "abstract": "This paper investigates into the colorization problem which converts a grayscale image to a colorful version. This is a very difficult problem and normally requires manual adjustment to achieve artifact-free quality. For instance, it normally requires human-labelled color scribbles on the grayscale target image or a careful selection of colorful reference images (e.g., capturing the same scene in the grayscale target image). Unlike the previous methods, this paper aims at a high-quality fully-automatic colorization method. With the assumption of a perfect patch matching technique, the use of an extremely large-scale reference database (that contains sufficient color images) is the most reliable solution to the colorization problem. However, patch matching noise will increase with respect to the size of the reference database in practice. Inspired by the recent success in deep learning techniques which provide amazing modeling of large-scale data, this paper re-formulates the colorization problem so that deep learning techniques can be directly employed. To ensure artifact-free quality, a joint bilateral filtering based post-processing step is proposed. Numerous experiments demonstrate that our method outperforms the state-of-art algorithms both in terms of quality and speed.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Cheng_Deep_Colorization_ICCV_2015_paper.pdf",
        "aff": "Shanghai Jiao Tong University; City University of Hong Kong; Shanghai Jiao Tong University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2103413,
        "gs_citation": 821,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12455232730149289375&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": "sjtu.edu.cn;cityu.edu.hk;sjtu.edu.cn",
        "email": "sjtu.edu.cn;cityu.edu.hk;sjtu.edu.cn",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Cheng_Deep_Colorization_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Shanghai Jiao Tong University;City University of Hong Kong",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.sjtu.edu.cn;https://www.cityu.edu.hk",
        "aff_unique_abbr": "SJTU;CityU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Cheng_2015_ICCV,\n    \n    author = {\n    Cheng,\n    Zezhou and Yang,\n    Qingxiong and Sheng,\n    Bin\n},\n    title = {\n    Deep Colorization\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "798d04cb72",
        "title": "Deep Fried Convnets",
        "session": "statistical methods and learning",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "Zichao Yang, Marcin Moczulski, Misha Denil, Nando de Freitas, Alex Smola, Le Song, Ziyu Wang",
        "author": "Zichao Yang; Marcin Moczulski; Misha Denil; Nando de Freitas; Alex Smola; Le Song; Ziyu Wang",
        "abstract": "The fully connected layers of a deep convolutional neural network typically contain over 90% of the network parameters, and consume the majority of the memory required to store the network. Reducing the number of parameters while preserving predictive performance is critically important for deploying deep neural networks in memory constrained environments such as GPUs or embedded devices. In this paper we show how kernel methods, in particular a single Fastfood layer, can be used to replace the fully connected layers in a deep convolutional neural network. This deep fried network is end-to-end trainable in conjunction with convolutional layers. Our new architecture substantially reduces the memory footprint of convolutional networks trained on MNIST and ImageNet with no drop in predictive performance",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Yang_Deep_Fried_Convnets_ICCV_2015_paper.pdf",
        "aff": ";;;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 381736,
        "gs_citation": 364,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5084451682192638940&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "author_num": 7,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Yang_Deep_Fried_Convnets_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Yang_2015_ICCV,\n    \n    author = {\n    Yang,\n    Zichao and Moczulski,\n    Marcin and Denil,\n    Misha and de Freitas,\n    Nando and Smola,\n    Alex and Song,\n    Le and Wang,\n    Ziyu\n},\n    title = {\n    Deep Fried Convnets\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "d4f32ca648",
        "title": "Deep Learning Face Attributes in the Wild",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Ziwei Liu, Ping Luo, Xiaogang Wang, Xiaoou Tang",
        "author": "Ziwei Liu; Ping Luo; Xiaogang Wang; Xiaoou Tang",
        "abstract": "Predicting face attributes in the wild is challenging due to complex face variations. We propose a novel deep learning framework for attribute prediction in the wild. It cascades two CNNs, LNet and ANet, which are fine-tuned jointly with attribute tags, but pre-trained differently. LNet is pre-trained by massive general object categories for face localization, while ANet is pre-trained by massive face identities for attribute prediction. This framework not only outperforms the state-of-the-art with a large margin, but also reveals valuable facts on learning face representation. (1) It shows how the performances of face localization (LNet) and attribute prediction (ANet) can be improved by different pre-training strategies. (2) It reveals that although the filters of LNet are fine-tuned only with image-level attribute tags, their response maps over entire images have strong indication of face locations. This fact enables training LNet for face localization with only image-level annotations, but without face bounding boxes or landmarks, which are required by all attribute recognition works. (3) It also demonstrates that the high-level hidden neurons of ANet automatically discover semantic concepts after pre-training with massive face identities, and such concepts are significantly enriched after fine-tuning with attribute tags. Each attribute can be well explained with a sparse linear combination of these concepts.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Liu_Deep_Learning_Face_ICCV_2015_paper.pdf",
        "aff": "Department of Information Engineering, The Chinese University of Hong Kong + Shenzhen Key Lab of Comp. Vis. & Pat. Rec., Shenzhen Institutes of Advanced Technology, CAS, China; Department of Information Engineering, The Chinese University of Hong Kong + Shenzhen Key Lab of Comp. Vis. & Pat. Rec., Shenzhen Institutes of Advanced Technology, CAS, China; Department of Electronic Engineering, The Chinese University of Hong Kong + Shenzhen Key Lab of Comp. Vis. & Pat. Rec., Shenzhen Institutes of Advanced Technology, CAS, China; Department of Information Engineering, The Chinese University of Hong Kong + Shenzhen Key Lab of Comp. Vis. & Pat. Rec., Shenzhen Institutes of Advanced Technology, CAS, China",
        "project": "http://personal.ie.cuhk.edu.hk/~lz013/projects/FaceAttributes.html",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2402765,
        "gs_citation": 10579,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9137261784815578205&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 27,
        "aff_domain": "ie.cuhk.edu.hk;ie.cuhk.edu.hk;ee.cuhk.edu.hk;ie.cuhk.edu.hk",
        "email": "ie.cuhk.edu.hk;ie.cuhk.edu.hk;ee.cuhk.edu.hk;ie.cuhk.edu.hk",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Liu_Deep_Learning_Face_ICCV_2015_paper.html",
        "aff_unique_index": "0+1;0+1;0+1;0+1",
        "aff_unique_norm": "The Chinese University of Hong Kong;Shenzhen Institutes of Advanced Technology",
        "aff_unique_dep": "Department of Information Engineering;Shenzhen Key Lab of Comp. Vis. & Pat. Rec.",
        "aff_unique_url": "https://www.cuhk.edu.hk;http://www.siat.ac.cn",
        "aff_unique_abbr": "CUHK;SIAT",
        "aff_campus_unique_index": "0+1;0+1;0+1;0+1",
        "aff_campus_unique": "Hong Kong SAR;Shenzhen",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Liu_2015_ICCV,\n    \n    author = {\n    Liu,\n    Ziwei and Luo,\n    Ping and Wang,\n    Xiaogang and Tang,\n    Xiaoou\n},\n    title = {\n    Deep Learning Face Attributes in the Wild\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "0ac611db7f",
        "title": "Deep Learning Strong Parts for Pedestrian Detection",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Yonglong Tian, Ping Luo, Xiaogang Wang, Xiaoou Tang",
        "author": "Yonglong Tian; Ping Luo; Xiaogang Wang; Xiaoou Tang",
        "abstract": "Recent advances in pedestrian detection are attained by transferring the learned features of Convolutional Neural Network (ConvNet) to pedestrians. This ConvNet is typically pre-trained with massive general object categories (e.g. ImageNet). Although these features are able to handle variations such as poses, viewpoints, and lightings, they may fail when pedestrian images with complex occlusions are present. Occlusion handling is one of the most important problem in pedestrian detection. Unlike previous deep models that directly learned a single detector for pedestrian detection, we propose DeepParts, which consists of extensive part detectors. DeepParts has several appealing properties. First, DeepParts can be trained on weakly labeled data, i.e. only pedestrian bounding boxes without part annotations are provided. Second, DeepParts is able to handle low IoU positive proposals that shift away from ground truth. Third, each part detector in DeepParts is a strong detector that can detect pedestrian by observing only a part of a proposal. Extensive experiments in Caltech dataset demonstrate the effectiveness of DeepParts, which yields a new state-of-the-art miss rate of 11:89%, outperforming the second best method by 10%.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Tian_Deep_Learning_Strong_ICCV_2015_paper.pdf",
        "aff": "Department of Information Engineering, The Chinese University of Hong Kong; Department of Information Engineering, The Chinese University of Hong Kong + Department of Electronic Engineering, The Chinese University of Hong Kong; Department of Electronic Engineering, The Chinese University of Hong Kong + Shenzhen Key Lab of Comp. Vis. & Pat. Rec., Shenzhen Institutes of Advanced Technology, CAS, China; Department of Information Engineering, The Chinese University of Hong Kong + Shenzhen Key Lab of Comp. Vis. & Pat. Rec., Shenzhen Institutes of Advanced Technology, CAS, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1244036,
        "gs_citation": 653,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7740499673489636663&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff_domain": "ie.cuhk.edu.hk;ie.cuhk.edu.hk;ee.cuhk.edu.hk;ie.cuhk.edu.hk",
        "email": "ie.cuhk.edu.hk;ie.cuhk.edu.hk;ee.cuhk.edu.hk;ie.cuhk.edu.hk",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Tian_Deep_Learning_Strong_ICCV_2015_paper.html",
        "aff_unique_index": "0;0+0;0+1;0+1",
        "aff_unique_norm": "The Chinese University of Hong Kong;Shenzhen Institutes of Advanced Technology",
        "aff_unique_dep": "Department of Information Engineering;Shenzhen Key Lab of Comp. Vis. & Pat. Rec.",
        "aff_unique_url": "https://www.cuhk.edu.hk;http://www.siat.ac.cn",
        "aff_unique_abbr": "CUHK;SIAT",
        "aff_campus_unique_index": "0;0+0;0+1;0+1",
        "aff_campus_unique": "Hong Kong SAR;Shenzhen",
        "aff_country_unique_index": "0;0+0;0+0;0+0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Tian_2015_ICCV,\n    \n    author = {\n    Tian,\n    Yonglong and Luo,\n    Ping and Wang,\n    Xiaogang and Tang,\n    Xiaoou\n},\n    title = {\n    Deep Learning Strong Parts for Pedestrian Detection\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "77e5bb4b50",
        "title": "Deep Multi-Patch Aggregation Network for Image Style, Aesthetics, and Quality Estimation",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Xin Lu, Zhe Lin, Xiaohui Shen, Radomír Měch, James Z. Wang",
        "author": "Xin Lu; Zhe Lin; Xiaohui Shen; Radomir Mech; James Z. Wang",
        "abstract": "This paper investigates problems of image style, aesthetics, and quality estimation, which require fine-grained details from high-resolution images, utilizing deep neural network training approach. Existing deep convolutional neural networks mostly extracted one patch such as a down-sized crop from each image as a training example. However, one patch may not always well represent the entire image, which may cause ambiguity during training. We propose a deep multi-patch aggregation network training approach, which allows us to train models using multiple patches generated from one image. We achieve this by constructing multiple, shared columns in the neural network and feeding multiple patches to each of the columns. More importantly, we propose two novel network layers (statistics and sorting) to support aggregation of those patches. The proposed deep multi-patch aggregation network integrates shared feature learning and aggregation function learning into a unified framework. We demonstrate the effectiveness of the deep multi-patch aggregation network on the three problems, i.e., image style recognition, aesthetic quality categorization, and image quality estimation. Our models trained using the proposed networks significantly outperformed the state of the art in all three applications.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Lu_Deep_Multi-Patch_Aggregation_ICCV_2015_paper.pdf",
        "aff": "The Pennsylvania State University, University Park, Pennsylvania + Adobe Research, San Jose, California; Adobe Research, San Jose, California; Adobe Research, San Jose, California; Adobe Research, San Jose, California; The Pennsylvania State University, University Park, Pennsylvania + Adobe Research, San Jose, California",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 955558,
        "gs_citation": 399,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14080119512446876506&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "psu.edu;adobe.com;adobe.com;adobe.com;psu.edu",
        "email": "psu.edu;adobe.com;adobe.com;adobe.com;psu.edu",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Lu_Deep_Multi-Patch_Aggregation_ICCV_2015_paper.html",
        "aff_unique_index": "0+1;1;1;1;0+1",
        "aff_unique_norm": "The Pennsylvania State University;Adobe Research",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.psu.edu;https://research.adobe.com",
        "aff_unique_abbr": "PSU;Adobe",
        "aff_campus_unique_index": "0+1;1;1;1;0+1",
        "aff_campus_unique": "University Park;San Jose",
        "aff_country_unique_index": "0+0;0;0;0;0+0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Lu_2015_ICCV,\n    \n    author = {\n    Lu,\n    Xin and Lin,\n    Zhe and Shen,\n    Xiaohui and Mech,\n    Radomir and Wang,\n    James Z.\n},\n    title = {\n    Deep Multi-Patch Aggregation Network for Image Style,\n    Aesthetics,\n    and Quality Estimation\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "d40810f8f3",
        "title": "Deep Networks for Image Super-Resolution With Sparse Prior",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Zhaowen Wang, Ding Liu, Jianchao Yang, Wei Han, Thomas Huang",
        "author": "Zhaowen Wang; Ding Liu; Jianchao Yang; Wei Han; Thomas Huang",
        "abstract": "Deep learning techniques have been successfully applied in many areas of computer vision, including low-level image restoration problems. For image super-resolution, several models based on deep neural networks have been recently proposed and attained superior performance that overshadows all previous handcrafted models. The question then arises whether large-capacity and data-driven models have become the dominant solution to the ill-posed super-resolution problem. In this paper, we argue that domain expertise represented by the conventional sparse coding model is still valuable, and it can be combined with the key ingredients of deep learning to achieve further improved results. We show that a sparse coding model particularly designed for super-resolution can be incarnated as a neural network, and trained in a cascaded structure from end to end. The interpretation of the network based on sparse coding leads to much more efficient and effective training, as well as a reduced model size. Our model is evaluated on a wide range of images, and shows clear advantage over existing state-of-the-art methods in terms of both restoration accuracy and human subjective quality.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Wang_Deep_Networks_for_ICCV_2015_paper.pdf",
        "aff": "Beckman Institute, University of Illinois at Urbana-Champaign; Beckman Institute, University of Illinois at Urbana-Champaign; Snapchat, Venice, CA; Beckman Institute, University of Illinois at Urbana-Champaign; Beckman Institute, University of Illinois at Urbana-Champaign",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1209901,
        "gs_citation": 1001,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16776231602849831052&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 13,
        "aff_domain": "adobe.com;ifp.uiuc.edu;gmail.com;ifp.uiuc.edu;ifp.uiuc.edu",
        "email": "adobe.com;ifp.uiuc.edu;gmail.com;ifp.uiuc.edu;ifp.uiuc.edu",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Wang_Deep_Networks_for_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "University of Illinois at Urbana-Champaign;Snapchat",
        "aff_unique_dep": "Beckman Institute;",
        "aff_unique_url": "https://www.illinois.edu;https://www.snapchat.com",
        "aff_unique_abbr": "UIUC;Snapchat",
        "aff_campus_unique_index": "0;0;1;0;0",
        "aff_campus_unique": "Urbana-Champaign;Venice",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Wang_2015_ICCV,\n    \n    author = {\n    Wang,\n    Zhaowen and Liu,\n    Ding and Yang,\n    Jianchao and Han,\n    Wei and Huang,\n    Thomas\n},\n    title = {\n    Deep Networks for Image Super-Resolution With Sparse Prior\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "d437652886",
        "title": "Deep Neural Decision Forests",
        "session": "statistical methods and learning",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "Peter Kontschieder, Madalina Fiterau, Antonio Criminisi, Samuel Rota Bulò",
        "author": "Peter Kontschieder; Madalina Fiterau; Antonio Criminisi; Samuel Rota Bulo",
        "abstract": "We present Deep Neural Decision Forests - a novel approach that unifies classification trees with the representation learning functionality known from deep convolutional networks, by training them in an end-to-end manner. To combine these two worlds, we introduce a stochastic and differentiable decision tree model, which steers the representation learning usually conducted in the initial layers of a (deep) convolutional network. Our model differs from conventional deep networks because a decision forest provides the final predictions and it differs from conventional decision forests since we propose a principled, joint and global optimization of split and leaf node parameters. We show experimental results on benchmark machine learning datasets like MNIST and ImageNet and find on-par or superior results when compared to state-of-the-art deep models. Most remarkably, we obtain Top5-Errors of only 7.84%/6.38% on ImageNet validation data when integrating our forests in a single-crop, single/seven model GoogLeNet architecture, respectively. Thus, even without any form of training data set augmentation we are improving on the 6.67% error obtained by the best GoogLeNet architecture (7 models, 144 crops).",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Kontschieder_Deep_Neural_Decision_ICCV_2015_paper.pdf",
        "aff": "Microsoft Research1; Carnegie Mellon University2 + Microsoft Research1; Microsoft Research1; Fondazione Bruno Kessler3",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1408120,
        "gs_citation": 708,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2615348809059085741&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "microsoft.com;cmu.edu;microsoft.com;fbk.eu",
        "email": "microsoft.com;cmu.edu;microsoft.com;fbk.eu",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Kontschieder_Deep_Neural_Decision_ICCV_2015_paper.html",
        "aff_unique_index": "0;1+0;0;2",
        "aff_unique_norm": "Microsoft Research;Carnegie Mellon University;Fondazione Bruno Kessler",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.microsoft.com/en-us/research;https://www.cmu.edu;https://www.fbk.eu",
        "aff_unique_abbr": "MSR;CMU;FBK",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;1",
        "aff_country_unique": "United States;Italy",
        "bibtex": "@InProceedings{Kontschieder_2015_ICCV,\n    \n    author = {\n    Kontschieder,\n    Peter and Fiterau,\n    Madalina and Criminisi,\n    Antonio and Bulo,\n    Samuel Rota\n},\n    title = {\n    Deep Neural Decision Forests\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "223a9cbeac",
        "title": "DeepBox: Learning Objectness With Convolutional Networks",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Weicheng Kuo, Bharath Hariharan, Jitendra Malik",
        "author": "Weicheng Kuo; Bharath Hariharan; Jitendra Malik",
        "abstract": "Existing object proposal approaches use primarily bottom-up cues to rank proposals, while we believe that \"objectness\" is in fact a high level construct. We argue for a data-driven, semantic approach for ranking object proposals. Our framework, which we call DeepBox, uses convolutional neural networks (CNNs) to rerank proposals from a bottom-up method.  We use a novel four-layer CNN architecture that is as good as much larger networks on the task of evaluating objectness while being much faster. We show that DeepBox significantly improves over the bottom-up ranking, achieving the same recall with 500 proposals as achieved by bottom-up methods with 2000. This improvement generalizes to categories the CNN has never seen before and leads to a 4.5-point gain in detection mAP. Our implementation achieves this performance while running at 260 ms per image.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Kuo_DeepBox_Learning_Objectness_ICCV_2015_paper.pdf",
        "aff": "University of California, Berkeley; University of California, Berkeley; University of California, Berkeley",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1054858,
        "gs_citation": 228,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14372714509714539442&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu",
        "email": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Kuo_DeepBox_Learning_Objectness_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Kuo_2015_ICCV,\n    \n    author = {\n    Kuo,\n    Weicheng and Hariharan,\n    Bharath and Malik,\n    Jitendra\n},\n    title = {\n    DeepBox: Learning Objectness With Convolutional Networks\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "b445817a8d",
        "title": "DeepDriving: Learning Affordance for Direct Perception in Autonomous Driving",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Chenyi Chen, Ari Seff, Alain Kornhauser, Jianxiong Xiao",
        "author": "Chenyi Chen; Ari Seff; Alain Kornhauser; Jianxiong Xiao",
        "abstract": "Today, there are two major paradigms for vision-based autonomous driving systems: mediated perception approaches that parse an entire scene to make a driving decision, and behavior reflex approaches that directly map an input image to a driving action by a regressor. In this paper, we propose a third paradigm: a direct perception approach to estimate the affordance for driving. We propose to map an input image to a small number of key perception indicators that directly relate to the affordance of a road/traffic state for driving. Our representation provides a set of compact yet complete descriptions of the scene to enable a simple controller to drive autonomously. Falling in between the two extremes of mediated perception and behavior reflex, we argue that our direct perception representation provides the right level of abstraction. To demonstrate this, we train a deep Convolutional Neural Network using recording from 12 hours of human driving in a video game and show that our model can work well to drive a car in a very diverse set of virtual environments. We also train a model for car distance estimation on the KITTI dataset. Results show that our direct perception approach can generalize well to real driving images. Source code and data are available on our project website.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Chen_DeepDriving_Learning_Affordance_ICCV_2015_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1838725,
        "gs_citation": 2530,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6173219959812350972&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 25,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Chen_DeepDriving_Learning_Affordance_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Chen_2015_ICCV,\n    \n    author = {\n    Chen,\n    Chenyi and Seff,\n    Ari and Kornhauser,\n    Alain and Xiao,\n    Jianxiong\n},\n    title = {\n    DeepDriving: Learning Affordance for Direct Perception in Autonomous Driving\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "8a2539abb7",
        "title": "DeepProposal: Hunting Objects by Cascading Deep Convolutional Layers",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Amir Ghodrati, Ali Diba, Marco Pedersoli, Tinne Tuytelaars, Luc Van Gool",
        "author": "Amir Ghodrati; Ali Diba; Marco Pedersoli; Tinne Tuytelaars; Luc Van Gool",
        "abstract": "In this paper we evaluate the quality of the activation layers of a convolutional neural network (CNN) for the generation of object proposals. We generate hypotheses in a sliding-window fashion over different activation layers and show that the final convolutional layers can find the object of interest with high recall but poor localization due to the coarseness of the feature maps. Instead, the first layers of the network can better localize the object of interest but with a reduced recall. Based on this observation we design a method for proposing object locations that is based on CNN features and that combines the best of both worlds. We build an inverse cascade that, going from the final to the initial convolutional layers of the CNN, selects the most promising object locations and refines their boxes in a coarse-to-fine manner. The method is efficient, because i) it uses the same features extracted for detection, ii) it aggregates features using integral images, and iii) it avoids a dense evaluation of the proposals due to the inverse coarse-to-fine cascade. The method is also accurate; it outperforms most of the previously proposed object proposals approaches and when plugged into a CNN-based detector produces state-of-the-art detection performance.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Ghodrati_DeepProposal_Hunting_Objects_ICCV_2015_paper.pdf",
        "aff": "KU Leuven, ESAT-PSI, iMinds; KU Leuven, ESAT-PSI, iMinds; Inria+LEAR project, Inria Grenoble Rhone-Alpes, LJK, CNRS, Univ. Grenoble Alpes; KU Leuven, ESAT-PSI, iMinds; KU Leuven, ESAT-PSI, iMinds+CVL, ETH Zurich",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 481111,
        "gs_citation": 150,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12183960202381368225&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 20,
        "aff_domain": "esat.kuleuven.be;esat.kuleuven.be;inria.fr;esat.kuleuven.be;esat.kuleuven.be",
        "email": "esat.kuleuven.be;esat.kuleuven.be;inria.fr;esat.kuleuven.be;esat.kuleuven.be",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Ghodrati_DeepProposal_Hunting_Objects_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;1+2;0;0+3",
        "aff_unique_norm": "KU Leuven;Inria;Inria Grenoble Rhone-Alpes;ETH Zurich",
        "aff_unique_dep": "ESAT-PSI;;LEAR project, LJK;Computer Vision Laboratory",
        "aff_unique_url": "https://www.kuleuven.be;https://www.inria.fr;https://www.inria.fr/grenoble;https://www.ethz.ch",
        "aff_unique_abbr": "KU Leuven;Inria;Inria;ETHZ",
        "aff_campus_unique_index": "1;",
        "aff_campus_unique": ";Grenoble",
        "aff_country_unique_index": "0;0;1+1;0;0+2",
        "aff_country_unique": "Belgium;France;Switzerland",
        "bibtex": "@InProceedings{Ghodrati_2015_ICCV,\n    \n    author = {\n    Ghodrati,\n    Amir and Diba,\n    Ali and Pedersoli,\n    Marco and Tuytelaars,\n    Tinne and Van Gool,\n    Luc\n},\n    title = {\n    DeepProposal: Hunting Objects by Cascading Deep Convolutional Layers\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "e084b6a927",
        "title": "Deformable 3D Fusion: From Partial Dynamic 3D Observations to Complete 4D Models",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Weipeng Xu, Mathieu Salzmann, Yongtian Wang, Yue Liu",
        "author": "Weipeng Xu; Mathieu Salzmann; Yongtian Wang; Yue Liu",
        "abstract": "Capturing the 3D motion of dynamic, non-rigid objects has attracted significant attention in computer vision. Existing methods typically require either complete 3D volumetric observations, or a shape template. In this paper, we introduce a template-less 4D reconstruction method that incrementally fuses highly-incomplete 3D observations of a deforming object, and generates a complete, temporally-coherent shape representation of the object. To this end, we design an online algorithm that alternatively registers new observations to the current model estimate and updates the model. We demonstrate the effectiveness of our approach at reconstructing non-rigidly moving objects from highly-incomplete measurements on both sequences of partial 3D point clouds and Kinect videos.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Xu_Deformable_3D_Fusion_ICCV_2015_paper.pdf",
        "aff": "Beijing Institute of Technology, China+ NICTA, Canberra, Australia; NICTA, Canberra, Australia+ CVLab, EPFL, Switzerland; Beijing Institute of Technology, China; Beijing Institute of Technology, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1808843,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15668532070694459248&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "bit.edu.cn;epfl.ch;bit.edu.cn;bit.edu.cn",
        "email": "bit.edu.cn;epfl.ch;bit.edu.cn;bit.edu.cn",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Xu_Deformable_3D_Fusion_ICCV_2015_paper.html",
        "aff_unique_index": "0+1;1+2;0;0",
        "aff_unique_norm": "Beijing Institute of Technology;NICTA;École Polytechnique Fédérale de Lausanne",
        "aff_unique_dep": ";;CVLab",
        "aff_unique_url": "http://www.bit.edu.cn/;;https://cvlab.epfl.ch",
        "aff_unique_abbr": "BIT;;EPFL",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Canberra",
        "aff_country_unique_index": "0+1;1+2;0;0",
        "aff_country_unique": "China;Australia;Switzerland",
        "bibtex": "@InProceedings{Xu_2015_ICCV,\n    \n    author = {\n    Xu,\n    Weipeng and Salzmann,\n    Mathieu and Wang,\n    Yongtian and Liu,\n    Yue\n},\n    title = {\n    Deformable 3D Fusion: From Partial Dynamic 3D Observations to Complete 4D Models\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "8d7a38d850",
        "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",
        "author": "Kaiming He; Xiangyu Zhang; Shaoqing Ren; Jian Sun",
        "abstract": "Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66%). To our knowledge, our result is the first to surpass the reported human-level performance (5.1%) on this dataset.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1941738,
        "gs_citation": 27645,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6243061688889140249&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/He_Delving_Deep_into_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{He_2015_ICCV,\n    \n    author = {\n    He,\n    Kaiming and Zhang,\n    Xiangyu and Ren,\n    Shaoqing and Sun,\n    Jian\n},\n    title = {\n    Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "e540a7a275",
        "title": "Dense Continuous-Time Tracking and Mapping With Rolling Shutter RGB-D Cameras",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Christian Kerl, Jörg Stückler, Daniel Cremers",
        "author": "Christian Kerl; Jorg Stuckler; Daniel Cremers",
        "abstract": "We propose a dense continuous-time tracking and mapping method for RGB-D cameras. We parametrize the camera trajectory using continuous B-splines and optimize the trajectory through dense, direct image alignment. Our method also directly models rolling shutter in both RGB and depth images within the optimization, which improves tracking and reconstruction quality for low-cost CMOS sensors.  Using a continuous trajectory representation has a number of advantages over a discrete-time representation (e.g. camera poses at the frame interval). With splines, less variables need to be optimized than with a discrete representation, since the trajectory can be represented with fewer control points than frames. Splines also naturally include smoothness constraints on derivatives of the trajectory estimate. Finally, the continuous trajectory representation allows to compensate for rolling shutter effects, since a pose estimate is available at any exposure time of an image. Our approach demonstrates superior quality in tracking and reconstruction compared to approaches with discrete-time or global shutter assumptions.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Kerl_Dense_Continuous-Time_Tracking_ICCV_2015_paper.pdf",
        "aff": "Technische Universität München; Technische Universität München; Technische Universität München",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 958655,
        "gs_citation": 107,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12036456200466190280&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "in.tum.de;in.tum.de;in.tum.de",
        "email": "in.tum.de;in.tum.de;in.tum.de",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Kerl_Dense_Continuous-Time_Tracking_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Technische Universität München",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.tum.de",
        "aff_unique_abbr": "TUM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany",
        "bibtex": "@InProceedings{Kerl_2015_ICCV,\n    \n    author = {\n    Kerl,\n    Christian and Stuckler,\n    Jorg and Cremers,\n    Daniel\n},\n    title = {\n    Dense Continuous-Time Tracking and Mapping With Rolling Shutter RGB-D Cameras\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "613e092def",
        "title": "Dense Image Registration and Deformable Surface Reconstruction in Presence of Occlusions and Minimal Texture",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Dat Tien Ngo, Sanghyuk Park, Anne Jorstad, Alberto Crivellaro, Chang D. Yoo, Pascal Fua",
        "author": "Dat Tien Ngo; Sanghyuk Park; Anne Jorstad; Alberto Crivellaro; Chang D. Yoo; Pascal Fua",
        "abstract": "Deformable surface tracking from monocular images is well-known to be under-constrained. Occlusions often make the task even more challenging, and can result in failure if the surface is not sufficiently textured. In this work, we explicitly address the problem of 3D reconstruction of poorly textured, occluded surfaces, proposing a framework based on a template-matching approach that scales dense robust features by a relevancy score. Our approach is extensively compared to current methods employing both local feature matching and dense template alignment. We test on standard datasets as well as on a new dataset (that will be made publicly available) of a sparsely textured, occluded surface. Our framework achieves state-of-the-art results for both well and poorly textured, occluded surfaces.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Ngo_Dense_Image_Registration_ICCV_2015_paper.pdf",
        "aff": "Computer Vision Laboratory, EPFL, Switzerland; School of Electrical Engineering, KAIST, Korea; Computer Vision Laboratory, EPFL, Switzerland; Computer Vision Laboratory, EPFL, Switzerland; School of Electrical Engineering, KAIST, Korea; Computer Vision Laboratory, EPFL, Switzerland",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 7366891,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11856126760839161550&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "epfl.ch;kaist.ac.kr;epfl.ch;epfl.ch;kaist.ac.kr;epfl.ch",
        "email": "epfl.ch;kaist.ac.kr;epfl.ch;epfl.ch;kaist.ac.kr;epfl.ch",
        "author_num": 6,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Ngo_Dense_Image_Registration_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;0;0;1;0",
        "aff_unique_norm": "École Polytechnique Fédérale de Lausanne;KAIST",
        "aff_unique_dep": "Computer Vision Laboratory;School of Electrical Engineering",
        "aff_unique_url": "https://cvl.epfl.ch;https://www.kaist.ac.kr",
        "aff_unique_abbr": "EPFL;KAIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0;1;0",
        "aff_country_unique": "Switzerland;South Korea",
        "bibtex": "@InProceedings{Ngo_2015_ICCV,\n    \n    author = {\n    Ngo,\n    Dat Tien and Park,\n    Sanghyuk and Jorstad,\n    Anne and Crivellaro,\n    Alberto and Yoo,\n    Chang D. and Fua,\n    Pascal\n},\n    title = {\n    Dense Image Registration and Deformable Surface Reconstruction in Presence of Occlusions and Minimal Texture\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "ba0aa153c3",
        "title": "Dense Optical Flow Prediction From a Static Image",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Jacob Walker, Abhinav Gupta, Martial Hebert",
        "author": "Jacob Walker; Abhinav Gupta; Martial Hebert",
        "abstract": "Given a scene, what is going to move, and in what direction will it move? Such a question could be considered a non-semantic form of action prediction. In this work, we present a convolutional neural network (CNN) based approach for motion prediction. Given a static image, this CNN predicts the future motion of each and every pixel in the image in terms of optical flow. Our CNN model leverages the data in tens of thousands of realistic videos to train our model. Our method relies on absolutely no human labeling and is able to predict motion based on the context of the scene. Because our CNN model makes no assumptions about the underlying scene, it can predict future optical flow on a diverse set of scenarios. We outperform all previous approaches by large margins.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Walker_Dense_Optical_Flow_ICCV_2015_paper.pdf",
        "aff": "Robotics Institute, Carnegie Mellon University; Robotics Institute, Carnegie Mellon University; Robotics Institute, Carnegie Mellon University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1261594,
        "gs_citation": 268,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5044313105631009409&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Walker_Dense_Optical_Flow_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Robotics Institute",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Walker_2015_ICCV,\n    \n    author = {\n    Walker,\n    Jacob and Gupta,\n    Abhinav and Hebert,\n    Martial\n},\n    title = {\n    Dense Optical Flow Prediction From a Static Image\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "0242dd265a",
        "title": "Dense Semantic Correspondence Where Every Pixel is a Classifier",
        "session": "motion and correspondence",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "Hilton Bristow, Jack Valmadre, Simon Lucey",
        "author": "Hilton Bristow; Jack Valmadre; Simon Lucey",
        "abstract": "Determining dense semantic correspondences across objects and scenes is a difficult problem that underpins many higher-level computer vision algorithms. Unlike canonical dense correspondence problems which consider images that are spatially or temporally adjacent, semantic correspondence is characterized by images that share similar high-level structures whose exact appearance and geometry may differ.  Motivated by object recognition literature and recent work on rapidly estimating linear classifiers, we treat semantic correspondence as a constrained detection problem, where an exemplar LDA classifier is learned for each pixel. LDA classifiers have two distinct benefits: (i) they exhibit higher average precision than similarity metrics typically used in correspondence problems, and (ii) unlike exemplar SVM, can output globally interpretable posterior probabilities without calibration, whilst also being significantly faster to train.  We pose the correspondence problem as a graphical model, where the unary potentials are computed via convolution with the set of exemplar classifiers, and the joint potentials enforce smoothly varying correspondence assignment.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Bristow_Dense_Semantic_Correspondence_ICCV_2015_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2960206,
        "gs_citation": 70,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11100657716465387859&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Bristow_Dense_Semantic_Correspondence_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Bristow_2015_ICCV,\n    \n    author = {\n    Bristow,\n    Hilton and Valmadre,\n    Jack and Lucey,\n    Simon\n},\n    title = {\n    Dense Semantic Correspondence Where Every Pixel is a Classifier\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "6c747cc184",
        "title": "Depth Map Estimation and Colorization of Anaglyph Images Using Local Color Prior and Reverse Intensity Distribution",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Williem, Ramesh Raskar, In Kyu Park",
        "author": "W. Williem; Ramesh Raskar; In Kyu Park",
        "abstract": "In this paper, we present a joint iterative anaglyph stereo matching and colorization framework for obtaining a set of disparity maps and colorized images. Conventional stereo matching algorithms fail when addressing anaglyph images that do not have similar intensities on their two respective view images. To resolve this problem, we propose two novel data costs using local color prior and reverse intensity distribution factor for obtaining accurate depth maps. To colorize an anaglyph image, each pixel in one view is warped to another view using the obtained disparity values of non-occluded regions. A colorization algorithm using optimization is then employed with additional constraint to colorize the remaining occluded regions. Experimental results confirm that the proposed unified framework is robust and produces accurate depth maps and colorized stereo images.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Williem_Depth_Map_Estimation_ICCV_2015_paper.pdf",
        "aff": "Inha University; MIT Media Lab; Inha University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2380725,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5374589689585884019&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 11,
        "aff_domain": "gmail.com;media.mit.edu;inha.ac.kr",
        "email": "gmail.com;media.mit.edu;inha.ac.kr",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Williem_Depth_Map_Estimation_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Inha University;Massachusetts Institute of Technology",
        "aff_unique_dep": ";Media Lab",
        "aff_unique_url": "https://www.inha.edu/;http://www.media.mit.edu/",
        "aff_unique_abbr": "Inha;MIT",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "South Korea;United States",
        "bibtex": "@InProceedings{Williem_2015_ICCV,\n    \n    author = {\n    Williem,\n    W. and Raskar,\n    Ramesh and Park,\n    In Kyu\n},\n    title = {\n    Depth Map Estimation and Colorization of Anaglyph Images Using Local Color Prior and Reverse Intensity Distribution\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "b95088f6f3",
        "title": "Depth Recovery From Light Field Using Focal Stack Symmetry",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Haiting Lin, Can Chen, Sing Bing Kang, Jingyi Yu",
        "author": "Haiting Lin; Can Chen; Sing Bing Kang; Jingyi Yu",
        "abstract": "We describe a technique to recover depth from a light field (LF) using two proposed features of the LF focal stack. One feature is the property that non-occluding pixels exhibit symmetry along the focal depth dimension centered at the in-focus slice. The other is a data consistency measure based on analysis-by-synthesis, i.e., the difference between the synthesized focal stack given the hypothesized depth map and that from the LF. These terms are used in an iterative optimization framework to extract scene depth. Experimental results on real Lytro and Raytrix data demonstrate that our technique outperforms state-of-the-art solutions and is significantly more robust to noise and under-sampling.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Lin_Depth_Recovery_From_ICCV_2015_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 174,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=253038238197216372&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Lin_Depth_Recovery_From_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Lin_2015_ICCV,\n    \n    author = {\n    Lin,\n    Haiting and Chen,\n    Can and Kang,\n    Sing Bing and Yu,\n    Jingyi\n},\n    title = {\n    Depth Recovery From Light Field Using Focal Stack Symmetry\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "10ab50bdc5",
        "title": "Depth Selective Camera: A Direct, On-Chip, Programmable Technique for Depth Selectivity in Photography",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Ryuichi Tadano, Adithya Kumar Pediredla, Ashok Veeraraghavan",
        "author": "Ryuichi Tadano; Adithya Kumar Pediredla; Ashok Veeraraghavan",
        "abstract": "Time of flight (ToF) cameras use a temporally modulated light source and measure correlation between the reflected light and a sensor modulation pattern, in order to infer scene depth. In this paper, we show that such correlational sensors can also be used to selectively accept or reject light rays from certain scene depths. The basic idea is to carefully select illumination and sensor modulation patterns such that the correlation is non-zero only in the selected depth range - thus light reflected from objects outside this depth range do not affect the correlational measurements. We demonstrate a prototype depth-selective camera and highlight two potential applications: imaging through scattering media and virtual blue screening. This depthselectivity can be used to reject back-scattering and reflection from media in front of the subjects of interest, thereby significantly enhancing the ability to image through scattering media- critical for applications such as car navigation in fog and rain. Similarly, such depth selectivity can also be utilized as a virtual blue-screen in cinematography by rejecting light reflecting from background, while selectively retaining light contributions from the foreground subject.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Tadano_Depth_Selective_Camera_ICCV_2015_paper.pdf",
        "aff": "Sony Corporation, Tokyo, Japan; Rice University, Houston, TX, USA; Rice University, Houston, TX, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2731811,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6557144931472048344&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "jp.sony.com;rice.edu;rice.edu",
        "email": "jp.sony.com;rice.edu;rice.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Tadano_Depth_Selective_Camera_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Sony Corporation;Rice University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.sony.com;https://www.rice.edu",
        "aff_unique_abbr": "Sony;Rice",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Tokyo;Houston",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "Japan;United States",
        "bibtex": "@InProceedings{Tadano_2015_ICCV,\n    \n    author = {\n    Tadano,\n    Ryuichi and Pediredla,\n    Adithya Kumar and Veeraraghavan,\n    Ashok\n},\n    title = {\n    Depth Selective Camera: A Direct,\n    On-Chip,\n    Programmable Technique for Depth Selectivity in Photography\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "5f652d0c15",
        "title": "Depth-Based Hand Pose Estimation: Data, Methods, and Challenges",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "James S. Supančič III, Grégory Rogez, Yi Yang, Jamie Shotton, Deva Ramanan",
        "author": "James S. Supancic III; Gregory Rogez; Yi Yang; Jamie Shotton; Deva Ramanan",
        "abstract": "Hand pose estimation has matured rapidly in recent years. The introduction of commodity depth sensors and a multitude of practical applications have spurred new advances. We provide an extensive analysis of the state-of-the-art, focusing on hand pose estimation from a single depth frame. To do so, we have implemented a considerable number of systems, and will release all software and evaluation code. We summarize important conclusions here: (1) Pose estimation appears roughly solved for scenes with isolated hands. However, methods still struggle to analyze cluttered scenes where hands may be interacting with nearby objects and surfaces. To spur further progress we introduce a challenging new dataset with diverse, cluttered scenes. (2) Many methods evaluate themselves with disparate criteria, making comparisons difficult. We define a consistent evaluation criteria, rigorously motivated by human experiments. (3) We introduce a simple nearest-neighbor baseline that outperforms most existing systems. This implies that most systems do not generalize beyond their training sets. This also reinforces the under-appreciated point that training data is as important as the model itself. We conclude with directions for future progress.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Supancic_Depth-Based_Hand_Pose_ICCV_2015_paper.pdf",
        "aff": "University of California at Irvine; Inria; Baidu; Microsoft; Carnegie Mellon University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1239826,
        "gs_citation": 200,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11630673708066838304&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 25,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Supancic_Depth-Based_Hand_Pose_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;2;3;4",
        "aff_unique_norm": "University of California, Irvine;Inria;Baidu, Inc.;Microsoft Corporation;Carnegie Mellon University",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://www.uci.edu;https://www.inria.fr;https://www.baidu.com;https://www.microsoft.com;https://www.cmu.edu",
        "aff_unique_abbr": "UCI;Inria;Baidu;Microsoft;CMU",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Irvine;",
        "aff_country_unique_index": "0;1;2;0;0",
        "aff_country_unique": "United States;France;China",
        "bibtex": "@InProceedings{III_2015_ICCV,\n    \n    author = {\n    Supancic,\n    III,\n    James S. and Rogez,\n    Gregory and Yang,\n    Yi and Shotton,\n    Jamie and Ramanan,\n    Deva\n},\n    title = {\n    Depth-Based Hand Pose Estimation: Data,\n    Methods,\n    and Challenges\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "90ebeedb39",
        "title": "Describing Videos by Exploiting Temporal Structure",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Li Yao, Atousa Torabi, Kyunghyun Cho, Nicolas Ballas, Christopher Pal, Hugo Larochelle, Aaron Courville",
        "author": "Li Yao; Atousa Torabi; Kyunghyun Cho; Nicolas Ballas; Christopher Pal; Hugo Larochelle; Aaron Courville",
        "abstract": "Recent progress in using recurrent neural networks (RNNs) for image description has motivated the exploration of their application for video description. However, while images are static, working with videos requires modeling their dynamic temporal structure and then properly integrating that information into a natural language description model. In this context, we propose an approach that successfully takes into account both the local and global temporal structure of videos to produce descriptions. First, our approach incorporates a spatial temporal 3-D convolutional neural network (3-D CNN) representation of the short temporal dynamics. The 3-D CNN representation is trained on video action recognition tasks, so as to produce a representation that is tuned to human motion and behavior. Second we propose a temporal attention mechanism that allows to go beyond local temporal modeling and learns to automatically select the most relevant temporal segments given the text-generating RNN. Our approach exceeds the current state-of-art for both BLEU and METEOR metrics on the Youtube2Text dataset. We also present results on a new, larger and more challenging dataset of paired video and natural language descriptions.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Yao_Describing_Videos_by_ICCV_2015_paper.pdf",
        "aff": "Universit ´e de Montr ´eal; Universit ´e de Montr ´eal; Universit ´e de Montr ´eal; Universit ´e de Montr ´eal; ´Ecole Polytechnique de Montr ´eal; Universit ´e de Sherbrooke; Universit ´e de Montr ´eal",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 744992,
        "gs_citation": 1392,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17225606232504528023&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff_domain": "umontreal.ca;umontreal.ca;umontreal.ca;umontreal.ca;polymtl.ca;usherbrooke.ca;umontreal.ca",
        "email": "umontreal.ca;umontreal.ca;umontreal.ca;umontreal.ca;polymtl.ca;usherbrooke.ca;umontreal.ca",
        "author_num": 7,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Yao_Describing_Videos_by_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0;1;2;0",
        "aff_unique_norm": "Université de Montréal;Ecole Polytechnique de Montréal;Université de Sherbrooke",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.umontreal.ca;https://www.polymtl.ca;https://www.usherbrooke.ca",
        "aff_unique_abbr": "UdeM;Polytechnique Montréal;UdeS",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Montréal",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "Canada",
        "bibtex": "@InProceedings{Yao_2015_ICCV,\n    \n    author = {\n    Yao,\n    Li and Torabi,\n    Atousa and Cho,\n    Kyunghyun and Ballas,\n    Nicolas and Pal,\n    Christopher and Larochelle,\n    Hugo and Courville,\n    Aaron\n},\n    title = {\n    Describing Videos by Exploiting Temporal Structure\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "4c1c8b2779",
        "title": "Detailed Full-Body Reconstructions of Moving People From Monocular RGB-D Sequences",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Federica Bogo, Michael J. Black, Matthew Loper, Javier Romero",
        "author": "Federica Bogo; Michael J. Black; Matthew Loper; Javier Romero",
        "abstract": "We accurately estimate the 3D geometry and appearance of the human body from a monocular RGB-D sequence of a user moving freely in front of the sensor. Range data in each frame is first brought into alignment with a multi-resolution 3D body model in a coarse-to-fine process. The method then uses geometry and image texture over time to obtain accurate shape, pose, and appearance information despite unconstrained motion, partial views, varying resolution, occlusion, and soft tissue deformation. Our novel body model has variable shape detail, allowing it to capture faces with a high-resolution deformable head model and body shape with lower-resolution. Finally we combine range data from an entire sequence to estimate a high-resolution displacement map that captures fine shape details. We compare our recovered models with high-resolution scans from a professional system and with avatars created by a commercial product. We extract accurate 3D avatars from challenging motion sequences and even capture soft tissue dynamics.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Bogo_Detailed_Full-Body_Reconstructions_ICCV_2015_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2899548,
        "gs_citation": 259,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4054131937270479223&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Bogo_Detailed_Full-Body_Reconstructions_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Bogo_2015_ICCV,\n    \n    author = {\n    Bogo,\n    Federica and Black,\n    Michael J. and Loper,\n    Matthew and Romero,\n    Javier\n},\n    title = {\n    Detailed Full-Body Reconstructions of Moving People From Monocular RGB-D Sequences\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "de19760976",
        "title": "Detection and Segmentation of 2D Curved Reflection Symmetric Structures",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Ching L. Teo, Cornelia Fermüller, Yiannis Aloimonos",
        "author": "Ching L. Teo; Cornelia Fermuller; Yiannis Aloimonos",
        "abstract": "Symmetry, as one of the key components of Gestalt theory, provides an important mid-level cue that serves as input to higher visual processes such as segmentation. In this work, we propose a complete approach that links the detection of curved reflection symmetries to produce symmetry-constrained segments of structures/regions in real images with clutter. For curved reflection symmetry detection, we leverage on patch-based symmetric features to train a Structured Random Forest classifier that detects multiscaled curved symmetries in 2D images. Next, using these curved symmetries, we modulate a novel symmetry-constrained foreground-background segmentation by their symmetry scores so that we enforce global symmetrical consistency in the final segmentation. This is achieved by imposing a pairwise symmetry prior that encourages symmetric pixels to have the same labels over a MRF-based representation of the input image edges, and the final segmentation is obtained via graph-cuts. Experimental results over four publicly available datasets containing annotated symmetric structures: 1) SYMMAX-300, 2) BSD-Parts, 3) Weizmann Horse and 4) NY-roads demonstrate the approach's applicability to different environments with state-of-the-art performance.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Teo_Detection_and_Segmentation_ICCV_2015_paper.pdf",
        "aff": "Computer Vision Lab, University of Maryland, College Park, MD 20742, USA; Computer Vision Lab, University of Maryland, College Park, MD 20742, USA; Computer Vision Lab, University of Maryland, College Park, MD 20742, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3686454,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9970755390492865162&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "cs.umd.edu;umiacs.umd.edu;cs.umd.edu",
        "email": "cs.umd.edu;umiacs.umd.edu;cs.umd.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Teo_Detection_and_Segmentation_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Maryland",
        "aff_unique_dep": "Computer Vision Lab",
        "aff_unique_url": "https://www.umd.edu",
        "aff_unique_abbr": "UMD",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "College Park",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Teo_2015_ICCV,\n    \n    author = {\n    Teo,\n    Ching L. and Fermuller,\n    Cornelia and Aloimonos,\n    Yiannis\n},\n    title = {\n    Detection and Segmentation of 2D Curved Reflection Symmetric Structures\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "635f0d09ce",
        "title": "Differential Recurrent Neural Networks for Action Recognition",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Vivek Veeriah, Naifan Zhuang, Guo-Jun Qi",
        "author": "Vivek Veeriah; Naifan Zhuang; Guo-Jun Qi",
        "abstract": "The long short-term memory (LSTM) neural network is capable of processing complex sequential information since it utilizes special gating schemes for learning representations from long input sequences. It has the potential to model any time-series or sequential data, where the current hidden state has to be considered in the context of the past hidden states. This property makes LSTM an ideal choice to learn the complex dynamics of various actions.  Unfortunately, the conventional LSTMs do not consider the impact of spatio-temporal dynamics corresponding to the given salient motion patterns, when they gate the information that ought to be memorized through time. To address this problem, we propose a differential gating scheme for the LSTM neural network, which emphasizes on the change in information gain caused by the salient motions between the successive frames. This change in information gain is quantified by Derivative of States (DoS), and thus the proposed LSTM model is termed as differential Recurrent Neural Network (dRNN).  We demonstrate the effectiveness of the proposed model by automatically recognizing actions from the real-world 2D and 3D human action datasets. Our study is one of the first works towards demonstrating the potential of learning complex time-series representations via high-order derivatives of states.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Veeriah_Differential_Recurrent_Neural_ICCV_2015_paper.pdf",
        "aff": "University of Central Florida; University of Central Florida; University of Central Florida",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 608867,
        "gs_citation": 637,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11699479310351332027&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "knights.ucf.edu;knights.ucf.edu;ucf.edu",
        "email": "knights.ucf.edu;knights.ucf.edu;ucf.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Veeriah_Differential_Recurrent_Neural_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Central Florida",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ucf.edu",
        "aff_unique_abbr": "UCF",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Veeriah_2015_ICCV,\n    \n    author = {\n    Veeriah,\n    Vivek and Zhuang,\n    Naifan and Qi,\n    Guo-Jun\n},\n    title = {\n    Differential Recurrent Neural Networks for Action Recognition\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "4e4ff928e4",
        "title": "Direct Intrinsics: Learning Albedo-Shading Decomposition by Convolutional Regression",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Takuya Narihira, Michael Maire, Stella X. Yu",
        "author": "Takuya Narihira; Michael Maire; Stella X. Yu",
        "abstract": "We introduce a new approach to intrinsic image decomposition, the task of decomposing a single image into albedo and shading components.  Our strategy, which we term direct intrinsics, is to learn a convolutional neural network (CNN) that directly predicts output albedo and shading channels from an input RGB image patch.  Direct intrinsics is a departure from classical techniques for intrinsic image decomposition, which typically rely on physically-motivated priors and graph-based inference algorithms.  The large-scale synthetic ground-truth of the MPI Sintel dataset plays the key role in training direct intrinsics.  We demonstrate results on both the synthetic images of Sintel and the real images of the classic MIT intrinsic image dataset.  On Sintel, direct intrinsics, using only RGB input, outperforms all prior work, including methods that rely on RGB+Depth input.  Direct intrinsics also generalizes across modalities; our Sintel-trained CNN produces quite reasonable decompositions on the real images of the MIT dataset.  Our results indicate that the marriage of CNNs with synthetic training data may be a powerful new technique for tackling classic problems in computer vision.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Narihira_Direct_Intrinsics_Learning_ICCV_2015_paper.pdf",
        "aff": "UC Berkeley / ICSI / Sony Corp.; TTI Chicago; UC Berkeley / ICSI",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1182514,
        "gs_citation": 216,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15084859776579221812&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "jp.sony.com;ttic.edu;berkeley.edu",
        "email": "jp.sony.com;ttic.edu;berkeley.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Narihira_Direct_Intrinsics_Learning_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of California, Berkeley;Toyota Technological Institute at Chicago",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.berkeley.edu;https://www.tti-chicago.org",
        "aff_unique_abbr": "UC Berkeley;TTI",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Berkeley;Chicago",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Narihira_2015_ICCV,\n    \n    author = {\n    Narihira,\n    Takuya and Maire,\n    Michael and Yu,\n    Stella X.\n},\n    title = {\n    Direct Intrinsics: Learning Albedo-Shading Decomposition by Convolutional Regression\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "65f0df4fb5",
        "title": "Direct, Dense, and Deformable: Template-Based Non-Rigid 3D Reconstruction From RGB Video",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Rui Yu, Chris Russell, Neill D. F. Campbell, Lourdes Agapito",
        "author": "Rui Yu; Chris Russell; Neill D. F. Campbell; Lourdes Agapito",
        "abstract": "In this paper we tackle the problem of capturing the dense, detailed 3D geometry of generic, complex non-rigid meshes using a single RGB-only commodity video camera and a direct approach.  While robust and even real-time solutions exist to this problem  if the observed scene is static, for non-rigid dense shape capture current systems are typically restricted to the use of complex multi-camera rigs, take advantage of the additional depth channel available in RGB-D cameras, or deal with specific shapes such as faces or planar surfaces. In contrast, our method makes use of a single RGB video as input; it can capture the deformations of generic shapes; and the depth estimation is dense, per-pixel and direct.  We first compute a dense 3D template of the shape of the object, using a short rigid sequence, and subsequently perform online reconstruction of the non-rigid mesh as it evolves over time.  Our energy optimization approach minimizes a robust photometric cost that simultaneously  estimates the temporal correspondences and 3D deformations with respect to the template mesh. In our experimental evaluation we show a range of qualitative results on novel datasets; we compare against an existing method that requires multi-frame optical flow; and perform a quantitative evaluation against other template-based approaches on a ground truth dataset.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Yu_Direct_Dense_and_ICCV_2015_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 9060086,
        "gs_citation": 115,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3104304413687153085&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Yu_Direct_Dense_and_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Yu_2015_ICCV,\n    \n    author = {\n    Yu,\n    Rui and Russell,\n    Chris and Campbell,\n    Neill D. F. and Agapito,\n    Lourdes\n},\n    title = {\n    Direct,\n    Dense,\n    and Deformable: Template-Based Non-Rigid 3D Reconstruction From RGB Video\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "36c69272fa",
        "title": "Discovering the Spatial Extent of Relative Attributes",
        "session": "learning representations and attributes",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "Fanyi Xiao, Yong Jae Lee",
        "author": "Fanyi Xiao; Yong Jae Lee",
        "abstract": "We present a weakly-supervised approach that discovers the spatial extent of relative attributes, given only pairs of ordered images. In contrast to traditional approaches that use global appearance features or rely on keypoint detectors, our goal is to automatically discover the image regions that are relevant to the attribute, even when the attribute's appearance changes drastically across its attribute spectrum. To accomplish this, we first develop a novel formulation that combines a detector with local smoothness to discover a set of coherent visual chains across the image collection. We then introduce an efficient way to generate additional chains anchored on the initial discovered ones. Finally, we automatically identify the most relevant visual chains, and create an ensemble image representation to model the attribute. Through extensive experiments, we demonstrate our method's promise relative to several baselines in modeling relative attributes.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Xiao_Discovering_the_Spatial_ICCV_2015_paper.pdf",
        "aff": "University of California, Davis; University of California, Davis",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1383190,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5124631927921961036&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": "cs.ucdavis.edu;cs.ucdavis.edu",
        "email": "cs.ucdavis.edu;cs.ucdavis.edu",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Xiao_Discovering_the_Spatial_ICCV_2015_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Davis",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ucdavis.edu",
        "aff_unique_abbr": "UC Davis",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Davis",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Xiao_2015_ICCV,\n    \n    author = {\n    Xiao,\n    Fanyi and Lee,\n    Yong Jae\n},\n    title = {\n    Discovering the Spatial Extent of Relative Attributes\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "29e7aee282",
        "title": "Discrete Tabu Search for Graph Matching",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Kamil Adamczewski, Yumin Suh, Kyoung Mu Lee",
        "author": "Kamil Adamczewski; Yumin Suh; Kyoung Mu Lee",
        "abstract": "Graph matching is a fundamental problem in computer vision. In this paper, we propose a novel graph matching algorithm based on tabu search. The proposed method solves graph matching problem by casting it into an equivalent weighted maximum clique problem of the corresponding association graph, which we further penalize through introducing negative weights. Subsequent tabu search optimization allows for overcoming the convention of using positive weights. The method's distinct feature is that it utilizes the history of search to make more strategic decisions while looking for the optimal solution, thus effectively escaping local optima and in practice achieving superior results. The proposed method, unlike the existing algorithms, enables direct optimization in the original discrete space while encouraging rather than artificially enforcing hard one-to-one constraint, thus resulting in better solution. The experiments demonstrate the robustness of the algorithm in a variety of settings, presenting the state-of-the-art results. The code is available at http://cv.snu.ac.kr/research/ DTSGM/",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Adamczewski_Discrete_Tabu_Search_ICCV_2015_paper.pdf",
        "aff": "Department of ECE, ASRI, Seoul National University; Department of ECE, ASRI, Seoul National University; Department of ECE, ASRI, Seoul National University",
        "project": "http://cv.snu.ac.kr/research/~DTSGM/",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2284809,
        "gs_citation": 58,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2091361011730693878&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "gmail.com;snu.ac.kr;snu.ac.kr",
        "email": "gmail.com;snu.ac.kr;snu.ac.kr",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Adamczewski_Discrete_Tabu_Search_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Seoul National University",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.snu.ac.kr",
        "aff_unique_abbr": "SNU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Seoul",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "South Korea",
        "bibtex": "@InProceedings{Adamczewski_2015_ICCV,\n    \n    author = {\n    Adamczewski,\n    Kamil and Suh,\n    Yumin and Lee,\n    Kyoung Mu\n},\n    title = {\n    Discrete Tabu Search for Graph Matching\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "8d8399b817",
        "title": "Discriminative Learning of Deep Convolutional Feature Point Descriptors",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Edgar Simo-Serra, Eduard Trulls, Luis Ferraz, Iasonas Kokkinos, Pascal Fua, Francesc Moreno-Noguer",
        "author": "Edgar Simo-Serra; Eduard Trulls; Luis Ferraz; Iasonas Kokkinos; Pascal Fua; Francesc Moreno-Noguer",
        "abstract": "Deep learning has revolutionalized image-level tasks such as classification, but patch-level tasks, such as correspondence, still rely on hand-crafted features, e.g. SIFT. In this paper we use Convolutional Neural Networks (CNNs) to learn discriminant patch representations and in particular train a Siamese network with pairs of (non-)corresponding patches. We deal with the large number of potential pairs with the combination of a stochastic sampling of the training set and an aggressive mining strategy biased towards patches that are hard to classify.  By using the L2 distance during both training and testing we develop 128-D descriptors whose euclidean distances reflect patch similarity, and which can be used as a drop-in replacement for any task involving SIFT. We demonstrate consistent performance gains over the state of the art, and generalize well against scaling and rotation, perspective transformation, non-rigid deformation, and illumination changes. Our descriptors are efficient to compute and amenable to modern GPUs, and are publicly available.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Simo-Serra_Discriminative_Learning_of_ICCV_2015_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3213247,
        "gs_citation": 1022,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13199477557343029706&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 30,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Simo-Serra_Discriminative_Learning_of_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Simo-Serra_2015_ICCV,\n    \n    author = {\n    Simo-Serra,\n    Edgar and Trulls,\n    Eduard and Ferraz,\n    Luis and Kokkinos,\n    Iasonas and Fua,\n    Pascal and Moreno-Noguer,\n    Francesc\n},\n    title = {\n    Discriminative Learning of Deep Convolutional Feature Point Descriptors\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "ece7b8ae55",
        "title": "Discriminative Low-Rank Tracking",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Yao Sui, Yafei Tang, Li Zhang",
        "author": "Yao Sui; Yafei Tang; Li Zhang",
        "abstract": "Good tracking performance is in general attributed to accurate representation over previously obtained targets or reliable discrimination between the target and the surrounding background. In this work, we exploit the advantages of the both approaches to achieve a robust tracker. We construct a subspace to represent the target and the neighboring background, and simultaneously propagate their class labels via the learned subspace. Moreover, we propose a novel criterion to identify the target from numerous target candidates on each frame, which takes into account both discrimination reliability and representation accuracy. In addition, with the proposed criterion, the ambiguity in the class labels of the neighboring background samples, which often influences the reliability of discriminative tracking model, is effectively alleviated, while the training set is still kept small. Extensive experiments demonstrate that our tracker performs favourably against many other state-of-the-art trackers.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Sui_Discriminative_Low-Rank_Tracking_ICCV_2015_paper.pdf",
        "aff": "Department of Electronic Engineering, Tsinghua University, Beijing 100084, China; Department of Electronic Engineering, Tsinghua University, Beijing 100084, China; Department of Electronic Engineering, Tsinghua University, Beijing 100084, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1355329,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10378475306387349447&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "gmail.com;chinaunicom.cn;tsinghua.edu.cn",
        "email": "gmail.com;chinaunicom.cn;tsinghua.edu.cn",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Sui_Discriminative_Low-Rank_Tracking_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Tsinghua University",
        "aff_unique_dep": "Department of Electronic Engineering",
        "aff_unique_url": "https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "THU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Sui_2015_ICCV,\n    \n    author = {\n    Sui,\n    Yao and Tang,\n    Yafei and Zhang,\n    Li\n},\n    title = {\n    Discriminative Low-Rank Tracking\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "a7a2fa7767",
        "title": "Discriminative Pose-Free Descriptors for Face and Object Matching",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Soubhik Sanyal, Sivaram Prasad Mudunuri, Soma Biswas",
        "author": "Soubhik Sanyal; Sivaram Prasad Mudunuri; Soma Biswas",
        "abstract": "Pose invariant matching is a very important and challenging problem with various applications like recognizing faces in uncontrolled scenarios, matching objects taken from different view points, etc. In this paper, we propose a discriminative pose-free descriptor (DPFD) which can be used to match faces/objects across pose variations. Training examples at very few representative poses are used to generate virtual intermediate pose subspaces. An image or image region is then represented by a feature set obtained by projecting it on all these subspaces and a discriminative transform is applied on this feature set to make it suitable for classification tasks. Finally, this discriminative feature set is represented by a single feature vector, termed as DPFD. The DPFD of images taken from different viewpoints can be directly compared for matching. Extensive experiments on recognizing faces across pose, pose and resolution on the Multi-PIE and Surveillance Cameras Face datasets and comparisons with state-of-the-art approaches show the effectiveness of the proposed approach. Experiments on matching general objects across viewpoints show the generalizability of the proposed approach beyond faces.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Sanyal_Discriminative_Pose-Free_Descriptors_ICCV_2015_paper.pdf",
        "aff": "Indian Institute of Science, Bangalore, India; Indian Institute of Science, Bangalore, India; Indian Institute of Science, Bangalore, India",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 924773,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5804918930813695803&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "ee.iisc.ernet.in;ee.iisc.ernet.in;ee.iisc.ernet.in",
        "email": "ee.iisc.ernet.in;ee.iisc.ernet.in;ee.iisc.ernet.in",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Sanyal_Discriminative_Pose-Free_Descriptors_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Indian Institute of Science",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iisc.ac.in",
        "aff_unique_abbr": "IISc",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Bangalore",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India",
        "bibtex": "@InProceedings{Sanyal_2015_ICCV,\n    \n    author = {\n    Sanyal,\n    Soubhik and Mudunuri,\n    Sivaram Prasad and Biswas,\n    Soma\n},\n    title = {\n    Discriminative Pose-Free Descriptors for Face and Object Matching\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "3679b73050",
        "title": "Domain Generalization for Object Recognition With Multi-Task Autoencoders",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Muhammad Ghifary, W. Bastiaan Kleijn, Mengjie Zhang, David Balduzzi",
        "author": "Muhammad Ghifary; W. Bastiaan Kleijn; Mengjie Zhang; David Balduzzi",
        "abstract": "The problem of domain generalization is to take knowledge acquired from a number of related domains, where training data is available, and to then successfully apply it to previously unseen domains. We propose a new feature learning algorithm, Multi-Task Autoencoder (MTAE), that provides good generalization performance for cross-domain object recognition.  The algorithm extends the standard denoising autoencoder framework by substituting artificially induced corruption with naturally occurring inter-domain variability in the appearance of objects. Instead of reconstructing images from noisy versions, MTAE learns to transform the original image into analogs in multiple related domains. It thereby learns features that are robust to variations across domains. The learnt features are then used as inputs to a classifier.  We evaluated the performance of the algorithm on benchmark image recognition datasets, where the task is to learn features from multiple datasets and to then predict the image label from unseen datasets. We found that (denoising) MTAE outperforms alternative autoencoder-based models as well as the current state-of-the-art algorithms for domain generalization.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Ghifary_Domain_Generalization_for_ICCV_2015_paper.pdf",
        "aff": "Victoria University of Wellington; Victoria University of Wellington; Victoria University of Wellington; Victoria University of Wellington",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 898457,
        "gs_citation": 833,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4992538266369979207&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 14,
        "aff_domain": "ecs.vuw.ac.nz;ecs.vuw.ac.nz;ecs.vuw.ac.nz;vuw.ac.nz",
        "email": "ecs.vuw.ac.nz;ecs.vuw.ac.nz;ecs.vuw.ac.nz;vuw.ac.nz",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Ghifary_Domain_Generalization_for_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Victoria University of Wellington",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.victoria.ac.nz",
        "aff_unique_abbr": "VUW",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "New Zealand",
        "bibtex": "@InProceedings{Ghifary_2015_ICCV,\n    \n    author = {\n    Ghifary,\n    Muhammad and Kleijn,\n    W. Bastiaan and Zhang,\n    Mengjie and Balduzzi,\n    David\n},\n    title = {\n    Domain Generalization for Object Recognition With Multi-Task Autoencoders\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "b4becdc53b",
        "title": "Dual-Feature Warping-Based Motion Model Estimation",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Shiwei Li, Lu Yuan, Jian Sun, Long Quan",
        "author": "Shiwei Li; Lu Yuan; Jian Sun; Long Quan",
        "abstract": "To break down the geometry assumptions of traditional motion models (e.g., homography, affine), warping-based motion model recently becomes very popular and is adopted in many latest applications (e.g., image stitching, video stabilization). With high degrees of freedom, the accuracy of model heavily relies on data-terms (keypoint correspondences). In some low-texture environments (e.g., indoor) where keypoint feature is insufficient or unreliable, the warping model is often erroneously estimated.  In this paper we propose a simple and effective approach by considering both keypoint and line segment correspondences as data-term. Line segment is a prominent feature in artificial environments and it can supply sufficient geometrical and structural information of scenes, which not only helps guild to a correct warp in low-texture condition, but also prevents the undesired distortion induced by warping. The combination aims to complement each other and benefit for a wider range of scenes. Our method is general and can be ported to many existing applications. Experiments demonstrate that using dual-feature yields more robust and accurate result especially for those low-texture images.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Li_Dual-Feature_Warping-Based_Motion_ICCV_2015_paper.pdf",
        "aff": "Hong Kong University of Science and Technology; Microsoft Research; Microsoft Research; Hong Kong University of Science and Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2815358,
        "gs_citation": 134,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13638256176826971669&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "cse.ust.hk;cse.ust.hk;microsoft.com;microsoft.com",
        "email": "cse.ust.hk;cse.ust.hk;microsoft.com;microsoft.com",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Li_Dual-Feature_Warping-Based_Motion_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "Hong Kong University of Science and Technology;Microsoft Corporation",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.ust.hk;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "HKUST;MSR",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0;1;1;0",
        "aff_country_unique": "China;United States",
        "bibtex": "@InProceedings{Li_2015_ICCV,\n    \n    author = {\n    Li,\n    Shiwei and Yuan,\n    Lu and Sun,\n    Jian and Quan,\n    Long\n},\n    title = {\n    Dual-Feature Warping-Based Motion Model Estimation\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "1f107c3a1e",
        "title": "Dynamic Texture Recognition via Orthogonal Tensor Dictionary Learning",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Yuhui Quan, Yan Huang, Hui Ji",
        "author": "Yuhui Quan; Yan Huang; Hui Ji",
        "abstract": "Dynamic textures (DTs) are video sequences with stationary properties, which exhibit repetitive patterns over space and time. This paper aims at investigating the sparse coding based approach to characterizing local DT patterns for recognition. Owing to the high dimensionality of DT sequences, existing dictionary learning algorithms are not suitable for our purpose due to their high computational costs as well as poor scalability. To overcome these obstacles, we proposed a structured tensor dictionary learning method for sparse coding, which learns a dictionary structured with orthogonality and separability. The proposed method is very fast and more scalable to high-dimensional data than the existing ones. In addition, based on the proposed dictionary learning method, a DT descriptor is developed, which has better adaptivity, discriminability and scalability than the existing approaches. These advantages are demonstrated by the experiments on multiple datasets.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Quan_Dynamic_Texture_Recognition_ICCV_2015_paper.pdf",
        "aff": "Department of Mathematics, National University of Singapore, Singapore 119076; School of Computer Science &Engineering, South China University of Technology, China 510006; Department of Mathematics, National University of Singapore, Singapore 119076",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 558390,
        "gs_citation": 79,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=719026259914878707&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "nus.edu.sg;gmail.com;nus.edu.sg",
        "email": "nus.edu.sg;gmail.com;nus.edu.sg",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Quan_Dynamic_Texture_Recognition_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "National University of Singapore;South China University of Technology",
        "aff_unique_dep": "Department of Mathematics;School of Computer Science & Engineering",
        "aff_unique_url": "https://www.nus.edu.sg;https://www.scut.edu.cn",
        "aff_unique_abbr": "NUS;SCUT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Singapore;China",
        "bibtex": "@InProceedings{Quan_2015_ICCV,\n    \n    author = {\n    Quan,\n    Yuhui and Huang,\n    Yan and Ji,\n    Hui\n},\n    title = {\n    Dynamic Texture Recognition via Orthogonal Tensor Dictionary Learning\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "db3f74d689",
        "title": "Efficient Classifier Training to Minimize False Merges in Electron Microscopy Segmentation",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Toufiq Parag, Dan C. Ciresan, Alessandro Giusti",
        "author": "Toufiq Parag; Dan C. Ciresan; Alessandro Giusti",
        "abstract": "The prospect of neural reconstruction from Electron Microscopy (EM) images has been elucidated by the automatic segmentation algorithms. Although segmentation algorithms eliminate the necessity of tracing the neurons by hand, significant manual effort is still essential for correcting the mistakes they make. A considerable amount of human labor is also required for annotating groundtruth volumes for training the classifiers of a segmentation framework. It is critically important to diminish the dependence on human interaction in the overall reconstruction system.  This study proposes a novel classifier training algorithm for EM segmentation aimed to reduce the amount of manual effort demanded by the groundtruth annotation and error refinement tasks. Instead of using an exhaustive pixel level groundtruth, an active learning algorithm is proposed for sparse labeling of pixel and boundaries of superpixels. Because over-segmentation errors are in general more tolerable and easier to correct than the under-segmentation errors, our algorithm is designed to prioritize minimization of false-merges over false-split mistakes. Our experiments on both 2D and 3D data  suggest that the proposed method yields segmentation outputs that are more amenable to neural reconstruction than those of existing methods.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Parag_Efficient_Classifier_Training_ICCV_2015_paper.pdf",
        "aff": "HHMI Janelia Research Campus, Ashburn, V A; IDSIA, USI-SUPSI, Lugano, Switzerland; IDSIA, USI-SUPSI, Lugano, Switzerland",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2127163,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4488987699805600073&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "janelia.hhmi.org;idsia.ch;idsia.ch",
        "email": "janelia.hhmi.org;idsia.ch;idsia.ch",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Parag_Efficient_Classifier_Training_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "HHMI Janelia Research Campus;IDSIA",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.janelia.org;https://www.idsia.ch",
        "aff_unique_abbr": "Janelia;IDSIA",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Ashburn;Lugano",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "United States;Switzerland",
        "bibtex": "@InProceedings{Parag_2015_ICCV,\n    \n    author = {\n    Parag,\n    Toufiq and Ciresan,\n    Dan C. and Giusti,\n    Alessandro\n},\n    title = {\n    Efficient Classifier Training to Minimize False Merges in Electron Microscopy Segmentation\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "d108c2e87f",
        "title": "Efficient Decomposition of Image and Mesh Graphs by Lifted Multicuts",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Margret Keuper, Evgeny Levinkov, Nicolas Bonneel, Guillaume Lavoué, Thomas Brox, Björn Andres",
        "author": "Margret Keuper; Evgeny Levinkov; Nicolas Bonneel; Guillaume Lavoue; Thomas Brox; Bjorn Andres",
        "abstract": "Formulations of the Image Decomposition Problem as a Multicut Problem (MP) w.r.t. a superpixel graph have received considerable attention. In contrast, instances of the MP w.r.t. a pixel grid graph have received little attention, firstly, because the MP is NP-hard and instances w.r.t. a pixel grid graph are hard to solve in practice, and, secondly, due to the lack of long-range terms in the objective function of the MP. We propose a generalization of the MP with long-range terms (LMP). We design and implement two efficient algorithms (primal feasible heuristics) for the MP and LMP which allow us to study instances of both problems w.r.t. the pixel grid graphs of the images in the BSDS-500 benchmark. The decompositions we obtain do not differ significantly from the state of the art, suggesting that the LMP is a competitive formulation of the Image Decomposition Problem. To demonstrate the generality of the LMP, we apply it also to the Mesh Decomposition Problem posed by the Princeton benchmark, obtaining state-of-the-art decompositions.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Keuper_Efficient_Decomposition_of_ICCV_2015_paper.pdf",
        "aff": "Department of Computer Science, University of Freiburg; Combinatorial Image Analysis, MPI for Informatics, Saarbr ¨ucken; Laboratoire d’Informatique en Image et Syst `emes d’Information, CNRS Lyon; Laboratoire d’Informatique en Image et Syst `emes d’Information, CNRS Lyon; Department of Computer Science, University of Freiburg; Combinatorial Image Analysis, MPI for Informatics, Saarbr ¨ucken",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 13133972,
        "gs_citation": 159,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16603011495847824404&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 21,
        "aff_domain": "cs.uni-freiburg.de;mpi-inf.mpg.de;liris.cnrs.fr;liris.cnrs.fr;cs.uni-freiburg.de;mpi-inf.mpg.de",
        "email": "cs.uni-freiburg.de;mpi-inf.mpg.de;liris.cnrs.fr;liris.cnrs.fr;cs.uni-freiburg.de;mpi-inf.mpg.de",
        "author_num": 6,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Keuper_Efficient_Decomposition_of_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;2;2;0;1",
        "aff_unique_norm": "University of Freiburg;Max Planck Institute for Informatics;CNRS",
        "aff_unique_dep": "Department of Computer Science;Combinatorial Image Analysis;Laboratoire d’Informatique en Image et Systèmes d’Information",
        "aff_unique_url": "https://www.uni-freiburg.de;https://mpi-inf.mpg.de;https://www.cnrs.fr",
        "aff_unique_abbr": ";MPII;CNRS",
        "aff_campus_unique_index": "1;2;2;1",
        "aff_campus_unique": ";Saarbrücken;Lyon",
        "aff_country_unique_index": "0;0;1;1;0;0",
        "aff_country_unique": "Germany;France",
        "bibtex": "@InProceedings{Keuper_2015_ICCV,\n    \n    author = {\n    Keuper,\n    Margret and Levinkov,\n    Evgeny and Bonneel,\n    Nicolas and Lavoue,\n    Guillaume and Brox,\n    Thomas and Andres,\n    Bjorn\n},\n    title = {\n    Efficient Decomposition of Image and Mesh Graphs by Lifted Multicuts\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "2bd112c00d",
        "title": "Efficient PSD Constrained Asymmetric Metric Learning for Person Re-Identification",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Shengcai Liao, Stan Z. Li",
        "author": "Shengcai Liao; Stan Z. Li",
        "abstract": "Person re-identification is becoming a hot research topic due to its value in both machine learning research and video surveillance applications. For this challenging problem, distance metric learning is shown to be effective in matching person images. However, existing approaches either require a heavy computation due to the positive semidefinite (PSD) constraint, or ignore the PSD constraint and learn a free distance function that makes the learned metric potentially noisy. We argue that the PSD constraint provides a useful regularization to smooth the solution of the metric, and hence the learned metric is more robust than without the PSD constraint. Another problem with metric learning algorithms is that the number of positive sample pairs is very limited, and the learning process is largely dominated by the large amount of negative sample pairs. To address the above issues, we derive a logistic metric learning approach with the PSD constraint and an asymmetric sample weighting strategy. Besides, we successfully apply the accelerated proximal gradient approach to find a global minimum solution of the proposed formulation, with a convergence rate of O(1/t^2) where t is the number of iterations. The proposed algorithm termed MLAPG is shown to be computationally efficient and able to perform low rank selection. We applied the proposed method for person re-identification, achieving state-of-the-art performance on four challenging databases (VIPeR, QMUL GRID, CUHK Campus, and CUHK03), compared to existing metric learning methods as well as published results.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Liao_Efficient_PSD_Constrained_ICCV_2015_paper.pdf",
        "aff": ";",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 685746,
        "gs_citation": 511,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6659171086321633098&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Liao_Efficient_PSD_Constrained_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Liao_2015_ICCV,\n    \n    author = {\n    Liao,\n    Shengcai and Li,\n    Stan Z.\n},\n    title = {\n    Efficient PSD Constrained Asymmetric Metric Learning for Person Re-Identification\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "728fcbb9ff",
        "title": "Efficient Solution to the Epipolar Geometry for Radially Distorted Cameras",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Zuzana Kukelova, Jan Heller, Martin Bujnak, Andrew Fitzgibbon, Tomas Pajdla",
        "author": "Zuzana Kukelova; Jan Heller; Martin Bujnak; Andrew Fitzgibbon; Tomas Pajdla",
        "abstract": "The estimation of the epipolar geometry of two cameras from image matches is a fundamental problem of computer vision with many applications. While the closely related problem of estimating relative pose of two different uncalibrated cameras with radial distortion is of particular importance, none of the previously published methods is suitable for practical applications. These solutions are either numerically unstable, sensitive to noise, based on a large number of point correspondences, or simply too slow for real-time applications. In this paper, we present a new efficient solution to this problem that uses 10 image correspondences. By manipulating ten input polynomial equations, we derive a degree 10 polynomial equation in one variable. The solutions to this equation are efficiently found using the Sturm sequences method. In the experiments, we show that the proposed solution is stable, noise resistant, and fast, and as such efficiently usable in a practical Structure-from-Motion pipeline.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Kukelova_Efficient_Solution_to_ICCV_2015_paper.pdf",
        "aff": "Microsoft Research Ltd; Czech Technical University in Prague; Capturing Reality s.r.o. + Czech Technical University in Prague; Microsoft Research Ltd; Czech Technical University in Prague",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3866671,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9467076691587637130&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "microsoft.com;cmp.felk.cvut.cz;capturingreality.com;microsoft.com;cmp.felk.cvut.cz",
        "email": "microsoft.com;cmp.felk.cvut.cz;capturingreality.com;microsoft.com;cmp.felk.cvut.cz",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Kukelova_Efficient_Solution_to_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;2+1;0;1",
        "aff_unique_norm": "Microsoft Research;Czech Technical University;Capturing Reality",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.microsoft.com/en-us/research;https://www.ctu.cz;",
        "aff_unique_abbr": "MSR;CTU;CR",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Prague",
        "aff_country_unique_index": "0;1;1+1;0;1",
        "aff_country_unique": "United Kingdom;Czech Republic",
        "bibtex": "@InProceedings{Kukelova_2015_ICCV,\n    \n    author = {\n    Kukelova,\n    Zuzana and Heller,\n    Jan and Bujnak,\n    Martin and Fitzgibbon,\n    Andrew and Pajdla,\n    Tomas\n},\n    title = {\n    Efficient Solution to the Epipolar Geometry for Radially Distorted Cameras\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "e765d45ce5",
        "title": "Efficient Video Segmentation Using Parametric Graph Partitioning",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Chen-Ping Yu, Hieu Le, Gregory Zelinsky, Dimitris Samaras",
        "author": "Chen-Ping Yu; Hieu Le; Gregory Zelinsky; Dimitris Samaras",
        "abstract": "Video segmentation is the task of grouping similar pixels in the spatio-temporal domain, and has become an important preprocessing step for subsequent video analysis. Most video segmentation and supervoxel methods output a hierarchy of segmentations, but while this provides useful multiscale information, it also adds difficulty in selecting the appropriate level for a task. In this work, we propose an efficient and robust video segmentation framework based on parametric graph partitioning (PGP), a fast, almost parameter free graph partitioning method that identifies and removes between-cluster edges to form node clusters. Apart from its computational efficiency, PGP performs clustering of the spatio-temporal volume without requiring a pre-specified cluster number or bandwidth parameters, thus making video segmentation more practical to use in applications. The PGP framework also allows processing sub-volumes, which further improves performance,  contrary to other streaming video segmentation methods where sub-volume processing reduces performance. We evaluate the PGP method using the  SegTrack v2 and Chen Xiph.org datasets, and show that it outperforms related state-of-the-art algorithms in 3D segmentation metrics and running time.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Yu_Efficient_Video_Segmentation_ICCV_2015_paper.pdf",
        "aff": "Computer Science Department; Computer Science Department; Computer Science Department + Psychology Department; Computer Science Department",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1236090,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14409386271408018398&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "cs.stonybrook.edu;cs.stonybrook.edu;stonybrook.edu;cs.stonybrook.edu",
        "email": "cs.stonybrook.edu;cs.stonybrook.edu;stonybrook.edu;cs.stonybrook.edu",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Yu_Efficient_Video_Segmentation_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0+1;0",
        "aff_unique_norm": "Computer Science Department;Psychology Department",
        "aff_unique_dep": "Computer Science;Psychology",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": "",
        "bibtex": "@InProceedings{Yu_2015_ICCV,\n    \n    author = {\n    Yu,\n    Chen-Ping and Le,\n    Hieu and Zelinsky,\n    Gregory and Samaras,\n    Dimitris\n},\n    title = {\n    Efficient Video Segmentation Using Parametric Graph Partitioning\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "e1cad623dc",
        "title": "Enhancing Road Maps by Parsing Aerial Images Around the World",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Gellért Máttyus, Shenlong Wang, Sanja Fidler, Raquel Urtasun",
        "author": "Gellert Mattyus; Shenlong Wang; Sanja Fidler; Raquel Urtasun",
        "abstract": "In recent years, contextual models that exploit maps have been shown to be very effective for many recognition and localization tasks.  In this paper we propose to exploit aerial images in order to enhance  freely available world maps. Towards this goal, we make use of OpenStreetMap and formulate the problem as the one of inference in a Markov random field parameterized in terms of  the location of the road-segment centerlines as well as  their width.  This parameterization enables very efficient inference and  returns only topologically correct roads. In particular, we can segment all OSM roads in the  whole world in a single day using a small cluster of 10 computers. Importantly, our approach generalizes very well; it can be trained using only 1.5 km2 aerial imagery and produce very accurate results in any location across the globe.   We demonstrate the effectiveness of our approach outperforming the state-of-the-art in two new benchmarks that we collect. We then show how our enhanced maps are beneficial for semantic segmentation of ground images.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Mattyus_Enhancing_Road_Maps_ICCV_2015_paper.pdf",
        "aff": "Remote Sensing Technology Institute, German Aerospace Center; Department of Computer Science, University of Toronto; Department of Computer Science, University of Toronto; Department of Computer Science, University of Toronto",
        "project": "www.openstreetmap.org",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2085728,
        "gs_citation": 157,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12092453047766024197&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "dlr.de;cs.toronto.edu;cs.toronto.edu;cs.toronto.edu",
        "email": "dlr.de;cs.toronto.edu;cs.toronto.edu;cs.toronto.edu",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Mattyus_Enhancing_Road_Maps_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "German Aerospace Center;University of Toronto",
        "aff_unique_dep": "Remote Sensing Technology Institute;Department of Computer Science",
        "aff_unique_url": "https://www.dlr.de;https://www.utoronto.ca",
        "aff_unique_abbr": "DLR;U of T",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Toronto",
        "aff_country_unique_index": "0;1;1;1",
        "aff_country_unique": "Germany;Canada",
        "bibtex": "@InProceedings{Mattyus_2015_ICCV,\n    \n    author = {\n    Mattyus,\n    Gellert and Wang,\n    Shenlong and Fidler,\n    Sanja and Urtasun,\n    Raquel\n},\n    title = {\n    Enhancing Road Maps by Parsing Aerial Images Around the World\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "e5c09f0015",
        "title": "Entropy Minimization for Convex Relaxation Approaches",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Mohamed Souiai, Martin R. Oswald, Youngwook Kee, Junmo Kim, Marc Pollefeys, Daniel Cremers",
        "author": "Mohamed Souiai; Martin R. Oswald; Youngwook Kee; Junmo Kim; Marc Pollefeys; Daniel Cremers",
        "abstract": "Despite their enormous success in solving hard combinatorial problems, convex relaxation approaches often suffer from the fact that the computed solutions are far from binary and that subsequent heuristic binarization may substantially degrade the quality of computed solutions.  In this paper, we propose a novel relaxation technique which incorporates the entropy of the objective variable as a measure of relaxation tightness. We show both theoretically and experimentally that augmenting the objective function with an entropy term gives rise to more binary solutions and consequently solutions with a substantially tighter optimality gap.  We use difference of convex function (DC) programming as an efficient and provably convergent solver for the arising convex-concave minimization problem.  We evaluate this approach on three prominent non-convex computer vision challenges: multi-label inpainting, image segmentation and spatio-temporal multi-view reconstruction.  These experiments show that our approach consistently yields better solutions with respect to the original integral optimization problem",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Souiai_Entropy_Minimization_for_ICCV_2015_paper.pdf",
        "aff": "TU Munich, Germany; ETH Zürich, Switzerland; KAIST, South Korea; KAIST, South Korea; ETH Zürich, Switzerland; TU Munich, Germany",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1455062,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10725065274518983552&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Souiai_Entropy_Minimization_for_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;2;2;1;0",
        "aff_unique_norm": "Technical University of Munich;ETH Zürich;Korea Advanced Institute of Science and Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.tum.de;https://www.ethz.ch;https://www.kaist.ac.kr",
        "aff_unique_abbr": "TUM;ETHZ;KAIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;2;2;1;0",
        "aff_country_unique": "Germany;Switzerland;South Korea",
        "bibtex": "@InProceedings{Souiai_2015_ICCV,\n    \n    author = {\n    Souiai,\n    Mohamed and Oswald,\n    Martin R. and Kee,\n    Youngwook and Kim,\n    Junmo and Pollefeys,\n    Marc and Cremers,\n    Daniel\n},\n    title = {\n    Entropy Minimization for Convex Relaxation Approaches\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "b427a40e6c",
        "title": "Entropy-Based Latent Structured Output Prediction",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Diane Bouchacourt, Sebastian Nowozin, M. Pawan Kumar",
        "author": "Diane Bouchacourt; Sebastian Nowozin; M. Pawan Kumar",
        "abstract": "Recently several generalizations of the popular latent structural SVM framework have been proposed in the literature. Broadly speaking, the generalizations can be divided into two categories: (i) those that predict the output variables while either marginalizing the latent variables or estimating their most likely values; and (ii) those that predict the output variables by minimizing an entropy-based uncertainty measure over the latent space. In order to aid their application in computer vision, we study these generalizations with the aim of identifying their strengths and weaknesses. To this end, we propose a novel prediction criterion that includes as special cases all previous prediction criteria that have been used in the literature. Specifically, our framework's prediction criterion minimizes the Aczel and Daroczy entropy of the output. This in turn allows us to design a learning objective that provides a unified framework (UF) for latent structured prediction. We develop a single optimization algorithm and empirically show that it is as effective as the more complex approaches that have been previously employed for latent structured prediction. Using this algorithm, we provide empirical evidence that lends support to prediction via the minimization of the latent space uncertainty.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Bouchacourt_Entropy-Based_Latent_Structured_ICCV_2015_paper.pdf",
        "aff": "CentraleSupélec and INRIA Saclay; Microsoft Research; CentraleSupélec and INRIA Saclay",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1140347,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13238907561859273934&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 19,
        "aff_domain": "ecp.fr;microsoft.com;ecp.fr",
        "email": "ecp.fr;microsoft.com;ecp.fr",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Bouchacourt_Entropy-Based_Latent_Structured_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "CentraleSupélec;Microsoft Corporation",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.centralesupelec.fr;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "CentraleSupélec;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "France;United States",
        "bibtex": "@InProceedings{Bouchacourt_2015_ICCV,\n    \n    author = {\n    Bouchacourt,\n    Diane and Nowozin,\n    Sebastian and Kumar,\n    M. Pawan\n},\n    title = {\n    Entropy-Based Latent Structured Output Prediction\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "23aff8ee41",
        "title": "Example-Based Modeling of Facial Texture From Deficient Data",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Arnaud Dessein, William A. P. Smith, Richard C. Wilson, Edwin R. Hancock",
        "author": "Arnaud Dessein; William A. P. Smith; Richard C. Wilson; Edwin R. Hancock",
        "abstract": "We present an approach to modeling ear-to-ear, high-quality texture from one or more partial views of a face with possibly poor resolution and noise. Our approach is example-based in that we reconstruct texture with patches from a database composed of previously seen faces. A 3D morphable model is used to establish shape correspondence between the observed data across views and training faces. The database is built on the mesh surface by segmenting it into uniform overlapping patches. Texture patches are selected by belief propagation so as to be consistent with neighbors and with observations in an appropriate image formation model. We also develop a variant that is insensitive to light and camera parameters, and incorporate soft symmetry constraints. We obtain textures of higher quality for degraded views as small as 10 pixels wide, than a standard model fitted to non-degraded data. We further show applications to super-resolution where we substantially improve quality compared to a state-of-the-art algorithm, and to texture completion where we fill in missing regions and remove facial clutter in a photorealistic manner.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Dessein_Example-Based_Modeling_of_ICCV_2015_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1027401,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13705645617055047755&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Dessein_Example-Based_Modeling_of_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Dessein_2015_ICCV,\n    \n    author = {\n    Dessein,\n    Arnaud and Smith,\n    William A. P. and Wilson,\n    Richard C. and Hancock,\n    Edwin R.\n},\n    title = {\n    Example-Based Modeling of Facial Texture From Deficient Data\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "db56a338be",
        "title": "Exploiting High Level Scene Cues in Stereo Reconstruction",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Simon Hadfield, Richard Bowden",
        "author": "Simon Hadfield; Richard Bowden",
        "abstract": "We present a novel approach to 3D reconstruction which is inspired by the human visual system. This system unifies standard appearance matching and triangulation techniques with higher level reasoning and scene understanding, in order to resolve ambiguities between different interpretations of the scene. The types of reasoning integrated in the approach includes recognising common configurations of surface normals and semantic edges (e.g. convex, concave and occlusion boundaries). We also recognise the coplanar, collinear and symmetric structures which are especially common in man made environments.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Hadfield_Exploiting_High_Level_ICCV_2015_paper.pdf",
        "aff": "University of Surrey; University of Surrey",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2857565,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=924967012361300357&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "surrey.ac.uk;surrey.ac.uk",
        "email": "surrey.ac.uk;surrey.ac.uk",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Hadfield_Exploiting_High_Level_ICCV_2015_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Surrey",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.surrey.ac.uk",
        "aff_unique_abbr": "Surrey",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom",
        "bibtex": "@InProceedings{Hadfield_2015_ICCV,\n    \n    author = {\n    Hadfield,\n    Simon and Bowden,\n    Richard\n},\n    title = {\n    Exploiting High Level Scene Cues in Stereo Reconstruction\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "3a2c976923",
        "title": "Exploiting Object Similarity in 3D Reconstruction",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Chen Zhou, Fatma Güney, Yizhou Wang, Andreas Geiger",
        "author": "Chen Zhou; Fatma Guney; Yizhou Wang; Andreas Geiger",
        "abstract": "Despite recent progress, reconstructing outdoor scenes in 3D from movable platforms remains a highly difficult endeavour. Challenges include low frame rates, occlusions, large distortions and difficult lighting conditions. In this paper, we leverage the fact that the larger the reconstructed area, the more likely objects of similar type and shape will occur in the scene. This is particularly true for outdoor scenes where buildings and vehicles often suffer from missing texture or reflections, but share similarity in 3D shape. We take advantage of this shape similarity by localizing objects using detectors and jointly reconstructing them while learning a volumetric model of their shape. This allows us to reduce noise while completing missing surfaces as objects of similar shape benefit from all observations for the respective category. We evaluate our approach with respect to LIDAR ground truth on a novel challenging suburban dataset and show its advantages over the state-of-the-art.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Zhou_Exploiting_Object_Similarity_ICCV_2015_paper.pdf",
        "aff": "Nat’l Engineering Laboratory for Video Technology + Cooperative Medianet Innovation Center, Peking University, China; MPI for Intelligent Systems T¨ubingen; Nat’l Engineering Laboratory for Video Technology + Cooperative Medianet Innovation Center, Peking University, China; MPI for Intelligent Systems T¨ubingen",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2004496,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13251369900444134270&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "pku.edu.cn;tue.mpg.de;pku.edu.cn;tue.mpg.de",
        "email": "pku.edu.cn;tue.mpg.de;pku.edu.cn;tue.mpg.de",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Zhou_Exploiting_Object_Similarity_ICCV_2015_paper.html",
        "aff_unique_index": "0+1;2;0+1;2",
        "aff_unique_norm": "National Engineering Laboratory for Video Technology;Peking University;Max Planck Institute for Intelligent Systems",
        "aff_unique_dep": ";Cooperative Medianet Innovation Center;",
        "aff_unique_url": ";http://www.pku.edu.cn;https://www.mpi-is.mpg.de",
        "aff_unique_abbr": ";Peking U;MPI-IS",
        "aff_campus_unique_index": ";1;;1",
        "aff_campus_unique": ";Tübingen",
        "aff_country_unique_index": "0+0;1;0+0;1",
        "aff_country_unique": "China;Germany",
        "bibtex": "@InProceedings{Zhou_2015_ICCV,\n    \n    author = {\n    Zhou,\n    Chen and Guney,\n    Fatma and Wang,\n    Yizhou and Geiger,\n    Andreas\n},\n    title = {\n    Exploiting Object Similarity in 3D Reconstruction\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "15dae868f1",
        "title": "Exploring Causal Relationships in Visual Object Tracking",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Karel Lebeda, Simon Hadfield, Richard Bowden",
        "author": "Karel Lebeda; Simon Hadfield; Richard Bowden",
        "abstract": "Causal relationships can often be found in visual object tracking between the motions of the camera and that of the tracked object. This object motion may be an effect of the camera motion, e.g. an unsteady handheld camera. But it may also be the cause, e.g. the cameraman framing the ob- ject. In this paper we explore these relationships, and pro- vide statistical tools to detect and quantify them; these are based on transfer entropy and stem from information the- ory. The relationships are then exploited to make predic- tions about the object location. The approach is shown to be an excellent measure for describing such relationships. On the VOT2013 dataset the prediction accuracy is increased by 62 % over the best non-causal predictor. We show that the location predictions are robust to camera shake and sud- den motion, which is invaluable for any tracking algorithm and demonstrate this by applying causal prediction to two state-of-the-art trackers. Both of them benefit, Struck gain- ing a 7 % accuracy and 22 % robustness increase on the VTB1.1 benchmark, becoming the new state-of-the-art.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Lebeda_Exploring_Causal_Relationships_ICCV_2015_paper.pdf",
        "aff": "University of Surrey, Guildford, GU2 7XH, United Kingdom; University of Surrey, Guildford, GU2 7XH, United Kingdom; University of Surrey, Guildford, GU2 7XH, United Kingdom",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 881544,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13792926635731353289&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff_domain": "Surrey.ac.uk;Surrey.ac.uk;Surrey.ac.uk",
        "email": "Surrey.ac.uk;Surrey.ac.uk;Surrey.ac.uk",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Lebeda_Exploring_Causal_Relationships_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Surrey",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.surrey.ac.uk",
        "aff_unique_abbr": "Surrey",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Guildford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom",
        "bibtex": "@InProceedings{Lebeda_2015_ICCV,\n    \n    author = {\n    Lebeda,\n    Karel and Hadfield,\n    Simon and Bowden,\n    Richard\n},\n    title = {\n    Exploring Causal Relationships in Visual Object Tracking\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "4d74e927e8",
        "title": "Extended Depth of Field Catadioptric Imaging Using Focal Sweep",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Ryunosuke Yokoya, Shree K. Nayar",
        "author": "Ryunosuke Yokoya; Shree K. Nayar",
        "abstract": "Catadioptric imaging systems use curved mirrors to capture wide fields of view. However, due to the curvature of the mirror, these systems tend to have very limited depth of field (DOF), with the point spread function (PSF) varying dramatically over the field of view and as a function of scene depth. In recent years, focal sweep has been used extensively to extend the DOF of conventional imaging systems. It has been shown that focal sweep produces an integrated point spread function (IPSF) that is nearly space-invariant and depth-invariant, enabling the recovery of an extended depth of field (EDOF) image by deconvolving the captured focal sweep image with a single IPSF. In this paper, we use focal sweep to extend the DOF of a catadioptric imaging system. We show that while the IPSF is spatially varying when a curved mirror is used, it remains quasi depth-invariant over the wide field of view of the imaging system. We have developed a focal sweep system where mirrors of different shapes can be used to capture wide field of view EDOF images. In particular, we show experimental results using spherical and paraboloidal mirrors.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Yokoya_Extended_Depth_of_ICCV_2015_paper.pdf",
        "aff": "Columbia University; Columbia University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4271637,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5063741123235215726&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "cs.columbia.edu;cs.columbia.edu",
        "email": "cs.columbia.edu;cs.columbia.edu",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Yokoya_Extended_Depth_of_ICCV_2015_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Columbia University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.columbia.edu",
        "aff_unique_abbr": "Columbia",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Yokoya_2015_ICCV,\n    \n    author = {\n    Yokoya,\n    Ryunosuke and Nayar,\n    Shree K.\n},\n    title = {\n    Extended Depth of Field Catadioptric Imaging Using Focal Sweep\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "9cd80aa219",
        "title": "External Patch Prior Guided Internal Clustering for Image Denoising",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Fei Chen, Lei Zhang, Huimin Yu",
        "author": "Fei Chen; Lei Zhang; Huimin Yu",
        "abstract": "Natural image modeling plays a key role in many vision problems such as image denoising. Image priors are widely used to regularize the denoising process, which is an illposed inverse problem. One category of denoising methods exploit the priors (e.g., TV, sparsity) learned from external clean images to reconstruct the given noisy image, while another category of methods exploit the internal prior (e.g., self-similarity) to reconstruct the latent image. Though the internal prior based methods have achieved impressive denoising results, the improvement of visual quality will become very difficult with the increase of noise level. In this paper, we propose to exploit image external patch prior and internal self-similarity prior jointly, and develop an external patch prior guided internal clustering algorithm for image denoising. It is known that natural image patches form multiple subspaces. By utilizing Gaussian mixture models (GMMs) learning, image similar patches can be clustered and the subspaces can be learned. The learned GMMs from clean images are then used to guide the clustering of noisypatches of the input noisy images, followed by a low-rank approximation process to estimate the latent subspace for image recovery. Numerical experiments show that the proposed method outperforms many state-of-the-art denoising algorithms such as BM3D and WNNM.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Chen_External_Patch_Prior_ICCV_2015_paper.pdf",
        "aff": "College of Mathematics and Computer Science, Fuzhou University, Fuzhou, China; Dept. of Computing, The Hong Kong Polytechnic University, Hong Kong, China; College of Information Science & Electronic Engineering, Zhejiang University, China + State Key Laboratory of CAD & CG, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 868382,
        "gs_citation": 186,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16042957188085346189&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "fzu.edu.cn;comp.polyu.edu.hk;zju.edu.cn",
        "email": "fzu.edu.cn;comp.polyu.edu.hk;zju.edu.cn",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Chen_External_Patch_Prior_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;2+3",
        "aff_unique_norm": "Fuzhou University;The Hong Kong Polytechnic University;Zhejiang University;State Key Laboratory of CAD & CG",
        "aff_unique_dep": "College of Mathematics and Computer Science;Dept. of Computing;College of Information Science & Electronic Engineering;",
        "aff_unique_url": "https://www.fzu.edu.cn;https://www.polyu.edu.hk;http://www.zju.edu.cn;",
        "aff_unique_abbr": "FZU;PolyU;ZJU;",
        "aff_campus_unique_index": "0;1;",
        "aff_campus_unique": "Fuzhou;Hong Kong;",
        "aff_country_unique_index": "0;0;0+0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Chen_2015_ICCV,\n    \n    author = {\n    Chen,\n    Fei and Zhang,\n    Lei and Yu,\n    Huimin\n},\n    title = {\n    External Patch Prior Guided Internal Clustering for Image Denoising\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "b865618b2d",
        "title": "Extraction of Virtual Baselines From Distorted Document Images Using Curvilinear Projection",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Gaofeng Meng, Zuming Huang, Yonghong Song, Shiming Xiang, Chunhong Pan",
        "author": "Gaofeng Meng; Zuming Huang; Yonghong Song; Shiming Xiang; Chunhong Pan",
        "abstract": "The baselines of a document page are a set of virtual horizontal and parallel lines, to which the printed contents of document, e.g., text lines, tables or inserted photos, are aligned. Accurate baseline extraction is of great importance in the geometric correction of curved document images. In this paper, we propose an efficient method for accurate extraction of these virtual visual cues from a curved document image. Our method comes from two basic observations that the baselines of documents do not intersect with each other and that within a narrow strip, the baselines can be well approximated by linear segments. Based upon these observations, we propose a curvilinear projection based method and model the estimation of curved baselines as a constrained sequential optimization problem. A dynamic programming algorithm is then developed to efficiently solve the problem. The proposed method can extract the complete baselines through each pixel of document images in a high accuracy. It is also scripts insensitive and highly robust to image noises, non-textual objects, image resolutions and image quality degradation like blurring and non-uniform illumination. Extensive experiments on a number of captured document images demonstrate the effectiveness of the proposed method.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Meng_Extraction_of_Virtual_ICCV_2015_paper.pdf",
        "aff": "National Laboratory of Pattern Recognition, Institute of Automation Chinese Academy of Sciences, Beijing P.R.China, 100190; National Laboratory of Pattern Recognition, Institute of Automation Chinese Academy of Sciences, Beijing P.R.China, 100190; Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University Xi'an, Shaanxi Province, P.R.China 710049; National Laboratory of Pattern Recognition, Institute of Automation Chinese Academy of Sciences, Beijing P.R.China, 100190; National Laboratory of Pattern Recognition, Institute of Automation Chinese Academy of Sciences, Beijing P.R.China, 100190",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4261811,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13736388704795053100&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 6,
        "aff_domain": "nlpr.ia.ac.cn;ia.ac.cn;mail.xjtu.edu.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "email": "nlpr.ia.ac.cn;ia.ac.cn;mail.xjtu.edu.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Meng_Extraction_of_Virtual_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "Institute of Automation Chinese Academy of Sciences;Xi'an Jiaotong University",
        "aff_unique_dep": "National Laboratory of Pattern Recognition;Institute of Artificial Intelligence and Robotics",
        "aff_unique_url": "http://www.ia.cas.cn;http://www.xjtu.edu.cn",
        "aff_unique_abbr": "IAS;XJTU",
        "aff_campus_unique_index": "0;0;1;0;0",
        "aff_campus_unique": "Beijing;Xi'an",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Meng_2015_ICCV,\n    \n    author = {\n    Meng,\n    Gaofeng and Huang,\n    Zuming and Song,\n    Yonghong and Xiang,\n    Shiming and Pan,\n    Chunhong\n},\n    title = {\n    Extraction of Virtual Baselines From Distorted Document Images Using Curvilinear Projection\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "f9d87e4047",
        "title": "FASText: Efficient Unconstrained Scene Text Detector",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Michal Bušta, Lukáš Neumann, Jiří Matas",
        "author": "Michal Busta; Lukas Neumann; Jiri Matas",
        "abstract": "We propose a novel easy-to-implement stroke detector based on an efficient pixel intensity comparison to surrounding pixels. Stroke-specific keypoints are efficiently detected and text fragments are subsequently extracted by local thresholding guided by keypoint properties. Classification based on effectively calculated features then eliminates non-text regions.  The stroke-specific keypoints produce 2 times less region segmentations and still detect 25% more characters than the commonly exploited MSER detector and the process is 4 times faster. After a novel efficient classification step, the number of regions is reduced to 7 times less than the standard method and is still almost 3 times faster.  All stages of the proposed pipeline are scale- and rotation-invariant and support a wide variety of scripts (Latin, Hebrew, Chinese, etc.) and fonts. When the proposed detector is plugged into a scene text localization and recognition pipeline, a state-of-the-art text localization accuracy is maintained  whilst the processing time is significantly reduced.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Busta_FASText_Efficient_Unconstrained_ICCV_2015_paper.pdf",
        "aff": "Centre for Machine Perception, Department of Cybernetics, Czech Technical University, Prague, Czech Republic; Centre for Machine Perception, Department of Cybernetics, Czech Technical University, Prague, Czech Republic; Centre for Machine Perception, Department of Cybernetics, Czech Technical University, Prague, Czech Republic",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 28157550,
        "gs_citation": 197,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=517107417937255110&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "fel.cvut.cz;cmp.felk.cvut.cz;cmp.felk.cvut.cz",
        "email": "fel.cvut.cz;cmp.felk.cvut.cz;cmp.felk.cvut.cz",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Busta_FASText_Efficient_Unconstrained_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Czech Technical University",
        "aff_unique_dep": "Department of Cybernetics",
        "aff_unique_url": "https://www.cvut.cz",
        "aff_unique_abbr": "CTU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Prague",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Czech Republic",
        "bibtex": "@InProceedings{Busta_2015_ICCV,\n    \n    author = {\n    Busta,\n    Michal and Neumann,\n    Lukas and Matas,\n    Jiri\n},\n    title = {\n    FASText: Efficient Unconstrained Scene Text Detector\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "4d5dbed720",
        "title": "Face Flow",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Patrick Snape, Anastasios Roussos, Yannis Panagakis, Stefanos Zafeiriou",
        "author": "Patrick Snape; Anastasios Roussos; Yannis Panagakis; Stefanos Zafeiriou",
        "abstract": "In this paper, we propose a method for the robust and efficient computation of  multi-frame optical flow in an expressive sequence of facial images. We formulate a novel energy minimisation problem for establishing dense  correspondences between a neutral template and every frame of a sequence. We exploit the highly correlated nature of human expressions by representing dense facial motion using a deformation basis. Furthermore, we exploit the even higher correlation between deformations in  a given input sequence by imposing a low-rank prior on the coefficients of the  deformation basis, yielding temporally consistent optical flow. Our proposed model-based formulation, in conjunction with the inverse  compositional strategy and low-rank matrix optimisation that we adopt, leads to  a highly efficient algorithm for calculating facial flow.  As experimental evaluation, we show quantitative experiments on a challenging  novel benchmark of face sequences, with dense ground truth optical flow provided by motion  capture data. We also provide qualitative results on a real sequence displaying fast motion and occlusions. Extensive quantitative and qualitative comparisons demonstrate that the proposed method  outperforms state-of-the-art optical flow and dense non-rigid registration techniques,  whilst running an order of magnitude faster.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Snape_Face_Flow_ICCV_2015_paper.pdf",
        "aff": "Department of Computing, Imperial College London; Department of Computing, Imperial College London; Department of Computing, Imperial College London; Department of Computing, Imperial College London",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 957910,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18107751641640580860&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 20,
        "aff_domain": "imperial.ac.uk;imperial.ac.uk;imperial.ac.uk;imperial.ac.uk",
        "email": "imperial.ac.uk;imperial.ac.uk;imperial.ac.uk;imperial.ac.uk",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Snape_Face_Flow_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Imperial College London",
        "aff_unique_dep": "Department of Computing",
        "aff_unique_url": "https://www.imperial.ac.uk",
        "aff_unique_abbr": "Imperial",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "London",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United Kingdom",
        "bibtex": "@InProceedings{Snape_2015_ICCV,\n    \n    author = {\n    Snape,\n    Patrick and Roussos,\n    Anastasios and Panagakis,\n    Yannis and Zafeiriou,\n    Stefanos\n},\n    title = {\n    Face Flow\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "f208511e77",
        "title": "FaceDirector: Continuous Control of Facial Performance in Video",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Charles Malleson, Jean-Charles Bazin, Oliver Wang, Derek Bradley, Thabo Beeler, Adrian Hilton, Alexander Sorkine-Hornung",
        "author": "Charles Malleson; Jean-Charles Bazin; Oliver Wang; Derek Bradley; Thabo Beeler; Adrian Hilton; Alexander Sorkine-Hornung",
        "abstract": "We present a method to continuously blend between multiple facial performances of an actor, which can contain different facial expressions or emotional states. As an example, given sad and angry video takes of a scene, our method empowers the movie director to specify arbitrary weighted combinations and smooth transitions between the two takes in post-production. Our contributions include (1) a robust nonlinear audio-visual synchronization technique that exploits complementary properties of audio and visual cues to automatically determine robust, dense spatiotemporal correspondences between takes, and (2) a seamless facial blending approach that provides the director full control to interpolate timing, facial expression, and local appearance, in order to generate novel performances after filming. In contrast to most previous works, our approach operates entirely in image space, avoiding the need of 3D facial reconstruction. We demonstrate that our method can synthesize visually believable performances with applications in emotion transition, performance correction, and timing control.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Malleson_FaceDirector_Continuous_Control_ICCV_2015_paper.pdf",
        "aff": ";;;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3798068,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6483257430007928398&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "author_num": 7,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Malleson_FaceDirector_Continuous_Control_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Malleson_2015_ICCV,\n    \n    author = {\n    Malleson,\n    Charles and Bazin,\n    Jean-Charles and Wang,\n    Oliver and Bradley,\n    Derek and Beeler,\n    Thabo and Hilton,\n    Adrian and Sorkine-Hornung,\n    Alexander\n},\n    title = {\n    FaceDirector: Continuous Control of Facial Performance in Video\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "ef1aa42efa",
        "title": "Fast Orthogonal Projection Based on Kronecker Product",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Xu Zhang, Felix X. Yu, Ruiqi Guo, Sanjiv Kumar, Shengjin Wang, Shi-Fu Chang",
        "author": "Xu Zhang; Felix X. Yu; Ruiqi Guo; Sanjiv Kumar; Shengjin Wang; Shi-Fu Chang",
        "abstract": "We propose a family of structured matrices to speed up orthogonal projections for high-dimensional data commonly seen in computer vision applications. In this, a structured matrix is formed by the Kronecker product of a series of smaller orthogonal matrices. This achieves O(dlogd) computational complexity and O(logd) space complexity for d-dimensional data, a drastic improvement over the standard unstructured projections whose computational and space complexities are both O(d^2). The proposed structured matrices are applicable to a number of application domains, and are faster and more compact than other structured matrices used in the past. We also introduce an efficient learning procedure for optimizing such matrices in a data dependent fashion. We demonstrate the significant advantages of the proposed approach in solving the approximate nearest neighbor (ANN) image search problem with both binary embedding and quantization. We find that the orthogonality plays a very important role in solving ANN problem, since the random orthogonal Kronecker projection has already provided promising performance. Comprehensive experiments show that the proposed approach can achieve similar or better accuracy as the existing state-of-the-art but with significantly less time and memory.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Zhang_Fast_Orthogonal_Projection_ICCV_2015_paper.pdf",
        "aff": "Tsinghua University; Columbia University + Google Research; Google Research; Google Research; Tsinghua University; Columbia University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 725871,
        "gs_citation": 55,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16904676947770447833&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Zhang_Fast_Orthogonal_Projection_ICCV_2015_paper.html",
        "aff_unique_index": "0;1+2;2;2;0;1",
        "aff_unique_norm": "Tsinghua University;Columbia University;Google",
        "aff_unique_dep": ";;Google Research",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.columbia.edu;https://research.google",
        "aff_unique_abbr": "THU;Columbia;Google Research",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;1+1;1;1;0;1",
        "aff_country_unique": "China;United States",
        "bibtex": "@InProceedings{Zhang_2015_ICCV,\n    \n    author = {\n    Zhang,\n    Xu and Yu,\n    Felix X. and Guo,\n    Ruiqi and Kumar,\n    Sanjiv and Wang,\n    Shengjin and Chang,\n    Shi-Fu\n},\n    title = {\n    Fast Orthogonal Projection Based on Kronecker Product\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "edb6043a25",
        "title": "Fast R-CNN",
        "session": "learning representations and attributes",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "Ross Girshick",
        "author": "Ross Girshick",
        "abstract": "This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection.  Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks.  Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy.  Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012.  Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate.  Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf",
        "aff": "Microsoft Research",
        "project": "",
        "github": "https://github.com/rbgirshick/fast-rcnn",
        "supp": "",
        "arxiv": "",
        "pdf_size": 589236,
        "gs_citation": 40802,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16324699838103945745&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 36,
        "aff_domain": "microsoft.com",
        "email": "microsoft.com",
        "author_num": 1,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Girshick_Fast_R-CNN_ICCV_2015_paper.html",
        "aff_unique_index": "0",
        "aff_unique_norm": "Microsoft Corporation",
        "aff_unique_dep": "Microsoft Research",
        "aff_unique_url": "https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "MSR",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Girshick_2015_ICCV,\n    \n    author = {\n    Girshick,\n    Ross\n},\n    title = {\n    Fast R-CNN\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "f1e7a2557c",
        "title": "Fast and Accurate Head Pose Estimation via Random Projection Forests",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Donghoon Lee, Ming-Hsuan Yang, Songhwai Oh",
        "author": "Donghoon Lee; Ming-Hsuan Yang; Songhwai Oh",
        "abstract": "In this paper, we consider the problem of estimating the gaze direction of a person from a low-resolution image.  Under this condition, reliably extracting facial features is very difficult.  We propose a novel head pose estimation algorithm based on compressive sensing.  Head image patches are mapped to a large feature space using the proposed extensive, yet efficient filter bank.  The filter bank is designed to generate sparse responses of color and gradient information, which can be compressed using random projection, and classified by a random forest.  Extensive experiments on challenging datasets show that the proposed algorithm performs favorably against the state-of-the-art methods on head pose estimation in low-resolution images degraded by noise, occlusion, and blurring.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Lee_Fast_and_Accurate_ICCV_2015_paper.pdf",
        "aff": "Electrical and Computer Engineering, Seoul National University, Korea; Electrical Engineering and Computer Science, University of California at Merced; Electrical and Computer Engineering, Seoul National University, Korea",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1412930,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17560566674359975957&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "cpslab.snu.ac.kr;ucmerced.edu;snu.ac.kr",
        "email": "cpslab.snu.ac.kr;ucmerced.edu;snu.ac.kr",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Lee_Fast_and_Accurate_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Seoul National University;University of California, Merced",
        "aff_unique_dep": "Electrical and Computer Engineering;Electrical Engineering and Computer Science",
        "aff_unique_url": "https://www.snu.ac.kr;https://www.ucmerced.edu",
        "aff_unique_abbr": "SNU;UC Merced",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Merced",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "South Korea;United States",
        "bibtex": "@InProceedings{Lee_2015_ICCV,\n    \n    author = {\n    Lee,\n    Donghoon and Yang,\n    Ming-Hsuan and Oh,\n    Songhwai\n},\n    title = {\n    Fast and Accurate Head Pose Estimation via Random Projection Forests\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "4a3619a230",
        "title": "Fast and Effective L0 Gradient Minimization by Region Fusion",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Rang M. H. Nguyen, Michael S. Brown",
        "author": "Rang M. H. Nguyen; Michael S. Brown",
        "abstract": "L_0 gradient minimization can be applied to an input signal to control the number of non-zero gradients.  This is useful in reducing small gradients generally associated with signal noise, while preserving important signal features.  In computer vision, L_0 gradient minimization has found applications in image denoising, 3D mesh denoising, and image enhancement.  Minimizing the L_0 norm, however, is an NP-hard problem because of its non-convex property. As a result, existing methods rely on approximation strategies to perform the minimization.  In this paper, we present a new method to perform L_0 gradient minimization that is fast and effective. Our method uses a descent approach based on region fusion that converges faster than other methods while providing a better approximation of the optimal L_0 norm.  In addition, our method can be applied to both 2D images and 3D mesh topologies.  The effectiveness of our approach is demonstrated on a number of examples.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Nguyen_Fast_and_Effective_ICCV_2015_paper.pdf",
        "aff": ";",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 89,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10330103006465158877&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Nguyen_Fast_and_Effective_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Nguyen_2015_ICCV,\n    \n    author = {\n    Nguyen,\n    Rang M. H. and Brown,\n    Michael S.\n},\n    title = {\n    Fast and Effective L0 Gradient Minimization by Region Fusion\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "0b28c30f0f",
        "title": "Fill and Transfer: A Simple Physics-Based Approach for Containability Reasoning",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Lap-Fai Yu, Noah Duncan, Sai-Kit Yeung",
        "author": "Lap-Fai Yu; Noah Duncan; Sai-Kit Yeung",
        "abstract": "The visual perception of object affordances has emerged as a useful ingredient for building powerful computer vision and robotic applications. In this paper we introduce a novel approach to reason about liquid containability - the affordance of containing liquid. Our approach analyzes container objects based on two simple physical processes: the Fill and Transfer of liquid. First, it reasons about whether a given 3D object is a liquid container and its best filling direction. Second, it proposes directions to transfer its contained liquid to the outside while avoiding spillage. We compare our simplified model with a common fluid dynamics simulation and demonstrate that our algorithm makes human-like choices about the best directions to fill containers and transfer liquid from them. We apply our approach to reason about the containability of several real-world objects acquired using a consumer-grade depth camera.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Yu_Fill_and_Transfer_ICCV_2015_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2729862,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3971166767606556170&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Yu_Fill_and_Transfer_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Yu_2015_ICCV,\n    \n    author = {\n    Yu,\n    Lap-Fai and Duncan,\n    Noah and Yeung,\n    Sai-Kit\n},\n    title = {\n    Fill and Transfer: A Simple Physics-Based Approach for Containability Reasoning\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "e6e4ae6be6",
        "title": "Fine-Grained Change Detection of Misaligned Scenes With Varied Illuminations",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Wei Feng, Fei-Peng Tian, Qian Zhang, Nan Zhang, Liang Wan, Jizhou Sun",
        "author": "Wei Feng; Fei-Peng Tian; Qian Zhang; Nan Zhang; Liang Wan; Jizhou Sun",
        "abstract": "Detecting fine-grained subtle changes among a scene is critically important in practice. Previous change detection methods, focusing on detecting large-scale significant changes, cannot do this well. This paper proposes a feasible end-to-end approach to this challenging problem. We start from active camera relocation that quickly relocates camera to nearly the same pose and position of the last time observation. To guarantee detection sensitivity and accuracy of minute changes, in an observation, we capture a group of images under multiple illuminations, which need only to be roughly aligned to the last time lighting conditions. Given two times observations, we formulate fine-grained change detection as a joint optimization problem of three related factors, i.e., normal-aware lighting difference, camera geometry correction flow, and real scene change mask. We solve the three factors in a coarse-to-fine manner and achieve reliable change decision by rank minimization. We build three real-world datasets to benchmark fine-grained change detection of misaligned scenes under varied multiple lighting conditions. Extensive experiments show the superior performance of our approach over state-of-the-art change detection methods and its ability to distinguish real scene changes from false ones caused by lighting variations.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Feng_Fine-Grained_Change_Detection_ICCV_2015_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 8641136,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7850966195346334154&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Feng_Fine-Grained_Change_Detection_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Feng_2015_ICCV,\n    \n    author = {\n    Feng,\n    Wei and Tian,\n    Fei-Peng and Zhang,\n    Qian and Zhang,\n    Nan and Wan,\n    Liang and Sun,\n    Jizhou\n},\n    title = {\n    Fine-Grained Change Detection of Misaligned Scenes With Varied Illuminations\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "26a486ed2a",
        "title": "Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Bryan A. Plummer, Liwei Wang, Chris M. Cervantes, Juan C. Caicedo, Julia Hockenmaier, Svetlana Lazebnik",
        "author": "Bryan A. Plummer; Liwei Wang; Chris M. Cervantes; Juan C. Caicedo; Julia Hockenmaier; Svetlana Lazebnik",
        "abstract": "The Flickr30k dataset has become a standard benchmark for sentence-based image description. This paper presents Flickr30k Entities, which augments the 158k captions from Flickr30k with 244k coreference chains linking mentions of the same entities in images, as well as  276k manually annotated bounding boxes corresponding to each entity. Such annotation is essential for continued progress in automatic image description and grounded language understanding. We present experiments demonstrating the usefulness of our annotations for text-to-image reference resolution, or the task of localizing textual entity mentions in an image, and for bidirectional image-sentence retrieval. These experiments confirm that we can further improve the accuracy of state-of-the-art retrieval methods by training with explicit region-to-phrase correspondence, but at the same time, they show that accurately inferring this correspondence given an image and a caption remains really challenging.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Plummer_Flickr30k_Entities_Collecting_ICCV_2015_paper.pdf",
        "aff": "Univ. of Illinois at Urbana-Champaign; Univ. of Illinois at Urbana-Champaign; Univ. of Illinois at Urbana-Champaign; Fundaci ´on Univ. Konrad Lorenz; Univ. of Illinois at Urbana-Champaign; Univ. of Illinois at Urbana-Champaign",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 915731,
        "gs_citation": 2475,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7850289731690963804&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 25,
        "aff_domain": "illinois.edu;illinois.edu;illinois.edu;konradlorenz.edu.co;illinois.edu;illinois.edu",
        "email": "illinois.edu;illinois.edu;illinois.edu;konradlorenz.edu.co;illinois.edu;illinois.edu",
        "author_num": 6,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Plummer_Flickr30k_Entities_Collecting_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;1;0;0",
        "aff_unique_norm": "University of Illinois at Urbana-Champaign;Fundación Universitaria Konrad Lorenz",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://illinois.edu;https://www.fundacionkl.org.co",
        "aff_unique_abbr": "UIUC;FUKL",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Urbana-Champaign;",
        "aff_country_unique_index": "0;0;0;1;0;0",
        "aff_country_unique": "United States;Colombia",
        "bibtex": "@InProceedings{Plummer_2015_ICCV,\n    \n    author = {\n    Plummer,\n    Bryan A. and Wang,\n    Liwei and Cervantes,\n    Chris M. and Caicedo,\n    Juan C. and Hockenmaier,\n    Julia and Lazebnik,\n    Svetlana\n},\n    title = {\n    Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "d4d6f6a654",
        "title": "Flow Fields: Dense Correspondence Fields for Highly Accurate Large Displacement Optical Flow Estimation",
        "session": "motion and correspondence",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "Christian Bailer, Bertram Taetz, Didier Stricker",
        "author": "Christian Bailer; Bertram Taetz; Didier Stricker",
        "abstract": "Modern large displacement optical flow algorithms usually use an initialization by either sparse descriptor matching techniques or dense approximate nearest neighbor fields. While the latter have the advantage of being dense, they have the major disadvantage of being very outlier prone as they are not designed to find the optical flow, but the visually most similar correspondence. In this paper we present a dense correspondence field approach that is much less outlier prone and thus much better suited for optical flow estimation than approximate nearest neighbor fields. Our approach is conceptually novel as it does not require explicit regularization, smoothing (like median filtering) or a new data term, but solely our novel purely data based search strategy that finds most inliers (even for small objects), while it effectively avoids finding outliers. Moreover, we present novel enhancements for outlier filtering. We show that our approach is better suited for large displacement optical flow estimation than state-of-the-art descriptor matching techniques. We do so by initializing EpicFlow (so far the best method on MPI-Sintel) with our Flow Fields instead of their originally used state-of-the-art descriptor matching technique. We significantly outperform the original EpicFlow on MPI-Sintel, KITTI and Middlebury.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Bailer_Flow_Fields_Dense_ICCV_2015_paper.pdf",
        "aff": "German Research Center for Artificial Intelligence (DFKI) + University of Kaiserslautern; German Research Center for Artificial Intelligence (DFKI) + University of Kaiserslautern; German Research Center for Artificial Intelligence (DFKI) + University of Kaiserslautern",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3486285,
        "gs_citation": 325,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12540170961157454281&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 21,
        "aff_domain": "dfki.de;dfki.de;dfki.de",
        "email": "dfki.de;dfki.de;dfki.de",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Bailer_Flow_Fields_Dense_ICCV_2015_paper.html",
        "aff_unique_index": "0+1;0+1;0+1",
        "aff_unique_norm": "German Research Center for Artificial Intelligence;University of Kaiserslautern",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.dFKI.de;https://www.uni-kl.de",
        "aff_unique_abbr": "DFKI;Uni KL",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "Germany",
        "bibtex": "@InProceedings{Bailer_2015_ICCV,\n    \n    author = {\n    Bailer,\n    Christian and Taetz,\n    Bertram and Stricker,\n    Didier\n},\n    title = {\n    Flow Fields: Dense Correspondence Fields for Highly Accurate Large Displacement Optical Flow Estimation\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "a02158d33c",
        "title": "FlowNet: Learning Optical Flow With Convolutional Networks",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Häusser, Caner Hazırbaş, Vladimir Golkov, Patrick van der Smagt, Daniel Cremers, Thomas Brox",
        "author": "Alexey Dosovitskiy; Philipp Fischer; Eddy Ilg; Philip Hausser; Caner Hazirbas; Vladimir Golkov; Patrick van der Smagt; Daniel Cremers; Thomas Brox",
        "abstract": "Convolutional neural networks (CNNs) have recently been very successful in a variety of computer vision tasks, especially on those linked to recognition. Optical flow estimation has not been among the tasks CNNs succeeded at. In this paper we construct CNNs which are capable of solving the optical flow estimation problem as a supervised learning task. We propose and compare two architectures: a generic architecture and another one including a layer that correlates feature vectors at different image locations. Since existing ground truth data sets are not sufficiently large to train a CNN, we generate a large synthetic Flying Chairs dataset. We show that networks trained on this unrealistic data still generalize very well to existing datasets such as Sintel and KITTI, achieving competitive accuracy at frame rates of 5 to 10 fps.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Dosovitskiy_FlowNet_Learning_Optical_ICCV_2015_paper.pdf",
        "aff": "University of Freiburg+Technical University of Munich; University of Freiburg+Technical University of Munich; University of Freiburg+Technical University of Munich; Technical University of Munich; Technical University of Munich; Technical University of Munich; Technical University of Munich; Technical University of Munich; University of Freiburg",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 7409877,
        "gs_citation": 4909,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14306873502916215398&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": "cs.uni-freiburg.de;cs.uni-freiburg.de;cs.uni-freiburg.de;cs.tum.edu;cs.tum.edu;cs.tum.edu;brml.org;tum.de;cs.uni-freiburg.de",
        "email": "cs.uni-freiburg.de;cs.uni-freiburg.de;cs.uni-freiburg.de;cs.tum.edu;cs.tum.edu;cs.tum.edu;brml.org;tum.de;cs.uni-freiburg.de",
        "author_num": 9,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Dosovitskiy_FlowNet_Learning_Optical_ICCV_2015_paper.html",
        "aff_unique_index": "0+1;0+1;0+1;1;1;1;1;1;0",
        "aff_unique_norm": "University of Freiburg;Technical University of Munich",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uni-freiburg.de;https://www.tum.de",
        "aff_unique_abbr": "UoF;TUM",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;0;0;0;0;0;0",
        "aff_country_unique": "Germany",
        "bibtex": "@InProceedings{Dosovitskiy_2015_ICCV,\n    \n    author = {\n    Dosovitskiy,\n    Alexey and Fischer,\n    Philipp and Ilg,\n    Eddy and Hausser,\n    Philip and Hazirbas,\n    Caner and Golkov,\n    Vladimir and van der Smagt,\n    Patrick and Cremers,\n    Daniel and Brox,\n    Thomas\n},\n    title = {\n    FlowNet: Learning Optical Flow With Convolutional Networks\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "d8f4cf4a74",
        "title": "Flowing ConvNets for Human Pose Estimation in Videos",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Tomas Pfister, James Charles, Andrew Zisserman",
        "author": "Tomas Pfister; James Charles; Andrew Zisserman",
        "abstract": "The objective of this work is human pose estimation in videos, where multiple frames are available.  We investigate a ConvNet architecture that is able to benefit from temporal context by combining information across the multiple frames using optical flow.  To this end we propose a network architecture with the following novelties: (i) a deeper network than previously investigated for regressing heatmaps; (ii) spatial fusion layers that learn an implicit spatial model; (iii) optical flow is used to align heatmap predictions from neighbouring frames; and (iv) a final parametric pooling layer which learns to combine the aligned heatmaps into a pooled confidence map.  We show that this architecture outperforms a number of others, including one that uses optical flow solely at the input layers, one that regresses joint coordinates directly, and one that predicts heatmaps without spatial fusion.  The new architecture outperforms the state of the art by a large margin on three video pose estimation datasets, including the very challenging Poses in the Wild dataset, and outperforms other deep methods that don't use a graphical model on the single-image FLIC benchmark (and also Chen & Yuille and Tompson et al. in the high precision region).",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Pfister_Flowing_ConvNets_for_ICCV_2015_paper.pdf",
        "aff": "Dept. of Engineering Science, University of Oxford; School of Computing, University of Leeds; Dept. of Engineering Science, University of Oxford",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1289844,
        "gs_citation": 741,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13561683774802539240&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff_domain": "robots.ox.ac.uk;leeds.ac.uk;robots.ox.ac.uk",
        "email": "robots.ox.ac.uk;leeds.ac.uk;robots.ox.ac.uk",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Pfister_Flowing_ConvNets_for_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Oxford;University of Leeds",
        "aff_unique_dep": "Dept. of Engineering Science;School of Computing",
        "aff_unique_url": "https://www.ox.ac.uk;https://www.leeds.ac.uk",
        "aff_unique_abbr": "Oxford;Leeds",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Oxford;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom",
        "bibtex": "@InProceedings{Pfister_2015_ICCV,\n    \n    author = {\n    Pfister,\n    Tomas and Charles,\n    James and Zisserman,\n    Andrew\n},\n    title = {\n    Flowing ConvNets for Human Pose Estimation in Videos\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "0083283afd",
        "title": "FollowMe: Efficient Online Min-Cost Flow Tracking With Bounded Memory and Computation",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Philip Lenz, Andreas Geiger, Raquel Urtasun",
        "author": "Philip Lenz; Andreas Geiger; Raquel Urtasun",
        "abstract": "One of the most popular approaches to multi-target tracking is tracking-by-detection. Current  min-cost flow algorithms which solve the data association problem optimally have three main drawbacks: they are computationally expensive, they assume that the whole video is given as a batch, and they scale badly in memory and computation with the length of the video sequence. In this paper, we address each of these issues, resulting in a computationally and memory-bounded solution. First, we introduce a dynamic version of the successive shortest-path algorithm  which solves the data association problem optimally while reusing computation, resulting in  faster inference than standard solvers. Second, we address the optimal solution to the data association problem when dealing with an incoming stream of data (i.e., online setting). Finally, we present our main contribution which is an approximate online solution with bounded memory and computation which is capable of handling videos of arbitrary length while performing tracking in real time. We demonstrate the effectiveness of our algorithms on the KITTI and PETS2009 benchmarks and show state-of-the-art performance, while being significantly faster than existing solvers.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Lenz_FollowMe_Efficient_Online_ICCV_2015_paper.pdf",
        "aff": "Karlsruhe Institute of Technology; MPI Tübingen; University of Toronto",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1016112,
        "gs_citation": 132,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17306060670456878809&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff_domain": "kit.edu;tue.mpg.de;cs.toronto.edu",
        "email": "kit.edu;tue.mpg.de;cs.toronto.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Lenz_FollowMe_Efficient_Online_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Karlsruhe Institute of Technology;Max Planck Institute for Biological Cybernetics;University of Toronto",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.kit.edu;https://www.cbs.mpg.de;https://www.utoronto.ca",
        "aff_unique_abbr": "KIT;MPI CBS;U of T",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Tübingen",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "Germany;Canada",
        "bibtex": "@InProceedings{Lenz_2015_ICCV,\n    \n    author = {\n    Lenz,\n    Philip and Geiger,\n    Andreas and Urtasun,\n    Raquel\n},\n    title = {\n    FollowMe: Efficient Online Min-Cost Flow Tracking With Bounded Memory and Computation\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "a9b55bf609",
        "title": "Frequency-Based Environment Matting by Compressive Sensing",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Yiming Qian, Minglun Gong, Yee-Hong Yang",
        "author": "Yiming Qian; Minglun Gong; Yee-Hong Yang",
        "abstract": "Extracting environment mattes using existing approaches often requires either thousands of captured images or a long processing time, or both. In this paper, we propose a novel approach to capturing and extracting the matte of a real scene effectively and efficiently. Grown out of the traditional frequency-based signal analysis, our approach can accurately locate contributing sources. By exploiting the recently developed compressive sensing theory, we simplify the data acquisition process of frequency-based environment matting. Incorporating phase information in a frequency signal into data acquisition further accelerates the matte extraction procedure. Compared with the state-of-the-art method, our approach achieves superior performance on both synthetic and real data, while consuming only a fraction of the processing time.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Qian_Frequency-Based_Environment_Matting_ICCV_2015_paper.pdf",
        "aff": "University of Alberta; Memorial University of Newfoundland; University of Alberta",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 5874650,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=416781764871172186&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "ualberta.ca;cs.mun.ca;cs.ualberta.ca",
        "email": "ualberta.ca;cs.mun.ca;cs.ualberta.ca",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Qian_Frequency-Based_Environment_Matting_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Alberta;Memorial University of Newfoundland",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ualberta.ca;https://www.mun.ca",
        "aff_unique_abbr": "UAlberta;MUN",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada",
        "bibtex": "@InProceedings{Qian_2015_ICCV,\n    \n    author = {\n    Qian,\n    Yiming and Gong,\n    Minglun and Yang,\n    Yee-Hong\n},\n    title = {\n    Frequency-Based Environment Matting by Compressive Sensing\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "736b3e4cf5",
        "title": "From Emotions to Action Units With Hidden and Semi-Hidden-Task Learning",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Adria Ruiz, Joost Van de Weijer, Xavier Binefa",
        "author": "Adria Ruiz; Joost Van de Weijer; Xavier Binefa",
        "abstract": "Limited annotated training data is a challenging problem in Action Unit recognition. In this paper, we investigate how the use of large databases labelled according to the 6 universal facial expressions can increase the generalization ability of Action Unit classifiers. For this purpose, we propose a novel learning framework: Hidden-Task Learning. HTL aims to learn a set of Hidden-Tasks (Action Units) for which samples are not available but, in contrast, training data is easier to obtain from a set of related Visible-Tasks (Facial Expressions). To that end, HTL is able to exploit prior knowledge about the relation between Hidden and Visible-Tasks. In our case, we base this prior knowledge on empirical psychological studies providing statistical correlations between Action Units and universal facial expressions. Additionally, we extend HTL to Semi-Hidden Task Learning (SHTL) assuming that Action Unit training samples are also provided. Performing exhaustive experiments over four different datasets, we show that HTL and SHTL improve the generalization ability of AU classifiers by training them with additional facial expression data. Additionally, we show that SHTL achieves competitive performance compared with state-of-the-art Transductive Learning approaches which face the problem of limited training data by using unlabelled test samples during training.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Ruiz_From_Emotions_to_ICCV_2015_paper.pdf",
        "aff": "Universitat Pompeu Fabra; Centre de Visio per Computador; Universitat Pompeu Fabra",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1506415,
        "gs_citation": 68,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1987307785070478299&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "upf.es;cvc.uab.es;upf.es",
        "email": "upf.es;cvc.uab.es;upf.es",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Ruiz_From_Emotions_to_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Universitat Pompeu Fabra;Centre de Visio per Computador",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.upf.edu/;https://www.cvc.uab.cat/",
        "aff_unique_abbr": "UPF;CVC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Spain",
        "bibtex": "@InProceedings{Ruiz_2015_ICCV,\n    \n    author = {\n    Ruiz,\n    Adria and Van de Weijer,\n    Joost and Binefa,\n    Xavier\n},\n    title = {\n    From Emotions to Action Units With Hidden and Semi-Hidden-Task Learning\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "e9b1844b30",
        "title": "From Facial Parts Responses to Face Detection: A Deep Learning Approach",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Shuo Yang, Ping Luo, Chen-Change Loy, Xiaoou Tang",
        "author": "Shuo Yang; Ping Luo; Chen-Change Loy; Xiaoou Tang",
        "abstract": "In this paper, we propose a novel deep convolutional network (DCN) that achieves outstanding performance on FDDB, PASCAL Face, and AFW. Specifically, our method achieves a high recall rate of 90.99% on the challenging FDDB benchmark, outperforming the state-of-the-art method by a large margin of 2.91%. Importantly, we consider finding faces from a new perspective through scoring facial parts responses by their spatial structure and arrangement. The scoring mechanism is carefully formulated considering challenging cases where faces are only partially visible. This consideration allows our network to detect faces under severe occlusion and unconstrained pose variation, which are the main difficulty and bottleneck of most existing face detection approaches. We show that despite the use of DCN, our network can achieve practical runtime speed.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Yang_From_Facial_Parts_ICCV_2015_paper.pdf",
        "aff": "Department of Information Engineering, The Chinese University of Hong Kong + Shenzhen Key Lab of Comp. Vis. & Pat. Rec., Shenzhen Institutes of Advanced Technology, CAS, China; Department of Information Engineering, The Chinese University of Hong Kong + Shenzhen Key Lab of Comp. Vis. & Pat. Rec., Shenzhen Institutes of Advanced Technology, CAS, China; Department of Information Engineering, The Chinese University of Hong Kong + Shenzhen Key Lab of Comp. Vis. & Pat. Rec., Shenzhen Institutes of Advanced Technology, CAS, China; Department of Information Engineering, The Chinese University of Hong Kong + Shenzhen Key Lab of Comp. Vis. & Pat. Rec., Shenzhen Institutes of Advanced Technology, CAS, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 5903070,
        "gs_citation": 798,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1818335115841631894&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff_domain": "ie.cuhk.edu.hk;ie.cuhk.edu.hk;ie.cuhk.edu.hk;ie.cuhk.edu.hk",
        "email": "ie.cuhk.edu.hk;ie.cuhk.edu.hk;ie.cuhk.edu.hk;ie.cuhk.edu.hk",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Yang_From_Facial_Parts_ICCV_2015_paper.html",
        "aff_unique_index": "0+1;0+1;0+1;0+1",
        "aff_unique_norm": "The Chinese University of Hong Kong;Shenzhen Institutes of Advanced Technology",
        "aff_unique_dep": "Department of Information Engineering;Shenzhen Key Lab of Comp. Vis. & Pat. Rec.",
        "aff_unique_url": "https://www.cuhk.edu.hk;http://www.siat.ac.cn",
        "aff_unique_abbr": "CUHK;SIAT",
        "aff_campus_unique_index": "0+1;0+1;0+1;0+1",
        "aff_campus_unique": "Hong Kong SAR;Shenzhen",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Yang_2015_ICCV,\n    \n    author = {\n    Yang,\n    Shuo and Luo,\n    Ping and Loy,\n    Chen-Change and Tang,\n    Xiaoou\n},\n    title = {\n    From Facial Parts Responses to Face Detection: A Deep Learning Approach\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "b3c8436c35",
        "title": "Fully Connected Guided Image Filtering",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Longquan Dai, Mengke Yuan, Feihu Zhang, Xiaopeng Zhang",
        "author": "Longquan Dai; Mengke Yuan; Feihu Zhang; Xiaopeng Zhang",
        "abstract": "This paper presents a linear time fully connected guided filter by introducing the minimum spanning tree (MST) to the guided filter (GF). Since the intensity based filtering kernel of GF is apt to overly smooth edges and the fixed-shape local box support region adopted by GF is not geometric-adaptive, our filter introduces an extra spatial term, the tree similarity, to the filtering kernel of GF and substitutes the box window with the implicit support region by establishing all-pairs-connections among pixels in the image and assigning the spatial-intensity-aware similarity to these connections. The adaptive implicit support region composed by the pixels with large kernel weights in the entire image domain has a big advantage over the predefined local box window in presenting the structure of an image for the reason that: 1, MST can efficiently present the structure of an image; 2, the kernel weight of our filter considers the tree distance defined on the MST. Due to these reasons, our filter achieves better edge-preserving results. We demonstrate the strength of the proposed filter in several applications. Experimental results show that our method produces better results than state-of-the-art methods.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Dai_Fully_Connected_Guided_ICCV_2015_paper.pdf",
        "aff": "NLPR-LIAMA, Institute of Automation Chinese Academy of Sciences, BEIJING, CHINA; NLPR-LIAMA, Institute of Automation Chinese Academy of Sciences, BEIJING, CHINA; NLPR-LIAMA, Institute of Automation Chinese Academy of Sciences, BEIJING, CHINA; NLPR-LIAMA, Institute of Automation Chinese Academy of Sciences, BEIJING, CHINA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1585000,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13624768385305532164&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "ia.ac.cn;ia.ac.cn;gmail.com;ia.ac.cn",
        "email": "ia.ac.cn;ia.ac.cn;gmail.com;ia.ac.cn",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Dai_Fully_Connected_Guided_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Institute of Automation Chinese Academy of Sciences",
        "aff_unique_dep": "NLPR-LIAMA",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "BEIJING",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Dai_2015_ICCV,\n    \n    author = {\n    Dai,\n    Longquan and Yuan,\n    Mengke and Zhang,\n    Feihu and Zhang,\n    Xiaopeng\n},\n    title = {\n    Fully Connected Guided Image Filtering\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "b915a3d428",
        "title": "Fully Connected Object Proposals for Video Segmentation",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Federico Perazzi, Oliver Wang, Markus Gross, Alexander Sorkine-Hornung",
        "author": "Federico Perazzi; Oliver Wang; Markus Gross; Alexander Sorkine-Hornung",
        "abstract": "We present a novel approach to video segmentation using multiple object proposals. The problem is formulated as a minimization of a novel energy function defined over a fully connected graph of object proposals. Our model combines appearance with long-range point tracks, which is key to ensure robustness with respect to fast motion and occlusions over longer video sequences. As opposed to previous approaches based on object proposals, we do not seek the best per-frame object hypotheses to perform the segmentation. Instead, we combine multiple, potentially imperfect proposals to improve overall segmentation accuracy and ensure robustness to outliers. Overall, the basic algorithm consists of three steps. First, we generate a very large number of object proposals for each video frame using existing techniques. Next, we perform an SVM-based pruning step to retain only high quality proposals with sufficiently discriminative power. Finally, we determine the fore- and background classification by solving for the maximum a posteriori of a fully connected conditional random field, defined using our novel energy function. Experimental results on a well established dataset demonstrate that our method compares favorably to several recent state-of-the-art approaches.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Perazzi_Fully_Connected_Object_ICCV_2015_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1535593,
        "gs_citation": 210,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2357192075196348324&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Perazzi_Fully_Connected_Object_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Perazzi_2015_ICCV,\n    \n    author = {\n    Perazzi,\n    Federico and Wang,\n    Oliver and Gross,\n    Markus and Sorkine-Hornung,\n    Alexander\n},\n    title = {\n    Fully Connected Object Proposals for Video Segmentation\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "f6588e3284",
        "title": "General Dynamic Scene Reconstruction From Multiple View Video",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Armin Mustafa, Hansung Kim, Jean-Yves Guillemaut, Adrian Hilton",
        "author": "Armin Mustafa; Hansung Kim; Jean-Yves Guillemaut; Adrian Hilton",
        "abstract": "This paper introduces a general approach to dynamic scene   reconstruction from multiple moving cameras without prior knowledge or  limiting constraints on the scene structure, appearance, or  illumination. Existing techniques  or dynamic scene reconstruction  from multiple wide-baseline camera views primarily focus on accurate  reconstruction in controlled  environments, where the cameras are  fixed and calibrated and background is known. These approaches are not robust for general dynamic scenes captured with sparse moving cameras. Previous approaches for outdoor dynamic scene  reconstruction assume prior knowledge of the static background appearance and  structure. The primary contributions of this paper are twofold: an automatic method for initial coarse dynamic scene segmentation and reconstruction without prior knowledge of background appearance or structure; and a general robust approach for joint segmentation refinement and dense reconstruction of dynamic scenes from multiple wide-baseline static or moving cameras. Evaluation is performed on a variety of indoor and outdoor scenes with cluttered backgrounds and multiple dynamic non-rigid objects such as people. Comparison with state-of-the-art approaches demonstrates improved accuracy in both multiple view segmentation and dense reconstruction. The proposed approach also eliminates the requirement for prior knowledge of scene structure and appearance.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Mustafa_General_Dynamic_Scene_ICCV_2015_paper.pdf",
        "aff": "CVSSP, University of Surrey, Guildford, United Kingdom; CVSSP, University of Surrey, Guildford, United Kingdom; CVSSP, University of Surrey, Guildford, United Kingdom; CVSSP, University of Surrey, Guildford, United Kingdom",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 846107,
        "gs_citation": 85,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2238221909658262776&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff_domain": "surrey.ac.uk; ; ; ",
        "email": "surrey.ac.uk; ; ; ",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Mustafa_General_Dynamic_Scene_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Surrey",
        "aff_unique_dep": "CVSSP",
        "aff_unique_url": "https://www.surrey.ac.uk",
        "aff_unique_abbr": "Surrey",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Guildford",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United Kingdom",
        "bibtex": "@InProceedings{Mustafa_2015_ICCV,\n    \n    author = {\n    Mustafa,\n    Armin and Kim,\n    Hansung and Guillemaut,\n    Jean-Yves and Hilton,\n    Adrian\n},\n    title = {\n    General Dynamic Scene Reconstruction From Multiple View Video\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "f57d9b43ad",
        "title": "Generating Notifications for Missing Actions: Don't Forget to Turn the Lights Off!",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Bilge Soran, Ali Farhadi, Linda Shapiro",
        "author": "Bilge Soran; Ali Farhadi; Linda Shapiro",
        "abstract": "We all have experienced forgetting habitual actions among our daily activities. For example, we probably have forgotten to turn the lights off before leaving a room or turn the stove off after cooking. In this paper, we propose a solution to the problem of issuing notifications on actions that may be missed. This involves learning about interdependencies between actions and being able to predict an ongoing action while segmenting the input video stream. In order to show a proof of concept, we collected a new egocentric dataset, in which people wear a camera while making lattes. We show promising results on the extremely challenging task of issuing correct and timely reminders. We also show that our model reliably segments the actions, while predicting the ongoing one when only a few frames from the beginning of the action are observed. The overall prediction accuracy is 46.2% when only 10 frames of an action are seen (2/3 of a sec). Moreover, the overall recognition and segmentation accuracy is shown to be 72.7% when the whole activity sequence is observed. Finally, the online prediction and segmentation accuracy is 68.3% when the prediction is made at every time step.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Soran_Generating_Notifications_for_ICCV_2015_paper.pdf",
        "aff": "University of Washington; University of Washington + Allen Institute for Artificial Intelligence; University of Washington",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 796981,
        "gs_citation": 85,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=213325104941391815&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": "cs.washington.edu;cs.washington.edu;cs.washington.edu",
        "email": "cs.washington.edu;cs.washington.edu;cs.washington.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Soran_Generating_Notifications_for_ICCV_2015_paper.html",
        "aff_unique_index": "0;0+1;0",
        "aff_unique_norm": "University of Washington;Allen Institute for Artificial Intelligence",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.washington.edu;https://allenai.org",
        "aff_unique_abbr": "UW;AI2",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Soran_2015_ICCV,\n    \n    author = {\n    Soran,\n    Bilge and Farhadi,\n    Ali and Shapiro,\n    Linda\n},\n    title = {\n    Generating Notifications for Missing Actions: Don't Forget to Turn the Lights Off!\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "110447cf19",
        "title": "Generic Promotion of Diffusion-Based Salient Object Detection",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Peng Jiang, Nuno Vasconcelos, Jingliang Peng",
        "author": "Peng Jiang; Nuno Vasconcelos; Jingliang Peng",
        "abstract": "In this work, we propose a generic scheme to promote any diffusion-based salient object detection algorithm by original ways to re-synthesize the diffusion matrix and construct the seed vector. We first make a novel analysis of the working mechanism of the diffusion matrix, which reveals the close relationship between saliency diffusion and spectral clustering. Following this analysis, we propose to re-synthesize the diffusion matrix from the most discriminative eigenvectors after adaptive re-weighting. Further, we propose to generate the seed vector based on the readily available diffusion maps, avoiding extra computation for color-based seed search. As a particular instance, we use inverse normalized Laplacian matrix as the original diffusion matrix and promote the corresponding salient object detection algorithm, which leads to superior performance as experimentally demonstrated.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Jiang_Generic_Promotion_of_ICCV_2015_paper.pdf",
        "aff": "Shandong University; University of California, San Diego; Shandong University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1183704,
        "gs_citation": 66,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16118482781289316947&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "mail.sdu.edu.cn;ucsd.edu;sdu.edu.cn",
        "email": "mail.sdu.edu.cn;ucsd.edu;sdu.edu.cn",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Jiang_Generic_Promotion_of_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Shandong University;University of California, San Diego",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.sdu.edu.cn;https://www.ucsd.edu",
        "aff_unique_abbr": "SDU;UCSD",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";San Diego",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "China;United States",
        "bibtex": "@InProceedings{Jiang_2015_ICCV,\n    \n    author = {\n    Jiang,\n    Peng and Vasconcelos,\n    Nuno and Peng,\n    Jingliang\n},\n    title = {\n    Generic Promotion of Diffusion-Based Salient Object Detection\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "315d8111f8",
        "title": "Geometry-Aware Deep Transform",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Jiaji Huang, Qiang Qiu, Robert Calderbank, Guillermo Sapiro",
        "author": "Jiaji Huang; Qiang Qiu; Robert Calderbank; Guillermo Sapiro",
        "abstract": "Many recent efforts have been devoted to designing sophisticated deep learning structures, obtaining revolutionary results on benchmark datasets. The success of these deep learning methods mostly relies on an enormous volume of labeled  training samples to learn a huge number of parameters in a network; therefore,  understanding  the generalization ability of a learned deep network cannot be overlooked, especially when restricted to a small training set, which is  the case for many applications. In this paper, we  propose a novel deep learning objective formulation that unifies both the classification and metric learning criteria.   We then introduce a geometry-aware deep transform to enable a non-linear discriminative and robust feature transform, which shows competitive performance on small training sets for  both synthetic and real-world data. We further support the proposed framework with a formal (K,epsilon)-robustness analysis.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Huang_Geometry-Aware_Deep_Transform_ICCV_2015_paper.pdf",
        "aff": "Duke University; Duke University; Duke University; Duke University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4572621,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8837690428722217264&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "duke.edu;duke.edu;duke.edu;duke.edu",
        "email": "duke.edu;duke.edu;duke.edu;duke.edu",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Huang_Geometry-Aware_Deep_Transform_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Duke University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.duke.edu",
        "aff_unique_abbr": "Duke",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Huang_2015_ICCV,\n    \n    author = {\n    Huang,\n    Jiaji and Qiu,\n    Qiang and Calderbank,\n    Robert and Sapiro,\n    Guillermo\n},\n    title = {\n    Geometry-Aware Deep Transform\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "33b07a6fc9",
        "title": "Global Structure-From-Motion by Similarity Averaging",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Zhaopeng Cui, Ping Tan",
        "author": "Zhaopeng Cui; Ping Tan",
        "abstract": "Global structure-from-motion (SfM) methods solve all cameras simultaneously from all available relative motions. It has better potential in both reconstruction accuracy and computation efficiency than incremental methods. However, global SfM is challenging, mainly because of two reasons. Firstly, translation averaging is difficult, since an essential matrix only tells the direction of relative translation. Secondly, it is also hard to filter out bad essential matrices due to feature matching failures. We propose to compute a sparse depth image at each camera to solve both problems. Depth images help to upgrade an essential matrix to a similarity transformation, which can determine the scale of relative translation. Thus, camera registration is formulated as a well-posed similarity averaging problem. Depth images also make the filtering of essential matrices simple and effective. In this way, translation averaging can be solved robustly in two convex L1 optimization problems, which reach the global optimum rapidly. We demonstrate this method in various examples including sequential data, Internet data, and ambiguous data with repetitive scene structures.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Cui_Global_Structure-From-Motion_by_ICCV_2015_paper.pdf",
        "aff": "Simon Fraser University; Simon Fraser University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3835983,
        "gs_citation": 203,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3820131647413683379&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "sfu.ca;sfu.ca",
        "email": "sfu.ca;sfu.ca",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Cui_Global_Structure-From-Motion_by_ICCV_2015_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Simon Fraser University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.sfu.ca",
        "aff_unique_abbr": "SFU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada",
        "bibtex": "@InProceedings{Cui_2015_ICCV,\n    \n    author = {\n    Cui,\n    Zhaopeng and Tan,\n    Ping\n},\n    title = {\n    Global Structure-From-Motion by Similarity Averaging\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "8823f0f312",
        "title": "Global, Dense Multiscale Reconstruction for a Billion Points",
        "session": "3d vision",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "Benjamin Ummenhofer, Thomas Brox",
        "author": "Benjamin Ummenhofer; Thomas Brox",
        "abstract": "We present a variational approach for surface reconstruction from a set of oriented points with scale information. We focus particularly on scenarios with non-uniform point densities due to images taken from different distances. In contrast to previous methods, we integrate the scale information in the objective and globally optimize the signed distance function of the surface on a balanced octree grid. We use a finite element discretization on the dual structure of the octree minimizing the number of variables. The tetrahedral mesh is generated efficiently from the dual structure, and also memory efficiency is optimized, such that robust data terms can be used even on very large scenes. The surface normals are explicitly optimized and used for surface extraction to improve the reconstruction at edges and corners.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Ummenhofer_Global_Dense_Multiscale_ICCV_2015_paper.pdf",
        "aff": "Computer Vision Group, University of Freiburg, Germany; Computer Vision Group, University of Freiburg, Germany",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4818529,
        "gs_citation": 85,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4120070617383713450&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 19,
        "aff_domain": "cs.uni-freiburg.de;cs.uni-freiburg.de",
        "email": "cs.uni-freiburg.de;cs.uni-freiburg.de",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Ummenhofer_Global_Dense_Multiscale_ICCV_2015_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Freiburg",
        "aff_unique_dep": "Computer Vision Group",
        "aff_unique_url": "https://www.uni-freiburg.de",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Freiburg",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany",
        "bibtex": "@InProceedings{Ummenhofer_2015_ICCV,\n    \n    author = {\n    Ummenhofer,\n    Benjamin and Brox,\n    Thomas\n},\n    title = {\n    Global,\n    Dense Multiscale Reconstruction for a Billion Points\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "502fa7645b",
        "title": "Globally Optimal 2D-3D Registration From Points or Lines Without Correspondences",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Mark Brown, David Windridge, Jean-Yves Guillemaut",
        "author": "Mark Brown; David Windridge; Jean-Yves Guillemaut",
        "abstract": "We present a novel approach to 2D-3D registration from points or lines without correspondences. While there exist established solutions in the case where correspondences are known, there are many situations where it is not possible to reliably extract such correspondences across modalities, thus requiring the use of a correspondence-free registration algorithm. Existing correspondence-free methods rely on local search strategies and consequently have no guarantee of finding the optimal solution. In contrast, we present the first globally optimal approach to 2D-3D registration without correspondences, achieved by a Branch-and-Bound algorithm. Furthermore, a deterministic annealing procedure is proposed to speed up the nested branch-and-bound algorithm used. The theoretical and practical advantages this brings are demonstrated on a range of synthetic and real data where it is observed that the proposed approach is significantly more robust to high proportions of outliers compared to existing approaches.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Brown_Globally_Optimal_2D-3D_ICCV_2015_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1005469,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12901234111386458669&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Brown_Globally_Optimal_2D-3D_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Brown_2015_ICCV,\n    \n    author = {\n    Brown,\n    Mark and Windridge,\n    David and Guillemaut,\n    Jean-Yves\n},\n    title = {\n    Globally Optimal 2D-3D Registration From Points or Lines Without Correspondences\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "c0528b53f7",
        "title": "Group Membership Prediction",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Ziming Zhang, Yuting Chen, Venkatesh Saligrama",
        "author": "Ziming Zhang; Yuting Chen; Venkatesh Saligrama",
        "abstract": "The group membership prediction (GMP) problem involves predicting whether or not a collection of instances share a certain semantic property. For instance, in kinship verification given a collection of images, the goal is to predict whether or not they share a familial relationship. In this context we propose a novel probability model and introduce latent view-specific and view-shared random variables to jointly account for the view-specific appearance and cross-view similarities among data instances. Our model posits that data from each view is independent conditioned on the shared variables. This postulate leads to a parametric probability model that decomposes group membership likelihood into a tensor product of data-independent parameters and data-dependent factors. We propose learning the data-independent parameters in a discriminative way with bilinear classifiers, and test our prediction algorithm on challenging visual recognition tasks such as multi-camera person re-identification and kinship verification. On most benchmark datasets, our method can significantly outperform the current state-of-the-art.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Zhang_Group_Membership_Prediction_ICCV_2015_paper.pdf",
        "aff": "Department of Electrical & Computer Engineering, Boston University; Department of Electrical & Computer Engineering, Boston University; Department of Electrical & Computer Engineering, Boston University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1747323,
        "gs_citation": 50,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2301806650374396615&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "bu.edu;bu.edu;bu.edu",
        "email": "bu.edu;bu.edu;bu.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Zhang_Group_Membership_Prediction_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Boston University",
        "aff_unique_dep": "Department of Electrical & Computer Engineering",
        "aff_unique_url": "https://www.bu.edu",
        "aff_unique_abbr": "BU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Zhang_2015_ICCV,\n    \n    author = {\n    Zhang,\n    Ziming and Chen,\n    Yuting and Saligrama,\n    Venkatesh\n},\n    title = {\n    Group Membership Prediction\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "3709041920",
        "title": "Guaranteed Outlier Removal for Rotation Search",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Álvaro Parra Bustos, Tat-Jun Chin",
        "author": "Alvaro Parra Bustos; Tat-Jun Chin",
        "abstract": "Rotation search has become a core routine for solving many computer vision problems. The aim is to rotationally align two input point sets with correspondences. Recently, there is significant interest in developing globally optimal rotation search algorithms. A notable weakness of global algorithms, however, is their relatively high computational cost, especially on large problem sizes and data with a high proportion of outliers. In this paper, we propose a novel outlier removal technique for rotation search. Our method guarantees that any correspondence it discards as an outlier does not exist in the inlier set of the globally optimal rotation for the original data. Based on simple geometric operations, our algorithm is deterministic and fast. Used as a preprocessor to prune a large portion of the outliers from the input data, our method enables substantial speed-up of rotation search algorithms without compromising global optimality. We demonstrate the efficacy of our method in various synthetic and real data experiments.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Bustos_Guaranteed_Outlier_Removal_ICCV_2015_paper.pdf",
        "aff": "School of Computer Science, The University of Adelaide, South Australia; School of Computer Science, The University of Adelaide, South Australia",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4640841,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7611286268665779752&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "cs.adelaide.edu.au;cs.adelaide.edu.au",
        "email": "cs.adelaide.edu.au;cs.adelaide.edu.au",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Bustos_Guaranteed_Outlier_Removal_ICCV_2015_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "The University of Adelaide",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.adelaide.edu.au",
        "aff_unique_abbr": "Adelaide",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Adelaide",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Australia",
        "bibtex": "@InProceedings{Bustos_2015_ICCV,\n    \n    author = {\n    Bustos,\n    Alvaro Parra and Chin,\n    Tat-Jun\n},\n    title = {\n    Guaranteed Outlier Removal for Rotation Search\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "8dfdeeea11",
        "title": "Guiding the Long-Short Term Memory Model for Image Caption Generation",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Xu Jia, Efstratios Gavves, Basura Fernando, Tinne Tuytelaars",
        "author": "Xu Jia; Efstratios Gavves; Basura Fernando; Tinne Tuytelaars",
        "abstract": "In this work we focus on the problem of image caption generation. We propose an extension of the long short term memory (LSTM)  model, which we coin gLSTM for short. In particular, we add semantic information extracted from the image as extra input to each unit of the LSTM block, with the aim of guiding the model towards solutions that are more tightly coupled to the image content. Additionally, we explore different length normalization strategies for beam search to avoid bias towards short sentences. On various benchmark datasets such as Flickr8K, Flickr30K and MS COCO, we obtain results that are on par with or better than the current state-of-the-art.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Jia_Guiding_the_Long-Short_ICCV_2015_paper.pdf",
        "aff": "KU Leuven ESAT-PSI, iMinds; QUV A Lab, University of Amsterdam; ACRV, The Australian National University + KU Leuven ESAT-PSI, iMinds; KU Leuven ESAT-PSI, iMinds",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 608838,
        "gs_citation": 587,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6026018115607373782&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff_domain": "esat.kuleuven.be;uva.nl;anu.edu.au;esat.kuleuven.be",
        "email": "esat.kuleuven.be;uva.nl;anu.edu.au;esat.kuleuven.be",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Jia_Guiding_the_Long-Short_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;2+0;0",
        "aff_unique_norm": "KU Leuven;University of Amsterdam;The Australian National University",
        "aff_unique_dep": "ESAT-PSI;QUV A Lab;ACRV",
        "aff_unique_url": "https://www.kuleuven.be;https://www.uva.nl;https://www.anu.edu.au",
        "aff_unique_abbr": "KU Leuven;UvA;ANU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;2+0;0",
        "aff_country_unique": "Belgium;Netherlands;Australia",
        "bibtex": "@InProceedings{Jia_2015_ICCV,\n    \n    author = {\n    Jia,\n    Xu and Gavves,\n    Efstratios and Fernando,\n    Basura and Tuytelaars,\n    Tinne\n},\n    title = {\n    Guiding the Long-Short Term Memory Model for Image Caption Generation\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "f1e6252e69",
        "title": "HARF: Hierarchy-Associated Rich Features for Salient Object Detection",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Wenbin Zou, Nikos Komodakis",
        "author": "Wenbin Zou; Nikos Komodakis",
        "abstract": "The state-of-the-art salient object detection models are able to perform well for relatively simple scenes, yet for more complex ones, they still have difficulties in highlighting salient objects completely from background, largely due to the lack of sufficiently robust features for saliency prediction. To address such an issue, this paper proposes a novel hierarchy-associated feature construction framework for salient object detection, which is based on integrating elementary features from multi-level regions in a hierarchy. Furthermore, multi-layered deep learning features are introduced and incorporated as elementary features into this framework through a compact integration scheme. This leads to a rich feature representation, which is able to represent the context of the whole object/background and is much more discriminative as well as robust for salient object detection. Extensive experiments on the most widely used and challenging benchmark datasets demonstrate that the proposed approach substantially outperforms the state-of-the-art on salient object detection.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Zou_HARF_Hierarchy-Associated_Rich_ICCV_2015_paper.pdf",
        "aff": "Shenzhen Key Lab of Advanced Telecommunication and Information Processing, College of Information Engineering, Shenzhen University; Universite Paris-Est, Ecole des Ponts ParisTech",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1339567,
        "gs_citation": 78,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15352223392054019796&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "sina.com;enpc.fr",
        "email": "sina.com;enpc.fr",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Zou_HARF_Hierarchy-Associated_Rich_ICCV_2015_paper.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Shenzhen University;Ecole des Ponts ParisTech",
        "aff_unique_dep": "College of Information Engineering;",
        "aff_unique_url": "https://www.szu.edu.cn;https://www.ponts.org",
        "aff_unique_abbr": "SZU;ENPC",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Shenzhen;",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "China;France",
        "bibtex": "@InProceedings{Zou_2015_ICCV,\n    \n    author = {\n    Zou,\n    Wenbin and Komodakis,\n    Nikos\n},\n    title = {\n    HARF: Hierarchy-Associated Rich Features for Salient Object Detection\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "ba527a2886",
        "title": "HD-CNN: Hierarchical Deep Convolutional Neural Networks for Large Scale Visual Recognition",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Zhicheng Yan, Hao Zhang, Robinson Piramuthu, Vignesh Jagadeesh, Dennis DeCoste, Wei Di, Yizhou Yu",
        "author": "Zhicheng Yan; Hao Zhang; Robinson Piramuthu; Vignesh Jagadeesh; Dennis DeCoste; Wei Di; Yizhou Yu",
        "abstract": "In image classification, visual separability between different object categories is highly uneven, and some categories are more difficult to distinguish than others. Such difficult categories demand more dedicated classifiers. However, existing deep convolutional neural networks (CNN) are trained as flat N-way classifiers, and few efforts have been made to leverage the hierarchical structure of categories. In this paper, we introduce hierarchical deep CNNs (HD-CNNs) by embedding deep CNNs into a category hierarchy. An HD-CNN separates easy classes using a coarse category classifier while distinguishing difficult classes using fine category classifiers. During HD-CNN training, component-wise pretraining is followed by global finetuning with a multinomial logistic loss regularized by a coarse category consistency term. In addition, conditional executions of fine category classifiers  and layer parameter compression make HD-CNNs scalable for large-scale visual recognition. We achieve state-of-the-art results on both CIFAR100 and large-scale ImageNet 1000-class benchmark datasets. In our experiments, we build up three different HD-CNNs and they lower the top-1 error of the standard CNNs by 2.65%, 3.1% and 1.1%, respectively",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Yan_HD-CNN_Hierarchical_Deep_ICCV_2015_paper.pdf",
        "aff": "University of Illinois at Urbana-Champaign; Carnegie Mellon University; eBay Research Lab; eBay Research Lab; eBay Research Lab; eBay Research Lab; The University of Hong Kong + University of Illinois at Urbana-Champaign",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 602613,
        "gs_citation": 520,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8575196851568618448&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 21,
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "author_num": 7,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Yan_HD-CNN_Hierarchical_Deep_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;2;2;2;2;3+0",
        "aff_unique_norm": "University of Illinois at Urbana-Champaign;Carnegie Mellon University;eBay;The University of Hong Kong",
        "aff_unique_dep": ";;eBay Research Lab;",
        "aff_unique_url": "https://illinois.edu;https://www.cmu.edu;https://www.ebayinc.com;https://www.hku.hk",
        "aff_unique_abbr": "UIUC;CMU;eBay;HKU",
        "aff_campus_unique_index": "0;2+0",
        "aff_campus_unique": "Urbana-Champaign;;Hong Kong SAR",
        "aff_country_unique_index": "0;0;0;0;0;0;1+0",
        "aff_country_unique": "United States;China",
        "bibtex": "@InProceedings{Yan_2015_ICCV,\n    \n    author = {\n    Yan,\n    Zhicheng and Zhang,\n    Hao and Piramuthu,\n    Robinson and Jagadeesh,\n    Vignesh and DeCoste,\n    Dennis and Di,\n    Wei and Yu,\n    Yizhou\n},\n    title = {\n    HD-CNN: Hierarchical Deep Convolutional Neural Networks for Large Scale Visual Recognition\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "2161c495f7",
        "title": "HICO: A Benchmark for Recognizing Human-Object Interactions in Images",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Yu-Wei Chao, Zhan Wang, Yugeng He, Jiaxuan Wang, Jia Deng",
        "author": "Yu-Wei Chao; Zhan Wang; Yugeng He; Jiaxuan Wang; Jia Deng",
        "abstract": "We introduce a new benchmark \"Humans Interacting with Common Objects\" (HICO) for recognizing human-object interactions (HOI). We demonstrate the key features of HICO: a diverse set of interactions with common object categories, a list of well-defined, sense-based HOI categories, and an exhaustive labeling of co-occurring interactions with an object category in each image. We perform an in-depth analysis of representative current approaches and show that DNNs enjoy a significant edge. In addition, we show that semantic knowledge can significantly improve HOI recognition, especially for uncommon categories.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Chao_HICO_A_Benchmark_ICCV_2015_paper.pdf",
        "aff": "Computer Science and Engineering, University of Michigan, Ann Arbor; Computer Science and Engineering, University of Michigan, Ann Arbor; Computer Science and Engineering, University of Michigan, Ann Arbor; Computer Science and Engineering, University of Michigan, Ann Arbor; Computer Science and Engineering, University of Michigan, Ann Arbor",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1051054,
        "gs_citation": 385,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5864578571911068795&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "umich.edu;umich.edu;umich.edu;umich.edu;umich.edu",
        "email": "umich.edu;umich.edu;umich.edu;umich.edu;umich.edu",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Chao_HICO_A_Benchmark_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "Computer Science and Engineering",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Ann Arbor",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Chao_2015_ICCV,\n    \n    author = {\n    Chao,\n    Yu-Wei and Wang,\n    Zhan and He,\n    Yugeng and Wang,\n    Jiaxuan and Deng,\n    Jia\n},\n    title = {\n    HICO: A Benchmark for Recognizing Human-Object Interactions in Images\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "43db8988dc",
        "title": "Harvesting Discriminative Meta Objects With Deep CNN Features for Scene Classification",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Ruobing Wu, Baoyuan Wang, Wenping Wang, Yizhou Yu",
        "author": "Ruobing Wu; Baoyuan Wang; Wenping Wang; Yizhou Yu",
        "abstract": "Recent work on scene classification still makes use of generic CNN features in a rudimentary manner. In this paper, we present a novel pipeline built upon deep CNN features to harvest discriminative visual objects and parts for scene classification. We first use a region proposal technique to generate a set of high-quality patches potentially containing objects, and apply a pre-trained CNN to extract generic deep features from these patches. Then we perform both unsupervised and weakly supervised learning to screen these patches and discover discriminative ones representing category-specific objects and parts. We further apply discriminative clustering enhanced with local CNN fine-tuning to aggregate similar objects and parts into groups, called meta objects. A scene image representation is constructed by pooling the feature response maps of all the learned meta objects at multiple spatial scales. We have confirmed that the scene image representation obtained using this new pipeline is capable of delivering state-of-the-art performance on two popular scene benchmark datasets, MIT Indoor 67 [22] and Sun397 [31].",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Wu_Harvesting_Discriminative_Meta_ICCV_2015_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1215034,
        "gs_citation": 153,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2205457302018047217&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Wu_Harvesting_Discriminative_Meta_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Wu_2015_ICCV,\n    \n    author = {\n    Wu,\n    Ruobing and Wang,\n    Baoyuan and Wang,\n    Wenping and Yu,\n    Yizhou\n},\n    title = {\n    Harvesting Discriminative Meta Objects With Deep CNN Features for Scene Classification\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "c5e26feb98",
        "title": "Hierarchical Convolutional Features for Visual Tracking",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Chao Ma, Jia-Bin Huang, Xiaokang Yang, Ming-Hsuan Yang",
        "author": "Chao Ma; Jia-Bin Huang; Xiaokang Yang; Ming-Hsuan Yang",
        "abstract": "Visual object tracking is challenging as target objects often undergo significant appearance changes caused by deformation, abrupt motion, background clutter and occlusion. In this paper, we exploit features extracted from deep convolutional neural networks trained on object recognition datasets to improve tracking accuracy and robustness. The outputs of the last convolutional layers encode the semantic information of targets and such representations are robust to significant appearance variations. However, their spatial resolution is too coarse to precisely localize targets. In contrast, earlier convolutional layers provide more precise localization but are less invariant to appearance changes. We interpret the hierarchies of convolutional layers as a nonlinear counterpart of an image pyramid representation and exploit these multiple levels of abstraction for visual tracking. Specifically, we adaptively learn correlation filters on each convolutional layer to encode the target appearance. We hierarchically infer the maximum response of each layer to locate targets. Extensive experimental results on a largescale benchmark dataset show that the proposed algorithm performs favorably against state-of-the-art methods.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Ma_Hierarchical_Convolutional_Features_ICCV_2015_paper.pdf",
        "aff": "SJTU; UIUC; SJTU; UC Merced",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4122038,
        "gs_citation": 2242,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18443068716629032032&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": "sjtu.edu.cn;illinois.edu;sjtu.edu.cn;ucmerced.edu",
        "email": "sjtu.edu.cn;illinois.edu;sjtu.edu.cn;ucmerced.edu",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Ma_Hierarchical_Convolutional_Features_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;0;2",
        "aff_unique_norm": "Shanghai Jiao Tong University;University of Illinois at Urbana-Champaign;University of California, Merced",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.sjtu.edu.cn;https://www illinois.edu;https://www.ucmerced.edu",
        "aff_unique_abbr": "SJTU;UIUC;UCM",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Urbana-Champaign;Merced",
        "aff_country_unique_index": "0;1;0;1",
        "aff_country_unique": "China;United States",
        "bibtex": "@InProceedings{Ma_2015_ICCV,\n    \n    author = {\n    Ma,\n    Chao and Huang,\n    Jia-Bin and Yang,\n    Xiaokang and Yang,\n    Ming-Hsuan\n},\n    title = {\n    Hierarchical Convolutional Features for Visual Tracking\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "c166516a5c",
        "title": "Hierarchical Higher-Order Regression Forest Fields: An Application to 3D Indoor Scene Labelling",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Trung T. Pham, Ian Reid, Yasir Latif, Stephen Gould",
        "author": "Trung T. Pham; Ian Reid; Yasir Latif; Stephen Gould",
        "abstract": "This paper addresses the problem of semantic segmentation of 3D indoor scenes reconstructed from RGB-D images.Traditionally label prediction for 3D points is tackled by employing graphical models that capture scene features and complex relations between different class labels. However, the existing work is restricted to pairwise conditional random fields, which are insufficient when encoding rich scene context. In this work we propose models with higher-order potentials to describe complex relational information from the 3D scenes. Specifically, we relax the labelling problem to a regression, and generalize the higher-order associative P n Potts model to a new family of arbitrary higher-order models based on regression forests. We show that these models, like the robust P n models, can still be decomposed into the sum of pairwise terms by introducing auxiliary variables. Moreover, our proposed higher-order models also permit extension to hierarchical random fields, which allows for the integration of scene context and features computed at different scales. Our potential functions are constructed based on regression forests encoding Gaussian densities that admit efficient inference. The parameters of our model are learned from training data using a structured learning approach. Results on two datasets show clear improvements over current state-of-the-art methods.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Pham_Hierarchical_Higher-Order_Regression_ICCV_2015_paper.pdf",
        "aff": "School of Computer Science, The University of Adelaide; School of Computer Science, The University of Adelaide; School of Computer Science, The University of Adelaide; Research School of Computer Science, The Australian National University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 985145,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15605065199841784829&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "adelaide.edu.au;adelaide.edu.au;adelaide.edu.au;anu.edu.au",
        "email": "adelaide.edu.au;adelaide.edu.au;adelaide.edu.au;anu.edu.au",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Pham_Hierarchical_Higher-Order_Regression_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "The University of Adelaide;The Australian National University",
        "aff_unique_dep": "School of Computer Science;Research School of Computer Science",
        "aff_unique_url": "https://www.adelaide.edu.au;https://www.anu.edu.au",
        "aff_unique_abbr": "Adelaide;ANU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Australia",
        "bibtex": "@InProceedings{Pham_2015_ICCV,\n    \n    author = {\n    Pham,\n    Trung T. and Reid,\n    Ian and Latif,\n    Yasir and Gould,\n    Stephen\n},\n    title = {\n    Hierarchical Higher-Order Regression Forest Fields: An Application to 3D Indoor Scene Labelling\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "ac2c57eb53",
        "title": "High Quality Structure From Small Motion for Rolling Shutter Cameras",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Sunghoon Im, Hyowon Ha, Gyeongmin Choe, Hae-Gon Jeon, Kyungdon Joo, In So Kweon",
        "author": "Sunghoon Im; Hyowon Ha; Gyeongmin Choe; Hae-Gon Jeon; Kyungdon Joo; In So Kweon",
        "abstract": "We present a practical 3D reconstruction method to obtain a high-quality dense depth map from narrow-baseline image sequences captured by commercial digital cameras, such as DSLRs or mobile phones. Depth estimation from small motion has gained interest as a means of various photographic editing, but important limitations present themselves in the form of depth uncertainty due to a narrow baseline and rolling shutter. To address these problems, we introduce a novel 3D reconstruction method from narrow-baseline image sequences that effectively handles the effects of a rolling shutter that occur from most of commercial digital cameras. Additionally, we present a depth propagation method to fill in the holes associated with the unknown pixels based on our novel geometric guidance model. Both qualitative and quantitative experimental results show that our new algorithm consistently generates better 3D depth maps than those by the state-of-the-art method.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Im_High_Quality_Structure_ICCV_2015_paper.pdf",
        "aff": "Korea Advanced Institute of Science and Technology (KAIST), Republic of Korea; Korea Advanced Institute of Science and Technology (KAIST), Republic of Korea; Korea Advanced Institute of Science and Technology (KAIST), Republic of Korea; Korea Advanced Institute of Science and Technology (KAIST), Republic of Korea; Korea Advanced Institute of Science and Technology (KAIST), Republic of Korea; Korea Advanced Institute of Science and Technology (KAIST), Republic of Korea",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 12746530,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9588813647292825706&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "rcv.kaist.ac.kr;rcv.kaist.ac.kr;rcv.kaist.ac.kr;rcv.kaist.ac.kr;rcv.kaist.ac.kr;kaist.ac.kr",
        "email": "rcv.kaist.ac.kr;rcv.kaist.ac.kr;rcv.kaist.ac.kr;rcv.kaist.ac.kr;rcv.kaist.ac.kr;kaist.ac.kr",
        "author_num": 6,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Im_High_Quality_Structure_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Korea Advanced Institute of Science and Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.kaist.ac.kr",
        "aff_unique_abbr": "KAIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "South Korea",
        "bibtex": "@InProceedings{Im_2015_ICCV,\n    \n    author = {\n    Im,\n    Sunghoon and Ha,\n    Hyowon and Choe,\n    Gyeongmin and Jeon,\n    Hae-Gon and Joo,\n    Kyungdon and Kweon,\n    In So\n},\n    title = {\n    High Quality Structure From Small Motion for Rolling Shutter Cameras\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "2cf3421fd7",
        "title": "High-for-Low and Low-for-High: Efficient Boundary Detection From Deep Object Features and its Applications to High-Level Vision",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Gedas Bertasius, Jianbo Shi, Lorenzo Torresani",
        "author": "Gedas Bertasius; Jianbo Shi; Lorenzo Torresani",
        "abstract": "Most of the current boundary detection systems rely exclusively on low-level features, such as color and texture. However, perception studies suggest that humans employ object-level reasoning when judging if a particular pixel is a boundary. Inspired by this observation, in this work we show how to predict boundaries by exploiting object-level features from a pretrained object-classification network. Our method can be viewed as a \"High-for-Low\" approach where high-level object features inform the low-level boundary detection process. Our model achieves state-of-the-art performance on an established boundary detection benchmark and it is efficient to run.  Additionally, we show that due to the semantic nature of our boundaries we can use them to aid a number of high-level vision tasks. We demonstrate that using our boundaries we improve the performance of state-of-the-art methods on the problems of semantic boundary labeling, semantic segmentation and object proposal generation. We can view this process as a \"Low-for-High'\" scheme, where low-level boundaries aid high-level vision tasks.   Thus, our contributions include a boundary detection system that is accurate, efficient, generalizes well to multiple datasets, and is also shown to improve existing state-of-the-art high-level vision methods on three distinct tasks.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Bertasius_High-for-Low_and_Low-for-High_ICCV_2015_paper.pdf",
        "aff": "University of Pennsylvania; University of Pennsylvania; Dartmouth College",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1111526,
        "gs_citation": 227,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6429592123688911770&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "seas.upenn.edu;seas.upenn.edu;dartmouth.edu",
        "email": "seas.upenn.edu;seas.upenn.edu;dartmouth.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Bertasius_High-for-Low_and_Low-for-High_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of Pennsylvania;Dartmouth College",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.upenn.edu;https://www.dartmouth.edu",
        "aff_unique_abbr": "UPenn;Dartmouth",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Bertasius_2015_ICCV,\n    \n    author = {\n    Bertasius,\n    Gedas and Shi,\n    Jianbo and Torresani,\n    Lorenzo\n},\n    title = {\n    High-for-Low and Low-for-High: Efficient Boundary Detection From Deep Object Features and its Applications to High-Level Vision\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "f2c0a449f2",
        "title": "Higher-Order CRF Structural Segmentation of 3D Reconstructed Surfaces",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Jingbo Liu, Jinglu Wang, Tian Fang, Chiew-Lan Tai, Long Quan",
        "author": "Jingbo Liu; Jinglu Wang; Tian Fang; Chiew-Lan Tai; Long Quan",
        "abstract": "In this paper, we propose a structural segmentation algorithm to partition multi-view stereo reconstructed surfaces of large-scale urban environments into structural segments. Each segment corresponds to a structural component describable by a surface primitive of up to the second order. This segmentation is for use in subsequent urban object modeling, vectorization, and recognition.  To overcome the high geometrical and topological noise levels in the 3D reconstructed urban surfaces, we formulate the structural segmentation as a higher-order Conditional Random Field (CRF) labeling problem. It not only incorporates classical lower-order 2D and 3D local cues, but also encodes contextual geometric regularities to disambiguate the noisy local cues. A general higher-order CRF is difficult to solve. We develop a bottom-up progressive approach through a patch-based surface representation, which iteratively evolves from the initial mesh triangles to the final segmentation. Each iteration alternates between performing a prior discovery step, which finds the contextual regularities of the patch-based representation, and an inference step that leverages the regularities as higher-order priors to construct a more stable and regular segmentation.  The efficiency and robustness of the proposed method is extensively demonstrated on real reconstruction models, yielding significantly better performance than classical mesh segmentation methods.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Liu_Higher-Order_CRF_Structural_ICCV_2015_paper.pdf",
        "aff": "The Hong Kong University of Science and Technology; The Hong Kong University of Science and Technology; The Hong Kong University of Science and Technology; The Hong Kong University of Science and Technology; The Hong Kong University of Science and Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3529295,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1240846837180965096&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "ust.hk;ust.hk;ust.hk;ust.hk;ust.hk",
        "email": "ust.hk;ust.hk;ust.hk;ust.hk;ust.hk",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Liu_Higher-Order_CRF_Structural_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Hong Kong University of Science and Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ust.hk",
        "aff_unique_abbr": "HKUST",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Liu_2015_ICCV,\n    \n    author = {\n    Liu,\n    Jingbo and Wang,\n    Jinglu and Fang,\n    Tian and Tai,\n    Chiew-Lan and Quan,\n    Long\n},\n    title = {\n    Higher-Order CRF Structural Segmentation of 3D Reconstructed Surfaces\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "f983f18f29",
        "title": "Higher-Order Inference for Multi-Class Log-Supermodular Models",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Jian Zhang, Josip Djolonga, Andreas Krause",
        "author": "Jian Zhang; Josip Djolonga; Andreas Krause",
        "abstract": "Higher-order models have been shown to be very useful for a plethora of computer vision tasks. However, existing techniques have focused mainly on MAP inference. In this paper, we present the first efficient approach towards approximate Bayesian marginal inference in a general class of high-order, multi-label attractive models, where previous techniques slow down exponentially with the order (clique size). We formalize this task as performing inference in log-supermodular models under partition constraints, and present an efficient variational inference technique. The resulting optimization problems are convex and yield bounds on the partition function. We also obtain a fully factorized approximation to the posterior, which can be used in lieu of the true complicated distribution. We empirically demonstrate the performance of our approach by comparing it to traditional inference methods on a challenging high-fidelity multi-label image segmentation dataset. We obtain state-of-the-art classification accuracy for MAP inference, and substantially improved ROC curves using the approximate marginals.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Zhang_Higher-Order_Inference_for_ICCV_2015_paper.pdf",
        "aff": "Department of Computer Science, ETH Zurich; Department of Computer Science, ETH Zurich; Department of Computer Science, ETH Zurich",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1368219,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12266851017294884495&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "student.ethz.ch;inf.ethz.ch;inf.ethz.ch",
        "email": "student.ethz.ch;inf.ethz.ch;inf.ethz.ch",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Zhang_Higher-Order_Inference_for_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "ETH Zurich",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ethz.ch",
        "aff_unique_abbr": "ETHZ",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Switzerland",
        "bibtex": "@InProceedings{Zhang_2015_ICCV,\n    \n    author = {\n    Zhang,\n    Jian and Djolonga,\n    Josip and Krause,\n    Andreas\n},\n    title = {\n    Higher-Order Inference for Multi-Class Log-Supermodular Models\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "14dd9bbec1",
        "title": "Highly-Expressive Spaces of Well-Behaved Transformations: Keeping It Simple",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Oren Freifeld, Søren Hauberg, Kayhan Batmanghelich, John W. Fisher III",
        "author": "Oren Freifeld; Soren Hauberg; Kayhan Batmanghelich; John W. Fisher III",
        "abstract": "We propose novel finite-dimensional spaces of R - R transformations, n [?] 1, 2, 3, derived from (continuously-defined) parametric stationary velocity fields. Particularly, we obtain these transformations, which are diffeomorphisms, by fast and highly-accurate integration of continuous piecewise-affine velocity fields; we also provide an exact solution for n = 1. The simple-yet-highly-expressive proposed representation handles optional constraints (e.g., volume preservation) easily and supports convenient modeling choices and rapid likelihood evaluations (facilitating tractable inference over latent transformations). Its applications include, but are not limited to: unconstrained optimization over monotonic functions; modeling cumulative distribution functions or histograms; time warping; image registration; landmark-based warping; real-time diffeomorphic image editing. Our code is available at https://github.com/freifeld/cpabDiffeo",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Freifeld_Highly-Expressive_Spaces_of_ICCV_2015_paper.pdf",
        "aff": "MIT CSAIL; DTU Compute; MIT CSAIL; MIT CSAIL",
        "project": "",
        "github": "https://github.com/freifeld/cpabDiffeo",
        "supp": "",
        "arxiv": "",
        "pdf_size": 9062852,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4394205371554505779&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "csail.mit.edu;dtu.dk;csail.mit.edu;csail.mit.edu",
        "email": "csail.mit.edu;dtu.dk;csail.mit.edu;csail.mit.edu",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Freifeld_Highly-Expressive_Spaces_of_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology;Technical University of Denmark",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory;DTU Compute",
        "aff_unique_url": "https://www.csail.mit.edu;https://www.compute.dtu.dk",
        "aff_unique_abbr": "MIT CSAIL;DTU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "United States;Denmark",
        "bibtex": "@InProceedings{Freifeld_2015_ICCV,\n    \n    author = {\n    Freifeld,\n    Oren and Hauberg,\n    Soren and Batmanghelich,\n    Kayhan and Fisher,\n    III,\n    John W.\n},\n    title = {\n    Highly-Expressive Spaces of Well-Behaved Transformations: Keeping It Simple\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "9fabdd5679",
        "title": "Holistically-Nested Edge Detection",
        "session": "segmentation, edges and saliency",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "Saining Xie, Zhuowen Tu",
        "author": "Saining Xie; Zhuowen Tu",
        "abstract": "We develop a new edge detection algorithm that addresses two critical issues in this long-standing vision problem: (1) holistic image training; and (2) multi-scale feature learning. Our proposed method, holistically-nested edge detection (HED), turns pixel-wise edge classification into image-to-image prediction by means of a deep learning model that leverages fully convolutional neural networks and deeply-supervised nets.  HED automatically learns rich hierarchical representations (guided by deep supervision on side responses) that are crucially important in order to approach the human ability to resolve the challenging ambiguity in edge and object boundary detection. We significantly advance the state-of-the-art on the BSD500 dataset (ODS F-score of 0.782) and the NYU Depth dataset (ODS F-score of 0.746), and do so with an improved speed (0.4 second per image) that is orders of magnitude faster than recent CNN-based edge detection algorithms.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Xie_Holistically-Nested_Edge_Detection_ICCV_2015_paper.pdf",
        "aff": ";",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 4673,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18154299256265143241&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 23,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Xie_Holistically-Nested_Edge_Detection_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Xie_2015_ICCV,\n    \n    author = {\n    Xie,\n    Saining and Tu,\n    Zhuowen\n},\n    title = {\n    Holistically-Nested Edge Detection\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "dc2427e74f",
        "title": "Hot or Not: Exploring Correlations Between Appearance and Temperature",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Daniel Glasner, Pascal Fua, Todd Zickler, Lihi Zelnik-Manor",
        "author": "Daniel Glasner; Pascal Fua; Todd Zickler; Lihi Zelnik-Manor",
        "abstract": "In this paper we explore interactions between the appearance of an outdoor scene and the ambient temperature. By studying statistical correlations between image sequences from outdoor cameras and temperature measurements we identify two interesting interactions. First, semantically meaningful regions such as foliage and reflective oriented surfaces are often highly indicative of the temperature. Second, small camera motions are correlated with the temperature in some scenes. We propose simple scene-specific temperature prediction algorithms which can be used to turn a camera into a crude temperature sensor. We find that for this task, simple features such as local pixel intensities outperform sophisticated, global features such as from a semantically-trained convolutional neural network.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Glasner_Hot_or_Not_ICCV_2015_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 937545,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17116558292591611117&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Glasner_Hot_or_Not_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Glasner_2015_ICCV,\n    \n    author = {\n    Glasner,\n    Daniel and Fua,\n    Pascal and Zickler,\n    Todd and Zelnik-Manor,\n    Lihi\n},\n    title = {\n    Hot or Not: Exploring Correlations Between Appearance and Temperature\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "4c227cc475",
        "title": "Human Action Recognition Using Factorized Spatio-Temporal Convolutional Networks",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Lin Sun, Kui Jia, Dit-Yan Yeung, Bertram E. Shi",
        "author": "Lin Sun; Kui Jia; Dit-Yan Yeung; Bertram E. Shi",
        "abstract": "Human actions in video sequences are three-dimensional (3D) spatio-temporal signals characterizing both the visual appearance and motion dynamics of the involved humans and objects. Inspired by the success of convolutional neural networks (CNN) for image classification, recent attempts have been made to learn 3D CNNs for recognizing human actions in videos. However, partly due to the high complexity of training 3D convolution kernels and the need for large quantities of training videos, only limited success has been reported. This has triggered us to investigate in this paper a new deep architecture which can handle 3D signals more effectively. Specifically, we propose factorized spatio-temporal convolutional networks (FstCN) that factorize the original 3D convolution kernel learning as a sequential process of learning 2D spatial kernels in the lower layers (called spatial convolutional layers), followed by learning 1D temporal kernels in the upper layers (called temporal convolutional layers). We introduce a novel transformation and permutation operator to make factorization in FstCN possible. Moreover, to address the issue of sequence alignment, we propose an effective training and inference strategy based on sampling multiple video clips from a given action video sequence. We have tested FstCN on two commonly used benchmark datasets (UCF-101 and HMDB-51). Without using auxiliary training videos to boost the performance, FstCN outperforms existing CNN based methods and achieves comparable performance with a recent method that benefits from using auxiliary training videos.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Sun_Human_Action_Recognition_ICCV_2015_paper.pdf",
        "aff": "Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology + Lenovo Corporate Research Hong Kong Branch; Faculty of Science and Technology, University of Macau; Department of Computer Science and Engineering, Hong Kong University of Science and Technology; Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 845906,
        "gs_citation": 745,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9473835785663262260&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "ust.hk;gmail.com;cse.ust.hk;ust.hk",
        "email": "ust.hk;gmail.com;cse.ust.hk;ust.hk",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Sun_Human_Action_Recognition_ICCV_2015_paper.html",
        "aff_unique_index": "0+1;2;0;0",
        "aff_unique_norm": "Hong Kong University of Science and Technology;Lenovo Corporate Research;University of Macau",
        "aff_unique_dep": "Department of Electronic and Computer Engineering;Corporate Research;Faculty of Science and Technology",
        "aff_unique_url": "https://www.ust.hk;https://www.lenovo.com;https://www.um.edu.mo",
        "aff_unique_abbr": "HKUST;LCR;UM",
        "aff_campus_unique_index": "0+0;1;0;0",
        "aff_campus_unique": "Hong Kong SAR;Macau SAR",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Sun_2015_ICCV,\n    \n    author = {\n    Sun,\n    Lin and Jia,\n    Kui and Yeung,\n    Dit-Yan and Shi,\n    Bertram E.\n},\n    title = {\n    Human Action Recognition Using Factorized Spatio-Temporal Convolutional Networks\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "a8c88ee275",
        "title": "Human Parsing With Contextualized Convolutional Neural Network",
        "session": "segmentation, edges and saliency",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "Xiaodan Liang, Chunyan Xu, Xiaohui Shen, Jianchao Yang, Si Liu, Jinhui Tang, Liang Lin, Shuicheng Yan",
        "author": "Xiaodan Liang; Chunyan Xu; Xiaohui Shen; Jianchao Yang; Si Liu; Jinhui Tang; Liang Lin; Shuicheng Yan",
        "abstract": "In this work, we address the human parsing task with a novel Contextualized Convolutional Neural Network (Co-CNN) architecture, which well integrates the cross-layer context, global image-level context, within-super-pixel context and cross-super-pixel neighborhood context into a unified network. Given an input human image, Co-CNN produces the pixel-wise categorization in an end-to-end way. First, the cross-layer context is captured by our basic local-to-global-to-local structure, which hierarchically combines the global semantic structure and the local fine details within the cross-layers. Second, the global image-level label prediction is used as an auxiliary objective in the intermediate layer of the Co-CNN, and its outputs are further used for guiding the feature learning in subsequent convolutional layers to leverage the global image-level context. Finally, to further utilize the local super-pixel contexts, the within-super-pixel smoothing and cross-super-pixel neighbourhood voting are formulated as natural sub-components of the Co-CNN to achieve the local label consistency in both training and testing process. Comprehensive evaluations on two public datasets well demonstrate the significant superiority of our Co-CNN architecture over other state-of-the-arts for human parsing. In particular, the F-1 score on the large dataset reaches 76.95% by Co-CNN, significantly higher than 62.81% and 64.38% by the state-of-the-art algorithms, M-CNN and ATR, respectively.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Liang_Human_Parsing_With_ICCV_2015_paper.pdf",
        "aff": "Sun Yat-sen University+National University of Singapore; National University of Singapore; Adobe Research; Snapchat Research; State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences; Nanjing University of Science and Technology; Sun Yat-sen University; National University of Singapore",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2631924,
        "gs_citation": 356,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15335078639563077696&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 24,
        "aff_domain": "ieee.org; ; ; ; ; ; ; ",
        "email": "ieee.org; ; ; ; ; ; ; ",
        "author_num": 8,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Liang_Human_Parsing_With_ICCV_2015_paper.html",
        "aff_unique_index": "0+1;1;2;3;4;5;0;1",
        "aff_unique_norm": "Sun Yat-sen University;National University of Singapore;Adobe;Snapchat;Chinese Academy of Sciences;Nanjing University of Science and Technology",
        "aff_unique_dep": ";;Adobe Research;Snapchat Research;State Key Laboratory of Information Security, Institute of Information Engineering;",
        "aff_unique_url": "http://www.sysu.edu.cn/;https://www.nus.edu.sg;https://research.adobe.com;https://www.snapchat.com;http://www.cas.cn;http://www.nust.edu.cn/",
        "aff_unique_abbr": "SYSU;NUS;Adobe;Snapchat;CAS;NUST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;1;2;2;0;0;0;1",
        "aff_country_unique": "China;Singapore;United States",
        "bibtex": "@InProceedings{Liang_2015_ICCV,\n    \n    author = {\n    Liang,\n    Xiaodan and Xu,\n    Chunyan and Shen,\n    Xiaohui and Yang,\n    Jianchao and Liu,\n    Si and Tang,\n    Jinhui and Lin,\n    Liang and Yan,\n    Shuicheng\n},\n    title = {\n    Human Parsing With Contextualized Convolutional Neural Network\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "0b5130726f",
        "title": "Human Pose Estimation in Videos",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Dong Zhang, Mubarak Shah",
        "author": "Dong Zhang; Mubarak Shah",
        "abstract": "In this paper, we present a method to estimate a sequence of human poses in unconstrained videos. In contrast to the commonly employed graph optimization framework, which is NP-hard and needs approximate solutions, we formulate this problem into a unified two stage tree-based optimization problem for which an efficient and exact solution exists. Although the proposed method finds an exact solution, it does not sacrifice the ability to model the spatial and temporal constraints between body parts in the video frames; indeed it even models the symmetric parts better than the existing methods. The proposed method is based on two main ideas: `Abstraction' and `Association' to enforce the intra- and inter-frame body part constraints respectively without inducing extra computational complexity to the polynomial time solution. Using the idea of `Abstraction', a new concept of `abstract body part' is introduced to model not only the tree based body part structure similar to existing methods, but also extra constraints between symmetric parts. Using the idea of `Association', the optimal tracklets are generated for each abstract body part, in order to enforce the spatiotemporal constraints between body parts in adjacent frames. Finally, a sequence of the best poses is inferred from the abstract body part tracklets through the tree-based optimization. We evaluated the proposed method on three publicly available video based human pose estimation datasets, and obtained dramatically improved performance compared to the state-of-the-art methods.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Zhang_Human_Pose_Estimation_ICCV_2015_paper.pdf",
        "aff": ";",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 67,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5774120219312576&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Zhang_Human_Pose_Estimation_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Zhang_2015_ICCV,\n    \n    author = {\n    Zhang,\n    Dong and Shah,\n    Mubarak\n},\n    title = {\n    Human Pose Estimation in Videos\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "80515844d3",
        "title": "Hyperpoints and Fine Vocabularies for Large-Scale Location Recognition",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Torsten Sattler, Michal Havlena, Filip Radenović, Konrad Schindler, Marc Pollefeys",
        "author": "Torsten Sattler; Michal Havlena; Filip Radenovic; Konrad Schindler; Marc Pollefeys",
        "abstract": "Structure-based localization is the task of finding the absolute pose of a given query image w.r.t. a pre-computed 3D model. While this is almost trivial at small scale, special care must be taken as the size of the 3D model grows, because straight-forward descriptor matching becomes ineffective due to the large memory footprint of the model, as well as the strictness of the ratio test in 3D. Recently, several authors have tried to overcome these problems, either by a smart compression of the 3D model or by clever sampling strategies for geometric verification. Here we explore an orthogonal strategy, which uses all the 3D points and standard sampling, but performs feature matching implicitly, by quantization into a fine vocabulary. We show that although this matching is ambiguous and gives rise to 3D hyperpoints when matching each 2D query feature in isolation, a simple voting strategy, which enforces the fact that the selected 3D points shall be co-visible, can reliably find a locally unique 2D-3D point assignment. Experiments on two large-scale datasets demonstrate that our method achieves state-of-the-art performance, while the memory footprint is greatly reduced, since only visual word labels but no 3D point descriptors need to be stored.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Sattler_Hyperpoints_and_Fine_ICCV_2015_paper.pdf",
        "aff": "Department of Computer Science, ETH Zürich, Switzerland; Institute of Geodesy and Photogrammetry, ETH Zürich, Switzerland; CMP, Faculty of Electrical Engineering, Czech Technical University in Prague; Institute of Geodesy and Photogrammetry, ETH Zürich, Switzerland; Department of Computer Science, ETH Zürich, Switzerland",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 826045,
        "gs_citation": 200,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5413457995883616766&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff_domain": "inf.ethz.ch;geod.baug.ethz.ch;cmp.felk.cvut.cz;geod.baug.ethz.ch;inf.ethz.ch",
        "email": "inf.ethz.ch;geod.baug.ethz.ch;cmp.felk.cvut.cz;geod.baug.ethz.ch;inf.ethz.ch",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Sattler_Hyperpoints_and_Fine_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "ETH Zürich;Czech Technical University in Prague",
        "aff_unique_dep": "Department of Computer Science;Faculty of Electrical Engineering",
        "aff_unique_url": "https://www.ethz.ch;https://www.cvut.cz",
        "aff_unique_abbr": "ETHZ;CTU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Prague",
        "aff_country_unique_index": "0;0;1;0;0",
        "aff_country_unique": "Switzerland;Czech Republic",
        "bibtex": "@InProceedings{Sattler_2015_ICCV,\n    \n    author = {\n    Sattler,\n    Torsten and Havlena,\n    Michal and Radenovic,\n    Filip and Schindler,\n    Konrad and Pollefeys,\n    Marc\n},\n    title = {\n    Hyperpoints and Fine Vocabularies for Large-Scale Location Recognition\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "6d1ad0b806",
        "title": "Hyperspectral Compressive Sensing Using Manifold-Structured Sparsity Prior",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Lei Zhang, Wei Wei, Yanning Zhang, Fei Li, Chunhua Shen, Qinfeng Shi",
        "author": "Lei Zhang; Wei Wei; Yanning Zhang; Fei Li; Chunhua Shen; Qinfeng Shi",
        "abstract": "To reconstruct hyperspectral image (HSI) accurately from a few noisy compressive measurements, we present a novel manifold-structured sparsity prior based hyperspectral compressive sensing (HCS) method in this study. A matrix based hierarchical prior is first proposed to represent the spectral structured sparsity and spatial unknown manifold structure of HSI simultaneously. Then, a latent variable Bayes model is introduced to learn the sparsity prior and estimate the noise jointly from measurements. The learned prior can fully represent the inherent 3D structure of HSI and regulate its shape based on the estimated noise level. Thus, with this learned prior, the proposed method improves the reconstruction accuracy significantly and shows strong robustness to unknown noise in HCS. Experiments on four real hyperspectral datasets show that the proposed method outperforms several state-of-the-art methods on the reconstruction accuracy of HSI.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Zhang_Hyperspectral_Compressive_Sensing_ICCV_2015_paper.pdf",
        "aff": "School of Computer Science and Engineering, Northwestern Polytechnical University, Xi’an, 710072, China; School of Computer Science and Engineering, Northwestern Polytechnical University, Xi’an, 710072, China; School of Computer Science and Engineering, Northwestern Polytechnical University, Xi’an, 710072, China; School of Computer Science and Engineering, Northwestern Polytechnical University, Xi’an, 710072, China; School of Computer Science, The University of Adelaide, Australia; School of Computer Science, The University of Adelaide, Australia",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1637445,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17600536289389070200&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "mail.nwpu.edu.cn;nwpu.edu.cn;nwpu.edu.cn;nwpu.edu.cn;adelaide.edu.au;adelaide.edu.au",
        "email": "mail.nwpu.edu.cn;nwpu.edu.cn;nwpu.edu.cn;nwpu.edu.cn;adelaide.edu.au;adelaide.edu.au",
        "author_num": 6,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Zhang_Hyperspectral_Compressive_Sensing_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0;1;1",
        "aff_unique_norm": "Northwestern Polytechnical University;The University of Adelaide",
        "aff_unique_dep": "School of Computer Science and Engineering;School of Computer Science",
        "aff_unique_url": "http://www.nwpu.edu.cn;https://www.adelaide.edu.au",
        "aff_unique_abbr": "NWPU;Adelaide",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Xi'an;",
        "aff_country_unique_index": "0;0;0;0;1;1",
        "aff_country_unique": "China;Australia",
        "bibtex": "@InProceedings{Zhang_2015_ICCV,\n    \n    author = {\n    Zhang,\n    Lei and Wei,\n    Wei and Zhang,\n    Yanning and Li,\n    Fei and Shen,\n    Chunhua and Shi,\n    Qinfeng\n},\n    title = {\n    Hyperspectral Compressive Sensing Using Manifold-Structured Sparsity Prior\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "606dfce57f",
        "title": "Hyperspectral Super-Resolution by Coupled Spectral Unmixing",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Charis Lanaras, Emmanuel Baltsavias, Konrad Schindler",
        "author": "Charis Lanaras; Emmanuel Baltsavias; Konrad Schindler",
        "abstract": "Hyperspectral cameras capture images with many narrow spectral channels, which densely sample the electromagnetic spectrum. The detailed spectral resolution is useful for many image analysis problems, but it comes at the cost of much lower spatial resolution. Hyperspectral super-resolution addresses this problem, by fusing a low-resolution hyperspectral image and a conventional high-resolution image into a product of both high spatial and high spectral resolution. In this paper, we propose a method which performs hyperspectral super-resolution by jointly unmixing the two input images into the pure reflectance spectra of the observed materials and the associated mixing coefficients. The formulation leads to a coupled matrix factorisation problem, with a number of useful constraints imposed by elementary physical properties of spectral mixing. In experiments with two benchmark datasets we show that the proposed approach delivers improved hyperspectral super-resolution.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Lanaras_Hyperspectral_Super-Resolution_by_ICCV_2015_paper.pdf",
        "aff": "Institute of Geodesy and Photogrammetry, ETH Zürich, Switzerland; Institute of Geodesy and Photogrammetry, ETH Zürich, Switzerland; Institute of Geodesy and Photogrammetry, ETH Zürich, Switzerland",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1821856,
        "gs_citation": 433,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=632440983753996995&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "geod.baug.ethz.ch;geod.baug.ethz.ch;geod.baug.ethz.ch",
        "email": "geod.baug.ethz.ch;geod.baug.ethz.ch;geod.baug.ethz.ch",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Lanaras_Hyperspectral_Super-Resolution_by_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "ETH Zürich",
        "aff_unique_dep": "Institute of Geodesy and Photogrammetry",
        "aff_unique_url": "https://www.ethz.ch",
        "aff_unique_abbr": "ETH",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Switzerland",
        "bibtex": "@InProceedings{Lanaras_2015_ICCV,\n    \n    author = {\n    Lanaras,\n    Charis and Baltsavias,\n    Emmanuel and Schindler,\n    Konrad\n},\n    title = {\n    Hyperspectral Super-Resolution by Coupled Spectral Unmixing\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "637cf03aa1",
        "title": "Illumination Robust Color Naming via Label Propagation",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Yuanliu liu, Zejian Yuan, Badong Chen, Jianru Xue, Nanning Zheng",
        "author": "Yuanliu liu; Zejian Yuan; Badong Chen; Jianru Xue; Nanning Zheng",
        "abstract": "Color composition is an important property for many computer vision tasks like image retrieval and object classification. In this paper we address the problem of inferring the color composition of the intrinsic reflectance of objects, where the shadows and highlights may change the observed color dramatically. We achieve this through color label propagation without recovering the intrinsic reflectance beforehand. Specifically, the color labels are propagated between regions sharing the same reflectance, and the direction of propagation is promoted to be from regions under full illumination and normal view angles to abnormal regions. We detect shadowed and highlighted regions as well as pairs of regions that have similar reflectance. A joint inference process is adopted to trim the inconsistent identities and connections. For evaluation we collect three datasets of images under noticeable highlights and shadows. Experimental results show that our model can effectively describe the color composition of real-world images.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/liu_Illumination_Robust_Color_ICCV_2015_paper.pdf",
        "aff": "Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University; Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University; Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University; Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University; Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 8699257,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2664686648813661250&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "gmail.com;mail.xjtu.edu.cn;mail.xjtu.edu.cn;mail.xjtu.edu.cn;mail.xjtu.edu.cn",
        "email": "gmail.com;mail.xjtu.edu.cn;mail.xjtu.edu.cn;mail.xjtu.edu.cn;mail.xjtu.edu.cn",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/liu_Illumination_Robust_Color_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Xi'an Jiaotong University",
        "aff_unique_dep": "Institute of Artificial Intelligence and Robotics",
        "aff_unique_url": "http://www.xjtu.edu.cn",
        "aff_unique_abbr": "XJTU",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Xi'an",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{liu_2015_ICCV,\n    \n    author = {\n    liu,\n    Yuanliu and Yuan,\n    Zejian and Chen,\n    Badong and Xue,\n    Jianru and Zheng,\n    Nanning\n},\n    title = {\n    Illumination Robust Color Naming via Label Propagation\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "e78d2edad9",
        "title": "Im2Calories: Towards an Automated Mobile Vision Food Diary",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Austin Meyers, Nick Johnston, Vivek Rathod, Anoop Korattikara, Alex Gorban, Nathan Silberman, Sergio Guadarrama, George Papandreou, Jonathan Huang, Kevin P. Murphy",
        "author": "Austin Meyers; Nick Johnston; Vivek Rathod; Anoop Korattikara; Alex Gorban; Nathan Silberman; Sergio Guadarrama; George Papandreou; Jonathan Huang; Kevin P. Murphy",
        "abstract": "We present a system which can recognize the contents of your meal from a single image, and then predict its nutritional contents, such as calories. The simplest version assumes that the user is eating at a restaurant for which we know the menu. In this case, we can collect images offline to train a multi-label classifier. At run time, we apply the classifier (running on your phone) to predict which foods are present in your meal, and we lookup the corresponding nutritional facts. We apply this method to a new dataset of images from 23 different restaurants, using a CNN-based classifier, significantly outperforming previous work. The more challenging setting works outside of restaurants. In this case, we need to estimate the size of the foods, as well as their labels. This requires solving segmentation and depth / volume estimation from a single image. We present CNN-based approaches to these problems, with promising preliminary results.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Meyers_Im2Calories_Towards_an_ICCV_2015_paper.pdf",
        "aff": "University of Maryland; Google; Google; Google; Google; Google; Google; Google; Google; Google",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1245903,
        "gs_citation": 602,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=666928001591037314&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "umd.edu;google.com;google.com;google.com;google.com;google.com;google.com;google.com;google.com;google.com",
        "email": "umd.edu;google.com;google.com;google.com;google.com;google.com;google.com;google.com;google.com;google.com",
        "author_num": 10,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Meyers_Im2Calories_Towards_an_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;1;1;1;1;1;1;1;1",
        "aff_unique_norm": "University of Maryland;Google",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www/umd.edu;https://www.google.com",
        "aff_unique_abbr": "UMD;Google",
        "aff_campus_unique_index": "1;1;1;1;1;1;1;1;1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Meyers_2015_ICCV,\n    \n    author = {\n    Meyers,\n    Austin and Johnston,\n    Nick and Rathod,\n    Vivek and Korattikara,\n    Anoop and Gorban,\n    Alex and Silberman,\n    Nathan and Guadarrama,\n    Sergio and Papandreou,\n    George and Huang,\n    Jonathan and Murphy,\n    Kevin P.\n},\n    title = {\n    Im2Calories: Towards an Automated Mobile Vision Food Diary\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "0974dd9218",
        "title": "Image Matting With KL-Divergence Based Sparse Sampling",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Levent Karacan, Aykut Erdem, Erkut Erdem",
        "author": "Levent Karacan; Aykut Erdem; Erkut Erdem",
        "abstract": "Previous sampling-based image matting methods typically rely on certain heuristics in collecting representative samples from known regions, and thus their performance deteriorates if the underlying assumptions are not satisfied. To alleviate this, in this paper we take an entirely new approach and formulate sampling as a sparse subset selection problem where we propose to pick a small set of candidate samples that best explains the unknown pixels. Moreover, we describe a new distance measure for comparing two samples which is based on KL-divergence between the distributions of features extracted in the vicinity of the samples. Using a standard benchmark dataset for image matting, we demonstrate that our approach provides more accurate results compared with the state-of-the-art methods.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Karacan_Image_Matting_With_ICCV_2015_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2010472,
        "gs_citation": 94,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3377678406633243216&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Karacan_Image_Matting_With_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Karacan_2015_ICCV,\n    \n    author = {\n    Karacan,\n    Levent and Erdem,\n    Aykut and Erdem,\n    Erkut\n},\n    title = {\n    Image Matting With KL-Divergence Based Sparse Sampling\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "2a79e44cfb",
        "title": "Improving Ferns Ensembles by Sparsifying and Quantising Posterior Probabilities",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Antonio L. Rodriguez, Vitor Sequeira",
        "author": "Antonio L. Rodriguez; Vitor Sequeira",
        "abstract": "Ferns ensembles offer an accurate and efficient multiclass non-linear classification, commonly at the expense of consuming a large amount of memory. We introduce a two-fold contribution that produces large reductions in their memory consumption. First, an efficient L0 regularised cost optimisation finds a sparse representation of the posterior probabilities in the ensemble by discarding elements with zero contribution to valid responses in the training samples. As a by-product this can produce a prediction accuracy gain that, if required, can be traded for further reductions in memory size and prediction time. Secondly, posterior probabilities are quantised and stored in a memory-friendly sparse data structure. We reported a minimum of 75% memory reduction for different types of classification problems using generative and discriminative ferns ensembles, without increasing prediction time or classification error. For image patch recognition our proposal produced a 90% memory reduction, and improved in several percentage points the prediction accuracy.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Rodriguez_Improving_Ferns_Ensembles_ICCV_2015_paper.pdf",
        "aff": "Joint Research Centre, Institute for Transuranium Elements, Ispra, Italy; Joint Research Centre, Institute for Transuranium Elements, Ispra, Italy",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 967819,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:WTHI-e1Y8yIJ:scholar.google.com/&scioq=Improving+Ferns+Ensembles+by+Sparsifying+and+Quantising+Posterior+Probabilities&hl=en&as_sdt=0,33",
        "gs_version_total": 9,
        "aff_domain": "jrc.ec.europa.eu;jrc.ec.europa.eu",
        "email": "jrc.ec.europa.eu;jrc.ec.europa.eu",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Rodriguez_Improving_Ferns_Ensembles_ICCV_2015_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Joint Research Centre",
        "aff_unique_dep": "Institute for Transuranium Elements",
        "aff_unique_url": "https://jrc.ec.europa.eu",
        "aff_unique_abbr": "JRC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Ispra",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Italy",
        "bibtex": "@InProceedings{Rodriguez_2015_ICCV,\n    \n    author = {\n    Rodriguez,\n    Antonio L. and Sequeira,\n    Vitor\n},\n    title = {\n    Improving Ferns Ensembles by Sparsifying and Quantising Posterior Probabilities\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "aae26c3233",
        "title": "Improving Image Classification With Location Context",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Kevin Tang, Manohar Paluri, Li Fei-Fei, Rob Fergus, Lubomir Bourdev",
        "author": "Kevin Tang; Manohar Paluri; Li Fei-Fei; Rob Fergus; Lubomir Bourdev",
        "abstract": "With the widespread availability of cellphones and cameras that have GPS capabilities, it is common for images being uploaded to the Internet today to have GPS coordinates associated with them. In addition to research that tries to predict GPS coordinates from visual features, this also opens up the door to problems that are conditioned on the availability of GPS coordinates. In this work, we tackle the problem of performing image classification with location context, in which we are given the GPS coordinates for images in both the train and test phases. We explore different ways of encoding and extracting features from the GPS coordinates, and show how to naturally incorporate these features into a Convolutional Neural Network (CNN), the current state-of-the-art for most image classification and recognition problems. We also show how it is possible to simultaneously learn the optimal pooling radii for a subset of our features within the CNN framework. To evaluate our model and to help promote research in this area, we identify a set of location-sensitive concepts and annotate a subset of the Yahoo Flickr Creative Commons 100M dataset that has GPS coordinates with these concepts, which we make publicly available. By leveraging location context, we are able to achieve almost a 7% gain in mean average precision.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Tang_Improving_Image_Classification_ICCV_2015_paper.pdf",
        "aff": "Computer Science Department, Stanford University; Facebook AI Research; Computer Science Department, Stanford University; Facebook AI Research; Facebook AI Research",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1166571,
        "gs_citation": 164,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15585268324006597517&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff_domain": "cs.stanford.edu;fb.com;cs.stanford.edu;fb.com;fb.com",
        "email": "cs.stanford.edu;fb.com;cs.stanford.edu;fb.com;fb.com",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Tang_Improving_Image_Classification_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;0;1;1",
        "aff_unique_norm": "Stanford University;Facebook",
        "aff_unique_dep": "Computer Science Department;Facebook AI Research",
        "aff_unique_url": "https://www.stanford.edu;https://research.facebook.com",
        "aff_unique_abbr": "Stanford;FAIR",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Tang_2015_ICCV,\n    \n    author = {\n    Tang,\n    Kevin and Paluri,\n    Manohar and Fei-Fei,\n    Li and Fergus,\n    Rob and Bourdev,\n    Lubomir\n},\n    title = {\n    Improving Image Classification With Location Context\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "8bc29aceed",
        "title": "Improving Image Restoration With Soft-Rounding",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Xing Mei, Honggang Qi, Bao-Gang Hu, Siwei Lyu",
        "author": "Xing Mei; Honggang Qi; Bao-Gang Hu; Siwei Lyu",
        "abstract": "Several important classes of images such as text, barcode and pattern images have the property that pixels can only take a distinct subset of values. This knowledge can benefit the restoration of such images, but it has not been widely considered in current restoration methods. In this work, we describe an effective and efficient approach to incorporate the knowledge of distinct pixel values of the pristine images into the general regularized least squares restoration framework. We introduce a new regularizer that attains zero at the designated pixel values and becomes a quadratic penalty function in the intervals between them. When incorporated into the regularized least squares restoration framework, this regularizer leads to a simple and efficient step that resembles and extends the rounding operation, which we term as soft-rounding. We apply the soft-rounding enhanced solution to the restoration of binary text/barcode images and pattern images with multiple distinct pixel values. Experimental results show that soft-rounding enhanced restoration methods achieve significant improvement in both visual quality and quantitative measures (PSNR and SSIM). Furthermore, we show that this regularizer can also benefit the restoration of general natural images.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Mei_Improving_Image_Restoration_ICCV_2015_paper.pdf",
        "aff": "Computer Science Department, University at Albany, SUNY; Computer Science Department, University of Chinese Academy of Sciences; NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Computer Science Department, University at Albany, SUNY",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2565308,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13831673769805492653&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "albany.edu;jdl.ac.cn;nlpr.ia.ac.cn;albany.edu",
        "email": "albany.edu;jdl.ac.cn;nlpr.ia.ac.cn;albany.edu",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Mei_Improving_Image_Restoration_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "University at Albany, SUNY;University of Chinese Academy of Sciences;Chinese Academy of Sciences",
        "aff_unique_dep": "Computer Science Department;Computer Science Department;Institute of Automation",
        "aff_unique_url": "https://www.albany.edu;http://www.ucas.ac.cn;http://www.ia.cas.cn",
        "aff_unique_abbr": "UAlbany;UCAS;CAS",
        "aff_campus_unique_index": "0;2;0",
        "aff_campus_unique": "Albany;;Beijing",
        "aff_country_unique_index": "0;1;1;0",
        "aff_country_unique": "United States;China",
        "bibtex": "@InProceedings{Mei_2015_ICCV,\n    \n    author = {\n    Mei,\n    Xing and Qi,\n    Honggang and Hu,\n    Bao-Gang and Lyu,\n    Siwei\n},\n    title = {\n    Improving Image Restoration With Soft-Rounding\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "cf083123eb",
        "title": "Inferring M-Best Diverse Labelings in a Single One",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Alexander Kirillov, Bogdan Savchynskyy, Dmitrij Schlesinger, Dmitry Vetrov, Carsten Rother",
        "author": "Alexander Kirillov; Bogdan Savchynskyy; Dmitrij Schlesinger; Dmitry Vetrov; Carsten Rother",
        "abstract": "We consider the task of finding M-best diverse solutions in a graphical model. In a previous work by Batra et al. an algorithmic approach for finding such solutions was proposed, and its usefulness was shown in numerous applications. Contrary to previous work we propose a novel formulation of the problem in form of a single energy minimization problem in a specially constructed graphical model. We show that the method of Batra et al. can be considered as a greedy approximate algorithm for our model, whereas we introduce an efficient specialized optimization technique for it, based on alpha-expansion. We evaluate our method on two application scenarios, interactive and semantic image segmentation, with binary and multiple labels. In both cases we achieve considerably better error rates than state-of-the art diversity methods. Furthermore, we empirically discover that in the binary label case we were able to reach global optimality for all test instances.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Kirillov_Inferring_M-Best_Diverse_ICCV_2015_paper.pdf",
        "aff": "TU Dresden, Dresden, Germany; TU Dresden, Dresden, Germany; TU Dresden, Dresden, Germany; Skoltech, Moscow, Russia; TU Dresden, Dresden, Germany",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1922130,
        "gs_citation": 51,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11323262743726273427&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "tu-dresden.de;tu-dresden.de;tu-dresden.de;yandex.ru;tu-dresden.de",
        "email": "tu-dresden.de;tu-dresden.de;tu-dresden.de;yandex.ru;tu-dresden.de",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Kirillov_Inferring_M-Best_Diverse_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "Technische Universität Dresden;Skolkovo Institute of Science and Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tu-dresden.de;https://www.skoltech.ru",
        "aff_unique_abbr": "TUD;Skoltech",
        "aff_campus_unique_index": "0;0;0;1;0",
        "aff_campus_unique": "Dresden;Moscow",
        "aff_country_unique_index": "0;0;0;1;0",
        "aff_country_unique": "Germany;Russia",
        "bibtex": "@InProceedings{Kirillov_2015_ICCV,\n    \n    author = {\n    Kirillov,\n    Alexander and Savchynskyy,\n    Bogdan and Schlesinger,\n    Dmitrij and Vetrov,\n    Dmitry and Rother,\n    Carsten\n},\n    title = {\n    Inferring M-Best Diverse Labelings in a Single One\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "d1de512d13",
        "title": "Infinite Feature Selection",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Giorgio Roffo, Simone Melzi, Marco Cristani",
        "author": "Giorgio Roffo; Simone Melzi; Marco Cristani",
        "abstract": "Filter-based feature selection has become crucial in many classification settings, especially object recognition, recently faced with feature learning strategies that originate thousands of cues. In this paper, we propose a feature selection method exploiting the convergence properties of power series of matrices, and introducing the concept of infinite feature selection (Inf-FS). Considering a selection of features as a path among feature distributions and letting these paths tend to an infinite number permits the investigation of the importance (relevance and redundancy) of a feature when injected into an arbitrary set of cues. Ranking the importance individuates candidate features, which turn out to be effective from a classification point of view, as proved by a thoroughly experimental section. The Inf-FS has been tested on thirteen diverse benchmarks, comparing against filters, embedded methods, and wrappers; in all the cases we achieve top performances, notably on the classification tasks of PASCAL VOC 2007-2012.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Roffo_Infinite_Feature_Selection_ICCV_2015_paper.pdf",
        "aff": "University of Verona, Department of Computer Science, Strada le Grazie 15, 37134, Verona, Italy; University of Verona, Department of Computer Science, Strada le Grazie 15, 37134, Verona, Italy; University of Verona, Department of Computer Science, Strada le Grazie 15, 37134, Verona, Italy",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 543523,
        "gs_citation": 392,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13878532147544819553&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff_domain": "univr.it;univr.it;univr.it",
        "email": "univr.it;univr.it;univr.it",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Roffo_Infinite_Feature_Selection_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Verona",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.univr.it",
        "aff_unique_abbr": "UniVR",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Verona",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Italy",
        "bibtex": "@InProceedings{Roffo_2015_ICCV,\n    \n    author = {\n    Roffo,\n    Giorgio and Melzi,\n    Simone and Cristani,\n    Marco\n},\n    title = {\n    Infinite Feature Selection\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "fc9b2b22b7",
        "title": "Integrating Dashcam Views Through Inter-Video Mapping",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Hsin-I Chen, Yi-Ling Chen, Wei-Tse Lee, Fan Wang, Bing-Yu Chen",
        "author": "Hsin-I Chen; Yi-Ling Chen; Wei-Tse Lee; Fan Wang; Bing-Yu Chen",
        "abstract": "In this paper, an inter-video mapping approach is proposed to integrate video footages from two dashcams installed on a preceding and its following vehicle to provide the illusion that the driver of the following vehicle can see-through the preceding one. The key challenge is to adapt the perspectives of the two videos based on a small number of common features since a large portion of the common region in the video captured by the following vehicle is occluded by the preceding one. Inspired by the observation that images with the most similar viewpoints yield dense and high-quality matches, the proposed inter-video mapping estimates spatially-varying motions across two videos utilizing images of very similar contents. Specifically, we estimate frame-to-frame motions of each two consecutive images and incrementally add new views into a merged representation. In this way, long-rang motion estimation is achieved, and the observed perspective discrepancy between the two videos can be well approximated our motion estimation. Once the inter-video mapping is established, the correspondences can be updated incrementally, so the proposed method is suitable for on-line applications. Our experiments demonstrate the effectiveness of our approach on real-world challenging videos.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Chen_Integrating_Dashcam_Views_ICCV_2015_paper.pdf",
        "aff": "National Taiwan University; National Taiwan University; National Taiwan University; National Taiwan University; National Taiwan University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1260629,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7192703020199669695&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "cmlab.csie.ntu.edu.tw;cmlab.csie.ntu.edu.tw;cmlab.csie.ntu.edu.tw;cmlab.csie.ntu.edu.tw;cmlab.csie.ntu.edu.tw",
        "email": "cmlab.csie.ntu.edu.tw;cmlab.csie.ntu.edu.tw;cmlab.csie.ntu.edu.tw;cmlab.csie.ntu.edu.tw;cmlab.csie.ntu.edu.tw",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Chen_Integrating_Dashcam_Views_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "National Taiwan University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ntu.edu.tw",
        "aff_unique_abbr": "NTU",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Taiwan",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Chen_2015_ICCV,\n    \n    author = {\n    Chen,\n    Hsin-I and Chen,\n    Yi-Ling and Lee,\n    Wei-Tse and Wang,\n    Fan and Chen,\n    Bing-Yu\n},\n    title = {\n    Integrating Dashcam Views Through Inter-Video Mapping\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "5662d61c3c",
        "title": "Interactive Visual Hull Refinement for Specular and Transparent Object Surface Reconstruction",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Xinxin Zuo, Chao Du, Sen Wang, Jiangbin Zheng, Ruigang Yang",
        "author": "Xinxin Zuo; Chao Du; Sen Wang; Jiangbin Zheng; Ruigang Yang",
        "abstract": "In this paper we present a method of using standard multi-view images for 3D surface reconstruction of non-Lambertian objects. We extend the original visual hull concept to incorporate 3D cues presented by internal occluding contours, i.e., occluding contours that are inside the object's silhouettes. We discovered that these internal contours, which are results of convex parts on an object's surface, can lead to a tighter fit than the original visual hull. We formulated a new visual hull refinement scheme - Locally Convex Carving that can completely reconstruct concavity caused by two or more intersecting convex surfaces. In addition we develop a novel approach for contour tracking given labeled contours in sparse key frames. It is designed specifically for highly specular or transparent objects, for which assumptions made in traditional contour detection/tracking methods, such as highest gradient and stationary texture edges, are no longer valid. It is formulated as an energy minimization function where several novel terms are developed to increase robustness. Based on the two core algorithms, we have developed an interactive system for 3D modeling. We have validated our system, both quantitatively and qualitatively, with four datasets of different object materials. Results show that we are able to generate visually pleasing models for very challenging cases.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Zuo_Interactive_Visual_Hull_ICCV_2015_paper.pdf",
        "aff": "Northwestern Polytechnical University, P.R.China+University of Kentucky, USA; University of Kentucky, USA; Northwestern Polytechnical University, P.R.China+University of Kentucky, USA; Northwestern Polytechnical University, P.R.China; University of Kentucky, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1433480,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11582475784684050018&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "uky.edu;uky.edu;uky.edu;nwpu.edu.cn;cs.uky.edu",
        "email": "uky.edu;uky.edu;uky.edu;nwpu.edu.cn;cs.uky.edu",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Zuo_Interactive_Visual_Hull_ICCV_2015_paper.html",
        "aff_unique_index": "0+1;1;0+1;0;1",
        "aff_unique_norm": "Northwestern Polytechnical University;University of Kentucky",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.nwpu.edu.cn;https://www.uky.edu",
        "aff_unique_abbr": "NWPU;UK",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;1;0+1;0;1",
        "aff_country_unique": "China;United States",
        "bibtex": "@InProceedings{Zuo_2015_ICCV,\n    \n    author = {\n    Zuo,\n    Xinxin and Du,\n    Chao and Wang,\n    Sen and Zheng,\n    Jiangbin and Yang,\n    Ruigang\n},\n    title = {\n    Interactive Visual Hull Refinement for Specular and Transparent Object Surface Reconstruction\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "4f91ec091d",
        "title": "Interpolation on the Manifold of K Component GMMs",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Hyunwoo J. Kim, Nagesh Adluru, Monami Banerjee, Baba C. Vemuri, Vikas Singh",
        "author": "Hyunwoo J. Kim; Nagesh Adluru; Monami Banerjee; Baba C. Vemuri; Vikas Singh",
        "abstract": "Probability density functions (PDFs) are fundamental \"objects\" in mathematics with numerous applications in computer vision, machine learning and medical imaging. The feasibility of basic operations such as computing the distance between two PDFs and estimating a mean of a set of PDFs is a direct function of the representation we choose to work with. In this paper, we study the Gaussian mixture model  (GMM) representation of the PDFs motivated by its numerous attractive features. (1) GMMs are arguably more interpretable than, say, square root parameterizations (2) the model complexity can be explicitly controlled by the number of components and (3) they are already widely used in many applications. The main contributions of  this paper are numerical algorithms to enable basic operations on such objects that strictly respect their underlying geometry.  For instance, when operating with a set of k component GMMs,  a first order expectation is that the result of  simple operations like interpolation and averaging should  provide an object that is also a k component GMM.  The literature provides very little guidance on enforcing such  requirements systematically. It turns out that these tasks are  important internal modules for analysis and processing of a field of ensemble average propagators (EAPs), common  in diffusion weighted magnetic resonance imaging. We provide proof of principle experiments showing how the proposed algorithms for interpolation can facilitate statistical  analysis of such data, essential to many neuroimaging studies. Separately, we also derive interesting connections of our algorithm  with functional spaces of Gaussians, that may be of independent interest.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Kim_Interpolation_on_the_ICCV_2015_paper.pdf",
        "aff": "University of Wisconsin–Madison; University of Wisconsin–Madison; University of Florida; University of Florida; University of Wisconsin–Madison",
        "project": "http://pages.cs.wisc.edu/~hwkim/projects/k-gmm",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2388870,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11349345082030799502&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Kim_Interpolation_on_the_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;1;1;0",
        "aff_unique_norm": "University of Wisconsin–Madison;University of Florida",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.wisc.edu;https://www.ufl.edu",
        "aff_unique_abbr": "UW–Madison;UF",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Madison;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Kim_2015_ICCV,\n    \n    author = {\n    Kim,\n    Hyunwoo J. and Adluru,\n    Nagesh and Banerjee,\n    Monami and Vemuri,\n    Baba C. and Singh,\n    Vikas\n},\n    title = {\n    Interpolation on the Manifold of K Component GMMs\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "7b0adc6f19",
        "title": "Intrinsic Decomposition of Image Sequences From Local Temporal Variations",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Pierre-Yves Laffont, Jean-Charles Bazin",
        "author": "Pierre-Yves Laffont; Jean-Charles Bazin",
        "abstract": "We present a method for intrinsic image decomposition, which aims to decompose images into reflectance and shading layers. Our input is a sequence of images with varying illumination acquired by a static camera, e.g. an indoor scene with a moving light source or an outdoor timelapse. We leverage the local color variations observed over time to infer constraints on the reflectance and solve the ill-posed image decomposition problem. In particular, we derive an adaptive local energy from the observations of each local neighborhood over time, and integrate distant pairwise constraints to enforce coherent decomposition across all surfaces with consistent shading changes. Our method is solely based on multiple observations of a Lambertian scene under varying illumination and does not require user interaction, scene geometry, or an explicit lighting model. We compare our results with several intrinsic decomposition methods on a number of synthetic and captured datasets.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Laffont_Intrinsic_Decomposition_of_ICCV_2015_paper.pdf",
        "aff": "ETH Zurich; ETH Zurich",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 928706,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14405636260603372629&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Laffont_Intrinsic_Decomposition_of_ICCV_2015_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "ETH Zurich",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ethz.ch",
        "aff_unique_abbr": "ETHZ",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Switzerland",
        "bibtex": "@InProceedings{Laffont_2015_ICCV,\n    \n    author = {\n    Laffont,\n    Pierre-Yves and Bazin,\n    Jean-Charles\n},\n    title = {\n    Intrinsic Decomposition of Image Sequences From Local Temporal Variations\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "c722446f72",
        "title": "Intrinsic Depth: Improving Depth Transfer With Intrinsic Images",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Naejin Kong, Michael J. Black",
        "author": "Naejin Kong; Michael J. Black",
        "abstract": "We formulate the estimation of dense depth maps from video sequences as a problem of intrinsic image estimation. Our approach synergistically integrates the estimation of multiple intrinsic images including depth, albedo, shading, optical flow, and surface contours. We build upon an example-based framework for depth estimation that uses label transfer from a database of RGB and depth pairs. We combine this with a method that extracts consistent albedo and shading from video. In contrast to raw RGB values, albedo and shading provide a richer, more physical, foundation for depth transfer. Additionally we train a new contour detector to predict surface boundaries from albedo, shading, and pixel values and use this to improve the estimation of depth boundaries. We also integrate sparse structure from motion with our method to improve the metric accuracy of the estimated depth maps. We evaluate our Intrinsic Depth method quantitatively by estimating depth from videos in the NYU RGB-D and SUN3D datasets. We find that combining the estimation of multiple intrinsic images improves depth estimation relative to the baseline method.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Kong_Intrinsic_Depth_Improving_ICCV_2015_paper.pdf",
        "aff": ";",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2724171,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2958950625397273015&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Kong_Intrinsic_Depth_Improving_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Kong_2015_ICCV,\n    \n    author = {\n    Kong,\n    Naejin and Black,\n    Michael J.\n},\n    title = {\n    Intrinsic Depth: Improving Depth Transfer With Intrinsic Images\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "900837a5c8",
        "title": "Intrinsic Scene Decomposition From RGB-D images",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Mohammed Hachama, Bernard Ghanem, Peter Wonka",
        "author": "Mohammed Hachama; Bernard Ghanem; Peter Wonka",
        "abstract": "In this paper, we address the problem of computing an intrinsic decomposition of the colors of a surface into an albedo and a shading term. The surface is reconstructed from a single or multiple RGB-D images of a static scene obtained from different views. We thereby extend and improve existing works in the area of intrinsic image decomposition. In a variational framework, we formulate the problem as a minimization of an energy composed of two terms: a data term and a regularity term. The first term is related to the image formation process and expresses the relation between the albedo, the surface normals, and the incident illumination. We use an affine shading model, a combination of a Lambertian model, and an ambient lighting term. This model is relevant for Lambertian surfaces. When available, multiple views can be used to handle view-dependent non-Lambertian reflections. The second term contains an efficient combination of l2 and l1-regularizers on the illumination vector field and albedo respectively. Unlike most previous approaches, especially Retinex-like techniques, these terms do not depend on the image gradient or texture, thus reducing the mixing shading/reflectance artifacts and leading to better results. The obtained non-linear optimization problem is efficiently solved using a cyclic block coordinate descent algorithm. Our method outperforms a range of state-of-the-art algorithms on a popular benchmark dataset.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Hachama_Intrinsic_Scene_Decomposition_ICCV_2015_paper.pdf",
        "aff": "King Abdullah University of Science and Technology (KAUST); King Abdullah University of Science and Technology (KAUST); King Abdullah University of Science and Technology (KAUST)",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2430821,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9833689753779984469&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff_domain": "gmail.com;kaust.edu.sa;gmail.com",
        "email": "gmail.com;kaust.edu.sa;gmail.com",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Hachama_Intrinsic_Scene_Decomposition_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "King Abdullah University of Science and Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.kaust.edu.sa",
        "aff_unique_abbr": "KAUST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Saudi Arabia",
        "bibtex": "@InProceedings{Hachama_2015_ICCV,\n    \n    author = {\n    Hachama,\n    Mohammed and Ghanem,\n    Bernard and Wonka,\n    Peter\n},\n    title = {\n    Intrinsic Scene Decomposition From RGB-D images\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "521012f8c5",
        "title": "Introducing Geometry in Active Learning for Image Segmentation",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Ksenia Konyushkova, Raphael Sznitman, Pascal Fua",
        "author": "Ksenia Konyushkova; Raphael Sznitman; Pascal Fua",
        "abstract": "We propose  an Active  Learning approach to  training a  segmentation classifier that exploits geometric priors to streamline  the annotation process in 3D image volumes.  To  this end, we use  these priors not  only to select voxels  most in need of  annotation but  to guarantee that  they lie on  2D planar  patch, which makes it much easier  to annotate than if they were  randomly distributed in the volume. A simplified version of this approach is effective in natural 2D images.  We evaluated  our approach on  Electron Microscopy and Magnetic  Resonance image volumes, as well  as on natural images.  Comparing our  approach against several accepted baselines demonstrates a marked performance increase.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Konyushkova_Introducing_Geometry_in_ICCV_2015_paper.pdf",
        "aff": "EPFL; University of Bern; EPFL",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1224642,
        "gs_citation": 82,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18079345692282849891&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "epfl.ch;artorg.unibe.ch;epfl.ch",
        "email": "epfl.ch;artorg.unibe.ch;epfl.ch",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Konyushkova_Introducing_Geometry_in_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Ecole Polytechnique Fédérale de Lausanne;University of Bern",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.epfl.ch;https://www.unibe.ch",
        "aff_unique_abbr": "EPFL;UniBE",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Switzerland",
        "bibtex": "@InProceedings{Konyushkova_2015_ICCV,\n    \n    author = {\n    Konyushkova,\n    Ksenia and Sznitman,\n    Raphael and Fua,\n    Pascal\n},\n    title = {\n    Introducing Geometry in Active Learning for Image Segmentation\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "40c2951616",
        "title": "Joint Camera Clustering and Surface Segmentation for Large-Scale Multi-View Stereo",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Runze Zhang, Shiwei Li, Tian Fang, Siyu Zhu, Long Quan",
        "author": "Runze Zhang; Shiwei Li; Tian Fang; Siyu Zhu; Long Quan",
        "abstract": "In this paper, we propose an optimal decomposition approach to large-scale multi-view stereo from an initial sparse reconstruction. The success of the approach depends on the introduction of surface-segmentation-based camera clustering rather than sparse-point-based camera clustering, which suffers from the problems of non-uniform reconstruction coverage ratio and high redundancy. In details, we introduce three criteria for camera clustering and surface segmentation for reconstruction, and then we formulate these criteria into an energy minimization problem under constraints. To solve this problem, we propose a joint optimization in a hierarchical framework to obtain the final surface segments and corresponding optimal camera clusters. On each level of the hierarchical framework, the camera clustering problem is formulated as a parameter estimation problem of a probability model solved by a General Expectation-Maximization algorithm and the surface segmentation problem is formulated as a Markov Random Field model based on the probability estimated by the previous camera clustering process. The experiments on several Internet datasets and aerial photo datasets demonstrate that the proposed approach method generates more uniform and complete dense reconstruction with less redundancy, resulting in more efficient multi-view stereo algorithm.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Zhang_Joint_Camera_Clustering_ICCV_2015_paper.pdf",
        "aff": "The Hong Kong University of Science and Technology; The Hong Kong University of Science and Technology; The Hong Kong University of Science and Technology; The Hong Kong University of Science and Technology; The Hong Kong University of Science and Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 5534346,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15300824311483799820&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "cse.ust.hk;cse.ust.hk;cse.ust.hk;cse.ust.hk;cse.ust.hk",
        "email": "cse.ust.hk;cse.ust.hk;cse.ust.hk;cse.ust.hk;cse.ust.hk",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Zhang_Joint_Camera_Clustering_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Hong Kong University of Science and Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ust.hk",
        "aff_unique_abbr": "HKUST",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Zhang_2015_ICCV,\n    \n    author = {\n    Zhang,\n    Runze and Li,\n    Shiwei and Fang,\n    Tian and Zhu,\n    Siyu and Quan,\n    Long\n},\n    title = {\n    Joint Camera Clustering and Surface Segmentation for Large-Scale Multi-View Stereo\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "e60cc8f452",
        "title": "Joint Fine-Tuning in Deep Neural Networks for Facial Expression Recognition",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Heechul Jung, Sihaeng Lee, Junho Yim, Sunjeong Park, Junmo Kim",
        "author": "Heechul Jung; Sihaeng Lee; Junho Yim; Sunjeong Park; Junmo Kim",
        "abstract": "Temporal information has useful features for recognizing facial expressions. However, to manually design useful features requires a lot of effort. In this paper, to reduce this effort, a deep learning technique, which is regarded as a tool to automatically extract useful features from raw data, is adopted. Our deep network is based on two different models. The first deep network extracts temporal appearance features from image sequences, while the other deep network extracts temporal geometry features from temporal facial landmark points. These two models are combined using a new integration method in order to boost the performance of the facial expression recognition. Through several experiments, we show that the two models cooperate with each other. As a result, we achieve superior performance to other state-of-the-art methods in the CK+ and Oulu-CASIA databases. Furthermore, we show that our new integration method gives more accurate results than traditional methods, such as a weighted summation and a feature concatenation method.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Jung_Joint_Fine-Tuning_in_ICCV_2015_paper.pdf",
        "aff": "School of Electrical Engineering, Korea Advanced Institute of Science and Technology; School of Electrical Engineering, Korea Advanced Institute of Science and Technology; School of Electrical Engineering, Korea Advanced Institute of Science and Technology; School of Electrical Engineering, Korea Advanced Institute of Science and Technology; School of Electrical Engineering, Korea Advanced Institute of Science and Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 864154,
        "gs_citation": 967,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3147204119858789086&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr",
        "email": "kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Jung_Joint_Fine-Tuning_in_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Korea Advanced Institute of Science and Technology",
        "aff_unique_dep": "School of Electrical Engineering",
        "aff_unique_url": "https://www.kaist.ac.kr",
        "aff_unique_abbr": "KAIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "South Korea",
        "bibtex": "@InProceedings{Jung_2015_ICCV,\n    \n    author = {\n    Jung,\n    Heechul and Lee,\n    Sihaeng and Yim,\n    Junho and Park,\n    Sunjeong and Kim,\n    Junmo\n},\n    title = {\n    Joint Fine-Tuning in Deep Neural Networks for Facial Expression Recognition\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "d62b8433c4",
        "title": "Joint Object and Part Segmentation Using Deep Learned Potentials",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Peng Wang, Xiaohui Shen, Zhe Lin, Scott Cohen, Brian Price, Alan L. Yuille",
        "author": "Peng Wang; Xiaohui Shen; Zhe Lin; Scott Cohen; Brian Price; Alan L. Yuille",
        "abstract": "Segmenting semantic objects from images and parsing them into their respective semantic parts are fundamental steps towards detailed object understanding in computer vision. In this paper, we propose a joint solution that tackles semantic object and part segmentation simultaneously, in which higher object-level context is provided to guide part segmentation, and more detailed part-level localization is utilized to refine object segmentation. Specifically, we first introduce the concept of semantic compositional parts (SCP) in which similar semantic parts are grouped and shared among different objects. A two-stream fully convolutional network (FCN) is then trained to provide the SCP and object potentials at each pixel. At the same time, a compact set of segments can also be obtained from the SCP predictions of the network. Given the potentials and the generated segments, in order to explore long-range context, we finally construct an efficient fully connected conditional random field (FCRF) to jointly predict the final object and part labels. Extensive evaluation on three different datasets shows that our approach can mutually enhance the performance of object and part segmentation, and outperforms the current state-of-the-art on both tasks.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Wang_Joint_Object_and_ICCV_2015_paper.pdf",
        "aff": "University of California, Los Angeles; Adobe Research; Adobe Research; Adobe Research; Adobe Research; University of California, Los Angeles",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2039634,
        "gs_citation": 141,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4342156029683513177&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Wang_Joint_Object_and_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;1;1;1;0",
        "aff_unique_norm": "University of California, Los Angeles;Adobe",
        "aff_unique_dep": ";Adobe Research",
        "aff_unique_url": "https://www.ucla.edu;https://research.adobe.com",
        "aff_unique_abbr": "UCLA;Adobe",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Wang_2015_ICCV,\n    \n    author = {\n    Wang,\n    Peng and Shen,\n    Xiaohui and Lin,\n    Zhe and Cohen,\n    Scott and Price,\n    Brian and Yuille,\n    Alan L.\n},\n    title = {\n    Joint Object and Part Segmentation Using Deep Learned Potentials\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "15db3d8840",
        "title": "Joint Optimization of Segmentation and Color Clustering",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Ekaterina Lobacheva, Olga Veksler, Yuri Boykov",
        "author": "Ekaterina Lobacheva; Olga Veksler; Yuri Boykov",
        "abstract": "Binary energy optimization is a popular approach for segmenting a color image into foreground/background regions. To model the appearance of the regions, color, a relatively high dimensional feature, should be handled effectively. A full color histogram is usually too sparse to be reliable. One approach is to explicitly reduce dimensionality by clustering or quantizing the color space. Another popular approach is to fit GMMs for soft implicit clustering of the color space. These approaches work well when the foreground/background are sufficiently distinct. In cases of more subtle difference in appearance, both approaches may reduce or even eliminate foreground/background distinction. This happens because either color clustering is performed completely independently from the segmentation process, as a preprocessing step (in clustering), or independently for the foreground and independently for the background (in GMM). We propose to make clustering an integral part of segmentation, by including a new clustering term in the energy function. Our energy function with a clustering term favours clusterings that make foreground/background appearance more distinct. Thus our energy function jointly optimizes over color clustering, foreground/background models, and segmentation. Exact optimization is not feasible, therefore we develop an approximate algorithm. We show the advantage of including the color clustering term into the energy function on  camouflage images, as well as standard segmentation datasets.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Lobacheva_Joint_Optimization_of_ICCV_2015_paper.pdf",
        "aff": "National Research University Higher School of Economics, Russia; Computer Vision Group University of Western Ontario, Canada; Computer Vision Group University of Western Ontario, Canada",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 959240,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3593984816551646680&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": "hse.ru;csd.uwo.ca;csd.uwo.ca",
        "email": "hse.ru;csd.uwo.ca;csd.uwo.ca",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Lobacheva_Joint_Optimization_of_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "National Research University Higher School of Economics;University of Western Ontario",
        "aff_unique_dep": ";Computer Vision Group",
        "aff_unique_url": "https://hse.ru;https://www.uwo.ca",
        "aff_unique_abbr": "HSE;UWO",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "Russia;Canada",
        "bibtex": "@InProceedings{Lobacheva_2015_ICCV,\n    \n    author = {\n    Lobacheva,\n    Ekaterina and Veksler,\n    Olga and Boykov,\n    Yuri\n},\n    title = {\n    Joint Optimization of Segmentation and Color Clustering\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "e499365b00",
        "title": "Joint Probabilistic Data Association Revisited",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Seyed Hamid Rezatofighi, Anton Milan, Zhen Zhang, Qinfeng Shi, Anthony Dick, Ian Reid",
        "author": "Seyed Hamid Rezatofighi; Anton Milan; Zhen Zhang; Qinfeng Shi; Anthony Dick; Ian Reid",
        "abstract": "In this paper, we revisit the joint probabilistic data association (JPDA) technique and propose a novel solution based on recent developments in finding the m-best solutions to an integer linear program. The key advantage of this approach is that it makes JPDA computationally tractable in applications with high target and/or clutter density, such as spot tracking in fluorescence microscopy sequences and pedestrian tracking in surveillance footage. We also show that our JPDA algorithm embedded in a simple tracking framework is surprisingly competitive with state-of-the-art global tracking methods in these two applications, while needing considerably less processing time.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Rezatofighi_Joint_Probabilistic_Data_ICCV_2015_paper.pdf",
        "aff": "School of Computer Science, The University of Adelaide, Australia; School of Computer Science, The University of Adelaide, Australia; School of Computer Science and Technology, Northwestern Polytechnical University, Xian, China; School of Computer Science, The University of Adelaide, Australia; School of Computer Science, The University of Adelaide, Australia; School of Computer Science, The University of Adelaide, Australia",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1139754,
        "gs_citation": 454,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14784685094867763365&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "adelaide.edu.au; ; ; ; ; ",
        "email": "adelaide.edu.au; ; ; ; ; ",
        "author_num": 6,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Rezatofighi_Joint_Probabilistic_Data_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;1;0;0;0",
        "aff_unique_norm": "The University of Adelaide;Northwestern Polytechnical University",
        "aff_unique_dep": "School of Computer Science;School of Computer Science and Technology",
        "aff_unique_url": "https://www.adelaide.edu.au;http://www.nwpu.edu.cn",
        "aff_unique_abbr": "Adelaide;NPU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Xian",
        "aff_country_unique_index": "0;0;1;0;0;0",
        "aff_country_unique": "Australia;China",
        "bibtex": "@InProceedings{Rezatofighi_2015_ICCV,\n    \n    author = {\n    Rezatofighi,\n    Seyed Hamid and Milan,\n    Anton and Zhang,\n    Zhen and Shi,\n    Qinfeng and Dick,\n    Anthony and Reid,\n    Ian\n},\n    title = {\n    Joint Probabilistic Data Association Revisited\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "bf9eb47e92",
        "title": "Just Noticeable Differences in Visual Attributes",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Aron Yu, Kristen Grauman",
        "author": "Aron Yu; Kristen Grauman",
        "abstract": "We explore the problem of predicting \"just noticeable differences\" in a visual attribute.  While some pairs of images have a clear ordering for an attribute (e.g., A is more sporty than B), for others the difference may be indistinguishable to human observers.  However, existing relative attribute models are unequipped to infer partial orders on novel data.   Attempting to map relative attribute ranks to equality predictions is non-trivial, particularly since the span of indistinguishable pairs in attribute space may vary in different parts of the feature space.  We develop a Bayesian local learning strategy to infer when images are indistinguishable for a given attribute.  On the UT-Zap50K shoes and LFW-10 faces datasets, we outperform a variety of alternative methods.  In addition, we show the practical impact on fine-grained visual search.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Yu_Just_Noticeable_Differences_ICCV_2015_paper.pdf",
        "aff": ";",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 58,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3652283520757569020&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Yu_Just_Noticeable_Differences_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Yu_2015_ICCV,\n    \n    author = {\n    Yu,\n    Aron and Grauman,\n    Kristen\n},\n    title = {\n    Just Noticeable Differences in Visual Attributes\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "5b08b1d2e2",
        "title": "LEWIS: Latent Embeddings for Word Images and their Semantics",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Albert Gordo, Jon Almazán, Naila Murray, Florent Perronin",
        "author": "Albert Gordo; Jon Almazan; Naila Murray; Florent Perronin",
        "abstract": "The goal of this work is to bring semantics into the tasks of text recognition and retrieval in natural images. Although text recognition and retrieval have received a lot of attention in recent years, previous works have focused on recognizing or retrieving exactly the same word used as a query, without taking the                                                                                   In this paper, we ask the following question: can we predict semantic concepts directly from a word image, without explicitly trying to transcribe the word image or its characters at any point? For this goal we propose a convolutional neural network (CNN) with a weighted ranking loss objective that ensures that the concepts relevant to the query image are ranked ahead of those that are not relevant. This can also be interpreted as learning a Euclidean space where word images and concepts are jointly embedded. This model is learned in an end-to-end manner, from image pixels to semantic concepts, using a dataset of synthetically generated word images and concepts mined from a lexical database (WordNet). Our results show that, despite the complexity of the task, word images and concepts can indeed be associated with a high degree of accuracy.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Gordo_LEWIS_Latent_Embeddings_ICCV_2015_paper.pdf",
        "aff": "Xerox Research Centre Europe; Xerox Research Centre Europe; Xerox Research Centre Europe; Facebook AI Research*",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 849011,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13209727759600914708&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "xrce.xerox.com;xrce.xerox.com;xrce.xerox.com;fb.com",
        "email": "xrce.xerox.com;xrce.xerox.com;xrce.xerox.com;fb.com",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Gordo_LEWIS_Latent_Embeddings_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Xerox Research Centre Europe;Facebook",
        "aff_unique_dep": ";Facebook AI Research",
        "aff_unique_url": "https://www.xerox.com/en-us/innovation/research-centers/europe;https://research.facebook.com",
        "aff_unique_abbr": "XRCE;FAIR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "Unknown;United States",
        "bibtex": "@InProceedings{Gordo_2015_ICCV,\n    \n    author = {\n    Gordo,\n    Albert and Almazan,\n    Jon and Murray,\n    Naila and Perronin,\n    Florent\n},\n    title = {\n    LEWIS: Latent Embeddings for Word Images and their Semantics\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "79610c9e2b",
        "title": "Large Displacement 3D Scene Flow With Occlusion Reasoning",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Andrei Zanfir, Cristian Sminchisescu",
        "author": "Andrei Zanfir; Cristian Sminchisescu",
        "abstract": "3D motion estimation is a fundamental problem with many computer vision applications. With the emergence of modern, affordable and increasingly accurate RGB-D sensors, single view approaches for estimating 3D motion, also known as scene flow, are becoming popular. In this paper we propose a novel coarse to fine correspondence-based scene flow approach to account for the effects of large displacements and to model occlusion, based on explicit geometric reasoning. Our methodology enforces piecewise motion rigidity at the level of the depth point cloud without explicitly smoothing the parameters of adjacent neighborhoods. By integrating all geometric and photometric components in a single, consistent, occlusion-aware energy model our method is able to deal with fast motions and large occlusions areas, as present in challenging datasets like MPI Sintel Flow Dataset, which have recently been augmented with depth information. By explicitly modeling large displacements and occlusion, we can now more successfully work with difficult sequences which cannot be currently processed by state of the art scene flow methods that rely on small inter-frame motion assumptions. We also show that by leveraging depth information, we can obtain superior correspondence fields compared to the best state of the art large-displacement (2D) optical flow methods.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Zanfir_Large_Displacement_3D_ICCV_2015_paper.pdf",
        "aff": "Institute of Mathematics of the Romanian Academy; Department of Mathematics, Faculty of Engineering, Lund University + Institute of Mathematics of the Romanian Academy",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 594059,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6466711576039084675&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "imar.ro;math.lth.se",
        "email": "imar.ro;math.lth.se",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Zanfir_Large_Displacement_3D_ICCV_2015_paper.html",
        "aff_unique_index": "0;1+0",
        "aff_unique_norm": "Romanian Academy;Lund University",
        "aff_unique_dep": "Institute of Mathematics;Department of Mathematics",
        "aff_unique_url": "https://www.math.ro/;https://www.lunduniversity.lu.se",
        "aff_unique_abbr": "IMAR;LU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1+0",
        "aff_country_unique": "Romania;Sweden",
        "bibtex": "@InProceedings{Zanfir_2015_ICCV,\n    \n    author = {\n    Zanfir,\n    Andrei and Sminchisescu,\n    Cristian\n},\n    title = {\n    Large Displacement 3D Scene Flow With Occlusion Reasoning\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "b8f8559371",
        "title": "Learning Analysis-by-Synthesis for 6D Pose Estimation in RGB-D Images",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Alexander Krull, Eric Brachmann, Frank Michel, Michael Ying Yang, Stefan Gumhold, Carsten Rother",
        "author": "Alexander Krull; Eric Brachmann; Frank Michel; Michael Ying Yang; Stefan Gumhold; Carsten Rother",
        "abstract": "Analysis-by-synthesis has been a successful approach for many tasks in computer vision, such as 6D pose estimation of an object in an RGB-D image which is the topic of this work. The idea is to compare the observation with the output of a forward process, such as a rendered image of the object of interest in a particular pose. Due to occlusion or complicated sensor noise, it can be difficult to perform this comparison in a meaningful way. We propose an approach that ``learns to compare'', while taking these difficulties into account. This is done by describing the posterior density of a particular object pose with a convolutional neural network (CNN) that compares observed and rendered images. The network is trained with the maximum likelihood paradigm. We observe empirically that the CNN does not specialize to the geometry or appearance of specific objects. It can be used with objects of vastly different shapes and appearances, and in different backgrounds. Compared to state-of-the-art, we demonstrate a significant improvement on two different datasets which include a total of eleven objects, cluttered background, and heavy occlusion.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Krull_Learning_Analysis-by-Synthesis_for_ICCV_2015_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 263,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18296367264736726091&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Krull_Learning_Analysis-by-Synthesis_for_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Krull_2015_ICCV,\n    \n    author = {\n    Krull,\n    Alexander and Brachmann,\n    Eric and Michel,\n    Frank and Yang,\n    Michael Ying and Gumhold,\n    Stefan and Rother,\n    Carsten\n},\n    title = {\n    Learning Analysis-by-Synthesis for 6D Pose Estimation in RGB-D Images\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "84530196e1",
        "title": "Learning Binary Codes for Maximum Inner Product Search",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Fumin Shen, Wei Liu, Shaoting Zhang, Yang Yang, Heng Tao Shen",
        "author": "Fumin Shen; Wei Liu; Shaoting Zhang; Yang Yang; Heng Tao Shen",
        "abstract": "Binary coding or hashing techniques are recognized to accomplish efficient near neighbor search, and have thus attracted broad interests in the recent vision and learning studies. However, such studies have rarely been dedicated to Maximum Inner Product Search (MIPS), which plays a critical role in various vision applications. In this paper, we investigate learning binary codes to exclusively handle the MIPS problem. Inspired by the latest advance in asymmetric hashing schemes, we propose an asymmetric binary code learning framework based on inner product fitting. Specifically, two sets of coding functions are learned such that the inner products between their generated binary codes can reveal the inner products between original data vectors. We also propose an alternative simpler objective which maximizes the correlations between the inner products of the produced binary codes and raw data vectors. In both objectives, the binary codes and coding functions are simultaneously learned without continuous relaxations, which is the key to achieving high-quality binary codes. We evaluate the proposed method, dubbed Asymmetric Inner-product Binary Coding (AIBC), relying on the two objectives on several large-scale image datasets. Both of them are superior to the state-of-the-art binary coding and hashing methods in performing MIPS tasks.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Shen_Learning_Binary_Codes_ICCV_2015_paper.pdf",
        "aff": "University of Electronic Science and Technology of China; Xidian University; University of North Carolina at Charlotte; University of Electronic Science and Technology of China; University of Electronic Science and Technology of China + The University of Queensland",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 17597054,
        "gs_citation": 167,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16462535021494489947&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "gmail.com;gmail.com; ; ; ",
        "email": "gmail.com;gmail.com; ; ; ",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Shen_Learning_Binary_Codes_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;2;0;0+3",
        "aff_unique_norm": "University of Electronic Science and Technology of China;Xidian University;University of North Carolina at Charlotte;The University of Queensland",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.uestc.edu.cn;http://www.xidian.edu.cn/;https://www.uncc.edu;https://www.uq.edu.au",
        "aff_unique_abbr": "UESTC;Xidian;UNCC;UQ",
        "aff_campus_unique_index": "1;",
        "aff_campus_unique": ";Charlotte",
        "aff_country_unique_index": "0;0;1;0;0+2",
        "aff_country_unique": "China;United States;Australia",
        "bibtex": "@InProceedings{Shen_2015_ICCV,\n    \n    author = {\n    Shen,\n    Fumin and Liu,\n    Wei and Zhang,\n    Shaoting and Yang,\n    Yang and Shen,\n    Heng Tao\n},\n    title = {\n    Learning Binary Codes for Maximum Inner Product Search\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "a7e8f11ef3",
        "title": "Learning Common Sense Through Visual Abstraction",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Ramakrishna Vedantam, Xiao Lin, Tanmay Batra, C. Lawrence Zitnick, Devi Parikh",
        "author": "Ramakrishna Vedantam; Xiao Lin; Tanmay Batra; C. Lawrence Zitnick; Devi Parikh",
        "abstract": "Common sense is essential for building intelligent machines. While some commonsense knowledge is explicitly stated in human-generated text and can be learnt by mining the web, much of it is unwritten. It is often unnecessary and even unnatural to write about commonsense facts. While unwritten, this commonsense knowledge is not unseen! The visual world around us is full of structure modeled by commonsense knowledge. Can machines learn common sense simply by observing our visual world? Unfortunately, this requires automatic and accurate detection of objects, their attributes, poses, and interactions between objects, which remain challenging problems. Our key insight is that while visual common sense is depicted in visual content, it is the semantic features that are relevant and not low-level pixel information. In other words, photorealism is not necessary to learn common sense. We explore the use of human-generated abstract scenes made from clipart for learning common sense. In particular, we reason about the plausibility of an interaction or relation between a pair of nouns by measuring the similarity of the relation and nouns with other relations and nouns we have seen in abstract scenes. We show that the commonsense knowledge we learn is complementary to what can be learnt from sources of text.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Vedantam_Learning_Common_Sense_ICCV_2015_paper.pdf",
        "aff": "Virginia Tech; Virginia Tech; Carnegie Mellon University + Virginia Tech; Microsoft Research; Virginia Tech",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 904231,
        "gs_citation": 115,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13030447060873087804&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "vt.edu;vt.edu;andrew.cmu.edu;microsoft.com;vt.edu",
        "email": "vt.edu;vt.edu;andrew.cmu.edu;microsoft.com;vt.edu",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Vedantam_Learning_Common_Sense_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;1+0;2;0",
        "aff_unique_norm": "Virginia Tech;Carnegie Mellon University;Microsoft Corporation",
        "aff_unique_dep": ";;Microsoft Research",
        "aff_unique_url": "https://www.vt.edu;https://www.cmu.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "VT;CMU;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Vedantam_2015_ICCV,\n    \n    author = {\n    Vedantam,\n    Ramakrishna and Lin,\n    Xiao and Batra,\n    Tanmay and Zitnick,\n    C. Lawrence and Parikh,\n    Devi\n},\n    title = {\n    Learning Common Sense Through Visual Abstraction\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "edd74aa9b5",
        "title": "Learning Complexity-Aware Cascades for Deep Pedestrian Detection",
        "session": "vision and people",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "Zhaowei Cai, Mohammad Saberian, Nuno Vasconcelos",
        "author": "Zhaowei Cai; Mohammad Saberian; Nuno Vasconcelos",
        "abstract": "The design of complexity-aware cascaded detectors, combining features of very different complexities, is considered. A new cascade design procedure is introduced, by formulating cascade learning as the Lagrangian optimization of a risk that accounts for both accuracy and complexity. A boosting algorithm, denoted as complexity aware cascade training (CompACT), is then derived to solve this optimization. CompACT cascades are shown to seek an optimal trade-off between accuracy and complexity by pushing features of higher complexity to the later cascade stages, where only a few difficult candidate patches remain to be classified. This enables the use of features of vastly different complexities in a single detector. In result, the feature pool can be expanded to features previously impractical for cascade design, such as the responses of a deep convolutional neural network (CNN). This is demonstrated through the design of a pedestrian detector with a pool of features whose complexities span orders of magnitude. The resulting cascade generalizes the combination of a CNN with an object proposal mechanism: rather than a pre-processing stage, CompACT cascades seamlessly integrate CNNs in their stages. This enables state of the art performance on the Caltech and KITTI datasets, at fairly fast speeds.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Cai_Learning_Complexity-Aware_Cascades_ICCV_2015_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 457,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11752335122783750672&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 24,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Cai_Learning_Complexity-Aware_Cascades_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Cai_2015_ICCV,\n    \n    author = {\n    Cai,\n    Zhaowei and Saberian,\n    Mohammad and Vasconcelos,\n    Nuno\n},\n    title = {\n    Learning Complexity-Aware Cascades for Deep Pedestrian Detection\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "a87ff79601",
        "title": "Learning Concept Embeddings With Combined Human-Machine Expertise",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Michael Wilber, Iljung S. Kwak, David Kriegman, Serge Belongie",
        "author": "Michael Wilber; Iljung S. Kwak; David Kriegman; Serge Belongie",
        "abstract": "This paper presents our work on \"SNaCK,\" a low-dimensional concept embedding algorithm that combines human expertise with automatic machine similarity kernels. Both parts are complimentary: human insight can capture relationships that are not apparent from the object's visual similarity and the machine can help relieve the human from having to exhaustively specify many constraints. We show that our SNaCK embeddings are useful in several tasks: distinguishing prime and nonprime numbers on MNIST, discovering labeling mistakes in the Caltech UCSD Birds (CUB) dataset with the help of deep-learned features, creating training datasets for bird classifiers, capturing  subjective human taste on a new dataset of 10,000 foods, and qualitatively exploring an unstructured set of pictographic characters. Comparisons with the state-of-the-art in these tasks show that SNaCK produces better concept embeddings that require less human supervision than the leading methods.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Wilber_Learning_Concept_Embeddings_ICCV_2015_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 7336014,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12082383679960206623&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Wilber_Learning_Concept_Embeddings_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Wilber_2015_ICCV,\n    \n    author = {\n    Wilber,\n    Michael and Kwak,\n    Iljung S. and Kriegman,\n    David and Belongie,\n    Serge\n},\n    title = {\n    Learning Concept Embeddings With Combined Human-Machine Expertise\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "46f5159a81",
        "title": "Learning Data-Driven Reflectance Priors for Intrinsic Image Decomposition",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Tinghui Zhou, Philipp Krähenbühl, Alexei A. Efros",
        "author": "Tinghui Zhou; Philipp Krahenbuhl; Alexei A. Efros",
        "abstract": "We propose a data-driven approach for intrinsic image decomposition, which is the process of inferring the confounding factors of reflectance and shading in an image. We pose this as a two-stage learning problem. First, we train a model to predict relative reflectance ordering be- tween image patches ('brighter', 'darker', 'same') from large-scale human annotations, producing a data-driven reflectance prior. Second, we show how to naturally integrate this learned prior into existing energy minimization frame- works for intrinsic image decomposition. We compare our method to the state-of-the-art approach of Bell et al. [7] on both decomposition and image relighting tasks, demonstrating the benefits of the simple relative reflectance prior, especially for scenes under challenging lighting conditions.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Zhou_Learning_Data-Driven_Reflectance_ICCV_2015_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1555953,
        "gs_citation": 189,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6685553459436807174&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Zhou_Learning_Data-Driven_Reflectance_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Zhou_2015_ICCV,\n    \n    author = {\n    Zhou,\n    Tinghui and Krahenbuhl,\n    Philipp and Efros,\n    Alexei A.\n},\n    title = {\n    Learning Data-Driven Reflectance Priors for Intrinsic Image Decomposition\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "af0647fbd6",
        "title": "Learning Deconvolution Network for Semantic Segmentation",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Hyeonwoo Noh, Seunghoon Hong, Bohyung Han",
        "author": "Hyeonwoo Noh; Seunghoon Hong; Bohyung Han",
        "abstract": "We propose a novel semantic segmentation algorithm by learning a deep deconvolution network. We learn the network on top of the convolutional layers adopted from VGG 16-layer net. The deconvolution network is composed of deconvolution and unpooling layers, which identify pixelwise class labels and predict segmentation masks. We apply the trained network to each proposal in an input image, and construct the final semantic segmentation map by combining the results from all proposals in a simple manner. The proposed algorithm mitigates the limitations of the existing methods based on fully convolutional networks by integrating deep deconvolution network and proposal-wise prediction;our segmentation method typically identifies detailed structures and handles objects in multiple scales naturally. Our network demonstrates outstanding performance in PASCAL VOC 2012 dataset, and we achieve the best accuracy (72.5%) among the methods trained without using Microsoft COCO dataset through ensemble with the fully convolutional network.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Noh_Learning_Deconvolution_Network_ICCV_2015_paper.pdf",
        "aff": "Department of Computer Science and Engineering, POSTECH, Korea; Department of Computer Science and Engineering, POSTECH, Korea; Department of Computer Science and Engineering, POSTECH, Korea",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1425858,
        "gs_citation": 5924,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4896002303003783815&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 20,
        "aff_domain": "postech.ac.kr;postech.ac.kr;postech.ac.kr",
        "email": "postech.ac.kr;postech.ac.kr;postech.ac.kr",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Noh_Learning_Deconvolution_Network_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "POSTECH",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.postech.ac.kr",
        "aff_unique_abbr": "POSTECH",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "South Korea",
        "bibtex": "@InProceedings{Noh_2015_ICCV,\n    \n    author = {\n    Noh,\n    Hyeonwoo and Hong,\n    Seunghoon and Han,\n    Bohyung\n},\n    title = {\n    Learning Deconvolution Network for Semantic Segmentation\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "5b1887e269",
        "title": "Learning Deep Object Detectors From 3D Models",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Xingchao Peng, Baochen Sun, Karim Ali, Kate Saenko",
        "author": "Xingchao Peng; Baochen Sun; Karim Ali; Kate Saenko",
        "abstract": "Crowdsourced 3D CAD models are easily accessible online, and can potentially generate an infinite number of training images for almost any object category. We show that augmenting the training data of contemporary Deep Convolutional Neural Net (DCNN) models with such synthetic data can be effective, especially when real training data is limited or not well matched to the target domain. Most freely available CAD models capture 3D shape but are often missing other low level cues, such as realistic object texture, pose, or background. In a detailed analysis, we use synthetic CAD images to probe the ability of DCNN to learn without these cues, with surprising findings. In particular, we show that when the DCNN is fine-tuned on the target detection task, it exhibits a large degree of invariance to missing low-level cues, but, when pretrained on generic ImageNet classification, it learns better when the low-level cues are simulated. We show that our synthetic DCNN training approach significantly outperforms previous methods on the benchmark PASCAL VOC2007 dataset when learning in the few-shot scenario and improves performance in a domain shift scenario on the Office benchmark.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Peng_Learning_Deep_Object_ICCV_2015_paper.pdf",
        "aff": "University of Massachusetts Lowell; University of Massachusetts Lowell; University of Massachusetts Lowell; University of Massachusetts Lowell",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1312572,
        "gs_citation": 467,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10987351959071838326&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "cs.uml.edu;cs.uml.edu;cs.uml.edu;cs.uml.edu",
        "email": "cs.uml.edu;cs.uml.edu;cs.uml.edu;cs.uml.edu",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Peng_Learning_Deep_Object_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Massachusetts Lowell",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uml.edu",
        "aff_unique_abbr": "UMass Lowell",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Lowell",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Peng_2015_ICCV,\n    \n    author = {\n    Peng,\n    Xingchao and Sun,\n    Baochen and Ali,\n    Karim and Saenko,\n    Kate\n},\n    title = {\n    Learning Deep Object Detectors From 3D Models\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "60e0fa5694",
        "title": "Learning Deep Representation With Large-Scale Attributes",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Wanli Ouyang, Hongyang Li, Xingyu Zeng, Xiaogang Wang",
        "author": "Wanli Ouyang; Hongyang Li; Xingyu Zeng; Xiaogang Wang",
        "abstract": "Learning strong feature representations from large scale supervision has achieved remarkable success in computer vision as the emergence of deep learning techniques. It is driven by big visual data with rich annotations. This paper contributes a large-scale object attribute database (The dataset is available on  www.ee.cuhk.edu.hk/ xgwang/ImageNetAttribute.html) that contains  rich attribute annotations (over 300 attributes) for ~180k samples and 494 object classes. Based on the ImageNet object detection dataset, it annotates the rotation, viewpoint, object part location, part occlusion, part existence, common attributes, and class-specific attributes. Then we use this dataset to train deep representations and extensively evaluate how these attributes are useful on the general object detection task. In order to make better use of the attribute annotations, a deep learning scheme is proposed by modeling the relationship of attributes and hierarchically clustering them into semantically meaningful mixture types. Experimental results show that the attributes are helpful in learning better features and improving the object detection accuracy by 2.6% in mAP on the ILSVRC 2014 object detection dataset and 2.4% in mAP on PASCAL VOC 2007 object detection dataset. Such improvement is well generalized across datasets.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Ouyang_Learning_Deep_Representation_ICCV_2015_paper.pdf",
        "aff": "Department of Electronic Engineering, The Chinese University of Hong Kong; Department of Electronic Engineering, The Chinese University of Hong Kong; Department of Electronic Engineering, The Chinese University of Hong Kong; Department of Electronic Engineering, The Chinese University of Hong Kong",
        "project": "www.ee.cuhk.edu.hk/~xgwang/ImageNetAttribute.html",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4435423,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7105926653733622710&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "ee.cuhk.edu.hk;ee.cuhk.edu.hk;ee.cuhk.edu.hk;ee.cuhk.edu.hk",
        "email": "ee.cuhk.edu.hk;ee.cuhk.edu.hk;ee.cuhk.edu.hk;ee.cuhk.edu.hk",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Ouyang_Learning_Deep_Representation_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "The Chinese University of Hong Kong",
        "aff_unique_dep": "Department of Electronic Engineering",
        "aff_unique_url": "https://www.cuhk.edu.hk",
        "aff_unique_abbr": "CUHK",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Ouyang_2015_ICCV,\n    \n    author = {\n    Ouyang,\n    Wanli and Li,\n    Hongyang and Zeng,\n    Xingyu and Wang,\n    Xiaogang\n},\n    title = {\n    Learning Deep Representation With Large-Scale Attributes\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "cbe5808c76",
        "title": "Learning Discriminative Reconstructions for Unsupervised Outlier Removal",
        "session": "statistical methods and learning",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "Yan Xia, Xudong Cao, Fang Wen, Gang Hua, Jian Sun",
        "author": "Yan Xia; Xudong Cao; Fang Wen; Gang Hua; Jian Sun",
        "abstract": "We study the problem of automatically removing outliers from noisy data, with application for removing outlier images from an image collection. We address this problem by utilizing the reconstruction errors of an autoencoder. We observe that when data are reconstructed from low-dimensional representations, the inliers and the outliers can be well separated according to their reconstruction errors. Based on this basic observation, we gradually inject discriminative information in the learning process of an autoencoder to make the inliers and the outliers more separable. Experiments on a variety of image datasets validate our approach.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Xia_Learning_Discriminative_Reconstructions_ICCV_2015_paper.pdf",
        "aff": "University of Science and Technology of China; Microsoft Research; Microsoft Research; Microsoft Research; Microsoft Research",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1288963,
        "gs_citation": 392,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2628452871880589588&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Xia_Learning_Discriminative_Reconstructions_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;1;1;1",
        "aff_unique_norm": "University of Science and Technology of China;Microsoft Corporation",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "http://www.ustc.edu.cn;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "USTC;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;1;1",
        "aff_country_unique": "China;United States",
        "bibtex": "@InProceedings{Xia_2015_ICCV,\n    \n    author = {\n    Xia,\n    Yan and Cao,\n    Xudong and Wen,\n    Fang and Hua,\n    Gang and Sun,\n    Jian\n},\n    title = {\n    Learning Discriminative Reconstructions for Unsupervised Outlier Removal\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "1572398560",
        "title": "Learning Ensembles of Potential Functions for Structured Prediction With Latent Variables",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Hossein Hajimirsadeghi, Greg Mori",
        "author": "Hossein Hajimirsadeghi; Greg Mori",
        "abstract": "Many visual recognition tasks involve modeling variables which are structurally related. Hidden conditional random fields (HCRFs) are a powerful class of models for encoding structure in weakly supervised training examples. This paper presents HCRF-Boost, a novel and general framework for learning HCRFs in functional space. An algorithm is proposed to learn the potential functions of an HCRF as a combination of abstract nonlinear feature functions, expressed by regression models. Consequently, the resulting latent structured model is not restricted to traditional log-linear potential functions or any explicit parameterization. Further, functional optimization helps to avoid direct interactions with the possibly large parameter space of nonlinear models and improves efficiency. As a result, a complex and flexible ensemble method is achieved for structured prediction which can be successfully used in a variety of applications. We validate the effectiveness of this method on tasks such as group activity recognition, human action recognition, and multi-instance learning of video events.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Hajimirsadeghi_Learning_Ensembles_of_ICCV_2015_paper.pdf",
        "aff": "School of Computing Science, Simon Fraser University, Canada; School of Computing Science, Simon Fraser University, Canada",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3995511,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1522046914170352373&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "sfu.ca;cs.sfu.ca",
        "email": "sfu.ca;cs.sfu.ca",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Hajimirsadeghi_Learning_Ensembles_of_ICCV_2015_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Simon Fraser University",
        "aff_unique_dep": "School of Computing Science",
        "aff_unique_url": "https://www.sfu.ca",
        "aff_unique_abbr": "SFU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada",
        "bibtex": "@InProceedings{Hajimirsadeghi_2015_ICCV,\n    \n    author = {\n    Hajimirsadeghi,\n    Hossein and Mori,\n    Greg\n},\n    title = {\n    Learning Ensembles of Potential Functions for Structured Prediction With Latent Variables\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "a6499d4b75",
        "title": "Learning Image Representations Tied to Ego-Motion",
        "session": "learning representations and attributes",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "Dinesh Jayaraman, Kristen Grauman",
        "author": "Dinesh Jayaraman; Kristen Grauman",
        "abstract": "Understanding how images of objects and scenes behave in response to specific ego-motions is a crucial aspect of proper visual development, yet existing visual learning methods are conspicuously disconnected from the physical source of their images.  We propose to exploit proprioceptive motor signals to provide unsupervised regularization in convolutional neural networks to learn visual representations from egocentric video. Specifically, we enforce that our learned features exhibit equivariance, i.e, they respond predictably to transformations associated with distinct ego-motions.  With three datasets, we show that our unsupervised feature learning approach significantly outperforms previous approaches on visual recognition and next-best-view prediction tasks.  In the most challenging test, we show that features learned from video captured on an autonomous driving platform improve large-scale scene recognition in static images from a disjoint domain.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Jayaraman_Learning_Image_Representations_ICCV_2015_paper.pdf",
        "aff": "The University of Texas at Austin; The University of Texas at Austin",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1232015,
        "gs_citation": 328,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2924255388958742117&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "cs.utexas.edu;cs.utexas.edu",
        "email": "cs.utexas.edu;cs.utexas.edu",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Jayaraman_Learning_Image_Representations_ICCV_2015_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Jayaraman_2015_ICCV,\n    \n    author = {\n    Jayaraman,\n    Dinesh and Grauman,\n    Kristen\n},\n    title = {\n    Learning Image Representations Tied to Ego-Motion\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "88a5bf16f8",
        "title": "Learning Image and User Features for Recommendation in Social Networks",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Xue Geng, Hanwang Zhang, Jingwen Bian, Tat-Seng Chua",
        "author": "Xue Geng; Hanwang Zhang; Jingwen Bian; Tat-Seng Chua",
        "abstract": "Good representations of data do help in many machine learning tasks such as recommendation. It is often a great challenge for traditional recommender systems to learn representative features of both users and images in large social networks, in particular, social curation networks, which are characterized as the extremely sparse links between users and images, and the extremely diverse visual contents of images. To address the challenges, we propose a novel deep model which learns the unified feature representations for both users and images. This is done by transforming the heterogeneous user-image networks into homogeneous low-dimensional representations, which facilitate a recommender to trivially recommend images to users by feature similarity. We also develop a fast online algorithm that can be easily scaled up to large networks in an asynchronously parallel way. We conduct extensive experiments on a representative subset of Pinterest, containing 1,456,540 images and 1,000,000 users. Results of image recommendation experiments demonstrate that our feature learning approach significantly outperforms other state-of-the-art recommendation methods.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Geng_Learning_Image_and_ICCV_2015_paper.pdf",
        "aff": "School of Computing, National University of Singapore; School of Computing, National University of Singapore; School of Computing, National University of Singapore; School of Computing, National University of Singapore",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 941574,
        "gs_citation": 277,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11117435295161346828&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "gmail.com;gmail.com;comp.nus.edu.sg;comp.nus.edu.sg",
        "email": "gmail.com;gmail.com;comp.nus.edu.sg;comp.nus.edu.sg",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Geng_Learning_Image_and_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "National University of Singapore",
        "aff_unique_dep": "School of Computing",
        "aff_unique_url": "https://www.nus.edu.sg",
        "aff_unique_abbr": "NUS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Singapore",
        "bibtex": "@InProceedings{Geng_2015_ICCV,\n    \n    author = {\n    Geng,\n    Xue and Zhang,\n    Hanwang and Bian,\n    Jingwen and Chua,\n    Tat-Seng\n},\n    title = {\n    Learning Image and User Features for Recommendation in Social Networks\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "16820122c6",
        "title": "Learning Informative Edge Maps for Indoor Scene Layout Prediction",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Arun Mallya, Svetlana Lazebnik",
        "author": "Arun Mallya; Svetlana Lazebnik",
        "abstract": "In this paper, we introduce new edge-based features for the task of recovering the 3D layout of an indoor scene from a single image. Indoor scenes have certain edges that are very informative about the spatial layout of the room, namely, the edges formed by the pairwise intersections of room faces (two walls, wall and ceiling, wall and floor). In contrast with previous approaches that rely on area-based features like geometric context and orientation maps, our method attempts to directly detect these informative edges. We learn to predict 'informative edge' probability maps using two recent methods that exploit local and global context, respectively: structured edge detection forests, and a fully convolutional network for pixelwise labeling. We show that the fully convolutional network is quite successful at predicting the informative edges even when they lack contrast or are occluded, and that the accuracy can be further improved by training the network to jointly predict the edges and the geometric context. Using features derived from the 'informative edge' maps, we learn a maximum margin structured classifier that achieves state-of-the-art performance on layout prediction.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Mallya_Learning_Informative_Edge_ICCV_2015_paper.pdf",
        "aff": "Dept. of Computer Science, University of Illinois at Urbana-Champaign; Dept. of Computer Science, University of Illinois at Urbana-Champaign",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1642046,
        "gs_citation": 186,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13241640162098013645&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "illinois.edu;illinois.edu",
        "email": "illinois.edu;illinois.edu",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Mallya_Learning_Informative_Edge_ICCV_2015_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Illinois at Urbana-Champaign",
        "aff_unique_dep": "Dept. of Computer Science",
        "aff_unique_url": "https://illinois.edu",
        "aff_unique_abbr": "UIUC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Urbana-Champaign",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Mallya_2015_ICCV,\n    \n    author = {\n    Mallya,\n    Arun and Lazebnik,\n    Svetlana\n},\n    title = {\n    Learning Informative Edge Maps for Indoor Scene Layout Prediction\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "36b5a4c7dd",
        "title": "Learning Large-Scale Automatic Image Colorization",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Aditya Deshpande, Jason Rock, David Forsyth",
        "author": "Aditya Deshpande; Jason Rock; David Forsyth",
        "abstract": "We describe an automated method for image colorization that learns to colorize from examples.  Our method exploits a LEARCH framework to train a quadratic objective function in the chromaticity maps, comparable to a Gaussian random field.  The coefficients of the objective function are conditioned on image features, using a random forest.  The objective function admits correlations on long spatial scales, and can control spatial error in the colorization of the image.  Images are then colorized by minimizing this objective function.  We demonstrate that our method strongly outperforms a natural baseline on large-scale experiments with images of real scenes using a demanding loss function.  We demonstrate that learning a model that is conditioned on scene produces improved results. We show how to incorporate a desired color histogram into the objective function, and that doing so can lead to further improvements in results.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Deshpande_Learning_Large-Scale_Automatic_ICCV_2015_paper.pdf",
        "aff": "University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign",
        "project": "http://vision.cs.illinois.edu/projects/lscolor",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 930337,
        "gs_citation": 290,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6315004092075884342&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "illinois.edu;illinois.edu;illinois.edu",
        "email": "illinois.edu;illinois.edu;illinois.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Deshpande_Learning_Large-Scale_Automatic_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Illinois at Urbana-Champaign",
        "aff_unique_dep": "",
        "aff_unique_url": "https://illinois.edu",
        "aff_unique_abbr": "UIUC",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Urbana-Champaign",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Deshpande_2015_ICCV,\n    \n    author = {\n    Deshpande,\n    Aditya and Rock,\n    Jason and Forsyth,\n    David\n},\n    title = {\n    Learning Large-Scale Automatic Image Colorization\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "88281fcd60",
        "title": "Learning Like a Child: Fast Novel Visual Concept Learning From Sentence Descriptions of Images",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Junhua Mao, Xu Wei, Yi Yang, Jiang Wang, Zhiheng Huang, Alan L. Yuille",
        "author": "Junhua Mao; Xu Wei; Yi Yang; Jiang Wang; Zhiheng Huang; Alan L. Yuille",
        "abstract": "In this paper, we address the task of learning novel visual concepts, and their interactions with other concepts, from a few images with sentence descriptions. Using linguistic context and visual features, our method is able to efficiently hypothesize the semantic meaning of new words and add them to its word dictionary so that they can be used to describe images which contain these novel concepts. Our method has an image captioning module based on the m-RNN model with several improvements. In particular, we propose a transposed weight sharing scheme, which not only improves performance on image captioning, but also makes the model more suitable for the novel concept learning task. We propose methods to prevent overfitting the new concepts. In addition, three novel concept datasets are constructed for this new task, and are publicly available on the project page. In the experiments, we show that our method effectively learns novel visual concepts from a few examples without disturbing the previously learned concepts. The project page is: http://www.stat.ucla.edu/ junhua.mao/projects/child_learning.html",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Mao_Learning_Like_a_ICCV_2015_paper.pdf",
        "aff": "University of California, Los Angeles; Baidu Research; Baidu Research; Baidu Research; Baidu Research; University of California, Los Angeles",
        "project": "www.stat.ucla.edu/~junhua.mao/projects/child_learning.html",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1203728,
        "gs_citation": 195,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13076534685762078277&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff_domain": "ucla.edu;baidu.com;baidu.com;baidu.com;baidu.com;stat.ucla.edu",
        "email": "ucla.edu;baidu.com;baidu.com;baidu.com;baidu.com;stat.ucla.edu",
        "author_num": 6,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Mao_Learning_Like_a_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;1;1;1;0",
        "aff_unique_norm": "University of California, Los Angeles;Baidu",
        "aff_unique_dep": ";Baidu Research",
        "aff_unique_url": "https://www.ucla.edu;https://research.baidu.com",
        "aff_unique_abbr": "UCLA;Baidu",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0;1;1;1;1;0",
        "aff_country_unique": "United States;China",
        "bibtex": "@InProceedings{Mao_2015_ICCV,\n    \n    author = {\n    Mao,\n    Junhua and Wei,\n    Xu and Yang,\n    Yi and Wang,\n    Jiang and Huang,\n    Zhiheng and Yuille,\n    Alan L.\n},\n    title = {\n    Learning Like a Child: Fast Novel Visual Concept Learning From Sentence Descriptions of Images\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "8571b512dd",
        "title": "Learning Nonlinear Spectral Filters for Color Image Reconstruction",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Michael Moeller, Julia Diebold, Guy Gilboa, Daniel Cremers",
        "author": "Michael Moeller; Julia Diebold; Guy Gilboa; Daniel Cremers",
        "abstract": "This paper presents the idea of learning optimal filters for color image reconstruction based on a novel concept of nonlinear spectral image decompositions recently proposed by Guy Gilboa. We use a multiscale image decomposition approach based on total variation regularization and Bregman iterations to represent the input data as the sum of image layers containing features at different scales. Filtered images can be obtained by weighted linear combinations of the different frequency layers. We introduce the idea of learning optimal filters for the task of image denoising, and propose the idea of mixing high frequency components of different color channels. Our numerical experiments demonstrate that learning the optimal weights can significantly improve the results in comparison to the standard variational approach, and achieves state-of-the-art image denoising results.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Moeller_Learning_Nonlinear_Spectral_ICCV_2015_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3545581,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14407658992250453767&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Moeller_Learning_Nonlinear_Spectral_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Moeller_2015_ICCV,\n    \n    author = {\n    Moeller,\n    Michael and Diebold,\n    Julia and Gilboa,\n    Guy and Cremers,\n    Daniel\n},\n    title = {\n    Learning Nonlinear Spectral Filters for Color Image Reconstruction\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "13b965c72f",
        "title": "Learning Ordinal Relationships for Mid-Level Vision",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Daniel Zoran, Phillip Isola, Dilip Krishnan, William T. Freeman",
        "author": "Daniel Zoran; Phillip Isola; Dilip Krishnan; William T. Freeman",
        "abstract": "We propose a framework that infers mid-level visual properties of an image by learning about ordinal relation- ships. Instead of estimating metric quantities directly, the system proposes pairwise relationship estimates for points in the input image. These sparse probabilistic ordinal mea- surements are globalized to create a dense output map of continuous metric measurements. Estimating order rela- tionships between pairs of points has several advantages over metric estimation: it solves a simpler problem than metric regression; humans are better at relative judgements, so data collection is easier; ordinal relationships are invari- ant to monotonic transformations of the data, thereby in- creasing the robustness of the system and providing qualitatively different information. We demonstrate that this frame- work works well on two important mid-level vision tasks: intrinsic image decomposition and depth from an RGB im- age. We train two systems with the same architecture on data from these two modalities. We provide an analysis of the resulting models, showing that they learn a number of simple rules to make ordinal decisions. We apply our algo-rithm to depth estimation, with good results, and intrinsic image decomposition, with state-of-the-art results.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Zoran_Learning_Ordinal_Relationships_ICCV_2015_paper.pdf",
        "aff": "CSAIL, MIT; CSAIL, MIT; Google; Google + CSAIL, MIT",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2865392,
        "gs_citation": 220,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1860586168152347216&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "mit.edu;mit.edu;google.com;google.com",
        "email": "mit.edu;mit.edu;google.com;google.com",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Zoran_Learning_Ordinal_Relationships_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;1;1+0",
        "aff_unique_norm": "Massachusetts Institute of Technology;Google",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory;",
        "aff_unique_url": "https://www.csail.mit.edu;https://www.google.com",
        "aff_unique_abbr": "MIT;Google",
        "aff_campus_unique_index": "0;0;1;1+0",
        "aff_campus_unique": "Cambridge;Mountain View",
        "aff_country_unique_index": "0;0;0;0+0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Zoran_2015_ICCV,\n    \n    author = {\n    Zoran,\n    Daniel and Isola,\n    Phillip and Krishnan,\n    Dilip and Freeman,\n    William T.\n},\n    title = {\n    Learning Ordinal Relationships for Mid-Level Vision\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "d1eacdbe21",
        "title": "Learning Parametric Distributions for Image Super-Resolution: Where Patch Matching Meets Sparse Coding",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Yongbo Li, Weisheng Dong, Guangming Shi, Xuemei Xie",
        "author": "Yongbo Li; Weisheng Dong; Guangming Shi; Xuemei Xie",
        "abstract": "Existing approaches toward Image super-resolution (SR) is often either data-driven (e.g., based on internet-scale matching and web image retrieval) or model-based (e.g., formulated as an Maximizing a Posterior estimation problem). The former is conceptually simple yet heuristic; while the latter is constrained by the fundamental limit of frequency aliasing. In this paper, we propose to develop a hybrid approach toward SR by combining those two lines of ideas. More specifically, the parameters underlying sparse distributions of desirable HR image patches are learned from a pair of LR image and retrieved HR images. Our hybrid approach can be interpreted as the first attempt of reconciling the difference between parametric and nonparametric models for low-level vision tasks. Experimental results show that the proposed hybrid SR method performs much better than existing state-of-the-art methods in terms of both subjective and objective image qualities.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Li_Learning_Parametric_Distributions_ICCV_2015_paper.pdf",
        "aff": "School of Electronic Engineering, Xidian University, Xi’an, China, 710071; School of Electronic Engineering, Xidian University, Xi’an, China, 710071; School of Electronic Engineering, Xidian University, Xi’an, China, 710071; School of Electronic Engineering, Xidian University, Xi’an, China, 710071",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 973344,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17242422580485819348&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "stu.xidian.edu.cn;mail.xidian.edu.cn;xidian.edu.cn;mail.xidian.edu.cn",
        "email": "stu.xidian.edu.cn;mail.xidian.edu.cn;xidian.edu.cn;mail.xidian.edu.cn",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Li_Learning_Parametric_Distributions_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Xidian University",
        "aff_unique_dep": "School of Electronic Engineering",
        "aff_unique_url": "http://www.xidian.edu.cn",
        "aff_unique_abbr": "Xidian",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Xi'an",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Li_2015_ICCV,\n    \n    author = {\n    Li,\n    Yongbo and Dong,\n    Weisheng and Shi,\n    Guangming and Xie,\n    Xuemei\n},\n    title = {\n    Learning Parametric Distributions for Image Super-Resolution: Where Patch Matching Meets Sparse Coding\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "0092e71448",
        "title": "Learning Query and Image Similarities With Ranking Canonical Correlation Analysis",
        "session": "vision and language",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "Ting Yao, Tao Mei, Chong-Wah Ngo",
        "author": "Ting Yao; Tao Mei; Chong-Wah Ngo",
        "abstract": "One of the fundamental problems in image search is to learn the ranking functions, i.e., similarity between the query and image. The research on this topic has evolved through two paradigms: feature-based vector model and image ranker learning. The former relies on the image surrounding texts, while the latter learns a ranker based on human labeled query-image pairs. Each of the paradigms has its own limitation. The vector model is sensitive to the quality of text descriptions, and the learning paradigm is difficult to be scaled up as human labeling is always too expensive to obtain. We demonstrate in this paper that the above two limitations can be well mitigated by jointly exploring subspace learning and the use of click-through data. Specifically, we propose a novel Ranking Canonical Correlation Analysis (RCCA) for learning query and image similarities. RCCA initially finds a common subspace between query and image views by maximizing their correlations, and further simultaneously learns a bilinear query-image similarity function and adjusts the subspace to preserve the preference relations implicit in the click-through data. Once the subspace is finalized, query-image similarity can be computed by the bilinear similarity function on their mappings in this subspace. On a large-scale click-based image dataset with 11.7 million queries and one million images, RCCA is shown to be powerful for image search with superior performance over several state-of-the-art methods on both keyword-based and query-by-example tasks.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Yao_Learning_Query_and_ICCV_2015_paper.pdf",
        "aff": "Microsoft Research, Beijing, China; Microsoft Research, Beijing, China; City University of Hong Kong, Kowloon, Hong Kong",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 7042649,
        "gs_citation": 98,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16596657944380590946&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "microsoft.com;microsoft.com;cityu.edu.hk",
        "email": "microsoft.com;microsoft.com;cityu.edu.hk",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Yao_Learning_Query_and_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Microsoft Research;City University of Hong Kong",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.microsoft.com/en-us/research/group/microsoft-research-asia;https://www.cityu.edu.hk",
        "aff_unique_abbr": "MSR;CityU",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Beijing;Hong Kong SAR",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Yao_2015_ICCV,\n    \n    author = {\n    Yao,\n    Ting and Mei,\n    Tao and Ngo,\n    Chong-Wah\n},\n    title = {\n    Learning Query and Image Similarities With Ranking Canonical Correlation Analysis\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "9815e76e88",
        "title": "Learning Semi-Supervised Representation Towards a Unified Optimization Framework for Semi-Supervised Learning",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Chun-Guang Li, Zhouchen Lin, Honggang Zhang, Jun Guo",
        "author": "Chun-Guang Li; Zhouchen Lin; Honggang Zhang; Jun Guo",
        "abstract": "State of the art approaches for Semi-Supervised Learning (SSL) usually follow a two-stage framework -- constructing an affinity matrix from the data and then propagating the partial labels on this affinity matrix to infer those unknown labels. While such a two-stage framework has been successful in many applications, solving two subproblems separately only once is still suboptimal because it does not fully exploit the correlation between the affinity and the labels. In this paper, we formulate the two stages of SSL into a unified optimization framework, which learns both the affinity matrix and the unknown labels simultaneously. In the unified framework, both the given labels and the estimated labels are used to learn the affinity matrix and to infer the unknown labels. We solve the unified optimization problem via an alternating direction method of multipliers combined with label propagation. Extensive experiments on a synthetic data set and several benchmark data sets demonstrate the effectiveness of our approach.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Li_Learning_Semi-Supervised_Representation_ICCV_2015_paper.pdf",
        "aff": "School of Info. & Commu. Engineering, Beijing University of Posts and Telecommunications; Key Laboratory of Machine Perception (MOE), School of EECS, Peking University + Cooperative Medianet Innovation Center, Shanghai Jiaotong University; School of Info. & Commu. Engineering, Beijing University of Posts and Telecommunications; School of Info. & Commu. Engineering, Beijing University of Posts and Telecommunications",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1120091,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15425011816453291390&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "bupt.edu.cn;pku.edu.cn;bupt.edu.cn;bupt.edu.cn",
        "email": "bupt.edu.cn;pku.edu.cn;bupt.edu.cn;bupt.edu.cn",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Li_Learning_Semi-Supervised_Representation_ICCV_2015_paper.html",
        "aff_unique_index": "0;1+2;0;0",
        "aff_unique_norm": "Beijing University of Posts and Telecommunications;Peking University;Shanghai Jiaotong University",
        "aff_unique_dep": "School of Info. & Commu. Engineering;School of EECS;Cooperative Medianet Innovation Center",
        "aff_unique_url": "http://www.bupt.edu.cn/;http://www.pku.edu.cn;https://www.sjtu.edu.cn",
        "aff_unique_abbr": "BUPT;PKU;SJTU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Li_2015_ICCV,\n    \n    author = {\n    Li,\n    Chun-Guang and Lin,\n    Zhouchen and Zhang,\n    Honggang and Guo,\n    Jun\n},\n    title = {\n    Learning Semi-Supervised Representation Towards a Unified Optimization Framework for Semi-Supervised Learning\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "f3f9472d55",
        "title": "Learning Shape, Motion and Elastic Models in Force Space",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Antonio Agudo, Francesc Moreno-Noguer",
        "author": "Antonio Agudo; Francesc Moreno-Noguer",
        "abstract": "In this paper, we address the problem of simultaneously recovering the 3D shape and pose of a deformable and potentially elastic object from 2D motion. This is a highly ambiguous problem typically tackled by using  low-rank shape and trajectory constraints.  We show that formulating the problem in terms of a low-rank force space that induces the deformation, allows for a better physical interpretation of the resulting priors and a more accurate representation of the actual object's behavior. However, this comes at the price of, besides force and pose, having to estimate the elastic model of the object. For this, we use an Expectation Maximization strategy, where each of these parameters are successively learned within partial M-steps, while robustly dealing with missing observations. We thoroughly validate the approach on both mocap and real sequences, showing more accurate 3D reconstructions than state-of-the-art, and additionally providing an estimate of the full elastic model with no a priori information.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Agudo_Learning_Shape_Motion_ICCV_2015_paper.pdf",
        "aff": "Instituto de Investigaci ´on en Ingenier ´ıa de Arag ´on (I3A), Universidad de Zaragoza, Spain; Institut de Rob `otica i Inform `atica Industrial (CSIC-UPC), Barcelona, Spain",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 14618681,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12214864357873756347&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Agudo_Learning_Shape_Motion_ICCV_2015_paper.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Universidad de Zaragoza;Institut de Robòtica i Informàtica Industrial",
        "aff_unique_dep": "Instituto de Investigaci  ´on en Ingenier  ´ıa de Arag  ´on (I3A);Robotica i Informatica Industrial",
        "aff_unique_url": "https://www.unizar.es;http://www.iri.upc.edu/",
        "aff_unique_abbr": "UniZar;IRI",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Barcelona",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Spain",
        "bibtex": "@InProceedings{Agudo_2015_ICCV,\n    \n    author = {\n    Agudo,\n    Antonio and Moreno-Noguer,\n    Francesc\n},\n    title = {\n    Learning Shape,\n    Motion and Elastic Models in Force Space\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "2e6c692dcc",
        "title": "Learning Social Relation Traits From Face Images",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Zhanpeng Zhang, Ping Luo, Chen-Change Loy, Xiaoou Tang",
        "author": "Zhanpeng Zhang; Ping Luo; Chen-Change Loy; Xiaoou Tang",
        "abstract": "Social relation defines the association, e.g., warm, friendliness, and dominance, between two or more people. Motivated by psychological studies, we investigate if such fine grained and high-level relation traits can be characterised and quantified from face images in the wild. To address this challenging problem we propose a deep model that learns a rich face representation to capture gender, expression, head pose, and age-related attributes, and then performs pairwise-face reasoning for relation prediction. To learn from heterogeneous attribute sources, we formulate a new network architecture with a bridging layer to leverage the inherent correspondences among these datasets. It can also cope with missing target attribute labels. Extensive experiments show that our approach is effective for fine-grained social relation learning in images and videos.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Zhang_Learning_Social_Relation_ICCV_2015_paper.pdf",
        "aff": "Department of Information Engineering, The Chinese University of Hong Kong; Department of Information Engineering, The Chinese University of Hong Kong; Department of Information Engineering, The Chinese University of Hong Kong; Department of Information Engineering, The Chinese University of Hong Kong",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1959450,
        "gs_citation": 217,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11516924989070733800&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff_domain": "ie.cuhk.edu.hk;ie.cuhk.edu.hk;ie.cuhk.edu.hk;ie.cuhk.edu.hk",
        "email": "ie.cuhk.edu.hk;ie.cuhk.edu.hk;ie.cuhk.edu.hk;ie.cuhk.edu.hk",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Zhang_Learning_Social_Relation_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "The Chinese University of Hong Kong",
        "aff_unique_dep": "Department of Information Engineering",
        "aff_unique_url": "https://www.cuhk.edu.hk",
        "aff_unique_abbr": "CUHK",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Zhang_2015_ICCV,\n    \n    author = {\n    Zhang,\n    Zhanpeng and Luo,\n    Ping and Loy,\n    Chen-Change and Tang,\n    Xiaoou\n},\n    title = {\n    Learning Social Relation Traits From Face Images\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "b9a0ae6d41",
        "title": "Learning Spatially Regularized Correlation Filters for Visual Tracking",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Martin Danelljan, Gustav Häger, Fahad Shahbaz Khan, Michael Felsberg",
        "author": "Martin Danelljan; Gustav Hager; Fahad Shahbaz Khan; Michael Felsberg",
        "abstract": "Robust and accurate visual tracking is one of the most challenging computer vision problems. Due to the inherent lack of training data, a robust approach for constructing a target appearance model is crucial. Recently, discriminatively learned correlation filters (DCF) have been successfully applied to address this problem for tracking. These methods utilize a periodic assumption of the training samples to efficiently learn a classifier on all patches in the target neighborhood. However, the periodic assumption also introduces unwanted boundary effects, which severely degrade the quality of the tracking model.  We propose Spatially Regularized Discriminative Correlation Filters (SRDCF) for tracking. A spatial regularization component is introduced in the learning to penalize correlation filter coefficients depending on their spatial location. Our SRDCF formulation allows the correlation filters to be learned on a significantly larger set of negative training samples, without corrupting the positive samples. We further propose an optimization strategy, based on the iterative Gauss-Seidel method, for efficient online learning of our SRDCF. Experiments are performed on four benchmark datasets: OTB-2013, ALOV++, OTB-2015, and VOT2014. Our approach achieves state-of-the-art results on all four datasets. On OTB-2013 and OTB-2015, we obtain an absolute gain of 8.0% and 8.2% respectively, in mean overlap precision, compared to the best existing trackers.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Danelljan_Learning_Spatially_Regularized_ICCV_2015_paper.pdf",
        "aff": "Computer Vision Laboratory, Linkoping University, Sweden; Computer Vision Laboratory, Linkoping University, Sweden; Computer Vision Laboratory, Linkoping University, Sweden; Computer Vision Laboratory, Linkoping University, Sweden",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2740598,
        "gs_citation": 2649,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16700478325580262692&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff_domain": "liu.se;liu.se;liu.se;liu.se",
        "email": "liu.se;liu.se;liu.se;liu.se",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Danelljan_Learning_Spatially_Regularized_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Linkoping University",
        "aff_unique_dep": "Computer Vision Laboratory",
        "aff_unique_url": "https://www.liu.se",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Sweden",
        "bibtex": "@InProceedings{Danelljan_2015_ICCV,\n    \n    author = {\n    Danelljan,\n    Martin and Hager,\n    Gustav and Khan,\n    Fahad Shahbaz and Felsberg,\n    Michael\n},\n    title = {\n    Learning Spatially Regularized Correlation Filters for Visual Tracking\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "2094bb8b8c",
        "title": "Learning Spatiotemporal Features With 3D Convolutional Networks",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, Manohar Paluri",
        "author": "Du Tran; Lubomir Bourdev; Rob Fergus; Lorenzo Torresani; Manohar Paluri",
        "abstract": "We propose a simple, yet effective approach for spatiotemporal feature learning using deep 3-dimensional convolutional networks (3D ConvNets) trained on a large scale supervised video dataset. Our findings are three-fold: 1) 3D ConvNets are more suitable for spatiotemporal feature learning compared to 2D ConvNets; 2) A homogeneous architecture with small 3x3x3 convolution kernels in all layers is among the best performing architectures for 3D ConvNets; and 3) Our learned features, namely C3D (Convolutional 3D), with a simple linear classifier outperform state-of-the-art methods on 4 different benchmarks and are comparable with current best methods on the other 2 benchmarks. In addition, the features are compact: achieving 52.8% accuracy on UCF101 dataset with only 10 dimensions and also very efficient to compute due to the fast inference of ConvNets. Finally, they are conceptually very simple and easy to train and use.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Tran_Learning_Spatiotemporal_Features_ICCV_2015_paper.pdf",
        "aff": "Facebook AI Research+Dartmouth College; Facebook AI Research; Facebook AI Research; Dartmouth College; Facebook AI Research",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 7002419,
        "gs_citation": 11362,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6546377120299846276&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": "cs.dartmouth.edu;fb.com;fb.com;cs.dartmouth.edu;fb.com",
        "email": "cs.dartmouth.edu;fb.com;fb.com;cs.dartmouth.edu;fb.com",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Tran_Learning_Spatiotemporal_Features_ICCV_2015_paper.html",
        "aff_unique_index": "0+1;0;0;1;0",
        "aff_unique_norm": "Facebook;Dartmouth College",
        "aff_unique_dep": "Facebook AI Research;",
        "aff_unique_url": "https://research.facebook.com;https://www.dartmouth.edu",
        "aff_unique_abbr": "FAIR;Dartmouth",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Tran_2015_ICCV,\n    \n    author = {\n    Tran,\n    Du and Bourdev,\n    Lubomir and Fergus,\n    Rob and Torresani,\n    Lorenzo and Paluri,\n    Manohar\n},\n    title = {\n    Learning Spatiotemporal Features With 3D Convolutional Networks\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "2fe06cd81f",
        "title": "Learning Temporal Embeddings for Complex Video Analysis",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Vignesh Ramanathan, Kevin Tang, Greg Mori, Li Fei-Fei",
        "author": "Vignesh Ramanathan; Kevin Tang; Greg Mori; Li Fei-Fei",
        "abstract": "In this paper, we propose to learn temporal embeddings of video frames for complex video analysis. Large quantities of unlabeled video data can be easily obtained from the Internet. These videos possess the implicit weak label that they are sequences of temporally and semantically coherent images. We leverage this information to learn temporal embeddings for video frames by associating frames with the temporal context that they appear in. To do this, we propose a scheme for incorporating temporal context based on past and future frames in videos, and compare this to other contextual representations. In addition, we show how data augmentation using multi-resolution samples and hard negatives helps to significantly improve the quality of the learned embeddings. We evaluate various design decisions for learning temporal embeddings, and show that our embeddings can improve performance for multiple video tasks such as retrieval, classification, and temporal order recovery in unconstrained Internet video.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Ramanathan_Learning_Temporal_Embeddings_ICCV_2015_paper.pdf",
        "aff": "Department of Electrical Engineering, Stanford University; Computer Science Department, Stanford University; School of Computing Science, Simon Fraser University; Computer Science Department, Stanford University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2548538,
        "gs_citation": 98,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15363697784447994059&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff_domain": "cs.stanford.edu;cs.stanford.edu;cs.sfu.ca;cs.stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu;cs.sfu.ca;cs.stanford.edu",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Ramanathan_Learning_Temporal_Embeddings_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Stanford University;Simon Fraser University",
        "aff_unique_dep": "Department of Electrical Engineering;School of Computing Science",
        "aff_unique_url": "https://www.stanford.edu;https://www.sfu.ca",
        "aff_unique_abbr": "Stanford;SFU",
        "aff_campus_unique_index": "0;0;1;0",
        "aff_campus_unique": "Stanford;Burnaby",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "United States;Canada",
        "bibtex": "@InProceedings{Ramanathan_2015_ICCV,\n    \n    author = {\n    Ramanathan,\n    Vignesh and Tang,\n    Kevin and Mori,\n    Greg and Fei-Fei,\n    Li\n},\n    title = {\n    Learning Temporal Embeddings for Complex Video Analysis\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "3a137d64aa",
        "title": "Learning The Structure of Deep Convolutional Networks",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Jiashi Feng, Trevor Darrell",
        "author": "Jiashi Feng; Trevor Darrell",
        "abstract": "In this work,  we develop a novel method for automatically learning aspects of the structure of a deep model, in order to improve its  performance, especially when labeled training data are scarce. We propose a new convolutional neural network model with the Indian Buffet Process (IBP) prior, termed ibpCNN. The ibpCNN automatically adapts its structure to provided training data,  achieves an optimal balance among model complexity, data fidelity and training loss, and thus  offers  better generalization performance.   The proposed ibpCNN captures complicated data distribution in an unsupervised generative way. Therefore, ibpCNN can exploit unlabeled data -- which can be collected at low cost -- to learn its  structure. After determining the structure, ibpCNN further learns its parameters according to specified tasks, in an end-to-end fashion, and produces  discriminative yet compact representations.    We evaluate the  performance of ibpCNN, on fully- and semi-supervised image classification tasks; ibpCNN surpasses standard CNN models on benchmark datasets, with much smaller size and higher efficiency.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Feng_Learning_The_Structure_ICCV_2015_paper.pdf",
        "aff": "Department of EECS & ICSI, UC Berkeley; Department of EECS & ICSI, UC Berkeley",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 616275,
        "gs_citation": 85,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15632264510830730339&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "berkeley.edu;eecs.berkeley.edu",
        "email": "berkeley.edu;eecs.berkeley.edu",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Feng_Learning_The_Structure_ICCV_2015_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "Department of Electrical Engineering and Computer Sciences & International Computer Science Institute",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Feng_2015_ICCV,\n    \n    author = {\n    Feng,\n    Jiashi and Darrell,\n    Trevor\n},\n    title = {\n    Learning The Structure of Deep Convolutional Networks\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "dac3dde389",
        "title": "Learning Visual Clothing Style With Heterogeneous Dyadic Co-Occurrences",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Andreas Veit, Balazs Kovacs, Sean Bell, Julian McAuley, Kavita Bala, Serge Belongie",
        "author": "Andreas Veit; Balazs Kovacs; Sean Bell; Julian McAuley; Kavita Bala; Serge Belongie",
        "abstract": "With the rapid proliferation of smart mobile devices, users now take millions of photos every day. These include large numbers of clothing and accessory images. We would like to answer questions like `What outfit goes well with this pair of shoes?' To answer these types of questions, one has to go beyond learning visual similarity and learn a visual notion of compatibility across categories. In this paper, we propose a novel learning framework to help answer these types of questions. The main idea of this framework is to learn a feature transformation from images of items into a latent space that expresses compatibility. For the feature transformation, we use a Siamese Convolutional Neural Network (CNN) architecture, where training examples are pairs of items that are either compatible or incompatible. We model compatibility based on co-occurrence in large-scale user behavior data; in particular co-purchase data from Amazon.com. To learn cross-category fit, we introduce a strategic method to sample training data, where pairs of items are heterogeneous dyads, i.e., the two elements of a pair belong to different high-level categories. While this approach is applicable to a wide variety of settings, we focus on the representative problem of learning compatible clothing style. Our results indicate that the proposed framework is capable of learning semantic information about visual style and is able to generate outfits of clothes, with items from different categories, that go well together.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Veit_Learning_Visual_Clothing_ICCV_2015_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1473559,
        "gs_citation": 388,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=636007317105506418&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Veit_Learning_Visual_Clothing_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Veit_2015_ICCV,\n    \n    author = {\n    Veit,\n    Andreas and Kovacs,\n    Balazs and Bell,\n    Sean and McAuley,\n    Julian and Bala,\n    Kavita and Belongie,\n    Serge\n},\n    title = {\n    Learning Visual Clothing Style With Heterogeneous Dyadic Co-Occurrences\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "08b2bd548a",
        "title": "Learning Where to Position Parts in 3D",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Marco Pedersoli, Tinne Tuytelaars",
        "author": "Marco Pedersoli; Tinne Tuytelaars",
        "abstract": "A common issue in deformable object detection is finding a good way to position the parts. This issue is even more outspoken when considering detection and pose estimation for 3D objects, where parts should be placed in a three-dimensional space. Some methods extract the 3D shape of the object from 3D CAD models. This limits their applicability to categories for which such models are available. Others represent the object with a predefined and simple shape (e.g. a cuboid). This extends the applicability of the model, but in many cases the pre-defined shape is too simple to properly represent the object in 3D. In this paper we propose a new method for the detection and pose estimation of 3D objects, that does not use any 3D CAD model or other 3D information. Starting from a simple and general 3D shape, we learn in a weakly supervised manner the 3D part locations that best fit the training data. As this method builds on a iterative estimation of the part locations, we introduce several speedups to make the method fast enough for practical experiments. We evaluate our model for the detection and pose estimation of faces and cars. Our method obtains results comparable with the state of the art, it is faster than most of the other approaches and does not need any additional 3D information.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Pedersoli_Learning_Where_to_ICCV_2015_paper.pdf",
        "aff": "Inria∗†; PSI-iMinds KU Leuven, Belgium",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 604748,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=262191681994942361&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "inria.fr;esat.kuleuven.be",
        "email": "inria.fr;esat.kuleuven.be",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Pedersoli_Learning_Where_to_ICCV_2015_paper.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Inria;KU Leuven",
        "aff_unique_dep": ";PSI-iMinds",
        "aff_unique_url": "https://www.inria.fr;https://www.kuleuven.be",
        "aff_unique_abbr": "Inria;KU Leuven",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "France;Belgium",
        "bibtex": "@InProceedings{Pedersoli_2015_ICCV,\n    \n    author = {\n    Pedersoli,\n    Marco and Tuytelaars,\n    Tinne\n},\n    title = {\n    Learning Where to Position Parts in 3D\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "8833a7479a",
        "title": "Learning a Descriptor-Specific 3D Keypoint Detector",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Samuele Salti, Federico Tombari, Riccardo Spezialetti, Luigi Di Stefano",
        "author": "Samuele Salti; Federico Tombari; Riccardo Spezialetti; Luigi Di Stefano",
        "abstract": "Keypoint detection represents the first stage in the majority of modern computer vision pipelines based on automatically established correspondences between local descriptors. However, no standard solution has emerged yet in the case of 3D data such as point clouds or meshes, which exhibit high variability in level of detail and noise. More importantly, existing proposals for 3D keypoint detection rely on geometric saliency functions that attempt to maximize repeatability rather than distinctiveness of the selected regions, which may lead to sub-optimal performance of the overall pipeline. To overcome these shortcomings, we cast 3D keypoint detection as a binary classification between points whose support can be correctly matched by a predefined 3D descriptor or not, thereby learning a descriptor-specific detector that adapts seamlessly to different scenarios. Through  experiments on several public datasets, we show that this novel approach to the design of a keypoint detector represents a flexible solution that, nonetheless, can provide state-of-the-art descriptor matching performance.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Salti_Learning_a_Descriptor-Specific_ICCV_2015_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8794029423183434818&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Salti_Learning_a_Descriptor-Specific_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Salti_2015_ICCV,\n    \n    author = {\n    Salti,\n    Samuele and Tombari,\n    Federico and Spezialetti,\n    Riccardo and Di Stefano,\n    Luigi\n},\n    title = {\n    Learning a Descriptor-Specific 3D Keypoint Detector\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "cd5e0fd3f1",
        "title": "Learning a Discriminative Model for the Perception of Realism in Composite Images",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Jun-Yan Zhu, Philipp Krähenbühl, Eli Shechtman, Alexei A. Efros",
        "author": "Jun-Yan Zhu; Philipp Krahenbuhl; Eli Shechtman; Alexei A. Efros",
        "abstract": "What makes an image appear realistic? In this work, we are answering this question from a data-driven perspective by learning the perception of visual realism directly from large amounts of data. In particular, we train a Convolutional Neural Network (CNN) model that distinguishes natural photographs from automatically generated composite images. The model learns to predict visual realism of a scene in terms of color, lighting and texture compatibility, without any human annotations pertaining to it. Our model outperforms previous works that rely on hand-crafted heuristics, for the task of classifying realistic vs. unrealistic photos. Furthermore, we apply our learned model to compute optimal parameters of a compositing method, to maximize the visual realism score predicted by our CNN model. We demonstrate its advantage against existing methods via a human perception study.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Zhu_Learning_a_Discriminative_ICCV_2015_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 174,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14824505096062642528&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Zhu_Learning_a_Discriminative_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Zhu_2015_ICCV,\n    \n    author = {\n    Zhu,\n    Jun-Yan and Krahenbuhl,\n    Philipp and Shechtman,\n    Eli and Efros,\n    Alexei A.\n},\n    title = {\n    Learning a Discriminative Model for the Perception of Realism in Composite Images\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "021ae4055b",
        "title": "Learning to Boost Filamentary Structure Segmentation",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Lin Gu, Li Cheng",
        "author": "Lin Gu; Li Cheng",
        "abstract": "The challenging problem of filamentary structure segmentation has a broad range of applications in biological and medical fields. A critical yet challenging issue remains on how to detect and restore the small filamentary fragments from backgrounds: The small fragments are of diverse shapes and appearances, meanwhile the backgrounds could be cluttered and ambiguous. Focusing on this issue, this paper proposes an iterative two-step learning-based approach to boost the performance based on a base segmenter arbitrarily chosen from a number of existing segmenters: We start with an initial partial segmentation where the filamentary structure obtained is of high confidence based on this existing segmenter. We also define a scanning horizon as epsilon balls centred around the partial segmentation result. Step one of our approach centers on a data-driven latent classification tree model to detect the filamentary fragments. This model is learned via a training process, where a large number of distinct local figure/background separation scenarios are established and geometrically organized into a tree structure. Step two spatially restores the isolated fragments back to the current partial segmentation, which is accomplished by means of completion fields and matting. Both steps are then alternated with the growth of partial segmentation result, until the input image space is entirely explored. Our approach is rather generic and can be easily augmented to a wide range of existing supervised/unsupervised segmenters to produce an improved result. This has been empirically verified on specific filamentary structure segmentation tasks: retinal blood vessel segmentation as well as neuronal segmentations, where noticeable improvement has been shown over the original state-of-the-arts.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Gu_Learning_to_Boost_ICCV_2015_paper.pdf",
        "aff": "Bioinformatics Institute, A*STAR, Singapore; Bioinformatics Institute, A*STAR, Singapore",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1479701,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1251693135842928293&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": "bii.a-star.edu.sg;bii.a-star.edu.sg",
        "email": "bii.a-star.edu.sg;bii.a-star.edu.sg",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Gu_Learning_to_Boost_ICCV_2015_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Bioinformatics Institute",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.bii.a-star.edu.sg",
        "aff_unique_abbr": "BII",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Singapore",
        "bibtex": "@InProceedings{Gu_2015_ICCV,\n    \n    author = {\n    Gu,\n    Lin and Cheng,\n    Li\n},\n    title = {\n    Learning to Boost Filamentary Structure Segmentation\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "dc86af92e1",
        "title": "Learning to Combine Mid-Level Cues for Object Proposal Generation",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Tom Lee, Sanja Fidler, Sven Dickinson",
        "author": "Tom Lee; Sanja Fidler; Sven Dickinson",
        "abstract": "In recent years, region proposals have replaced sliding windows in support of object recognition, offering more discriminating shape and appearance information through improved localization.  One powerful approach for generating region proposals is based on minimizing parametric energy functions with parametric maxflow.  In this paper, we introduce Parametric Min-Loss (PML), a novel structured learning framework for parametric energy functions.  While PML is generally applicable to different domains, we use it in the context of region proposals to learn to combine a set of mid-level grouping cues to yield a small set of object region proposals with high recall.  Our learning framework accounts for multiple diverse outputs, and is complemented by diversification seeds based on image location and color.  This approach casts perceptual grouping and cue combination in a novel structured learning framework which yields baseline improvements on VOC 2012 and COCO 2014.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Lee_Learning_to_Combine_ICCV_2015_paper.pdf",
        "aff": "University of Toronto; University of Toronto; University of Toronto",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4197259,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15533151319875168071&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "cs.toronto.edu;cs.toronto.edu;cs.toronto.edu",
        "email": "cs.toronto.edu;cs.toronto.edu;cs.toronto.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Lee_Learning_to_Combine_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Toronto",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.utoronto.ca",
        "aff_unique_abbr": "U of T",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada",
        "bibtex": "@InProceedings{Lee_2015_ICCV,\n    \n    author = {\n    Lee,\n    Tom and Fidler,\n    Sanja and Dickinson,\n    Sven\n},\n    title = {\n    Learning to Combine Mid-Level Cues for Object Proposal Generation\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "e7278663f0",
        "title": "Learning to Divide and Conquer for Online Multi-Target Tracking",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Francesco Solera, Simone Calderara, Rita Cucchiara",
        "author": "Francesco Solera; Simone Calderara; Rita Cucchiara",
        "abstract": "Online Multiple Target Tracking (MTT) is often addressed within the tracking-by-detection paradigm. Detections are previously extracted independently in each frame and then objects trajectories are built by maximizing specifically designed coherence functions. Nevertheless, ambiguities arise in presence of occlusions or detection errors.  In this paper we claim that the ambiguities in tracking could be solved by a selective use of the features, by working with more reliable features if possible and exploiting a deeper representation of the target only if necessary. To this end, we propose an online divide and conquer tracker for static camera scenes, which partitions the assignment problem in local subproblems and solves them by selectively choosing and combining the best features. The complete framework is cast as a structural learning task that unifies these phases and learns tracker parameters from examples. Experiments on two different datasets highlights a significant improvement of tracking performances (MOTA +10%) over the state of the art.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Solera_Learning_to_Divide_ICCV_2015_paper.pdf",
        "aff": "Department of Engineering, University of Modena and Reggio Emilia; Department of Engineering, University of Modena and Reggio Emilia; Department of Engineering, University of Modena and Reggio Emilia",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1293102,
        "gs_citation": 76,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1764665180179535325&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "unimore.it;unimore.it;unimore.it",
        "email": "unimore.it;unimore.it;unimore.it",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Solera_Learning_to_Divide_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Modena and Reggio Emilia",
        "aff_unique_dep": "Department of Engineering",
        "aff_unique_url": "https://www.unimore.it",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Italy",
        "bibtex": "@InProceedings{Solera_2015_ICCV,\n    \n    author = {\n    Solera,\n    Francesco and Calderara,\n    Simone and Cucchiara,\n    Rita\n},\n    title = {\n    Learning to Divide and Conquer for Online Multi-Target Tracking\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "5dcc7711a5",
        "title": "Learning to Predict Saliency on Face Images",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Mai Xu, Yun Ren, Zulin Wang",
        "author": "Mai Xu; Yun Ren; Zulin Wang",
        "abstract": "This paper proposes a novel method, which learns to detect saliency of face images. To be more specific, we obtain a database of eye tracking over extensive face images, via conducting an eye tracking experiment. With analysis on eye tracking database, we verify that the fixations tend to cluster around facial features, when viewing images with large faces. For modeling attention on faces and facial features, the proposed method learns the Gaussian mixture model (GMM) distribution from the fixations of eye tracking data as the top-down features for saliency detection of face images. Then, in our method, the top-down features (i.e., face and facial features) upon the the learnt GMM are linearly combined with the conventional bottom-up features (i.e., color, intensity, and orientation), for saliency detection. In the linear combination, we argue that the weights corresponding to top-down feature channels depend on the face size in images, and the relationship between the weights and face size is thus investigated via learning from the training eye tracking data. Finally, experimental results show that our learning-based method is able to advance state-of-the-art saliency prediction for face images. The corresponding database and code are available online: www.ee.buaa.edu.cn/xumfiles/saliency_detection.html.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Xu_Learning_to_Predict_ICCV_2015_paper.pdf",
        "aff": "School of Electronic and Information Engineering, Beihang University, Beijing, 100191, China; School of Electronic and Information Engineering, Beihang University, Beijing, 100191, China; School of Electronic and Information Engineering, Beihang University, Beijing, 100191, China",
        "project": "www.ee.buaa.edu.cn/xumfiles/saliency detection.html",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2319794,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18258939971681457068&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "buaa.edu.cn; ; ",
        "email": "buaa.edu.cn; ; ",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Xu_Learning_to_Predict_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Beihang University",
        "aff_unique_dep": "School of Electronic and Information Engineering",
        "aff_unique_url": "http://www.buaa.edu.cn",
        "aff_unique_abbr": "BUAA",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Xu_2015_ICCV,\n    \n    author = {\n    Xu,\n    Mai and Ren,\n    Yun and Wang,\n    Zulin\n},\n    title = {\n    Learning to Predict Saliency on Face Images\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "3ae6de0315",
        "title": "Learning to Rank Based on Subsequences",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Basura Fernando, Efstratios Gavves, Damien Muselet, Tinne Tuytelaars",
        "author": "Basura Fernando; Efstratios Gavves; Damien Muselet; Tinne Tuytelaars",
        "abstract": "We present a supervised learning to rank algorithm that effectively orders images by exploiting the structure in image sequences. Most often in the supervised learning to rank literature, ranking is approached either by analysing pairs of images or by optimizing a list-wise surrogate loss function on full sequences. In this work we propose MidRank, which learns from moderately sized sub-sequences instead. These sub-sequences contain useful structural ranking information that leads to better learnability during training and better generalization during testing. By exploiting sub-sequences, the proposed MidRank improves ranking accuracy considerably on an extensive array of image ranking applications and datasets.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Fernando_Learning_to_Rank_ICCV_2015_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14248649997709141141&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Fernando_Learning_to_Rank_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Fernando_2015_ICCV,\n    \n    author = {\n    Fernando,\n    Basura and Gavves,\n    Efstratios and Muselet,\n    Damien and Tuytelaars,\n    Tinne\n},\n    title = {\n    Learning to Rank Based on Subsequences\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "4dcb8cff2b",
        "title": "Learning to See by Moving",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Pulkit Agrawal, Joao Carreira, Jitendra Malik",
        "author": "Pulkit Agrawal; Joao Carreira; Jitendra Malik",
        "abstract": "The current dominant paradigm for feature learning in computer vision relies on training neural networks for the task of object recognition using millions of hand labelled images. Is it also possible to learn features for a diverse set of visual tasks using any other form of supervision? In biology, living organisms developed the ability of visual perception for the purpose of moving and acting in the world. Drawing inspiration from this observation, in this work we investigated if the awareness of egomotion(i.e. self motion) can be used as a supervisory signal for feature learning. As opposed to the knowledge of class labels, information about egomotion is freely available to mobile agents. We found that using the same number of training images, features learnt using egomotion as supervision compare favourably to features learnt using class-label as supervision on the tasks of scene recognition, object recognition, visual odometry and keypoint matching.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Agrawal_Learning_to_See_ICCV_2015_paper.pdf",
        "aff": "UC Berkeley; UC Berkeley; UC Berkeley",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 492990,
        "gs_citation": 702,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15713100577731372816&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu",
        "email": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Agrawal_Learning_to_See_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Agrawal_2015_ICCV,\n    \n    author = {\n    Agrawal,\n    Pulkit and Carreira,\n    Joao and Malik,\n    Jitendra\n},\n    title = {\n    Learning to See by Moving\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "5fe52b65c8",
        "title": "Learning to Track for Spatio-Temporal Action Localization",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Philippe Weinzaepfel, Zaid Harchaoui, Cordelia Schmid",
        "author": "Philippe Weinzaepfel; Zaid Harchaoui; Cordelia Schmid",
        "abstract": "We propose an effective approach for spatio-temporal action localization in realistic videos. The approach first detects proposals at the frame-level and scores them with a combination of static and motion CNN features. It then tracks high-scoring proposals throughout the video using a tracking-by-detection approach. Our tracker relies simultaneously on instance-level and class-level detectors. The tracks are scored using a spatio-temporal motion histogram, a descriptor at the track level, in combination with the CNN features. Finally, we perform temporal localization of the action  using a sliding-window approach at the track level. We present experimental results for spatio-temporal localization on the UCF-Sports, J-HMDB and UCF-101 action localization datasets, where our approach outperforms the state of the art with a margin of 15%, 7% and 12% respectively in mAP.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Weinzaepfel_Learning_to_Track_ICCV_2015_paper.pdf",
        "aff": "Inria∗; Inria+NYU; Inria∗",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 819683,
        "gs_citation": 410,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1893083230530095061&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 40,
        "aff_domain": "inria.fr; ; ",
        "email": "inria.fr; ; ",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Weinzaepfel_Learning_to_Track_ICCV_2015_paper.html",
        "aff_unique_index": "0;0+1;0",
        "aff_unique_norm": "Inria;New York University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.inria.fr;https://www.nyu.edu",
        "aff_unique_abbr": "Inria;NYU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";New York",
        "aff_country_unique_index": "0;0+1;0",
        "aff_country_unique": "France;United States",
        "bibtex": "@InProceedings{Weinzaepfel_2015_ICCV,\n    \n    author = {\n    Weinzaepfel,\n    Philippe and Harchaoui,\n    Zaid and Schmid,\n    Cordelia\n},\n    title = {\n    Learning to Track for Spatio-Temporal Action Localization\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "5940ce768e",
        "title": "Learning to Track: Online Multi-Object Tracking by Decision Making",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Yu Xiang, Alexandre Alahi, Silvio Savarese",
        "author": "Yu Xiang; Alexandre Alahi; Silvio Savarese",
        "abstract": "Online Multi-Object Tracking (MOT) has wide applications in time-critical video analysis scenarios, such as robot navigation and autonomous driving. In tracking-by-detection, a major challenge of online MOT is how to robustly associate noisy object detections on a new video frame with previously tracked objects. In this work, we formulate the online MOT problem as decision making in Markov Decision Processes (MDPs), where the lifetime of an object is modeled with a MDP. Learning a similarity function for data association is equivalent to learning a policy for the MDP, and the policy learning is approached in a reinforcement learning fashion which benefits from both advantages of offline-learning and online-learning for data association. Moreover, our framework can naturally handle the birth/death and appearance/disappearance of targets by treating them as state transitions in the MDP while leveraging existing online single object tracking methods. We conduct experiments on the MOT Benchmark to verify the effectiveness of our method.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Xiang_Learning_to_Track_ICCV_2015_paper.pdf",
        "aff": "Stanford University+University of Michigan at Ann Arbor; Stanford University; Stanford University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1695218,
        "gs_citation": 859,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5560095188110845353&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 21,
        "aff_domain": "umich.edu;stanford.edu;stanford.edu",
        "email": "umich.edu;stanford.edu;stanford.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Xiang_Learning_to_Track_ICCV_2015_paper.html",
        "aff_unique_index": "0+1;0;0",
        "aff_unique_norm": "Stanford University;University of Michigan",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.stanford.edu;https://www.umich.edu",
        "aff_unique_abbr": "Stanford;UM",
        "aff_campus_unique_index": "0+1;0;0",
        "aff_campus_unique": "Stanford;Ann Arbor",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Xiang_2015_ICCV,\n    \n    author = {\n    Xiang,\n    Yu and Alahi,\n    Alexandre and Savarese,\n    Silvio\n},\n    title = {\n    Learning to Track: Online Multi-Object Tracking by Decision Making\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "4179be7195",
        "title": "Learning to Transfer: Transferring Latent Task Structures and Its Application to Person-Specific Facial Action Unit Detection",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Timur Almaev, Brais Martinez, Michel Valstar",
        "author": "Timur Almaev; Brais Martinez; Michel Valstar",
        "abstract": "In this article we explore the problem of constructing person-specific models for the detection of facial Action Units (AUs), addressing the problem from the point of view of Transfer Learning and Multi-Task Learning. Our starting point is the fact that some expressions, such as smiles, are very easily elicited, annotated, and automatically detected, while others are much harder to elicit and to annotate. We thus consider a novel problem:  all AU models for the target subject are to be learnt using person-specific annotated data for a reference AU (AU12 in our case), and no data or little data regarding the target AU. In order to design such a model, we propose a novel Multi-Task Learning and the associated Transfer Learning framework, in which we consider both relations across subjects and  AUs. That is to say, we consider a tensor structure among the tasks. Our approach hinges on learning the latent relations among tasks using one single reference AU, and then transferring these latent relations to other AUs. We show that we are able to effectively make use of the annotated data for AU12 when learning other person-specific AU models, even in the absence of data for the target task. Finally, we show the excellent performance of our method when small amounts of annotated data for the target tasks are made available.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Almaev_Learning_to_Transfer_ICCV_2015_paper.pdf",
        "aff": "The School of Computer Science, University of Nottingham; The School of Computer Science, University of Nottingham; The School of Computer Science, University of Nottingham",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 727583,
        "gs_citation": 57,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3191465524219642283&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "nottingham.ac.uk;nottingham.ac.uk;nottingham.ac.uk",
        "email": "nottingham.ac.uk;nottingham.ac.uk;nottingham.ac.uk",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Almaev_Learning_to_Transfer_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Nottingham",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.nottingham.ac.uk",
        "aff_unique_abbr": "UoN",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom",
        "bibtex": "@InProceedings{Almaev_2015_ICCV,\n    \n    author = {\n    Almaev,\n    Timur and Martinez,\n    Brais and Valstar,\n    Michel\n},\n    title = {\n    Learning to Transfer: Transferring Latent Task Structures and Its Application to Person-Specific Facial Action Unit Detection\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "6dcd25659e",
        "title": "Leave-One-Out Kernel Optimization for Shadow Detection",
        "session": "computational photography and image enhancement",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "Tomas F. Yago Vicente, Minh Hoai, Dimitris Samaras",
        "author": "Tomas F. Yago Vicente; Minh Hoai; Dimitris Samaras",
        "abstract": "The objective of this work is to detect shadows in images. We pose this as the problem of labeling image regions, where each region corresponds to a group of superpixels. To predict the label of each region, we train a kernel Least-Squares SVM for separating shadow and non-shadow regions. The parameters of the kernel and the classifier are jointly learned to minimize the leave-one-out cross validation error. Optimizing the leave-one-out cross validation error is typically difficult, but it can be done efficiently in our framework. Experiments on two challenging shadow datasets, UCF and UIUC, show that our region classifier outperforms more complex methods. We further enhance the performance of the region classifier by embedding it in an MRF framework and adding pairwise contextual cues. This leads to a method that significantly outperforms the state-of-the-art.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Vicente_Leave-One-Out_Kernel_Optimization_ICCV_2015_paper.pdf",
        "aff": "Stony Brook University, Stony Brook, NY 11794, USA; Stony Brook University, Stony Brook, NY 11794, USA; Stony Brook University, Stony Brook, NY 11794, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 917586,
        "gs_citation": 69,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1173981501749071365&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "cs.stonybrook.edu;cs.stonybrook.edu;cs.stonybrook.edu",
        "email": "cs.stonybrook.edu;cs.stonybrook.edu;cs.stonybrook.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Vicente_Leave-One-Out_Kernel_Optimization_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Stony Brook University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stonybrook.edu",
        "aff_unique_abbr": "SBU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stony Brook",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Vicente_2015_ICCV,\n    \n    author = {\n    Vicente,\n    Tomas F. Yago and Hoai,\n    Minh and Samaras,\n    Dimitris\n},\n    title = {\n    Leave-One-Out Kernel Optimization for Shadow Detection\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "05973e608f",
        "title": "Lending A Hand: Detecting Hands and Recognizing Activities in Complex Egocentric Interactions",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Sven Bambach, Stefan Lee, David J. Crandall, Chen Yu",
        "author": "Sven Bambach; Stefan Lee; David J. Crandall; Chen Yu",
        "abstract": "Hands appear very often in egocentric video, and their appearance and pose give important cues about what people are doing and what they are paying attention to. But existing work in hand detection has made strong assumptions that work well in only simple scenarios, such as with limited interaction with other people or in lab settings. We develop methods to locate and distinguish between hands in egocentric video using strong appearance models with Convolutional Neural Networks, and introduce a simple candidate region generation approach that outperforms existing techniques at a fraction of the computational cost. We show how these high-quality bounding boxes can be used to create accurate pixelwise hand regions, and as an application, we investigate the extent to which hand segmentation alone can distinguish between different activities.  We evaluate these techniques on a new dataset of 48 first-person videos (along with pixel-level ground truth for over 15,000 hand instances) of people interacting in realistic environments.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Bambach_Lending_A_Hand_ICCV_2015_paper.pdf",
        "aff": "School of Informatics and Computing, Indiana University; School of Informatics and Computing, Indiana University; School of Informatics and Computing, Indiana University; Psychological and Brain Sciences, Indiana University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1023289,
        "gs_citation": 541,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11913244108675828123&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff_domain": "indiana.edu;indiana.edu;indiana.edu;indiana.edu",
        "email": "indiana.edu;indiana.edu;indiana.edu;indiana.edu",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Bambach_Lending_A_Hand_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Indiana University",
        "aff_unique_dep": "School of Informatics and Computing",
        "aff_unique_url": "https://www.indiana.edu",
        "aff_unique_abbr": "IU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Bambach_2015_ICCV,\n    \n    author = {\n    Bambach,\n    Sven and Lee,\n    Stefan and Crandall,\n    David J. and Yu,\n    Chen\n},\n    title = {\n    Lending A Hand: Detecting Hands and Recognizing Activities in Complex Egocentric Interactions\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "5631e450b5",
        "title": "Leveraging Datasets With Varying Annotations for Face Alignment via Deep Regression Network",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Jie Zhang, Meina Kan, Shiguang Shan, Xilin Chen",
        "author": "Jie Zhang; Meina Kan; Shiguang Shan; Xilin Chen",
        "abstract": "Facial landmark detection, as a vital topic in computer vision, has been studied for many decades and lots of datasets have been collected for evaluation. These datasets usually have different annotations, e.g., 68-landmark markup for LFPW dataset, while 74-landmark markup for GTAV dataset. Intuitively, it is meaningful to fuse all the datasets to predict a union of all types of landmarks from multiple datasets (i.e., transfer the annotations of each dataset to all other datasets), but this problem is nontrivial due to the distribution discrepancy between datasets and incomplete annotations of all types for each dataset. In this work, we propose a deep regression network coupled with sparse shape regression (DRN-SSR) to predict the union of all types of landmarks by leveraging datasets with varying annotations, each dataset with one type of annotation. Specifically, the deep regression network intends to predict the union of all landmarks, and the sparse shape regression attempts to approximate those undefined landmarks on each dataset so as to guide the learning of the deep regression network for face alignment. Extensive experiments on two challenging datasets, IBUG and GLF, demonstrate that our method can effectively leverage the multiple datasets with different annotations to predict the union of all types of landmarks.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Zhang_Leveraging_Datasets_With_ICCV_2015_paper.pdf",
        "aff": "Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing 100190, China + University of Chinese Academy of Sciences, Beijing 100049, China; Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing 100190, China; Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing 100190, China + CAS Center for Excellence in Brain Science and Intelligence Technology; Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing 100190, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 921156,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18313032662692210069&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "vipl.ict.ac.cn;vipl.ict.ac.cn;vipl.ict.ac.cn;vipl.ict.ac.cn",
        "email": "vipl.ict.ac.cn;vipl.ict.ac.cn;vipl.ict.ac.cn;vipl.ict.ac.cn",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Zhang_Leveraging_Datasets_With_ICCV_2015_paper.html",
        "aff_unique_index": "0+1;0;0+0;0",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Computing Technology;",
        "aff_unique_url": "http://www.cas.ac.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "CAS;UCAS",
        "aff_campus_unique_index": "0+0;0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0+0;0;0+0;0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Zhang_2015_ICCV,\n    \n    author = {\n    Zhang,\n    Jie and Kan,\n    Meina and Shan,\n    Shiguang and Chen,\n    Xilin\n},\n    title = {\n    Leveraging Datasets With Varying Annotations for Face Alignment via Deep Regression Network\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "4dc7ead274",
        "title": "Linearization to Nonlinear Learning for Visual Tracking",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Bo Ma, Hongwei Hu, Jianbing Shen, Yuping Zhang, Fatih Porikli",
        "author": "Bo Ma; Hongwei Hu; Jianbing Shen; Yuping Zhang; Fatih Porikli",
        "abstract": "Due to unavoidable appearance variations caused by occlusion, deformation, and other factors, classifiers for visual tracking are nonlinear as a necessity. Building on the theory of globally linear approximations to nonlinear functions, we introduce an elegant method that jointly learns a nonlinear classifier and a visual dictionary for tracking objects in a semi-supervised sparse coding fashion. This establishes an obvious distinction from conventional sparse coding based discriminative tracking algorithms that usually maintain two-stage learning strategies, i.e., learning a dictionary in an unsupervised way then followed by training a classifier. However, the treating dictionary learning and classifier training as separate stages may not produce both descriptive and discriminative models for objects. By contrast, our method is capable of constructing a dictionary that not only fully reflects the intrinsic manifold structure of the data, but also possesses discriminative power. This paper presents an optimization method to obtain such an optimal dictionary, associated sparse coding, and a classifier in an iterative process. Our experiments on a benchmark show our tracker attains outstanding performance compared with the  state-of-the-art algorithms.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Ma_Linearization_to_Nonlinear_ICCV_2015_paper.pdf",
        "aff": "Beijing Lab of Intelligent Information Technology, School of Computer Science, Beijing Institute of Technology, China; Beijing Lab of Intelligent Information Technology, School of Computer Science, Beijing Institute of Technology, China; Beijing Lab of Intelligent Information Technology, School of Computer Science, Beijing Institute of Technology, China; Beijing Lab of Intelligent Information Technology, School of Computer Science, Beijing Institute of Technology, China; Research School of Engineering, Australian National University, and NICTA Australia",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1346689,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12805475331331723409&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "bit.edu.cn; ; ; ; ",
        "email": "bit.edu.cn; ; ; ; ",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Ma_Linearization_to_Nonlinear_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0;1",
        "aff_unique_norm": "Beijing Institute of Technology;Australian National University",
        "aff_unique_dep": "School of Computer Science;Research School of Engineering",
        "aff_unique_url": "http://www.bit.edu.cn;https://www.anu.edu.au",
        "aff_unique_abbr": "BIT;ANU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0;0;0;0;1",
        "aff_country_unique": "China;Australia",
        "bibtex": "@InProceedings{Ma_2015_ICCV,\n    \n    author = {\n    Ma,\n    Bo and Hu,\n    Hongwei and Shen,\n    Jianbing and Zhang,\n    Yuping and Porikli,\n    Fatih\n},\n    title = {\n    Linearization to Nonlinear Learning for Visual Tracking\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "f5a7e84022",
        "title": "Listening With Your Eyes: Towards a Practical Visual Speech Recognition System Using Deep Boltzmann Machines",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Chao Sui, Mohammed Bennamoun, Roberto Togneri",
        "author": "Chao Sui; Mohammed Bennamoun; Roberto Togneri",
        "abstract": "This paper presents a novel feature learning method for visual speech recognition using Deep Boltzmann Machines (DBM). Unlike all existing visual feature extraction techniques which solely extracts features from video sequences, our method is able to explore both acoustic information and visual information to learn a better visual feature representation in the training stage. During the test stage, instead of using both audio and visual signals, only the videos are used for generating the missing audio feature, and both the given visual and given audio features are used to obtain a joint representation. We carried out our experiments on a large scale audio-visual data corpus, and experimental results show that our proposed techniques outperforms the performance of the hadncrafted features and features learned by other commonly used deep learning techniques.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Sui_Listening_With_Your_ICCV_2015_paper.pdf",
        "aff": "School of Computer Science and Software Engineering, University of Western Australia, Perth, Australia; School of Computer Science and Software Engineering, University of Western Australia, Perth, Australia; School of Electrical, Electronic and Computer Engineering, University of Western Australia, Perth, Australia",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4637276,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9620540829450897983&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "csse.uwa.edu.au;uwa.edu.au;uwa.edu.au",
        "email": "csse.uwa.edu.au;uwa.edu.au;uwa.edu.au",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Sui_Listening_With_Your_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Western Australia",
        "aff_unique_dep": "School of Computer Science and Software Engineering",
        "aff_unique_url": "https://www.uwa.edu.au",
        "aff_unique_abbr": "UWA",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Perth",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Australia",
        "bibtex": "@InProceedings{Sui_2015_ICCV,\n    \n    author = {\n    Sui,\n    Chao and Bennamoun,\n    Mohammed and Togneri,\n    Roberto\n},\n    title = {\n    Listening With Your Eyes: Towards a Practical Visual Speech Recognition System Using Deep Boltzmann Machines\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "210ef286f4",
        "title": "Live Repetition Counting",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Ofir Levy, Lior Wolf",
        "author": "Ofir Levy; Lior Wolf",
        "abstract": "The task of counting the number of repetitions of approximately the same action in an input video sequence is addressed. The proposed method runs online and not on the complete pre-captured video. It analyzes sequentially blocks of 20 non-consecutive frames. The cycle length within each block is evaluated using a convolutional neural network and the information is then integrated over time. The entropy of the network's predictions is used in order to automatically start and stop the repetition counter and to select the appropriate time scale. Coupled with a region of interest detection mechanism, the method is robust enough to handle real world videos, even when the camera is moving. A unique property of our method is that it is shown to successfully train on entirely unrealistic data created by synthesizing moving random patches.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Levy_Live_Repetition_Counting_ICCV_2015_paper.pdf",
        "aff": "The Blavatnik School of Computer Science, Tel Aviv University; The Blavatnik School of Computer Science, Tel Aviv University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 834484,
        "gs_citation": 111,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8517787109718193465&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Levy_Live_Repetition_Counting_ICCV_2015_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Tel Aviv University",
        "aff_unique_dep": "Blavatnik School of Computer Science",
        "aff_unique_url": "https://www.tau.ac.il",
        "aff_unique_abbr": "TAU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Tel Aviv",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel",
        "bibtex": "@InProceedings{Levy_2015_ICCV,\n    \n    author = {\n    Levy,\n    Ofir and Wolf,\n    Lior\n},\n    title = {\n    Live Repetition Counting\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "03add9c840",
        "title": "Local Convolutional Features With Unsupervised Training for Image Retrieval",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Mattis Paulin, Matthijs Douze, Zaid Harchaoui, Julien Mairal, Florent Perronin, Cordelia Schmid",
        "author": "Mattis Paulin; Matthijs Douze; Zaid Harchaoui; Julien Mairal; Florent Perronin; Cordelia Schmid",
        "abstract": "Patch-level descriptors underlie several important computer vision tasks, such as  stereo-matching or content-based image retrieval.  We introduce a deep convolutional architecture that yields patch-level descriptors,  as an alternative to the popular SIFT descriptor for image retrieval.   The proposed family of descriptors, called Patch-CKN, adapt the recently introduced Convolutional Kernel Network (CKN), an unsupervised framework to learn convolutional architectures. We present a comparison framework to benchmark current deep convolutional approaches along with Patch-CKN for both patch and image retrieval, including our novel ``RomePatches'' dataset. Patch-CKN descriptors yield competitive results compared to supervised CNN alternatives on patch and image retrieval.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Paulin_Local_Convolutional_Features_ICCV_2015_paper.pdf",
        "aff": "Inria∗; Inria∗; Inria∗+NYU; Inria∗; Facebook AI Research†; Inria∗",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1112195,
        "gs_citation": 219,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13044799910553789945&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 28,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Paulin_Local_Convolutional_Features_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0+1;0;2;0",
        "aff_unique_norm": "Inria;New York University;Facebook",
        "aff_unique_dep": ";;Facebook AI Research",
        "aff_unique_url": "https://www.inria.fr;https://www.nyu.edu;https://research.facebook.com",
        "aff_unique_abbr": "Inria;NYU;FAIR",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";New York",
        "aff_country_unique_index": "0;0;0+1;0;1;0",
        "aff_country_unique": "France;United States",
        "bibtex": "@InProceedings{Paulin_2015_ICCV,\n    \n    author = {\n    Paulin,\n    Mattis and Douze,\n    Matthijs and Harchaoui,\n    Zaid and Mairal,\n    Julien and Perronin,\n    Florent and Schmid,\n    Cordelia\n},\n    title = {\n    Local Convolutional Features With Unsupervised Training for Image Retrieval\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "d6542ac942",
        "title": "Local Subspace Collaborative Tracking",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Lin Ma, Xiaoqin Zhang, Weiming Hu, Junliang Xing, Jiwen Lu, Jie Zhou",
        "author": "Lin Ma; Xiaoqin Zhang; Weiming Hu; Junliang Xing; Jiwen Lu; Jie Zhou",
        "abstract": "Subspace models have been widely used for appearance based object tracking. Most existing subspace based trackers employ a linear subspace to represent object appearances, which are not accurate enough to model large variations of objects. To address this, this paper presents a local subspace collaborative tracking method for robust visual tracking, where multiple linear and nonlinear subspaces are learned to better model the nonlinear relationship of object appearances. First, we retain a set of key samples and compute a set of local subspaces for each key sample. Then, we construct a hyper sphere to represent the local nonlinear subspace for each key sample. The hyper sphere of one key sample passes the local key samples and also is tangent to the local linear subspace of the specific key sample. In this way, we are able to represent the nonlinear distribution of the key samples and also approximate the local linear subspace near the specific key sample, so that local distributions of the samples can be represented more accurately. Experimental results on challenging video sequences demonstrate the effectiveness of our method.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Ma_Local_Subspace_Collaborative_ICCV_2015_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2117929,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13810207985274172233&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Ma_Local_Subspace_Collaborative_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Ma_2015_ICCV,\n    \n    author = {\n    Ma,\n    Lin and Zhang,\n    Xiaoqin and Hu,\n    Weiming and Xing,\n    Junliang and Lu,\n    Jiwen and Zhou,\n    Jie\n},\n    title = {\n    Local Subspace Collaborative Tracking\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "6e7dc84f15",
        "title": "Localize Me Anywhere, Anytime: A Multi-Task Point-Retrieval Approach",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Guoyu Lu, Yan Yan, Li Ren, Jingkuan Song, Nicu Sebe, Chandra Kambhamettu",
        "author": "Guoyu Lu; Yan Yan; Li Ren; Jingkuan Song; Nicu Sebe; Chandra Kambhamettu",
        "abstract": "Image-based localization is an essential complement to GPS localization. Current image-based localization methods are based on either 2D-to-3D or 3D-to-2D to find the correspondences, which ignore the real scene geometric attributes. The main contribution of our paper is that we use a 3D model reconstructed by a short video as the query to realize 3D-to-3D localization under a multi-task point retrieval framework. Firstly, the use of a 3D model as the query enables us to efficiently select location candidates. Furthermore, the reconstruction of 3D model exploits the correlations among different images, based on the fact that images captured from different views for SfM share information through matching features. By exploring shared information (matching features) across multiple related tasks (images of the same scene captured from different views), the visual feature's view-invariance property can be improved in order to get to a higher point retrieval accuracy. More specifically, we use multi-task point retrieval framework to explore the relationship between descriptors and the 3D points, which extracts the discriminant points for more accurate 3D-to-3D correspondences retrieval. We further apply multi-task learning (MTL) retrieval approach on thermal images to prove that our MTL retrieval framework also provides superior performance for the thermal domain. This application is exceptionally helpful to cope with the localization problem in an environment with limited light sources.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Lu_Localize_Me_Anywhere_ICCV_2015_paper.pdf",
        "aff": "VIMS Lab, Computer and Information Science Department, University of Delaware, USA; MHUG Lab, Department of Computer Science, University of Trento, Italy; VIMS Lab, Computer and Information Science Department, University of Delaware, USA; MHUG Lab, Department of Computer Science, University of Trento, Italy; MHUG Lab, Department of Computer Science, University of Trento, Italy; VIMS Lab, Computer and Information Science Department, University of Delaware, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 973690,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12168790294165858151&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "udel.edu;unitn.it;udel.edu;unitn.it;unitn.it;udel.edu",
        "email": "udel.edu;unitn.it;udel.edu;unitn.it;unitn.it;udel.edu",
        "author_num": 6,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Lu_Localize_Me_Anywhere_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;0;1;1;0",
        "aff_unique_norm": "University of Delaware;University of Trento",
        "aff_unique_dep": "Computer and Information Science Department;Department of Computer Science",
        "aff_unique_url": "https://www.udel.edu;https://www.unitn.it",
        "aff_unique_abbr": "UD;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;1;1;0",
        "aff_country_unique": "United States;Italy",
        "bibtex": "@InProceedings{Lu_2015_ICCV,\n    \n    author = {\n    Lu,\n    Guoyu and Yan,\n    Yan and Ren,\n    Li and Song,\n    Jingkuan and Sebe,\n    Nicu and Kambhamettu,\n    Chandra\n},\n    title = {\n    Localize Me Anywhere,\n    Anytime: A Multi-Task Point-Retrieval Approach\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "bbc1507458",
        "title": "Look and Think Twice: Capturing Top-Down Visual Attention With Feedback Convolutional Neural Networks",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Chunshui Cao, Xianming Liu, Yi Yang, Yinan Yu, Jiang Wang, Zilei Wang, Yongzhen Huang, Liang Wang, Chang Huang, Wei Xu, Deva Ramanan, Thomas S. Huang",
        "author": "Chunshui Cao; Xianming Liu; Yi Yang; Yinan Yu; Jiang Wang; Zilei Wang; Yongzhen Huang; Liang Wang; Chang Huang; Wei Xu; Deva Ramanan; Thomas S. Huang",
        "abstract": "While feedforward deep convolutional neural networks (CNNs) have been a great success in computer vision, it is important to remember that the human visual contex contains generally more feedback connections than foward connections. In this paper, we will briefly introduce the background of feedbacks in the human visual cortex, which motivates us to develop a computational feedback mechanism in the deep neural networks. The proposed networks perform inference from image features in a bottom-up manner as traditional convolutional networks; while during feedback loops it sets up high-level semantic labels as the agoala to infer the activation status of hidden layer neurons. The feedback networks help us better visualize and understand on how deep neural networks work as well as capture visual attention on expected objects, even in the images with cluttered background and multiple objects.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Cao_Look_and_Think_ICCV_2015_paper.pdf",
        "aff": ";;;;;;;;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1052393,
        "gs_citation": 530,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10486055372502967982&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": ";;;;;;;;;;;",
        "email": ";;;;;;;;;;;",
        "author_num": 12,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Cao_Look_and_Think_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Cao_2015_ICCV,\n    \n    author = {\n    Cao,\n    Chunshui and Liu,\n    Xianming and Yang,\n    Yi and Yu,\n    Yinan and Wang,\n    Jiang and Wang,\n    Zilei and Huang,\n    Yongzhen and Wang,\n    Liang and Huang,\n    Chang and Xu,\n    Wei and Ramanan,\n    Deva and Huang,\n    Thomas S.\n},\n    title = {\n    Look and Think Twice: Capturing Top-Down Visual Attention With Feedback Convolutional Neural Networks\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "fdcb5d1716",
        "title": "Lost Shopping! Monocular Localization in Large Indoor Spaces",
        "session": "3d representations for recognition and localization",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "Shenlong Wang, Sanja Fidler, Raquel Urtasun",
        "author": "Shenlong Wang; Sanja Fidler; Raquel Urtasun",
        "abstract": "In this paper we propose a novel approach to  localization  in very large indoor spaces (i.e., 200+ store shopping malls)  that takes   a single image and a floor plan of the environment as input.  We formulate  the localization problem as inference in a Markov random field, which jointly reasons about text detection (localizing shop's names in the image with precise bounding boxes), shop facade segmentation, as well as   camera's  rotation and translation within the entire shopping mall. The power of our approach is that it does not use any prior  information about appearance  and instead  exploits text detections corresponding to the shop names. This makes our method applicable to a variety of domains and robust to store appearance variation across countries, seasons, and illumination conditions. We demonstrate the performance of our approach in a new dataset we collected of two very large shopping malls, and show the power of holistic reasoning.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Wang_Lost_Shopping_Monocular_ICCV_2015_paper.pdf",
        "aff": "Department of Computer Science, University of Toronto; Department of Computer Science, University of Toronto; Department of Computer Science, University of Toronto",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1985126,
        "gs_citation": 101,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6164443088353166115&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "cs.toronto.edu;cs.toronto.edu;cs.toronto.edu",
        "email": "cs.toronto.edu;cs.toronto.edu;cs.toronto.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Wang_Lost_Shopping_Monocular_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Toronto",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utoronto.ca",
        "aff_unique_abbr": "U of T",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Toronto",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada",
        "bibtex": "@InProceedings{Wang_2015_ICCV,\n    \n    author = {\n    Wang,\n    Shenlong and Fidler,\n    Sanja and Urtasun,\n    Raquel\n},\n    title = {\n    Lost Shopping! Monocular Localization in Large Indoor Spaces\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "ccf889964a",
        "title": "Love Thy Neighbors: Image Annotation by Exploiting Image Metadata",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Justin Johnson, Lamberto Ballan, Li Fei-Fei",
        "author": "Justin Johnson; Lamberto Ballan; Li Fei-Fei",
        "abstract": "Some images that are difficult to recognize on their own may become more clear in the context of a neighborhood of related images with similar social-network metadata. We build on this intuition to improve multilabel image annotation. Our model uses image metadata nonparametrically to generate neighborhoods of related images using Jaccard similarities, then uses a deep neural network to blend visual information from the image and its neighbors. Prior work typically models image metadata parametrically; in contrast, our nonparametric treatment allows our model to perform well even when the vocabulary of metadata changes between training and testing. We perform comprehensive experiments on the NUS-WIDE dataset, where we show that our model outperforms state-of-the-art methods for multilabel image annotation even when our model is forced to generalize to new types of metadata.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Johnson_Love_Thy_Neighbors_ICCV_2015_paper.pdf",
        "aff": "Computer Science Department, Stanford University; Computer Science Department, Stanford University; Computer Science Department, Stanford University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1778428,
        "gs_citation": 158,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10233997843980298556&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Johnson_Love_Thy_Neighbors_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Johnson_2015_ICCV,\n    \n    author = {\n    Johnson,\n    Justin and Ballan,\n    Lamberto and Fei-Fei,\n    Li\n},\n    title = {\n    Love Thy Neighbors: Image Annotation by Exploiting Image Metadata\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "994ed1654e",
        "title": "Low Dimensional Explicit Feature Maps",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Ondřej Chum",
        "author": "Ondrej Chum",
        "abstract": "Approximating non-linear kernels by finite-dimensional feature maps is a popular approach for speeding up training and evaluation of support vector machines or to encode information into efficient match kernels. We propose a novel method of data independent construction of low dimensional feature maps. The problem is cast as a linear program which jointly considers competing objectives: the quality of the approximation and the dimensionality of the feature map.  For both shift-invariant and homogeneous kernels the proposed method achieves a better approximations at the same dimensionality or comparable approximations at lower dimensionality of the feature map compared with state-of-the-art methods.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Chum_Low_Dimensional_Explicit_ICCV_2015_paper.pdf",
        "aff": "CMP, Faculty of Electrical Engineering, CTU in Prague",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 950470,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11864027050938198672&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "cmp.felk.cvut.cz",
        "email": "cmp.felk.cvut.cz",
        "author_num": 1,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Chum_Low_Dimensional_Explicit_ICCV_2015_paper.html",
        "aff_unique_index": "0",
        "aff_unique_norm": "Czech Technical University in Prague",
        "aff_unique_dep": "Faculty of Electrical Engineering",
        "aff_unique_url": "https://www.fel.cvut.cz",
        "aff_unique_abbr": "CTU",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Czech Republic",
        "bibtex": "@InProceedings{Chum_2015_ICCV,\n    \n    author = {\n    Chum,\n    Ondrej\n},\n    title = {\n    Low Dimensional Explicit Feature Maps\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "df2690517d",
        "title": "Low-Rank Matrix Factorization Under General Mixture Noise Distributions",
        "session": "statistical methods and learning",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "Xiangyong Cao, Yang Chen, Qian Zhao, Deyu Meng, Yao Wang, Dong Wang, Zongben Xu",
        "author": "Xiangyong Cao; Yang Chen; Qian Zhao; Deyu Meng; Yao Wang; Dong Wang; Zongben Xu",
        "abstract": "Many computer vision problems can be posed as learning a low-dimensional subspace from high dimensional data. The low rank matrix factorization (LRMF) represents a commonly utilized subspace learning strategy. Most of the current LRMF techniques are constructed on the optimization problem using L_1 norm and L_2 norm, which mainly deal with Laplacian and Gaussian noise, respectively. To make LRMF capable of adapting more complex noise, this paper proposes a new LRMF model by assuming noise as Mixture of Exponential Power (MoEP) distributions and proposes a penalized MoEP model by combining the penalized likelihood method with MoEP distributions. Such setting facilitates the learned LRMF model capable of automatically fitting the real noise through MoEP distributions. Each component in this mixture is adapted from a series of preliminary  super- or sub-Gaussian candidates. An Expectation Maximization (EM) algorithm is also designed to infer the parameters involved in the proposed PMoEP model. The advantage of our method is demonstrated by extensive experiments on synthetic data, face modeling and hyperspectral image restoration.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Cao_Low-Rank_Matrix_Factorization_ICCV_2015_paper.pdf",
        "aff": ";;;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2856611,
        "gs_citation": 98,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15970622542610103323&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "author_num": 7,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Cao_Low-Rank_Matrix_Factorization_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Cao_2015_ICCV,\n    \n    author = {\n    Cao,\n    Xiangyong and Chen,\n    Yang and Zhao,\n    Qian and Meng,\n    Deyu and Wang,\n    Yao and Wang,\n    Dong and Xu,\n    Zongben\n},\n    title = {\n    Low-Rank Matrix Factorization Under General Mixture Noise Distributions\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "7b8f43c005",
        "title": "Low-Rank Tensor Approximation With Laplacian Scale Mixture Modeling for Multiframe Image Denoising",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Weisheng Dong, Guangyu Li, Guangming Shi, Xin Li, Yi Ma",
        "author": "Weisheng Dong; Guangyu Li; Guangming Shi; Xin Li; Yi Ma",
        "abstract": "Patch-based low-rank models have shown effective in exploiting spatial redundancy of natural images especially for the application of image denoising. However, two-dimensional low-rank model can not fully exploit the spatio-temporal correlation in larger data sets such as multispectral images and 3D MRIs. In this work, we propose a novel low-rank tensor approximation framework with Laplacian Scale Mixture (LSM)  modeling for multi-frame image denoising. First, similar 3D patches are grouped to form a tensor of d-order and high-order Singular Value Decomposition (HOSVD) is applied to the grouped tensor. Then the task of multiframe image denoising is formulated as a Maximum A Posterior (MAP) estimation problem with the LSM prior for tensor coefficients. Both unknown sparse coefficients and hidden LSM parameters can be efficiently estimated by the method of alternating optimization. Specifically, we have derived closed-form solutions for both subproblems. Experimental results on spectral and dynamic MRI images show that the proposed algorithm can better preserve the sharpness of important image structures and outperform several existing state-of-the-art multiframe denoising methods (e.g., BM4D and tensor dictionary learning).",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Dong_Low-Rank_Tensor_Approximation_ICCV_2015_paper.pdf",
        "aff": "Xidian University, China; Xidian University, China; Xidian University, China; West Virginia University; ShanghaiTech University, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 645449,
        "gs_citation": 82,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6816611586016228489&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "mail.xidian.edu.cn; ;xidian.edu.cn;ieee.org;shanghaitech.edu.cn",
        "email": "mail.xidian.edu.cn; ;xidian.edu.cn;ieee.org;shanghaitech.edu.cn",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Dong_Low-Rank_Tensor_Approximation_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;1;2",
        "aff_unique_norm": "Xidian University;West Virginia University;ShanghaiTech University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "http://www.xidian.edu.cn/;https://www.wvu.edu;http://www.shanghaitech.edu.cn",
        "aff_unique_abbr": "Xidian;WVU;ShanghaiTech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1;0",
        "aff_country_unique": "China;United States",
        "bibtex": "@InProceedings{Dong_2015_ICCV,\n    \n    author = {\n    Dong,\n    Weisheng and Li,\n    Guangyu and Shi,\n    Guangming and Li,\n    Xin and Ma,\n    Yi\n},\n    title = {\n    Low-Rank Tensor Approximation With Laplacian Scale Mixture Modeling for Multiframe Image Denoising\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "8686c00e03",
        "title": "Low-Rank Tensor Constrained Multiview Subspace Clustering",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Changqing Zhang, Huazhu Fu, Si Liu, Guangcan Liu, Xiaochun Cao",
        "author": "Changqing Zhang; Huazhu Fu; Si Liu; Guangcan Liu; Xiaochun Cao",
        "abstract": "In this paper, we explore the problem of multiview subspace clustering. We introduce a low-rank tensor constraint to explore the complementary information from multiple views and, accordingly, establish a novel method called Low-rank Tensor constrained Multiview Subspace Clustering (LT-MSC). Our method regards the subspace representation matrices of different views as a tensor, which captures dexterously the high order correlations underlying multiview data. Then the tensor is equipped with a low-rank constraint, which models elegantly the cross information among different views, reduces effectually the redundancy of the learned subspace representations, and improves the accuracy of clustering as well. The inference process of the affinity matrix for clustering is formulated as a tensor nuclear norm minimization problem, constrained with an additional L2,1-norm regularizer and some linear equalities. The minimization problem is convex and thus can be solved efficiently by an Augmented Lagrangian Alternating Direction Minimization (AL-ADM) method. Extensive experimental results on four benchmark datasets show the effectiveness of our proposed LT-MSC method.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Zhang_Low-Rank_Tensor_Constrained_ICCV_2015_paper.pdf",
        "aff": "School of Computer Science and Technology, Tianjin University; Institute for Infocomm Research, Agency for Science, Technology and Research, Singapore; State Key Laboratory of Information Security, IIE, Chinese Academy of Sciences; School of Information and Control, Nanjing University of Information Science and Technology; School of Computer Science and Technology, Tianjin University+State Key Laboratory of Information Security, IIE, Chinese Academy of Sciences",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 7388272,
        "gs_citation": 617,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5951113005512544590&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "tju.edu.cn; ; ; ;iie.ac.cn",
        "email": "tju.edu.cn; ; ; ;iie.ac.cn",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Zhang_Low-Rank_Tensor_Constrained_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;2;3;0+2",
        "aff_unique_norm": "Tianjin University;Agency for Science, Technology and Research;Chinese Academy of Sciences;Nanjing University of Information Science and Technology",
        "aff_unique_dep": "School of Computer Science and Technology;Institute for Infocomm Research;State Key Laboratory of Information Security;School of Information and Control",
        "aff_unique_url": "http://www.tju.edu.cn;https://www.a-star.edu.sg;http://www.ucas.ac.cn;http://www.nuist.edu.cn",
        "aff_unique_abbr": "Tianjin University;A*STAR;CAS;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0;0+0",
        "aff_country_unique": "China;Singapore",
        "bibtex": "@InProceedings{Zhang_2015_ICCV,\n    \n    author = {\n    Zhang,\n    Changqing and Fu,\n    Huazhu and Liu,\n    Si and Liu,\n    Guangcan and Cao,\n    Xiaochun\n},\n    title = {\n    Low-Rank Tensor Constrained Multiview Subspace Clustering\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "4c4c11c168",
        "title": "MANTRA: Minimum Maximum Latent Structural SVM for Image Classification and Ranking",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Thibaut Durand, Nicolas Thome, Matthieu Cord",
        "author": "Thibaut Durand; Nicolas Thome; Matthieu Cord",
        "abstract": "In this work, we propose a novel Weakly Supervised  Learning (WSL) framework dedicated to learn discriminative part detectors from images annotated with a global label. Our WSL method encompasses three main contributions. Firstly, we introduce a new structured output latent variable model, Minimum mAximum lateNt sTRucturAl SVM (MANTRA), which prediction relies on a pair of latent variables: h^+ (resp. h^-) provides positive (resp. negative) evidence for a given output y. Secondly, we instantiate MANTRA for two different visual recognition tasks: multi-class classification and ranking. For ranking, we propose efficient solutions to exactly solve the inference and the loss-augmented problems. Finally, extensive experiments highlight the relevance of the proposed method: MANTRA outperforms state-of-the art results on five different datasets.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Durand_MANTRA_Minimum_Maximum_ICCV_2015_paper.pdf",
        "aff": "Sorbonne Universit ´es, UPMC Univ Paris 06, CNRS, LIP6 UMR 7606, 4 place Jussieu, 75005 Paris; Sorbonne Universit ´es, UPMC Univ Paris 06, CNRS, LIP6 UMR 7606, 4 place Jussieu, 75005 Paris; Sorbonne Universit ´es, UPMC Univ Paris 06, CNRS, LIP6 UMR 7606, 4 place Jussieu, 75005 Paris",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1711585,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14082985993529344911&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff_domain": "lip6.fr;lip6.fr;lip6.fr",
        "email": "lip6.fr;lip6.fr;lip6.fr",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Durand_MANTRA_Minimum_Maximum_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Sorbonne Universités",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.sorbonne-universite.fr",
        "aff_unique_abbr": "Sorbonne",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Paris",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "France",
        "bibtex": "@InProceedings{Durand_2015_ICCV,\n    \n    author = {\n    Durand,\n    Thibaut and Thome,\n    Nicolas and Cord,\n    Matthieu\n},\n    title = {\n    MANTRA: Minimum Maximum Latent Structural SVM for Image Classification and Ranking\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "7cea0f1e91",
        "title": "MAP Disparity Estimation Using Hidden Markov Trees",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Eric T. Psota, Jędrzej Kowalczuk, Mateusz Mittek, Lance C. Pérez",
        "author": "Eric T. Psota; Jedrzej Kowalczuk; Mateusz Mittek; Lance C. Perez",
        "abstract": "A new method is introduced for stereo matching that operates on minimum spanning trees (MSTs) generated from the images. Disparity maps are represented as a collection of hidden states on MSTs, and each MST is modeled as a hidden Markov tree. An efficient recursive message-passing scheme designed to operate on hidden Markov trees, known as the upward-downward algorithm, is used to compute the maximum a posteriori (MAP) disparity estimate at each pixel. The messages processed by the upward-downward algorithm involve two types of probabilities: the probability of a pixel having a particular disparity given a set of per-pixel matching costs, and the probability of a disparity transition between a pair of connected pixels given their similarity. The distributions of these probabilities are modeled from a collection of images with ground truth disparities. Performance evaluation using the Middlebury stereo benchmark version 3 demonstrates that the proposed method ranks second and third in terms of overall accuracy when evaluated on the training and test image sets, respectively.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Psota_MAP_Disparity_Estimation_ICCV_2015_paper.pdf",
        "aff": "Department of Electrical & Computer Engineering, University of Nebraska-Lincoln; Department of Electrical & Computer Engineering, University of Nebraska-Lincoln; Department of Electrical & Computer Engineering, University of Nebraska-Lincoln; Department of Electrical & Computer Engineering, University of Nebraska-Lincoln",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1215780,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9147777734110275299&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "unl.edu;unl.edu;unl.edu;unl.edu",
        "email": "unl.edu;unl.edu;unl.edu;unl.edu",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Psota_MAP_Disparity_Estimation_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Nebraska-Lincoln",
        "aff_unique_dep": "Department of Electrical & Computer Engineering",
        "aff_unique_url": "https://www.unl.edu",
        "aff_unique_abbr": "UNL",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Lincoln",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Psota_2015_ICCV,\n    \n    author = {\n    Psota,\n    Eric T. and Kowalczuk,\n    Jedrzej and Mittek,\n    Mateusz and Perez,\n    Lance C.\n},\n    title = {\n    MAP Disparity Estimation Using Hidden Markov Trees\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "cae18e1487",
        "title": "ML-MG: Multi-Label Learning With Missing Labels Using a Mixed Graph",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Baoyuan Wu, Siwei Lyu, Bernard Ghanem",
        "author": "Baoyuan Wu; Siwei Lyu; Bernard Ghanem",
        "abstract": "This work focuses on the problem of multi-label learning with missing labels (MLML), which aims to label each test instance with multiple class labels given training instances that have an incomplete/partial set of these labels (i.e. some of their labels are missing). To handle missing labels, we propose a unified model of label dependencies by constructing a mixed graph, which jointly incorporates (i) instance-level similarity and class co-occurrence as undirected edges and (ii) semantic label hierarchy as directed edges. Unlike most MLML methods, We formulate this learning problem transductively as a convex quadratic matrix optimization problem that encourages training label consistency and encodes both types of label dependencies (i.e. undirected and directed edges) using quadratic terms and hard linear constraints. The alternating direction method of multipliers (ADMM) can be used to exactly and efficiently solve this problem. To evaluate our proposed method, we consider two popular applications (image and video annotation), where the label hierarchy can be derived from Wordnet. Experimental results show that our method achieves a significant improvement over state-of-the-art methods in performance and robustness to missing labels.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Wu_ML-MG_Multi-Label_Learning_ICCV_2015_paper.pdf",
        "aff": "KAUST, Saudi Arabia; University at Albany, SUNY, Albany, NY, USA; KAUST, Saudi Arabia",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2095042,
        "gs_citation": 124,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12085390243381078088&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "kaust.edu.sa;albany.edu;kaust.edu.sa",
        "email": "kaust.edu.sa;albany.edu;kaust.edu.sa",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Wu_ML-MG_Multi-Label_Learning_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "King Abdullah University of Science and Technology;University at Albany, SUNY",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.kaust.edu.sa;https://www.albany.edu",
        "aff_unique_abbr": "KAUST;UAlbany",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Albany",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Saudi Arabia;United States",
        "bibtex": "@InProceedings{Wu_2015_ICCV,\n    \n    author = {\n    Wu,\n    Baoyuan and Lyu,\n    Siwei and Ghanem,\n    Bernard\n},\n    title = {\n    ML-MG: Multi-Label Learning With Missing Labels Using a Mixed Graph\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "704391eabc",
        "title": "MMSS: Multi-Modal Sharable and Specific Feature Learning for RGB-D Object Recognition",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Anran Wang, Jianfei Cai, Jiwen Lu, Tat-Jen Cham",
        "author": "Anran Wang; Jianfei Cai; Jiwen Lu; Tat-Jen Cham",
        "abstract": "Most of the feature-learning methods for RGB-D object recognition either learn features from color and depth modalities separately, or simply treat RGB-D as undifferentiated four-channel data, which cannot adequately exploit the relationship between different modalities. Motivated by the intuition that different modalities should contain not only some modal-specific patterns but also some shared common patterns, we propose a multi-modal feature learning framework for RGB-D object recognition. We first construct deep CNN layers for color and depth separately, and then connect them with our carefully designed multi-modal layers, which fuse color and depth information by enforcing a common part to be shared by features of different modalities. In this way, we obtain features reflecting shared properties as well as modal-specific properties in different modalities. The information of the multi-modal learning frameworks is back-propagated to the early CNN layers. Experimental results show that our proposed multi-modal feature learning method outperforms state-of-the-art approaches on two widely used RGB-D object benchmark datasets.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Wang_MMSS_Multi-Modal_Sharable_ICCV_2015_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 118,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=47165821705646649&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Wang_MMSS_Multi-Modal_Sharable_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Wang_2015_ICCV,\n    \n    author = {\n    Wang,\n    Anran and Cai,\n    Jianfei and Lu,\n    Jiwen and Cham,\n    Tat-Jen\n},\n    title = {\n    MMSS: Multi-Modal Sharable and Specific Feature Learning for RGB-D Object Recognition\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "ed43ce130b",
        "title": "Massively Parallel Multiview Stereopsis by Surface Normal Diffusion",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Silvano Galliani, Katrin Lasinger, Konrad Schindler",
        "author": "Silvano Galliani; Katrin Lasinger; Konrad Schindler",
        "abstract": "We present a new, massively parallel method for high-quality multiview matching. Our work builds on the Patchmatch idea: starting from randomly generated 3D planes in scene space, the best-fitting planes are iteratively propagated and refined to obtain a 3D depth and normal field per view, such that a robust photo-consistency measure over all images is maximized. Our main novelties are on the one hand to formulate Patchmatch in scene space, which makes it possible to aggregate image similarity across multiple views and obtain more accurate depth maps. And on the other hand a modified, diffusion-like propagation scheme that can be massively parallelized and delivers dense multiview correspondence over ten 1.9-Megapixel images in 3 seconds, on a consumer-grade GPU. Our method uses a slanted support window and thus has no fronto-parallel bias; it is completely local and parallel, such that computation time scales linearly with image size, and inversely proportional to the number of parallel threads. Furthermore, it has low memory footprint (four values per pixel, independent of the depth range). It therefore scales exceptionally well and can handle multiple large images at high depth resolution. Experiments on the DTU and Middlebury multiview datasets as well as oblique aerial images show that our method achieves very competitive results with high accuracy and completeness, across a range of different scenarios.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Galliani_Massively_Parallel_Multiview_ICCV_2015_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2103566,
        "gs_citation": 685,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11990525219590730724&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Galliani_Massively_Parallel_Multiview_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Galliani_2015_ICCV,\n    \n    author = {\n    Galliani,\n    Silvano and Lasinger,\n    Katrin and Schindler,\n    Konrad\n},\n    title = {\n    Massively Parallel Multiview Stereopsis by Surface Normal Diffusion\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "69ad14c6fa",
        "title": "Matrix Backpropagation for Deep Networks With Structured Layers",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Catalin Ionescu, Orestis Vantzos, Cristian Sminchisescu",
        "author": "Catalin Ionescu; Orestis Vantzos; Cristian Sminchisescu",
        "abstract": "Deep neural network architectures have recently produced excellent results in a variety of areas in artificial intelligence and visual recognition, well surpassing traditional shallow architectures trained using hand-designed features. The power of deep networks stems both from their ability to perform local computations followed by pointwise non-linearities over increasingly larger receptive fields, and from the simplicity and scalability of the gradient-descent training procedure based on backpropagation. An open problem is the inclusion of layers that perform global, structured matrix computations like segmentation (e.g. normalized cuts) or higher-order pooling (e.g. log-tangent space metrics defined over the manifold of symmetric positive definite matrices) while preserving the validity and efficiency of an end-to-end deep training framework. In this paper we propose a sound mathematical apparatus to formally integrate global structured computation into deep computation architectures. At the heart of our methodology is the development of the theory and practice of backpropagation that generalizes to the calculus of adjoint matrix variations. We perform segmentation experiments using the BSDS and MSCOCO  benchmarks and demonstrate that deep networks relying on second-order pooling and normalized cuts layers, trained end-to-end using matrix backpropagation, outperform counterparts that do not take  advantage of such global layers.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Ionescu_Matrix_Backpropagation_for_ICCV_2015_paper.pdf",
        "aff": "Institute of Mathematics of the Romanian Academy+Institute for Numerical Simulation, University of Bonn; Institute for Numerical Simulation, University of Bonn; Department of Mathematics, Faculty of Engineering, Lund University+Institute for Numerical Simulation, University of Bonn",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1265070,
        "gs_citation": 344,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17387807402435828231&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "ins.uni-bonn.de;ins.uni-bonn.de;math.lth.se",
        "email": "ins.uni-bonn.de;ins.uni-bonn.de;math.lth.se",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Ionescu_Matrix_Backpropagation_for_ICCV_2015_paper.html",
        "aff_unique_index": "0+1;1;2+1",
        "aff_unique_norm": "Romanian Academy;University of Bonn;Lund University",
        "aff_unique_dep": "Institute of Mathematics;Institute for Numerical Simulation;Department of Mathematics",
        "aff_unique_url": "https://www.math.ro/;https://www.uni-bonn.de;https://www.lunduniversity.lu.se",
        "aff_unique_abbr": "IMAR;;LU",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;1;2+1",
        "aff_country_unique": "Romania;Germany;Sweden",
        "bibtex": "@InProceedings{Ionescu_2015_ICCV,\n    \n    author = {\n    Ionescu,\n    Catalin and Vantzos,\n    Orestis and Sminchisescu,\n    Cristian\n},\n    title = {\n    Matrix Backpropagation for Deep Networks With Structured Layers\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "766b04ed26",
        "title": "Maximum-Margin Structured Learning With Deep Networks for 3D Human Pose Estimation",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Sijin Li, Weichen Zhang, Antoni B. Chan",
        "author": "Sijin Li; Weichen Zhang; Antoni B. Chan",
        "abstract": "This paper focuses on structured-output learning using deep neural networks for 3D human pose estimation from monocular images. Our network takes an image and 3D pose as inputs and outputs a score value, which is high when the image-pose pair matches and low otherwise. The network structure consists of a convolutional neural network for image feature extraction, followed by two sub-networks for transforming the image features and pose into a joint embedding. The score function is then the dot-product between the image and pose embeddings. The image-pose embedding and score function are jointly trained using a maximum-margin cost function. Our proposed framework can be interpreted as a special form of structured support vector machines where the joint feature space is discriminatively learned using deep neural networks.  We test our framework on the Human3.6m dataset and obtain state-of-the-art results compared to other recent methods. Finally, we present visualizations of the image-pose embedding space, demonstrating the network has learned a high-level embedding of body-orientation and pose-configuration.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Li_Maximum-Margin_Structured_Learning_ICCV_2015_paper.pdf",
        "aff": "Department of Computer Science, City University of Hong Kong; Department of Computer Science, City University of Hong Kong; Department of Computer Science, City University of Hong Kong",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 5213189,
        "gs_citation": 295,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=947101803932781146&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff_domain": "my.cityu.edu.hk;my.cityu.edu.hk;cityu.edu.hk",
        "email": "my.cityu.edu.hk;my.cityu.edu.hk;cityu.edu.hk",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Li_Maximum-Margin_Structured_Learning_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "City University of Hong Kong",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.cityu.edu.hk",
        "aff_unique_abbr": "CityU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Li_2015_ICCV,\n    \n    author = {\n    Li,\n    Sijin and Zhang,\n    Weichen and Chan,\n    Antoni B.\n},\n    title = {\n    Maximum-Margin Structured Learning With Deep Networks for 3D Human Pose Estimation\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "e3f9ea0a8a",
        "title": "Merging the Unmatchable: Stitching Visually Disconnected SfM Models",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Andrea Cohen, Torsten Sattler, Marc Pollefeys",
        "author": "Andrea Cohen; Torsten Sattler; Marc Pollefeys",
        "abstract": "Recent advances in Structure-from-Motion not only enable the reconstruction of large scale scenes, but are also able to detect ambiguous structures caused by repeating elements that might result in incorrect reconstructions. Yet, it is not always possible to fully reconstruct a scene. The images required to merge different sub-models might be missing or it might be impossible to acquire such images in the first place due to occlusions or the structure of the scene. The problem of aligning multiple reconstructions that do not have visual overlap is impossible to solve in general. An important variant of this problem is the case in which individual sides of a building can be reconstructed but not joined due to the missing visual overlap. In this paper, we present a combinatorial approach for solving this variant by automatically stitching multiple sides of a building together. Our approach exploits symmetries and semantic information to reason about the possible geometric relations between the individual  models. We show that our approach is able to reconstruct complete building models where traditional SfM ends up with disconnected building sides.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Cohen_Merging_the_Unmatchable_ICCV_2015_paper.pdf",
        "aff": "Department of Computer Science, ETH Zurich, Switzerland; Department of Computer Science, ETH Zurich, Switzerland; Department of Computer Science, ETH Zurich, Switzerland",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2102705,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1281937638060983422&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "inf.ethz.ch;inf.ethz.ch;inf.ethz.ch",
        "email": "inf.ethz.ch;inf.ethz.ch;inf.ethz.ch",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Cohen_Merging_the_Unmatchable_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "ETH Zurich",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ethz.ch",
        "aff_unique_abbr": "ETHZ",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Switzerland",
        "bibtex": "@InProceedings{Cohen_2015_ICCV,\n    \n    author = {\n    Cohen,\n    Andrea and Sattler,\n    Torsten and Pollefeys,\n    Marc\n},\n    title = {\n    Merging the Unmatchable: Stitching Visually Disconnected SfM Models\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "5c767d07aa",
        "title": "MeshStereo: A Global Stereo Model With Mesh Alignment Regularization for View Interpolation",
        "session": "registration, alignment and stereo",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "Chi Zhang, Zhiwei Li, Yanhua Cheng, Rui Cai, Hongyang Chao, Yong Rui",
        "author": "Chi Zhang; Zhiwei Li; Yanhua Cheng; Rui Cai; Hongyang Chao; Yong Rui",
        "abstract": "We present a novel global stereo model designed for view interpolation. Unlike existing stereo models which only output a disparity map, our model is able to output a 3D triangular mesh, which can be directly used for view interpolation. To this aim, we partition the input stereo images into 2D triangles with shared vertices. Lifting the 2D triangulation to 3D naturally generates a corresponding mesh. A technical difficulty is to properly split vertices to multiple copies when they appear at depth discontinuous boundaries. To deal with this problem, we formulate our objective as a two-layer MRF, with the upper layer modeling the splitting properties of the vertices and the lower layer optimizing a region-based stereo matching. Experiments on the Middlebury and the Herodion datasets demonstrate that our model is able to synthesize visually coherent new view angles with high PSNR, as well as outputting high quality disparity maps which rank at the first place on the new challenging high resolution Middlebury 3.0 benchmark.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Zhang_MeshStereo_A_Global_ICCV_2015_paper.pdf",
        "aff": "Microsoft Research; Sun Yat-Sen University + SYSU-CMU Shunde International Joint Research Institute, P.R. China; Institute of Automation, Chinese Academy of Sciences; Microsoft Research; Sun Yat-Sen University + SYSU-CMU Shunde International Joint Research Institute, P.R. China; Microsoft Research",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2603102,
        "gs_citation": 206,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9879974104541449435&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "; ; ; ; ; ",
        "email": "; ; ; ; ; ",
        "author_num": 6,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Zhang_MeshStereo_A_Global_ICCV_2015_paper.html",
        "aff_unique_index": "0;1+2;3;0;1+2;0",
        "aff_unique_norm": "Microsoft Corporation;Sun Yat-Sen University;SYSU-CMU Shunde International Joint Research Institute;Chinese Academy of Sciences",
        "aff_unique_dep": "Microsoft Research;;;Institute of Automation",
        "aff_unique_url": "https://www.microsoft.com/en-us/research;http://www.sysu.edu.cn/;;http://www.ia.cas.cn",
        "aff_unique_abbr": "MSR;SYSU;;CAS",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Shunde",
        "aff_country_unique_index": "0;1+1;1;0;1+1;0",
        "aff_country_unique": "United States;China",
        "bibtex": "@InProceedings{Zhang_2015_ICCV,\n    \n    author = {\n    Zhang,\n    Chi and Li,\n    Zhiwei and Cheng,\n    Yanhua and Cai,\n    Rui and Chao,\n    Hongyang and Rui,\n    Yong\n},\n    title = {\n    MeshStereo: A Global Stereo Model With Mesh Alignment Regularization for View Interpolation\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "8878026bea",
        "title": "Minimal Solvers for 3D Geometry From Satellite Imagery",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Enliang Zheng, Ke Wang, Enrique Dunn, Jan-Michael Frahm",
        "author": "Enliang Zheng; Ke Wang; Enrique Dunn; Jan-Michael Frahm",
        "abstract": "We propose two novel minimal solvers which advance the state of the art in satellite imagery processing. Our methods are efficient and do not rely on the prior existence of complex inverse mapping functions to correlate 2D image coordinates and 3D terrain. Our first solver improves on the stereo correspondence problem for satellite imagery, in that we provide an exact image-to-object space mapping (where prior methods were inaccurate). Our second solver provides a novel mechanism for 3D point triangulation, which has improved robustness and accuracy over prior techniques. Given the usefulness and ubiquity of satellite imagery, our proposed methods allow for improved results in a variety of existing and future applications.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Zheng_Minimal_Solvers_for_ICCV_2015_paper.pdf",
        "aff": "The University of North Carolina at Chapel Hill; The University of North Carolina at Chapel Hill; The University of North Carolina at Chapel Hill; The University of North Carolina at Chapel Hill",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 6479732,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7316611597252768736&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "cs.unc.edu;cs.unc.edu;cs.unc.edu;cs.unc.edu",
        "email": "cs.unc.edu;cs.unc.edu;cs.unc.edu;cs.unc.edu",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Zheng_Minimal_Solvers_for_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of North Carolina at Chapel Hill",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.unc.edu",
        "aff_unique_abbr": "UNC Chapel Hill",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Chapel Hill",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Zheng_2015_ICCV,\n    \n    author = {\n    Zheng,\n    Enliang and Wang,\n    Ke and Dunn,\n    Enrique and Frahm,\n    Jan-Michael\n},\n    title = {\n    Minimal Solvers for 3D Geometry From Satellite Imagery\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "128c2ef0d4",
        "title": "Minimizing Human Effort in Interactive Tracking by Incremental Learning of Model Parameters",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Arridhana Ciptadi, James M. Rehg",
        "author": "Arridhana Ciptadi; James M. Rehg",
        "abstract": "We address the problem of minimizing human effort in interactive tracking by learning sequence-specific model parameters. Determining the optimal model parameters for each sequence is a critical problem in tracking. We demonstrate that by using the optimal model parameters for each sequence we can achieve high precision tracking results with significantly less effort. We leverage the sequential nature of interactive tracking to formulate an efficient method for learning model parameters through a maximum margin framework. By using our method we are able to save 60-90% of human effort to achieve high precision on two datasets: the VIRAT dataset and an Infant-Mother Interaction dataset.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Ciptadi_Minimizing_Human_Effort_ICCV_2015_paper.pdf",
        "aff": "School of Interactive Computing, Georgia Institute of Technology; School of Interactive Computing, Georgia Institute of Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1497776,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5598219595991335130&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "gatech.edu;gatech.edu",
        "email": "gatech.edu;gatech.edu",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Ciptadi_Minimizing_Human_Effort_ICCV_2015_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Georgia Institute of Technology",
        "aff_unique_dep": "School of Interactive Computing",
        "aff_unique_url": "https://www.gatech.edu",
        "aff_unique_abbr": "Georgia Tech",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Atlanta",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Ciptadi_2015_ICCV,\n    \n    author = {\n    Ciptadi,\n    Arridhana and Rehg,\n    James M.\n},\n    title = {\n    Minimizing Human Effort in Interactive Tracking by Incremental Learning of Model Parameters\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "e9d0221301",
        "title": "Minimum Barrier Salient Object Detection at 80 FPS",
        "session": "segmentation, edges and saliency",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "Jianming Zhang, Stan Sclaroff, Zhe Lin, Xiaohui Shen, Brian Price, Radomír Měch",
        "author": "Jianming Zhang; Stan Sclaroff; Zhe Lin; Xiaohui Shen; Brian Price; Radomir Mech",
        "abstract": "We propose a highly efficient, yet powerful, salient object detection method based on the Minimum Barrier Distance (MBD) Transform. The MBD transform is robust to pixel-value fluctuation, and thus can be effectively applied on raw pixels without region abstraction. We present an approximate MBD transform algorithm with 100X speedup over the exact algorithm. An error bound analysis is also provided. Powered by this fast MBD transform algorithm, the proposed salient object detection method runs at 80 FPS, and significantly outperforms previous methods with similar speed on four large benchmark datasets, and achieves comparable or better performance than state-of-the-art methods. Furthermore, a technique based on color whitening is proposed to extend our method to leverage the appearance-based backgroundness cue. This extended version further improves the performance, while still being one order of magnitude faster than all the other leading methods.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Zhang_Minimum_Barrier_Salient_ICCV_2015_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1336606,
        "gs_citation": 518,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15940698674242234772&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Zhang_Minimum_Barrier_Salient_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Zhang_2015_ICCV,\n    \n    author = {\n    Zhang,\n    Jianming and Sclaroff,\n    Stan and Lin,\n    Zhe and Shen,\n    Xiaohui and Price,\n    Brian and Mech,\n    Radomir\n},\n    title = {\n    Minimum Barrier Salient Object Detection at 80 FPS\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "6ba6c74349",
        "title": "Mining And-Or Graphs for Graph Matching and Object Discovery",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Quanshi Zhang, Ying Nian Wu, Song-Chun Zhu",
        "author": "Quanshi Zhang; Ying Nian Wu; Song-Chun Zhu",
        "abstract": "This paper reformulates the theory of graph mining on the technical basis of graph matching, and extends its scope of applications to computer vision. Given a set of attributed relational graphs (ARGs), we propose to use a hierarchical And-Or Graph (AoG) to model the pattern of maximal-size common subgraphs embedded in the ARGs, and we develop a general method to mine the AoG model from the unlabeled ARGs. This method provides a general solution to the problem of mining hierarchical models from unannotated visual data without exhaustive search of objects. We apply our method to RGB/RGB-D images and videos to demonstrate its generality and the wide range of applicability. The code will be available at https://sites.google.com/site/quanshizhang/mining-and-or-graphs.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Zhang_Mining_And-Or_Graphs_ICCV_2015_paper.pdf",
        "aff": "University of California, Los Angeles; University of California, Los Angeles; University of California, Los Angeles",
        "project": "https://sites.google.com/site/quanshizhang/mining-and-or-graphs",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2081535,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:Qr1L7sQGGZIJ:scholar.google.com/&scioq=Mining+And-Or+Graphs+for+Graph+Matching+and+Object+Discovery&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff_domain": "ucla.edu;ucla.edu;ucla.edu",
        "email": "ucla.edu;ucla.edu;ucla.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Zhang_Mining_And-Or_Graphs_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Los Angeles",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ucla.edu",
        "aff_unique_abbr": "UCLA",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Zhang_2015_ICCV,\n    \n    author = {\n    Zhang,\n    Quanshi and Wu,\n    Ying Nian and Zhu,\n    Song-Chun\n},\n    title = {\n    Mining And-Or Graphs for Graph Matching and Object Discovery\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "ffb7f66998",
        "title": "Mode-Seeking on Hypergraphs for Robust Geometric Model Fitting",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Hanzi Wang, Guobao Xiao, Yan Yan, David Suter",
        "author": "Hanzi Wang; Guobao Xiao; Yan Yan; David Suter",
        "abstract": "In this paper, we propose a novel geometric model fitting method, called Mode-Seeking on Hypergraphs (MSH), to deal with multi-structure data even in the presence of severe outliers. The proposed method formulates geometric model fitting as a mode seeking problem on a hypergraph in which vertices represent model hypotheses and hyperedges denote data points. MSH intuitively detects model instances by a simple and effective mode seeking algorithm. In addition to the mode seeking algorithm, MSH includes a similarity measure between vertices on the hypergraph and a \"weight-aware sampling\" technique. The proposed method not only alleviates sensitivity to the data distribution, but also is scalable to large scale problems. Experimental results further demonstrate that the proposed method has significant superiority over the state-of-the-art fitting methods on both synthetic data and real images.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Wang_Mode-Seeking_on_Hypergraphs_ICCV_2015_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 5846357,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4505431030745551243&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Wang_Mode-Seeking_on_Hypergraphs_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Wang_2015_ICCV,\n    \n    author = {\n    Wang,\n    Hanzi and Xiao,\n    Guobao and Yan,\n    Yan and Suter,\n    David\n},\n    title = {\n    Mode-Seeking on Hypergraphs for Robust Geometric Model Fitting\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "1ccf44290f",
        "title": "Model-Based Tracking at 300Hz Using Raw Time-of-Flight Observations",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Jan Stühmer, Sebastian Nowozin, Andrew Fitzgibbon, Richard Szeliski, Travis Perry, Sunil Acharya, Daniel Cremers, Jamie Shotton",
        "author": "Jan Stuhmer; Sebastian Nowozin; Andrew Fitzgibbon; Richard Szeliski; Travis Perry; Sunil Acharya; Daniel Cremers; Jamie Shotton",
        "abstract": "Consumer depth cameras have dramatically improved our ability to track rigid, articulated, and deformable 3D objects in real-time. However, depth cameras have a limited temporal resolution (frame-rate) that restricts the accuracy and robustness of tracking, especially for fast or unpredictable motion. In this paper, we show how to perform model-based object tracking which allows to reconstruct the object's depth at an order of magnitude higher frame-rate through simple modifications to an off-the-shelf depth camera. We focus on phase-based time-of-flight (ToF) sensing, which reconstructs each low frame-rate depth image from a set of short exposure 'raw' infrared captures. These raw captures are taken in quick succession near the beginning of each depth frame, and differ in the modulation of their active illumination. We make two contributions. First, we detail how to perform model-based tracking against these raw captures. Second, we show that by reprogramming the camera to space the raw captures uniformly in time, we obtain a 10x higher frame-rate, and thereby improve the ability to track fast-moving objects.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Stuhmer_Model-Based_Tracking_at_ICCV_2015_paper.pdf",
        "aff": ";;;;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 567282,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9592279085074884299&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 14,
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "author_num": 8,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Stuhmer_Model-Based_Tracking_at_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Stuhmer_2015_ICCV,\n    \n    author = {\n    Stuhmer,\n    Jan and Nowozin,\n    Sebastian and Fitzgibbon,\n    Andrew and Szeliski,\n    Richard and Perry,\n    Travis and Acharya,\n    Sunil and Cremers,\n    Daniel and Shotton,\n    Jamie\n},\n    title = {\n    Model-Based Tracking at 300Hz Using Raw Time-of-Flight Observations\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "3f5ea1950e",
        "title": "Monocular Object Instance Segmentation and Depth Ordering With CNNs",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Ziyu Zhang, Alexander G. Schwing, Sanja Fidler, Raquel Urtasun",
        "author": "Ziyu Zhang; Alexander G. Schwing; Sanja Fidler; Raquel Urtasun",
        "abstract": "In this paper we tackle the problem of instance-level segmentation and depth ordering from a single monocular image. Towards this goal, we take advantage of convolutional neural nets and train them to directly predict instance-level segmentations where the instance ID encodes the depth ordering within image patches. To provide a coherent single explanation of an image we develop a Markov random field which takes as input the predictions of convolutional neural nets applied at overlapping patches of different resolutions, as well as the output of a connected component algorithm. It aims to predict accurate instance-level segmentation and depth ordering. We demonstrate the effectiveness of our approach on the challenging KITTI benchmark and show good performance on both tasks.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Zhang_Monocular_Object_Instance_ICCV_2015_paper.pdf",
        "aff": "Department of Computer Science, University of Toronto; Department of Computer Science, University of Toronto; Department of Computer Science, University of Toronto; Department of Computer Science, University of Toronto",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1664616,
        "gs_citation": 194,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7431213548054053779&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "cs.toronto.edu;cs.toronto.edu;cs.toronto.edu;cs.toronto.edu",
        "email": "cs.toronto.edu;cs.toronto.edu;cs.toronto.edu;cs.toronto.edu",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Zhang_Monocular_Object_Instance_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Toronto",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utoronto.ca",
        "aff_unique_abbr": "U of T",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Toronto",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Canada",
        "bibtex": "@InProceedings{Zhang_2015_ICCV,\n    \n    author = {\n    Zhang,\n    Ziyu and Schwing,\n    Alexander G. and Fidler,\n    Sanja and Urtasun,\n    Raquel\n},\n    title = {\n    Monocular Object Instance Segmentation and Depth Ordering With CNNs\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "860682238d",
        "title": "Motion Trajectory Segmentation via Minimum Cost Multicuts",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Margret Keuper, Bjoern Andres, Thomas Brox",
        "author": "Margret Keuper; Bjoern Andres; Thomas Brox",
        "abstract": "For the segmentation of moving objects in videos, the analysis of long-term point trajectories has been very popular recently. In this paper, we formulate the segmentation of a video sequence based on point trajectories as a minimum cost multicut problem. Unlike the commonly used spectral clustering formulation, the minimum cost multicut formulation gives natural rise to optimize not only for a cluster assignment but also for the number of clusters while allowing for varying cluster sizes. In this setup, we provide a method to create a long-term point trajectory graph with attractive and repulsive binary terms and outperform state-of-the-art methods based on spectral clustering on the FBMS-59 dataset and on the motion subtask of the VSB100 dataset.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Keuper_Motion_Trajectory_Segmentation_ICCV_2015_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1912856,
        "gs_citation": 242,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10994850522154185472&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Keuper_Motion_Trajectory_Segmentation_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Keuper_2015_ICCV,\n    \n    author = {\n    Keuper,\n    Margret and Andres,\n    Bjoern and Brox,\n    Thomas\n},\n    title = {\n    Motion Trajectory Segmentation via Minimum Cost Multicuts\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "ab17763341",
        "title": "Multi-Class Multi-Annotator Active Learning With Robust Gaussian Process for Visual Recognition",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Chengjiang Long, Gang Hua",
        "author": "Chengjiang Long; Gang Hua",
        "abstract": "Active learning is an effective way to relieve the tedious work of manual annotation in many applications of visual recognition. However, less research attention has been focused on multi-class active learning. In this paper, we propose a novel Gaussian process classifier model with multiple annotators for multi-class visual recognition. Expectation propagation (EP) is adopted for efficient approximate Bayesian inference of our probabilistic model for classification. Based on the EP approximation inference, a generalized Expectation Maximization (GEM) algorithm is derived to estimate both the parameters for instances and the quality of each individual annotator. Also, we incorporate the idea of reinforcement learning to actively select both the informative samples and the high-quality annotators, which better explores the trade-off between exploitation and exploration. The experiments clearly demonstrate the efficacy of the proposed model.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Long_Multi-Class_Multi-Annotator_Active_ICCV_2015_paper.pdf",
        "aff": "Stevens Institute of Technology; Microsoft Research Asia",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 829445,
        "gs_citation": 80,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15082882904244247019&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "stevens.edu;gmail.com",
        "email": "stevens.edu;gmail.com",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Long_Multi-Class_Multi-Annotator_Active_ICCV_2015_paper.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Stevens Institute of Technology;Microsoft Research",
        "aff_unique_dep": ";Research",
        "aff_unique_url": "https://www.stevens.edu;https://www.microsoft.com/en-us/research/group/asia",
        "aff_unique_abbr": "SIT;MSR Asia",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Asia",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United States;China",
        "bibtex": "@InProceedings{Long_2015_ICCV,\n    \n    author = {\n    Long,\n    Chengjiang and Hua,\n    Gang\n},\n    title = {\n    Multi-Class Multi-Annotator Active Learning With Robust Gaussian Process for Visual Recognition\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "ff7c3ed18e",
        "title": "Multi-Conditional Latent Variable Model for Joint Facial Action Unit Detection",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Stefanos Eleftheriadis, Ognjen Rudovic, Maja Pantic",
        "author": "Stefanos Eleftheriadis; Ognjen Rudovic; Maja Pantic",
        "abstract": "We propose a novel multi-conditional latent variable model for simultaneous facial feature fusion and detection of facial action units. In our approach we exploit the structure-discovery capabilities of generative models such as Gaussian processes, and the discriminative power of classifiers such as logistic function. This leads to superior performance compared to existing classifiers for the target task that exploit either the discriminative or generative property, but not both. The model learning is performed via an efficient, newly proposed Bayesian learning strategy based on Monte Carlo sampling. Consequently, the learned model is robust to data overfitting, regardless of the number of both input features and jointly estimated facial action units. Extensive qualitative and quantitative experimental evaluations are performed on three publicly available datasets (CK+, Shoulder-pain and DISFA). We show that the proposed model outperforms the state-of-the-art methods for the target task on (i) feature fusion, and (ii) multiple facial action unit detection.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Eleftheriadis_Multi-Conditional_Latent_Variable_ICCV_2015_paper.pdf",
        "aff": "Department of Computing, Imperial College London, UK+ EEMCS, University of Twente, The Netherlands; Department of Computing, Imperial College London, UK+ EEMCS, University of Twente, The Netherlands; Department of Computing, Imperial College London, UK+ EEMCS, University of Twente, The Netherlands",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 925489,
        "gs_citation": 115,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13615927014481698193&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff_domain": "imperial.ac.uk;imperial.ac.uk;imperial.ac.uk",
        "email": "imperial.ac.uk;imperial.ac.uk;imperial.ac.uk",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Eleftheriadis_Multi-Conditional_Latent_Variable_ICCV_2015_paper.html",
        "aff_unique_index": "0+1;0+1;0+1",
        "aff_unique_norm": "Imperial College London;University of Twente",
        "aff_unique_dep": "Department of Computing;EEMCS",
        "aff_unique_url": "https://www.imperial.ac.uk;https://www.utwente.nl",
        "aff_unique_abbr": "Imperial;UT",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "London;",
        "aff_country_unique_index": "0+1;0+1;0+1",
        "aff_country_unique": "United Kingdom;Netherlands",
        "bibtex": "@InProceedings{Eleftheriadis_2015_ICCV,\n    \n    author = {\n    Eleftheriadis,\n    Stefanos and Rudovic,\n    Ognjen and Pantic,\n    Maja\n},\n    title = {\n    Multi-Conditional Latent Variable Model for Joint Facial Action Unit Detection\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "bdbb5e4825",
        "title": "Multi-Cue Structure Preserving MRF for Unconstrained Video Segmentation",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Saehoon Yi, Vladimir Pavlovic",
        "author": "Saehoon Yi; Vladimir Pavlovic",
        "abstract": "Video segmentation is a stepping stone to understanding video context.  Video segmentation enables one to represent a video by decomposing it into coherent regions which comprise whole or parts of objects.  However, the challenge originates from the fact that most of the video segmentation algorithms are based on unsupervised learning due to expensive cost of pixelwise video annotation and intra-class variability within similar unconstrained video classes. We propose a Markov Random Field model for unconstrained video segmentation that relies on tight integration of multiple cues: vertices are defined from contour based superpixels, unary potentials from temporally smooth label likelihood and pairwise potentials from global structure of a video.  Multi-cue structure is a breakthrough to extracting coherent object regions for unconstrained videos in absence of supervision.  Our experiments on VSB100 dataset show that the proposed model significantly outperforms competing state-of-the-art algorithms.  Qualitative analysis illustrates that video segmentation result of the proposed model is consistent with human perception of objects.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Yi_Multi-Cue_Structure_Preserving_ICCV_2015_paper.pdf",
        "aff": ";",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1237189,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14877766926505296526&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Yi_Multi-Cue_Structure_Preserving_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Yi_2015_ICCV,\n    \n    author = {\n    Yi,\n    Saehoon and Pavlovic,\n    Vladimir\n},\n    title = {\n    Multi-Cue Structure Preserving MRF for Unconstrained Video Segmentation\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "2972f2aff4",
        "title": "Multi-Image Matching via Fast Alternating Minimization",
        "session": "motion and correspondence",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "Xiaowei Zhou, Menglong Zhu, Kostas Daniilidis",
        "author": "Xiaowei Zhou; Menglong Zhu; Kostas Daniilidis",
        "abstract": "In this paper we propose a global optimization-based approach to jointly matching a set of images. The estimated correspondences simultaneously maximize pairwise feature affinities and cycle consistency across multiple images. Unlike previous convex methods relying on semidefinite programming, we formulate the problem as a low-rank matrix recovery problem and show that the desired semidefiniteness of a solution can be spontaneously fulfilled. The low-rank formulation enables us to derive a fast alternating minimization algorithm in order to handle practical problems with thousands of features. Both simulation and real experiments demonstrate that the proposed algorithm can achieve a competitive performance with an order of magnitude speedup compared to the state-of-the-art algorithm. In the end, we demonstrate the applicability of the proposed method to match the images of different object instances and as a result the potential to reconstruct category-specific object models from those images.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Zhou_Multi-Image_Matching_via_ICCV_2015_paper.pdf",
        "aff": "GRASP Laboratory, University of Pennsylvania; GRASP Laboratory, University of Pennsylvania; GRASP Laboratory, University of Pennsylvania",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2689697,
        "gs_citation": 178,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12235027989718717959&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "seas.upenn.edu;seas.upenn.edu;seas.upenn.edu",
        "email": "seas.upenn.edu;seas.upenn.edu;seas.upenn.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Zhou_Multi-Image_Matching_via_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Pennsylvania",
        "aff_unique_dep": "GRASP Laboratory",
        "aff_unique_url": "https://www.upenn.edu",
        "aff_unique_abbr": "UPenn",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Philadelphia",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Zhou_2015_ICCV,\n    \n    author = {\n    Zhou,\n    Xiaowei and Zhu,\n    Menglong and Daniilidis,\n    Kostas\n},\n    title = {\n    Multi-Image Matching via Fast Alternating Minimization\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "375984fd4a",
        "title": "Multi-Kernel Correlation Filter for Visual Tracking",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Ming Tang, Jiayi Feng",
        "author": "Ming Tang; Jiayi Feng",
        "abstract": "Correlation filter based trackers are ranked top in terms of performances. Nevertheless, they only employ a single kernel at a time. In this paper, we will derive a multi-kernel correlation filter (MKCF) based tracker which fully takes advantage of the invariance-discriminative power spectrums of various features to further improve the performance. Moreover, it may easily introduce location and representation errors to search several discrete scales for the proper one of the object bounding box, because normally the discrete candidate scales are determined and the corresponding feature pyramid are generated ahead of searching. In this paper, we will propose a novel and efficient scale estimation method based on optimal bisection search and fast evaluation of features. Our scale estimation method is the first one that uses the truly minimal number of layers of feature pyramid and avoids constructing the pyramid before searching for proper scales.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Tang_Multi-Kernel_Correlation_Filter_ICCV_2015_paper.pdf",
        "aff": "National Lab of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China; National Lab of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1312090,
        "gs_citation": 195,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9925652932186132318&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "email": "nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Tang_Multi-Kernel_Correlation_Filter_ICCV_2015_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Automation",
        "aff_unique_url": "http://www.ia.cas.cn",
        "aff_unique_abbr": "CAS",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Tang_2015_ICCV,\n    \n    author = {\n    Tang,\n    Ming and Feng,\n    Jiayi\n},\n    title = {\n    Multi-Kernel Correlation Filter for Visual Tracking\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "94f85a8d92",
        "title": "Multi-Label Cross-Modal Retrieval",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Viresh Ranjan, Nikhil Rasiwasia, C. V. Jawahar",
        "author": "Viresh Ranjan; Nikhil Rasiwasia; C. V. Jawahar",
        "abstract": "In this work, we address the problem of cross-modal retrieval in presence of multi-label annotations.  In particular, we introduce multi-label Canonical Correlation Analysis (ml-CCA), an extension of CCA, for learning shared subspaces taking into account high level semantic information in the form of multi-label annotations. Unlike CCA, ml-CCA does not rely on explicit pairing between modalities, instead it uses the multi-label information to establish correspondences.  This results in a discriminative subspace which is better suited for cross-modal retrieval tasks.  We also present Fast ml-CCA, a computationally efficient version of ml-CCA, which is able to handle large scale datasets.  We show the efficacy of our approach by conducting extensive cross-modal retrieval experiments on three standard benchmark datasets. The results show that the proposed approach achieves state of the art retrieval performance on the three datasets.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Ranjan_Multi-Label_Cross-Modal_Retrieval_ICCV_2015_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1046411,
        "gs_citation": 265,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15175875269477650622&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Ranjan_Multi-Label_Cross-Modal_Retrieval_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Ranjan_2015_ICCV,\n    \n    author = {\n    Ranjan,\n    Viresh and Rasiwasia,\n    Nikhil and Jawahar,\n    C. V.\n},\n    title = {\n    Multi-Label Cross-Modal Retrieval\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "3318f0fec0",
        "title": "Multi-Scale Learning for Low-Resolution Person Re-Identification",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Xiang Li, Wei-Shi Zheng, Xiaojuan Wang, Tao Xiang, Shaogang Gong",
        "author": "Xiang Li; Wei-Shi Zheng; Xiaojuan Wang; Tao Xiang; Shaogang Gong",
        "abstract": "In real world person re-identification (re-id), images of people captured at very different resolutions from different locations need be matched. Existing re-id models typically normalise all person images to the same size. However, a low-resolution (LR) image contains much less information about a person, and direct image scaling and simple size normalisation as done in conventional re-id methods cannot compensate for the loss of information. To solve this LR person re-id problem, we propose a novel joint multi-scale learning framework, termed joint multi-scale discriminant component analysis (JUDEA). The key component of this framework is a heterogeneous class mean discrepancy (HCMD) criterion for cross-scale image domain alignment, which is optimised simultaneously with discriminant modelling across multiple scales in the joint learning framework. Our experiments show that the proposed JUDEA framework outperforms existing representative re-id methods as well as other related LR visual matching models applied for the LR person re-id problem.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Li_Multi-Scale_Learning_for_ICCV_2015_paper.pdf",
        "aff": "School of Information Science and Technology, Sun Yat-sen University, China; School of Information Science and Technology, Sun Yat-sen University, China; School of Information Science and Technology, Sun Yat-sen University, China; School of Electronic Engineering and Computer Science, Queen Mary University of London, UK; School of Electronic Engineering and Computer Science, Queen Mary University of London, UK",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1024056,
        "gs_citation": 196,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15959319403364734211&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": "gmail.com;ieee.org;gmail.com;qmul.ac.uk;qmul.ac.uk",
        "email": "gmail.com;ieee.org;gmail.com;qmul.ac.uk;qmul.ac.uk",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Li_Multi-Scale_Learning_for_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;1;1",
        "aff_unique_norm": "Sun Yat-sen University;Queen Mary University of London",
        "aff_unique_dep": "School of Information Science and Technology;School of Electronic Engineering and Computer Science",
        "aff_unique_url": "http://www.sysu.edu.cn/;https://www.qmul.ac.uk",
        "aff_unique_abbr": "SYSU;QMUL",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";London",
        "aff_country_unique_index": "0;0;0;1;1",
        "aff_country_unique": "China;United Kingdom",
        "bibtex": "@InProceedings{Li_2015_ICCV,\n    \n    author = {\n    Li,\n    Xiang and Zheng,\n    Wei-Shi and Wang,\n    Xiaojuan and Xiang,\n    Tao and Gong,\n    Shaogang\n},\n    title = {\n    Multi-Scale Learning for Low-Resolution Person Re-Identification\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "44d81fc28c",
        "title": "Multi-Scale Recognition With DAG-CNNs",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Songfan Yang, Deva Ramanan",
        "author": "Songfan Yang; Deva Ramanan",
        "abstract": "We explore multi-scale convolutional neural nets (CNNs) for image classification. Contemporary approaches extract features from a single output layer. By extracting features from multiple layers, one can simultaneously reason about high, mid, and low-level features during classification. The resulting multi-scale architecture can itself be seen as a feed-forward model that is structured as a directed acyclic graph (DAG-CNNs). We use DAG-CNNs to learn a set of multi-scale features that can be effectively shared between coarse and fine-grained classification tasks. While fine-tuning such models helps performance, we show that even \"off-the-self\" multi-scale features perform quite well. We present extensive analysis and demonstrate state-of-the-art classification performance on three standard scene benchmarks (SUN397, MIT67, and Scene15). In terms of the heavily benchmarked MIT67 and Scene15 datasets, our results reduce the lowest previously-reported error by 23.9% and 9.5%, respectively.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Yang_Multi-Scale_Recognition_With_ICCV_2015_paper.pdf",
        "aff": "College of Electronics and Information Engineering, Sichuan University, China; Robotics Institute, Carnegie Mellon University, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2037538,
        "gs_citation": 258,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10295037766026645208&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "scu.edu.cn;cs.cmu.edu",
        "email": "scu.edu.cn;cs.cmu.edu",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Yang_Multi-Scale_Recognition_With_ICCV_2015_paper.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Sichuan University;Carnegie Mellon University",
        "aff_unique_dep": "College of Electronics and Information Engineering;Robotics Institute",
        "aff_unique_url": "https://www.scu.edu.cn;https://www.cmu.edu",
        "aff_unique_abbr": ";CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "China;United States",
        "bibtex": "@InProceedings{Yang_2015_ICCV,\n    \n    author = {\n    Yang,\n    Songfan and Ramanan,\n    Deva\n},\n    title = {\n    Multi-Scale Recognition With DAG-CNNs\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "58cf72043e",
        "title": "Multi-Task Learning With Low Rank Attribute Embedding for Person Re-Identification",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Chi Su, Fan Yang, Shiliang Zhang, Qi Tian, Larry S. Davis, Wen Gao",
        "author": "Chi Su; Fan Yang; Shiliang Zhang; Qi Tian; Larry S. Davis; Wen Gao",
        "abstract": "We propose a novel Multi-Task Learning with Low Rank Attribute Embedding (MTL-LORAE) framework for person re-identification. Re-identifications from multiple cameras are regarded as related tasks to exploit shared information to improve re-identification accuracy. Both low level features and semantic/data-driven attributes are utilized. Since attributes are generally correlated, we introduce a low rank attribute embedding into the MTL formulation to embed original binary attributes to a continuous attribute space, where incorrect and incomplete attributes are rectified and recovered to better describe people. The learning objective function consists of a quadratic loss regarding class labels and an attribute embedding error, which is solved by an alternating optimization procedure. Experiments on three person re-identification datasets have demonstrated that MTL-LORAE outperforms existing approaches by a large margin and produces state-of-the-art results.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Su_Multi-Task_Learning_With_ICCV_2015_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 641141,
        "gs_citation": 204,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8344764239527381370&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Su_Multi-Task_Learning_With_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Su_2015_ICCV,\n    \n    author = {\n    Su,\n    Chi and Yang,\n    Fan and Zhang,\n    Shiliang and Tian,\n    Qi and Davis,\n    Larry S. and Gao,\n    Wen\n},\n    title = {\n    Multi-Task Learning With Low Rank Attribute Embedding for Person Re-Identification\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "7de4fbd59e",
        "title": "Multi-Task Recurrent Neural Network for Immediacy Prediction",
        "session": "vision and people",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "Xiao Chu, Wanli Ouyang, Wei Yang, Xiaogang Wang",
        "author": "Xiao Chu; Wanli Ouyang; Wei Yang; Xiaogang Wang",
        "abstract": "In this paper, we propose to predict immediacy for interacting persons from still images. A complete immediacy set includes interactions, relative distance, body leaning direction and standing orientation. These measures are found to be related to the attitude, social relationship, social interaction, action, nationality, and religion of the communicators. A large-scale dataset with 10,000 images is constructed, in which all the immediacy measures and the human poses are annotated. We propose a rich set of immediacy representations that help to predict immediacy from imperfect 1-person and 2-person pose estimation results. A multi-task deep recurrent neural network is constructed to take the proposed rich immediacy representation as input and learn the complex relationship among immediacy predictions multiple steps of refinement. The effectiveness of the proposed approach is proved through extensive experiments on the large scale dataset.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Chu_Multi-Task_Recurrent_Neural_ICCV_2015_paper.pdf",
        "aff": "Department of Electronic Engineering, The Chinese University of Hong Kong; Department of Electronic Engineering, The Chinese University of Hong Kong; Department of Electronic Engineering, The Chinese University of Hong Kong; Department of Electronic Engineering, The Chinese University of Hong Kong",
        "project": "http://www.ee.cuhk.edu.hk/~xgwang/projectpage_immediacy.html",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1699041,
        "gs_citation": 67,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16335715250536256406&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "ee.cuhk.edu.hk;ee.cuhk.edu.hk;ee.cuhk.edu.hk;ee.cuhk.edu.hk",
        "email": "ee.cuhk.edu.hk;ee.cuhk.edu.hk;ee.cuhk.edu.hk;ee.cuhk.edu.hk",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Chu_Multi-Task_Recurrent_Neural_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "The Chinese University of Hong Kong",
        "aff_unique_dep": "Department of Electronic Engineering",
        "aff_unique_url": "https://www.cuhk.edu.hk",
        "aff_unique_abbr": "CUHK",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Chu_2015_ICCV,\n    \n    author = {\n    Chu,\n    Xiao and Ouyang,\n    Wanli and Yang,\n    Wei and Wang,\n    Xiaogang\n},\n    title = {\n    Multi-Task Recurrent Neural Network for Immediacy Prediction\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "9979b598d3",
        "title": "Multi-View Complementary Hash Tables for Nearest Neighbor Search",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Xianglong Liu, Lei Huang, Cheng Deng, Jiwen Lu, Bo Lang",
        "author": "Xianglong Liu; Lei Huang; Cheng Deng; Jiwen Lu; Bo Lang",
        "abstract": "Recent years have witnessed the success of hashing techniques in fast nearest neighbor search. In practice many applications (e.g., visual search, object detection, image matching, etc.) have enjoyed the benefits of complementary hash tables and information fusion over multiple views. However, most of prior research mainly focused on compact hash code cleaning, and rare work studies how to build multiple complementary hash tables, much less to adaptively integrate information stemming from multiple views. In this paper we first present a novel multi-view complementary hash table method that learns complementarity hash tables from the data with multiple views. For single multi-view table, using exemplar based feature fusion, we approximate the inherent data similarities with a low-rank matrix, and learn discriminative hash functions in an efficient way. To build complementary tables and meanwhile maintain scalable training and fast out-of-sample extension, an exemplar reweighting scheme is introduced to update the induced low-rank similarity in the sequential table construction framework, which indeed brings mutual benefits between tables by placing greater importance on exemplars shared by mis-separated neighbors. Extensive experiments on three large-scale image datasets demonstrate that the proposed method significantly outperforms various naive solutions and state-of-the-art multi-table methods.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Liu_Multi-View_Complementary_Hash_ICCV_2015_paper.pdf",
        "aff": "State Key Lab of Software Development Environment, Beihang University, Beijing, China; State Key Lab of Software Development Environment, Beihang University, Beijing, China; Xidian University, Xi’an, China; Department of Automation, Tsinghua University, Beijing, China; State Key Lab of Software Development Environment, Beihang University, Beijing, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1310023,
        "gs_citation": 60,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5556450958982929838&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "nlsde.buaa.edu.cn;nlsde.buaa.edu.cn;gmail.com;gmail.com;nlsde.buaa.edu.cn",
        "email": "nlsde.buaa.edu.cn;nlsde.buaa.edu.cn;gmail.com;gmail.com;nlsde.buaa.edu.cn",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Liu_Multi-View_Complementary_Hash_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;1;2;0",
        "aff_unique_norm": "Beihang University;Xidian University;Tsinghua University",
        "aff_unique_dep": "State Key Lab of Software Development Environment;;Department of Automation",
        "aff_unique_url": "http://www.buaa.edu.cn;http://www.xidian.edu.cn;https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "BUAA;Xidian;THU",
        "aff_campus_unique_index": "0;0;1;0;0",
        "aff_campus_unique": "Beijing;Xi'an",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Liu_2015_ICCV,\n    \n    author = {\n    Liu,\n    Xianglong and Huang,\n    Lei and Deng,\n    Cheng and Lu,\n    Jiwen and Lang,\n    Bo\n},\n    title = {\n    Multi-View Complementary Hash Tables for Nearest Neighbor Search\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "e17675c5b1",
        "title": "Multi-View Convolutional Neural Networks for 3D Shape Recognition",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Hang Su, Subhransu Maji, Evangelos Kalogerakis, Erik Learned-Miller",
        "author": "Hang Su; Subhransu Maji; Evangelos Kalogerakis; Erik Learned-Miller",
        "abstract": "A longstanding question in computer vision concerns the representation of 3D shapes for recognition: should 3D shapes be represented with descriptors operating on their native 3D formats, such as voxel grid or polygon mesh, or can they be effectively represented with view-based descriptors? We address this question in the context of learning to recognize 3D shapes from a collection of their rendered views on 2D images. We first present a standard CNN architecture trained to recognize the shapes' rendered views independently of each other, and show that a 3D shape can be recognized even from a single view at an accuracy far higher than using state-of-the-art 3D shape descriptors. Recognition rates further increase when multiple views of the shapes are provided. In addition, we present a novel CNN architecture that combines information from multiple views of a 3D shape into a single and compact shape descriptor offering even better recognition performance. The same architecture can be applied to accurately recognize human hand-drawn sketches of shapes. We conclude that a collection of 2D views can be highly informative for 3D shape recognition and is amenable to emerging CNN architectures and their derivatives.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Su_Multi-View_Convolutional_Neural_ICCV_2015_paper.pdf",
        "aff": "University of Massachusetts, Amherst; University of Massachusetts, Amherst; University of Massachusetts, Amherst; University of Massachusetts, Amherst",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1472858,
        "gs_citation": 4470,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4277219180010481756&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 23,
        "aff_domain": "cs.umass.edu;cs.umass.edu;cs.umass.edu;cs.umass.edu",
        "email": "cs.umass.edu;cs.umass.edu;cs.umass.edu;cs.umass.edu",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Su_Multi-View_Convolutional_Neural_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Massachusetts Amherst",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.umass.edu",
        "aff_unique_abbr": "UMass Amherst",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Amherst",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Su_2015_ICCV,\n    \n    author = {\n    Su,\n    Hang and Maji,\n    Subhransu and Kalogerakis,\n    Evangelos and Learned-Miller,\n    Erik\n},\n    title = {\n    Multi-View Convolutional Neural Networks for 3D Shape Recognition\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "6ad19683bb",
        "title": "Multi-View Domain Generalization for Visual Recognition",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Li Niu, Wen Li, Dong Xu",
        "author": "Li Niu; Wen Li; Dong Xu",
        "abstract": "In this paper, we propose a new multi-view domain generalization (MVDG) approach for visual recognition, in which we aim to use the source domain samples with multiple types of features (i.e., multi-view features) to learn robust classifiers that can generalize well to any unseen target domain. Considering the recent works show the domain generalization capability can be enhanced by fusing multiple SVM classifiers, we build upon exemplar SVMs to learn a set of SVM classifiers by using one positive sample and all negative samples in the source domain each time. When the source domain samples come from multiple latent domains, we expect the weight vectors of exemplar SVM classifiers can be organized into multiple hidden clusters. To exploit such cluster structure, we organize the weight vectors learnt on each view as a weight matrix and seek the low-rank representation by reconstructing this weight matrix using itself as the dictionary. To enforce the consistency of inherent cluster structures discovered from the weight matrices learnt on different views, we introduce a new regularizer to minimize the mismatch between any two representation matrices on different views. We also develop an efficient alternating optimization algorithm and further extend our MVDG approach for domain adaptation by exploiting the manifold structure of unlabeled target domain samples. Comprehensive experiments for visual recognition clearly demonstrate the effectiveness of our approaches for domain generalization and domain adaptation.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Niu_Multi-View_Domain_Generalization_ICCV_2015_paper.pdf",
        "aff": "Interdisciplinary Graduate School, Nanyang Technological University, Singapore; Computer Vision Laboratory, ETH Zurich, Switzerland; School of Computer Engineering, Nanyang Technological University, Singapore + School of Electrical and Information Engineering, The University of Sydney, Sydney, Australia",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 661961,
        "gs_citation": 81,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7877799648278710730&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "ntu.edu.sg;vision.ee.ethz.ch;gmail.com",
        "email": "ntu.edu.sg;vision.ee.ethz.ch;gmail.com",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Niu_Multi-View_Domain_Generalization_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;0+2",
        "aff_unique_norm": "Nanyang Technological University;ETH Zurich;The University of Sydney",
        "aff_unique_dep": "Interdisciplinary Graduate School;Computer Vision Laboratory;School of Electrical and Information Engineering",
        "aff_unique_url": "https://www.ntu.edu.sg;https://www.ethz.ch;https://www.sydney.edu.au",
        "aff_unique_abbr": "NTU;ETHZ;USYD",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Zurich;Sydney",
        "aff_country_unique_index": "0;1;0+2",
        "aff_country_unique": "Singapore;Switzerland;Australia",
        "bibtex": "@InProceedings{Niu_2015_ICCV,\n    \n    author = {\n    Niu,\n    Li and Li,\n    Wen and Xu,\n    Dong\n},\n    title = {\n    Multi-View Domain Generalization for Visual Recognition\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "bde84c6856",
        "title": "Multi-View Subspace Clustering",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Hongchang Gao, Feiping Nie, Xuelong Li, Heng Huang",
        "author": "Hongchang Gao; Feiping Nie; Xuelong Li; Heng Huang",
        "abstract": "For many computer vision applications, the data sets distribute on certain low-dimensional subspaces. Subspace clustering is to find such underlying subspaces and cluster the data points correctly. In this paper, we propose a novel multi-view subspace clustering method. The proposed method performs clustering on the subspace representation of each view simultaneously. Meanwhile, we propose to use a common cluster structure to guarantee the consistence among different views. In addition, an efficient algorithm is proposed to solve the problem. Experiments on four benchmark data sets have been performed to validate our proposed method. The promising results demonstrate the effectiveness of our method.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Gao_Multi-View_Subspace_Clustering_ICCV_2015_paper.pdf",
        "aff": "Computer Science and Engineering, University of Texas at Arlington, Arlington, TX, 76019, USA; Computer Science and Engineering, University of Texas at Arlington, Arlington, TX, 76019, USA; Center for OPTIMAL, XIOPM, Chinese Academy of Sciences, Xi’an, 710119, China; Computer Science and Engineering, University of Texas at Arlington, Arlington, TX, 76019, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1100491,
        "gs_citation": 673,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18096159011089020672&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "gmail.com;gmail.com;opt.ac.cn;uta.edu",
        "email": "gmail.com;gmail.com;opt.ac.cn;uta.edu",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Gao_Multi-View_Subspace_Clustering_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "University of Texas at Arlington;Chinese Academy of Sciences",
        "aff_unique_dep": "Computer Science and Engineering;Center for OPTIMAL",
        "aff_unique_url": "https://www.uta.edu;http://www.cas.cn",
        "aff_unique_abbr": "UTA;CAS",
        "aff_campus_unique_index": "0;0;1;0",
        "aff_campus_unique": "Arlington;Xi'an",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "United States;China",
        "bibtex": "@InProceedings{Gao_2015_ICCV,\n    \n    author = {\n    Gao,\n    Hongchang and Nie,\n    Feiping and Li,\n    Xuelong and Huang,\n    Heng\n},\n    title = {\n    Multi-View Subspace Clustering\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "44e1958812",
        "title": "Multimodal Convolutional Neural Networks for Matching Image and Sentence",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Lin Ma, Zhengdong Lu, Lifeng Shang, Hang Li",
        "author": "Lin Ma; Zhengdong Lu; Lifeng Shang; Hang Li",
        "abstract": "In this paper, we propose multimodal convolutional neural networks (m-CNNs) for matching image and sentence. Our m-CNN provides an end-to-end framework with convolutional architectures to exploit image representation, word composition, and the matching relations between the two modalities. More specifically, it consists of one image CNN encoding the image content and one matching CNN modeling the joint representation of image and sentence. The matching CNN composes different semantic fragments from words and learns the inter-modal relations between image and the composed fragments at different levels, thus fully exploit the matching relations between image and sentence. Experimental results demonstrate that the proposed m-CNNs can effectively capture the information necessary for image and sentence matching. More specifically, our proposed m-CNNs significantly outperform the state-of-the-art approaches for bidirectional image and sentence retrieval on the Flickr8K and Flickr30K datasets.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Ma_Multimodal_Convolutional_Neural_ICCV_2015_paper.pdf",
        "aff": "Noah’s Ark Lab, Huawei Technologies; Noah’s Ark Lab, Huawei Technologies; Noah’s Ark Lab, Huawei Technologies; Noah’s Ark Lab, Huawei Technologies",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 836337,
        "gs_citation": 447,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8242855245788745193&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 16,
        "aff_domain": "gmail.com;huawei.com;huawei.com;huawei.com",
        "email": "gmail.com;huawei.com;huawei.com;huawei.com",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Ma_Multimodal_Convolutional_Neural_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Huawei Technologies",
        "aff_unique_dep": "Noah’s Ark Lab",
        "aff_unique_url": "https://www.huawei.com",
        "aff_unique_abbr": "Huawei",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Ma_2015_ICCV,\n    \n    author = {\n    Ma,\n    Lin and Lu,\n    Zhengdong and Shang,\n    Lifeng and Li,\n    Hang\n},\n    title = {\n    Multimodal Convolutional Neural Networks for Matching Image and Sentence\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "afe0ad7edc",
        "title": "Multiple Feature Fusion via Weighted Entropy for Visual Tracking",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Lin Ma, Jiwen Lu, Jianjiang Feng, Jie Zhou",
        "author": "Lin Ma; Jiwen Lu; Jianjiang Feng; Jie Zhou",
        "abstract": "It is desirable to combine multiple feature descriptors to improve the visual tracking performance because different features can provide complementary information to describe objects of interest. However, how to effectively fuse multiple features remains a challenging problem in visual tracking, especially in a data-driven manner. In this paper, we propose a new data-adaptive visual tracking approach by using multiple feature fusion via weighted entropy. Unlike existing visual trackers which simply concatenate multiple feature vectors together for object representation, we employ the weighted entropy to evaluate the dissimilarity between the object state and the background state, and seek the optimal feature combination by minimizing the weighted entropy, so that more complementary information can be exploited for object representation. Experimental results demonstrate the effectiveness of our approach in tackling various challenges for visual object tracking.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Ma_Multiple_Feature_Fusion_ICCV_2015_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1714534,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12579524847629435722&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Ma_Multiple_Feature_Fusion_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Ma_2015_ICCV,\n    \n    author = {\n    Ma,\n    Lin and Lu,\n    Jiwen and Feng,\n    Jianjiang and Zhou,\n    Jie\n},\n    title = {\n    Multiple Feature Fusion via Weighted Entropy for Visual Tracking\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "bcfc692eec",
        "title": "Multiple Granularity Descriptors for Fine-Grained Categorization",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Dequan Wang, Zhiqiang Shen, Jie Shao, Wei Zhang, Xiangyang Xue, Zheng Zhang",
        "author": "Dequan Wang; Zhiqiang Shen; Jie Shao; Wei Zhang; Xiangyang Xue; Zheng Zhang",
        "abstract": "Fine-grained categorization, which aims to distinguish subordinate-level categories such as bird species or dog breeds, is an extremely challenging task. This is due to two main issues: how to localize discriminative regions for recognition and how to learn sophisticated features for representation. Neither of them is easy to handle if there is insufficient labeled data. We leverage the fact that a subordinate-level object already has other labels in its ontology tree. These \"free\" labels can be used to train a series of CNN-based classifiers, each specialized at one grain level. The internal representations of these networks have different region of interests, allowing the construction of multi-grained descriptors that encode informative and discriminative features covering all the grain levels. Our multiple granularity framework can be learned with the weakest supervision, requiring only image-level label and avoiding the use of labor-intensive bounding box or part annotations. Experimental results on three challenging fine-grained image datasets demonstrate that our approach outperforms state-of-the-art algorithms, including those requiring strong labels.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Wang_Multiple_Granularity_Descriptors_ICCV_2015_paper.pdf",
        "aff": "Shanghai Key Laboratory of Intelligent Information Processing, School of Computer Science, Fudan University; Shanghai Key Laboratory of Intelligent Information Processing, School of Computer Science, Fudan University; Shanghai Key Laboratory of Intelligent Information Processing, School of Computer Science, Fudan University; Shanghai Key Laboratory of Intelligent Information Processing, School of Computer Science, Fudan University; Shanghai Key Laboratory of Intelligent Information Processing, School of Computer Science, Fudan University; Department of Computer Science, New York University Shanghai",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 977427,
        "gs_citation": 286,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16113445214147377348&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;nyu.edu",
        "email": "fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;fudan.edu.cn;nyu.edu",
        "author_num": 6,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Wang_Multiple_Granularity_Descriptors_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0;0;1",
        "aff_unique_norm": "Fudan University;New York University Shanghai",
        "aff_unique_dep": "School of Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www.fudan.edu.cn;https://shanghai.nyu.edu",
        "aff_unique_abbr": "Fudan;NYU Shanghai",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Shanghai",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Wang_2015_ICCV,\n    \n    author = {\n    Wang,\n    Dequan and Shen,\n    Zhiqiang and Shao,\n    Jie and Zhang,\n    Wei and Xue,\n    Xiangyang and Zhang,\n    Zheng\n},\n    title = {\n    Multiple Granularity Descriptors for Fine-Grained Categorization\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "c96ed1da95",
        "title": "Multiple Hypothesis Tracking Revisited",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Chanho Kim, Fuxin Li, Arridhana Ciptadi, James M. Rehg",
        "author": "Chanho Kim; Fuxin Li; Arridhana Ciptadi; James M. Rehg",
        "abstract": "This paper revisits the classical multiple hypotheses tracking (MHT) algorithm in a tracking-by-detection framework. The success of MHT largely depends on the ability to maintain a small list of potential hypotheses, which can be facilitated with the accurate object detectors that are currently available. We demonstrate that a classical MHT implementation from the 90's can come surprisingly close to the performance of state-of-the-art methods on standard benchmark datasets. In order to further utilize the strength of MHT in exploiting higher-order information, we introduce a method for training online appearance models for each track hypothesis. We show that appearance models can be learned efficiently via a regularized least squares framework, requiring only a few extra operations for each hypothesis branch. We obtain state-of-the-art results on popular tracking-by-detection datasets such as PETS and the recent MOT challenge.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Kim_Multiple_Hypothesis_Tracking_ICCV_2015_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 873,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3234376622718270378&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Kim_Multiple_Hypothesis_Tracking_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Kim_2015_ICCV,\n    \n    author = {\n    Kim,\n    Chanho and Li,\n    Fuxin and Ciptadi,\n    Arridhana and Rehg,\n    James M.\n},\n    title = {\n    Multiple Hypothesis Tracking Revisited\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "7c333a4550",
        "title": "Multiple-Hypothesis Affine Region Estimation With Anisotropic LoG Filters",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Takahiro Hasegawa, Mitsuru Ambai, Kohta Ishikawa, Gou Koutaki, Yuji Yamauchi, Takayoshi Yamashita, Hironobu Fujiyoshi",
        "author": "Takahiro Hasegawa; Mitsuru Ambai; Kohta Ishikawa; Gou Koutaki; Yuji Yamauchi; Takayoshi Yamashita; Hironobu Fujiyoshi",
        "abstract": "We propose a method for estimating multiple-hypothesis affine regions from a keypoint by using an anisotropic Laplacian-of-Gaussian (LoG) filter. Although conventional affine region detectors, such as Hessian/Harris-Affine, iterate to find an affine region that fits a given image patch, such iterative searching is adversely affected by an initial point. To avoid this problem, we allow multiple detections from a single keypoint. We demonstrate that the responses of all possible anisotropic LoG filters can be efficiently computed by factorizing them in a similar manner to spectral SIFT. A large number of LoG filters that are densely sampled in a parameter space are reconstructed by a weighted combination of a limited number of representative filters, called ``eigenfilters\", by using singular value decomposition. Also, the reconstructed filter responses of the sampled parameters can be interpolated to a continuous representation by using a series of proper functions. This results in efficient multiple extrema searching in a continuous space. Experiments revealed that our method has higher repeatability than the conventional methods.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Hasegawa_Multiple-Hypothesis_Affine_Region_ICCV_2015_paper.pdf",
        "aff": ";;;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 6430760,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10333961515504429484&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "author_num": 7,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Hasegawa_Multiple-Hypothesis_Affine_Region_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Hasegawa_2015_ICCV,\n    \n    author = {\n    Hasegawa,\n    Takahiro and Ambai,\n    Mitsuru and Ishikawa,\n    Kohta and Koutaki,\n    Gou and Yamauchi,\n    Yuji and Yamashita,\n    Takayoshi and Fujiyoshi,\n    Hironobu\n},\n    title = {\n    Multiple-Hypothesis Affine Region Estimation With Anisotropic LoG Filters\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "b4214e2745",
        "title": "Multiresolution Hierarchy Co-Clustering for Semantic Segmentation in Sequences With Small Variations",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "David Varas, Mónica Alfaro, Ferran Marques",
        "author": "David Varas; Monica Alfaro; Ferran Marques",
        "abstract": "This paper presents a co-clustering technique that, given a collection of images and their hierarchies, clusters nodes from these hierarchies to obtain a coherent multiresolution representation of the image collection. We formalize the co-clustering as Quadratic Semi-Assignment Problem and solve it  with a linear programming relaxation approach that makes effective use of information from hierarchies. Initially, we address the problem of generating an optimal, coherent partition per image and, afterwards, we extend this method to a multiresolution framework. Finally, we particularize this framework to an iterative multiresolution video segmentation algorithm in sequences with small variations. We evaluate the algorithm on the Video Occlusion/Object Boundary Detection Dataset, showing that it produces state-of-the-art results in these scenarios.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Varas_Multiresolution_Hierarchy_Co-Clustering_ICCV_2015_paper.pdf",
        "aff": "Universitat Politècnica de Catalunya Barcelona Tech, Spain; Universitat Politècnica de Catalunya Barcelona Tech, Spain; Universitat Politècnica de Catalunya Barcelona Tech, Spain",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2043012,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10781163863543552583&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "upc.edu; ;upc.edu",
        "email": "upc.edu; ;upc.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Varas_Multiresolution_Hierarchy_Co-Clustering_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Universitat Politècnica de Catalunya Barcelona Tech",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.upc.edu",
        "aff_unique_abbr": "UPC",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Barcelona",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Spain",
        "bibtex": "@InProceedings{Varas_2015_ICCV,\n    \n    author = {\n    Varas,\n    David and Alfaro,\n    Monica and Marques,\n    Ferran\n},\n    title = {\n    Multiresolution Hierarchy Co-Clustering for Semantic Segmentation in Sequences With Small Variations\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "4717fea141",
        "title": "Mutual-Structure for Joint Filtering",
        "session": "computational photography and image enhancement",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "Xiaoyong Shen, Chao Zhou, Li Xu, Jiaya Jia",
        "author": "Xiaoyong Shen; Chao Zhou; Li Xu; Jiaya Jia",
        "abstract": "Previous joint/guided filters directly transfer the structural information in the reference image to the target one. In this paper, we first analyze its major drawback -- that is, there may be completely different edges in the two images. Simply passing all patterns to the target could introduce significant errors. To address this issue, we propose the concept of mutual-structure, which refers to the structural information that is contained in both images and thus can be safely enhanced by joint filtering, and an untraditional objective function that can be efficiently optimized to yield mutual structure. Our method results in necessary and important edge preserving, which greatly benefits depth completion, optical flow estimation, image enhancement, stereo matching, to name a few.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Shen_Mutual-Structure_for_Joint_ICCV_2015_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1817288,
        "gs_citation": 180,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12146431281788110980&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 21,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Shen_Mutual-Structure_for_Joint_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Shen_2015_ICCV,\n    \n    author = {\n    Shen,\n    Xiaoyong and Zhou,\n    Chao and Xu,\n    Li and Jia,\n    Jiaya\n},\n    title = {\n    Mutual-Structure for Joint Filtering\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "a2b9f92a8d",
        "title": "Naive Bayes Super-Resolution Forest",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Jordi Salvador, Eduardo Pérez-Pellitero",
        "author": "Jordi Salvador; Eduardo Perez-Pellitero",
        "abstract": "This paper presents a fast, high-performance method for super resolution with external learning. The first contribution leading to the excellent performance is a bimodal tree for clustering, which successfully exploits the antipodal invariance of the coarse-to-high-res mapping of natural image patches and provides scalability to finer partitions of the underlying coarse patch space. During training an ensemble of such bimodal trees is computed, providing different linearizations of the mapping. The second and main contribution is a fast inference algorithm, which selects the most suitable mapping function within the tree ensemble for each patch by adopting a Local Naive Bayes formulation. The experimental validation shows promising scalability properties that reflect the suitability of the proposed model, which may also be generalized to other tasks. The resulting method is beyond one order of magnitude faster and performs objectively and subjectively better than the current state of the art.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Salvador_Naive_Bayes_Super-Resolution_ICCV_2015_paper.pdf",
        "aff": "Technicolor R&I Hannover; Technicolor R&I Hannover",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1488967,
        "gs_citation": 151,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8139910932147038049&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "technicolor.com;technicolor.com",
        "email": "technicolor.com;technicolor.com",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Salvador_Naive_Bayes_Super-Resolution_ICCV_2015_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Technicolor",
        "aff_unique_dep": "R&I",
        "aff_unique_url": "https://www.technicolor.com",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hannover",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany",
        "bibtex": "@InProceedings{Salvador_2015_ICCV,\n    \n    author = {\n    Salvador,\n    Jordi and Perez-Pellitero,\n    Eduardo\n},\n    title = {\n    Naive Bayes Super-Resolution Forest\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "2064717cac",
        "title": "Near-Online Multi-Target Tracking With Aggregated Local Flow Descriptor",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Wongun Choi",
        "author": "Wongun Choi",
        "abstract": "In this paper, we tackle two key aspects of multiple target tracking problem: 1) designing an accurate affinity measure to associate detections and 2) implementing an efficient and accurate (near) online multiple target tracking algorithm. As for the first contribution, we introduce a novel Aggregated Local Flow Descriptor (ALFD) that encodes the relative motion pattern between a pair of temporally distant detections using long term interest point trajectories (IPTs). Leveraging on the IPTs, the ALFD provides a robust affinity measure for estimating the likelihood of matching detections regardless of the application scenarios. As for another contribution, we present a Near-Online Multi-target Tracking (NOMT) algorithm. The tracking problem is formulated as a data-association between targets and detections in a temporal window, that is performed repeatedly at every frame. While being efficient, NOMT achieves robustness via integrating multiple cues including ALFD metric, target dynamics, appearance similarity, and long term trajectory regularization into the model. Our ablative analysis verifies the superiority of the ALFD metric over the other conventional affinity metrics. We run a comprehensive experimental evaluation on two challenging tracking datasets, KITTI and MOT datasets. The NOMT method combined with ALFD metric achieves the best accuracy in both datasets with significant margins (about 10% higher MOTA) over the state-of-the-art.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Choi_Near-Online_Multi-Target_Tracking_ICCV_2015_paper.pdf",
        "aff": "NEC Laboratories America, Cupertino, CA, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2274838,
        "gs_citation": 546,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3120250776569516262&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": "nec-labs.com",
        "email": "nec-labs.com",
        "author_num": 1,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Choi_Near-Online_Multi-Target_Tracking_ICCV_2015_paper.html",
        "aff_unique_index": "0",
        "aff_unique_norm": "NEC Laboratories America",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.nec-labs.com",
        "aff_unique_abbr": "NEC Labs",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Cupertino",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Choi_2015_ICCV,\n    \n    author = {\n    Choi,\n    Wongun\n},\n    title = {\n    Near-Online Multi-Target Tracking With Aggregated Local Flow Descriptor\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "d0dd27189e",
        "title": "Neural Activation Constellations: Unsupervised Part Model Discovery With Convolutional Networks",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Marcel Simon, Erik Rodner",
        "author": "Marcel Simon; Erik Rodner",
        "abstract": "Part models of object categories are essential for challenging recognition tasks, where differences in categories are subtle and only reflected in appearances of small parts of the object. We present an approach that is able to learn part models in a  completely unsupervised manner, without part annotations and even without given bounding boxes during learning. The key idea is to find constellations of neural activation patterns computed using convolutional neural networks. In our experiments, we outperform existing approaches for fine-grained recognition on the CUB200-2011, Oxford PETS, and Oxford Flowers dataset in case no part or bounding box annotations are available and achieve state-of-the-art performance for the Stanford Dog dataset. We also show the benefits of neural constellation models as a data augmentation technique for fine-tuning. Furthermore, our paper unites the areas of generic and fine-grained classification, since our approach is suitable for both scenarios.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Simon_Neural_Activation_Constellations_ICCV_2015_paper.pdf",
        "aff": ";",
        "project": "http://www.inf-cv.uni-jena.de/constellation_model_revisited",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 731987,
        "gs_citation": 486,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4285624108192289380&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Simon_Neural_Activation_Constellations_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Simon_2015_ICCV,\n    \n    author = {\n    Simon,\n    Marcel and Rodner,\n    Erik\n},\n    title = {\n    Neural Activation Constellations: Unsupervised Part Model Discovery With Convolutional Networks\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "c836767a0f",
        "title": "Nighttime Haze Removal With Glow and Multiple Light Colors",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Yu Li, Robby T. Tan, Michael S. Brown",
        "author": "Yu Li; Robby T. Tan; Michael S. Brown",
        "abstract": "This paper focuses on dehazing nighttime images.  Most existing dehazing methods use models that are formulated to describe haze in daytime.  Daytime models assume a single uniform light color attributed to a light source not directly visible in the scene.  Nighttime scenes, however, commonly include visible lights sources with varying colors. These light sources also often introduce noticeable amounts of glow that is not present in daytime haze.   To address these effects, we introduce a new nighttime haze model that accounts for the varying light sources and their glow. Our model is a linear combination of three terms: the direct transmission, airlight and glow. The glow term represents light from the light sources that is scattered around before reaching the camera.  Based on the model, we propose a framework that first reduces the effect of the glow in the image, resulting in a nighttime image that consists of direct transmission and airlight only.  We then compute a spatially varying atmospheric light map that encodes light colors locally. This atmospheric map is used to predict the transmission, which we use to obtain our nighttime scene reflection image. We demonstrate the effectiveness of our nighttime haze model and correction method on a number of examples and compare our results with existing daytime and nighttime dehazing methods' results.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Li_Nighttime_Haze_Removal_ICCV_2015_paper.pdf",
        "aff": "National University of Singapore; Yale-NUS College; Advanced Digital Sciences Center",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1581539,
        "gs_citation": 329,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4322878443020855291&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "u.nus.edu;yale-nus.edu.sg;comp.nus.edu.sg",
        "email": "u.nus.edu;yale-nus.edu.sg;comp.nus.edu.sg",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Li_Nighttime_Haze_Removal_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "National University of Singapore;Yale-NUS College;Advanced Digital Sciences Center",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.nus.edu.sg;https://www.yale-nus.edu.sg;https://www.adsc.illinois.edu",
        "aff_unique_abbr": "NUS;Yale-NUS;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "Singapore;United States",
        "bibtex": "@InProceedings{Li_2015_ICCV,\n    \n    author = {\n    Li,\n    Yu and Tan,\n    Robby T. and Brown,\n    Michael S.\n},\n    title = {\n    Nighttime Haze Removal With Glow and Multiple Light Colors\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "d1055c7284",
        "title": "Non-Parametric Structure-Based Calibration of Radially Symmetric Cameras",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Federico Camposeco, Torsten Sattler, Marc Pollefeys",
        "author": "Federico Camposeco; Torsten Sattler; Marc Pollefeys",
        "abstract": "We propose a novel two-step method for estimating the intrinsic and extrinsic calibration of any radially symmetric camera, including non-central systems. The first step consists of estimating the camera pose, given a Structure from Motion (SfM) model, up to the translation along the optical axis. As a second step, we obtain the calibration by finding the translation of the camera center using an ordering constraint. The method makes use of the 1D radial camera model, which allows us to effectively handle any radially symmetric camera, including non-central ones. Using this ordering constraint, we show that the we are able to calibrate several different (central and non-central) Wide Field of View (WFOV) cameras, including fisheye, hyper-catadioptric and spherical catadioptric cameras, as well as pinhole cameras, using a single image or jointly solving for several views.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Camposeco_Non-Parametric_Structure-Based_Calibration_ICCV_2015_paper.pdf",
        "aff": "Department of Computer Science, ETH Zürich, Switzerland; Department of Computer Science, ETH Zürich, Switzerland; Department of Computer Science, ETH Zürich, Switzerland",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3161481,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13964309807049494252&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "inf.ethz.ch;inf.ethz.ch;inf.ethz.ch",
        "email": "inf.ethz.ch;inf.ethz.ch;inf.ethz.ch",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Camposeco_Non-Parametric_Structure-Based_Calibration_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "ETH Zürich",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ethz.ch",
        "aff_unique_abbr": "ETHZ",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Switzerland",
        "bibtex": "@InProceedings{Camposeco_2015_ICCV,\n    \n    author = {\n    Camposeco,\n    Federico and Sattler,\n    Torsten and Pollefeys,\n    Marc\n},\n    title = {\n    Non-Parametric Structure-Based Calibration of Radially Symmetric Cameras\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "72b84836a1",
        "title": "Object Detection Using Generalization and Efficiency Balanced Co-Occurrence Features",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Haoyu Ren, Ze-Nian Li",
        "author": "Haoyu Ren; Ze-Nian Li",
        "abstract": "In this paper, we propose a high-accuracy object detector based on co-occurrence features. Firstly, we introduce three kinds of local co-occurrence features constructed by the traditional Haar, LBP, and HOG respectively. Then the boosted detectors are learned, where each weak classifier corresponds to a local image region with a co-occurrence feature. In addition, we propose a Generalization and Efficiency Balanced (GEB) framework for boosting training. In the feature selection procedure, the discrimination ability, the generalization power, and the computation cost of the candidate features are all evaluated for decision. As a result, the boosted detector achieves both high accuracy and good efficiency. It also shows performance competitive with the state-of-the-art methods for pedestrian detection and general object detection tasks.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Ren_Object_Detection_Using_ICCV_2015_paper.pdf",
        "aff": ";",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1221912,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6712180849838494212&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Ren_Object_Detection_Using_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Ren_2015_ICCV,\n    \n    author = {\n    Ren,\n    Haoyu and Li,\n    Ze-Nian\n},\n    title = {\n    Object Detection Using Generalization and Efficiency Balanced Co-Occurrence Features\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "c02361b5be",
        "title": "Object Detection via a Multi-Region and Semantic Segmentation-Aware CNN Model",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Spyros Gidaris, Nikos Komodakis",
        "author": "Spyros Gidaris; Nikos Komodakis",
        "abstract": "We propose an object detection system that relies on a multi-region deep convolutional neural network (CNN) that also encodes semantic segmentation-aware features. The resulting CNN-based representation aims at capturing a diverse set of discriminative appearance factors and exhibits localization sensitivity that is essential for accurate object localization. We exploit the above properties of our recognition module by integrating it on an iterative localization mechanism that alternates between scoring a box proposal and refining its location with a deep CNN regression model. Thanks to the efficient use of our modules, we detect objects with very high localization accuracy. On the detection challenges of PASCAL VOC2007 and PASCAL VOC2012 we achieve mAP of 78.2% and 73.9% correspondingly, surpassing any other published work by a significant margin.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Gidaris_Object_Detection_via_ICCV_2015_paper.pdf",
        "aff": "Universite Paris Est, Ecole des Ponts ParisTech; Universite Paris Est, Ecole des Ponts ParisTech",
        "project": "",
        "github": "github.com/gidariss/mrcnn-object-detection/",
        "supp": "",
        "arxiv": "",
        "pdf_size": 720306,
        "gs_citation": 1043,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17076919334968493616&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff_domain": "imagine.enpc.fr;enpc.fr",
        "email": "imagine.enpc.fr;enpc.fr",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Gidaris_Object_Detection_via_ICCV_2015_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Universite Paris Est",
        "aff_unique_dep": "Ecole des Ponts ParisTech",
        "aff_unique_url": "https://www.univ-Paris-est.fr",
        "aff_unique_abbr": "UPE",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "France",
        "bibtex": "@InProceedings{Gidaris_2015_ICCV,\n    \n    author = {\n    Gidaris,\n    Spyros and Komodakis,\n    Nikos\n},\n    title = {\n    Object Detection via a Multi-Region and Semantic Segmentation-Aware CNN Model\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "3b8488506e",
        "title": "Objects2action: Classifying and Localizing Actions Without Any Video Example",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Mihir Jain, Jan C. van Gemert, Thomas Mensink, Cees G. M. Snoek",
        "author": "Mihir Jain; Jan C. van Gemert; Thomas Mensink; Cees G. M. Snoek",
        "abstract": "The goal of this paper is to recognize actions in video without the need for examples. Different from traditional zero-shot approaches we do not demand the design and specification of attribute classifiers and class-to-attribute mappings to allow for transfer from seen classes to unseen classes. Our key contribution is objects2action, a semantic word embedding that is spanned by a skip-gram model of thousands of object categories. Action labels are assigned to an object encoding of unseen video based on a convex combination of action and object affinities. Our semantic embedding has three main characteristics to accommodate for the specifics of actions. First, we propose a mechanism to exploit multiple-word descriptions of actions and objects. Second, we incorporate the automated selection of the most responsive objects per action. And finally, we demonstrate how to extend our zero-shot approach to the spatio-temporal localization of actions in video. Experiments on four action datasets demonstrate the potential of our approach.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Jain_Objects2action_Classifying_and_ICCV_2015_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1236128,
        "gs_citation": 189,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16306387507223425265&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 22,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Jain_Objects2action_Classifying_and_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Jain_2015_ICCV,\n    \n    author = {\n    Jain,\n    Mihir and van Gemert,\n    Jan C. and Mensink,\n    Thomas and Snoek,\n    Cees G. M.\n},\n    title = {\n    Objects2action: Classifying and Localizing Actions Without Any Video Example\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "2550ccf09e",
        "title": "Occlusion-Aware Depth Estimation Using Light-Field Cameras",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Ting-Chun Wang, Alexei A. Efros, Ravi Ramamoorthi",
        "author": "Ting-Chun Wang; Alexei A. Efros; Ravi Ramamoorthi",
        "abstract": "Consumer-level and high-end light-field cameras are now widely available.  Recent work has demonstrated practical methods for passive depth estimation from light-field images.  However, most previous approaches do not explicitly model occlusions, and therefore cannot capture sharp transitions around object boundaries.  A common assumption is that a pixel exhibits photo-consistency when focused to its correct depth, i.e., all viewpoints converge to a single (Lambertian) point in the scene.  This assumption does not hold in the presence of occlusions, making most current approaches unreliable precisely where accurate depth information is most important - at depth discontinuities.    In this paper, we develop a depth estimation algorithm that treats occlusion explicitly; the method also enables identification of occlusion edges, which may be useful in other applications.  We show that, although pixels at occlusions do not preserve photo-consistency in general, they are still consistent in approximately half the viewpoints. Moreover, the line separating the two view regions (correct depth vs. occluder) has the same orientation as the occlusion edge has in the spatial domain. By treating these two regions separately, depth estimation can be improved. Occlusion predictions can also be computed and used for regularization. Experimental results show that our method outperforms current state-of-the-art light-field depth estimation algorithms, especially near occlusion boundaries.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Wang_Occlusion-Aware_Depth_Estimation_ICCV_2015_paper.pdf",
        "aff": "UC Berkeley; UC Berkeley; UC San Diego",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2718615,
        "gs_citation": 438,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=329407496340572480&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "berkeley.edu;eecs.berkeley.edu;cs.ucsd.edu",
        "email": "berkeley.edu;eecs.berkeley.edu;cs.ucsd.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Wang_Occlusion-Aware_Depth_Estimation_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of California, Berkeley;University of California, San Diego",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.berkeley.edu;https://www.ucsd.edu",
        "aff_unique_abbr": "UC Berkeley;UCSD",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Berkeley;San Diego",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Wang_2015_ICCV,\n    \n    author = {\n    Wang,\n    Ting-Chun and Efros,\n    Alexei A. and Ramamoorthi,\n    Ravi\n},\n    title = {\n    Occlusion-Aware Depth Estimation Using Light-Field Cameras\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "5aefcb113f",
        "title": "On Linear Structure From Motion for Light Field Cameras",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Ole Johannsen, Antonin Sulc, Bastian Goldluecke",
        "author": "Ole Johannsen; Antonin Sulc; Bastian Goldluecke",
        "abstract": "We present a novel approach to relative pose estimation which is tailored to 4D light field cameras. From the relationships between scene geometry and light field structure and an analysis of the light field projection in terms of Pluecker ray coordinates, we deduce a set of linear constraints on ray space correspondences between a light field camera pair. These can be applied to infer relative pose of the light field cameras and thus obtain a point cloud reconstruction of the scene. While the proposed method has interesting relationships to pose estimation for generalized cameras based on ray-to-ray correspondence, our experiments demonstrate that our approach is both more accurate and computationally more efficient. It also compares favourably to direct linear pose estimation based on aligning the 3D point clouds obtained by reconstructing depth for each individual light field. To further validate the method, we employ the pose estimates to merge light fields captured with hand-held consumer light field cameras into refocusable panoramas.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Johannsen_On_Linear_Structure_ICCV_2015_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 908133,
        "gs_citation": 74,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1201960847940475600&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Johannsen_On_Linear_Structure_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Johannsen_2015_ICCV,\n    \n    author = {\n    Johannsen,\n    Ole and Sulc,\n    Antonin and Goldluecke,\n    Bastian\n},\n    title = {\n    On Linear Structure From Motion for Light Field Cameras\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "ff71a268e5",
        "title": "On Statistical Analysis of Neuroimages With Imperfect Registration",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Won Hwa Kim, Sathya N. Ravi, Sterling C. Johnson, Ozioma C. Okonkwo, Vikas Singh",
        "author": "Won Hwa Kim; Sathya N. Ravi; Sterling C. Johnson; Ozioma C. Okonkwo; Vikas Singh",
        "abstract": "A variety of studies in neuroscience/neuroimaging seek to perform statistical inference on the acquired brain image scans for diagnosis as well as understanding the pathological manifestation of diseases. To do so, an important first step is to register (or co-register) all of the image data into a common coordinate system. This permits meaningful comparison of the intensities at each voxel across groups (e.g., diseased versus healthy) to evaluate the effects of the disease and/or use machine learning algorithms in a subsequent step. But errors in the underlying registration make this problematic, they either decrease the statistical power or make the follow-up inference tasks less effective/accurate. In this paper, we derive a novel algorithm which offers immunity to local errors in the underlying deformation field obtained from registration procedures. By deriving a deformation invariant representation of the image, the downstream analysis can be made more robust as if one had access to a (hypothetical) far superior registration procedure. Our algorithm is based on recent work on Scattering coefficients. Using this as a starting point, we show how results from harmonic analysis (especially, non-Euclidean wavelets) yields strategies for designing deformation and additive noise invariant representations of large 3-D brain image volumes. We present a set of results on synthetic and real brain images where we achieve robust statistical analysis even in the presence of substantial deformation errors; here, standard analysis procedures significantly under-perform and fail to identify the true signal.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Kim_On_Statistical_Analysis_ICCV_2015_paper.pdf",
        "aff": "Dept. of Computer Sciences, University of Wisconsin, Madison, WI+Wisconsin Alzheimer’s Disease Research Center, University of Wisconsin, Madison, WI; Dept. of Industrial and Systems Engineering, University of Wisconsin, Madison, WI; GRECC, William S. Middleton V A Hospital, Madison, WI+Wisconsin Alzheimer’s Disease Research Center, University of Wisconsin, Madison, WI; Wisconsin Alzheimer’s Disease Research Center, University of Wisconsin, Madison, WI; Dept. of Biostatistics & Med. Informatics, University of Wisconsin, Madison, WI+Dept. of Computer Sciences, University of Wisconsin, Madison, WI+Wisconsin Alzheimer’s Disease Research Center, University of Wisconsin, Madison, WI",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1107301,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=87491070179073594&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Kim_On_Statistical_Analysis_ICCV_2015_paper.html",
        "aff_unique_index": "0+1;0;2+1;1;1+0+1",
        "aff_unique_norm": "University of Wisconsin-Madison;University of Wisconsin;William S. Middleton Memorial Veterans Hospital",
        "aff_unique_dep": "Department of Computer Sciences;Wisconsin Alzheimer’s Disease Research Center;GRECC (Geriatric Research, Education, and Clinical Center)",
        "aff_unique_url": "https://www.wisc.edu;https://www.wisc.edu;https://www.wisconsin.va.gov/",
        "aff_unique_abbr": "UW-Madison;UW;",
        "aff_campus_unique_index": "0+0;0;0+0;0;0+0+0",
        "aff_campus_unique": "Madison",
        "aff_country_unique_index": "0+0;0;0+0;0;0+0+0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Kim_2015_ICCV,\n    \n    author = {\n    Kim,\n    Won Hwa and Ravi,\n    Sathya N. and Johnson,\n    Sterling C. and Okonkwo,\n    Ozioma C. and Singh,\n    Vikas\n},\n    title = {\n    On Statistical Analysis of Neuroimages With Imperfect Registration\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "ca311a8852",
        "title": "On the Equivalence of Moving Entrance Pupil and Radial Distortion for Camera Calibration",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Avinash Kumar, Narendra Ahuja",
        "author": "Avinash Kumar; Narendra Ahuja",
        "abstract": "Radial distortion for ordinary (non-fisheye) camera lenses has traditionally been modeled as an infinite series function of radial location of an image pixel from the image center. While there has been enough empirical evidence  to show that such a model is accurate and sufficient for radial distortion calibration, there has not been much analysis on the geometric/physical understanding of radial distortion from a camera calibration perspective. In this paper, we show using a thick-lens imaging model, that the variation of entrance pupil location as a function of incident image ray angle is directly responsible for radial distortion in captured images. Thus, unlike as proposed in the current state-of-the-art in camera calibration, radial distortion  and entrance pupil movement are equivalent and need not be modeled together.  By modeling only entrance pupil motion instead of radial distortion,  we achieve two main benefits;  first, we obtain comparable if not better pixel re-projection error than traditional methods; second, and more importantly, we directly back-project a radially distorted image pixel along the  true image ray which formed it. Using a thick-lens setting, we show that such a back-projection is more  accurate than the two-step method of undistorting an image pixel and  then back-projecting it. We have applied this calibration method to the problem of generative depth-from-focus using focal stack to get accurate depth estimates.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Kumar_On_the_Equivalence_ICCV_2015_paper.pdf",
        "aff": "Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, Urbana, 61801, IL, USA; Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, Urbana, 61801, IL, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2573395,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15806865142009508141&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "illinois.edu;illinois.edu",
        "email": "illinois.edu;illinois.edu",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Kumar_On_the_Equivalence_ICCV_2015_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Illinois at Urbana-Champaign",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://illinois.edu",
        "aff_unique_abbr": "UIUC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Urbana",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Kumar_2015_ICCV,\n    \n    author = {\n    Kumar,\n    Avinash and Ahuja,\n    Narendra\n},\n    title = {\n    On the Equivalence of Moving Entrance Pupil and Radial Distortion for Camera Calibration\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "9d720dc6f4",
        "title": "On the Visibility of Point Clouds",
        "session": "3d vision",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "Sagi Katz, Ayellet Tal",
        "author": "Sagi Katz; Ayellet Tal",
        "abstract": "Is it possible to determine the visible subset of points directly from a given point cloud? Interestingly, in [7] it was shown that this is indeed the case - despite the fact that points cannot occlude each other, this task can be performed without surface reconstruction or normal estimation. The operator is very simple - it first transforms the points to a new domain and then constructs the convex hull in that domain. Points that lie on the convex hull of the transformed set of points are the images of the visible points. This operator found numerous applications in computer vision, including face reconstruction, keypoint detection, finding the best viewpoints, reduction of points, and many more. The current paper addresses a fundamental question: What properties should a transformation function satisfy, in order to be utilized in this operator? We show that three such properties are sufficient: the sign of the function, monotonicity, and a condition regarding the function's parameter.  The correctness of an algorithm that satisfies these three properties is proved. Finally, we show an interesting application of the operator - assignment of visibility-confidence score. This feature is missing from previous approaches, where a binary yes/no visibility is determined. This score can be utilized in various applications; we illustrate its use in view-dependent curvature estimation.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Katz_On_the_Visibility_ICCV_2015_paper.pdf",
        "aff": "Technion–Israel Institute of Technology; Technion–Israel Institute of Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 899618,
        "gs_citation": 74,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8974825779993832235&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "gmail.com;ee.technion.ac.il",
        "email": "gmail.com;ee.technion.ac.il",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Katz_On_the_Visibility_ICCV_2015_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Technion–Israel Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.technion.ac.il/en/",
        "aff_unique_abbr": "Technion",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel",
        "bibtex": "@InProceedings{Katz_2015_ICCV,\n    \n    author = {\n    Katz,\n    Sagi and Tal,\n    Ayellet\n},\n    title = {\n    On the Visibility of Point Clouds\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "ca780a9452",
        "title": "One Shot Learning via Compositions of Meaningful Patches",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Alex Wong, Alan L. Yuille",
        "author": "Alex Wong; Alan L. Yuille",
        "abstract": "The task of discriminating one object from another is almost trivial for a human being. However, this task is computationally taxing for most modern machine learning methods; whereas, we perform this task at ease given very few examples for learning. It has been proposed that the quick grasp of concept may come from the shared knowledge between the new example and examples previously learned. We believe that the key to one-shot learning is the sharing of common parts as each part holds immense amounts of information on how a visual concept is constructed. We propose an unsupervised method for learning a compact dictionary of image patches representing meaningful components of an objects. Using those patches as features, we build a compositional model that outperforms a number of popular algorithms on a one-shot learning task. We demonstrate the effectiveness of this approach on hand-written digits and show that this model generalizes to multiple datasets.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Wong_One_Shot_Learning_ICCV_2015_paper.pdf",
        "aff": "University of California, Los Angeles; University of California, Los Angeles",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1293287,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8753048707214986946&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "cs.ucla.edu;stat.ucla.edu",
        "email": "cs.ucla.edu;stat.ucla.edu",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Wong_One_Shot_Learning_ICCV_2015_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Los Angeles",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ucla.edu",
        "aff_unique_abbr": "UCLA",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Wong_2015_ICCV,\n    \n    author = {\n    Wong,\n    Alex and Yuille,\n    Alan L.\n},\n    title = {\n    One Shot Learning via Compositions of Meaningful Patches\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "6b16a448f9",
        "title": "Online Object Tracking With Proposal Selection",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Yang Hua, Karteek Alahari, Cordelia Schmid",
        "author": "Yang Hua; Karteek Alahari; Cordelia Schmid",
        "abstract": "Tracking-by-detection approaches are some of the most successful object trackers in recent years. Their success is largely determined by the detector model they learn initially and then update over time. However, under challenging conditions where an object can undergo transformations, e.g., severe rotation, these methods are found to be lacking. In this paper, we address this problem by formulating it as a proposal selection task and making two contributions. The first one is introducing novel proposals estimated from the geometric transformations undergone by the object, and building a rich candidate set for predicting the object location. The second one is devising a novel selection strategy using multiple cues, i.e., detection score and edgeness score computed from state-of-the-art object edges and motion boundaries. We extensively evaluate our approach on the visual object tracking 2014 challenge and online tracking benchmark datasets, and show the best performance.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Hua_Online_Object_Tracking_ICCV_2015_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1205739,
        "gs_citation": 130,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9253054280905996081&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 32,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Hua_Online_Object_Tracking_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Hua_2015_ICCV,\n    \n    author = {\n    Hua,\n    Yang and Alahari,\n    Karteek and Schmid,\n    Cordelia\n},\n    title = {\n    Online Object Tracking With Proposal Selection\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "38b7689a85",
        "title": "Opening the Black Box: Hierarchical Sampling Optimization for Estimating Human Hand Pose",
        "session": "vision and people",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "Danhang Tang, Jonathan Taylor, Pushmeet Kohli, Cem Keskin, Tae-Kyun Kim, Jamie Shotton",
        "author": "Danhang Tang; Jonathan Taylor; Pushmeet Kohli; Cem Keskin; Tae-Kyun Kim; Jamie Shotton",
        "abstract": "We address the problem of hand pose estimation, formulated as an inverse problem.  Typical approaches optimize an energy function over pose parameters using a `black box' image generation procedure. This procedure knows little about either the relationships between the parameters or the form of the energy function. In this paper, we show that we can significantly improving upon black box optimization by exploiting high-level knowledge of the structure of the parameters and using a local surrogate energy function. Our new framework, hierarchical sampling optimization, consists of a sequence of predictors organized into a kinematic hierarchy.  Each predictor is conditioned on its ancestors, and generates a set of samples over a subset of the pose parameters.  The highly-efficient surrogate energy is used to select among samples.  Having evaluated the full hierarchy, the partial pose samples are concatenated to generate a full-pose hypothesis.  Several hypotheses are generated using the same procedure, and finally the original full energy function selects the best result. Experimental evaluation on three publically available datasets show that our method is particularly impressive in low-compute scenarios where it significantly outperforms all other state-of-the-art methods.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Tang_Opening_the_Black_ICCV_2015_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 170,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4675066861039927147&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Tang_Opening_the_Black_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Tang_2015_ICCV,\n    \n    author = {\n    Tang,\n    Danhang and Taylor,\n    Jonathan and Kohli,\n    Pushmeet and Keskin,\n    Cem and Kim,\n    Tae-Kyun and Shotton,\n    Jamie\n},\n    title = {\n    Opening the Black Box: Hierarchical Sampling Optimization for Estimating Human Hand Pose\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "0298799f15",
        "title": "Optimizing Expected Intersection-Over-Union With Candidate-Constrained CRFs",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Faruk Ahmed, Dany Tarlow, Dhruv Batra",
        "author": "Faruk Ahmed; Dany Tarlow; Dhruv Batra",
        "abstract": "We study the question of how to make loss-aware predictions in image segmentation settings where the evaluation function is the Intersection-over-Union (IoU) measure that is used widely in evaluating image segmentation systems. Currently, there are two dominant approaches: the first approximates the Expected-IoU (EIoU) score as Expected-Intersection-over-Expected-Union (EIoEU); and the second approach is to compute exact EIoU but only over a small set of high-quality candidate solutions. We begin by asking which approach we should favor for two typical image segmentation tasks. Studying this question leads to two new methods that draw ideas from both existing approaches. Our new methods use the EIoEU approximation paired with high quality candidate solutions. Experimentally we show that our new approaches lead to improved performance on both image segmentation tasks.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Ahmed_Optimizing_Expected_Intersection-Over-Union_ICCV_2015_paper.pdf",
        "aff": "Université de Montréal; Microsoft Research; Virginia Tech",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1378108,
        "gs_citation": 83,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5499795983231771688&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "umontreal.ca;microsoft.com;vt.edu",
        "email": "umontreal.ca;microsoft.com;vt.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Ahmed_Optimizing_Expected_Intersection-Over-Union_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Université de Montréal;Microsoft Corporation;Virginia Tech",
        "aff_unique_dep": ";Microsoft Research;",
        "aff_unique_url": "https://www.umontreal.ca;https://www.microsoft.com/en-us/research;https://www.vt.edu",
        "aff_unique_abbr": "UdeM;MSR;VT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "Canada;United States",
        "bibtex": "@InProceedings{Ahmed_2015_ICCV,\n    \n    author = {\n    Ahmed,\n    Faruk and Tarlow,\n    Dany and Batra,\n    Dhruv\n},\n    title = {\n    Optimizing Expected Intersection-Over-Union With Candidate-Constrained CRFs\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "829ae618fd",
        "title": "Optimizing the Viewing Graph for Structure-From-Motion",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Chris Sweeney, Torsten Sattler, Tobias Höllerer, Matthew Turk, Marc Pollefeys",
        "author": "Chris Sweeney; Torsten Sattler; Tobias Hollerer; Matthew Turk; Marc Pollefeys",
        "abstract": "The viewing graph represents a set of views that are related by pairwise relative geometries. In the context of Structure-from-Motion (SfM), the viewing graph is the input to the incremental or global estimation pipeline. Much effort has been put towards developing robust algorithms to overcome potentially inaccurate relative geometries in the viewing graph during SfM. In this paper, we take a fundamentally different approach to SfM and instead focus on improving the quality of the viewing graph before applying SfM. Our main contribution is a novel optimization that improves the quality of the relative geometries in the viewing graph by enforcing loop consistency constraints with the epipolar point transfer. We show that this optimization greatly improves the accuracy of relative poses in the viewing graph and removes the need for filtering steps or robust algorithms typically used in global SfM methods. In addition, the optimized viewing graph can be used to efficiently calibrate cameras at scale. We combine our viewing graph optimization and focal length calibration into a global SfM pipeline that is more efficient than existing approaches. To our knowledge, ours is the first global SfM pipeline capable of handling uncalibrated image sets.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Sweeney_Optimizing_the_Viewing_ICCV_2015_paper.pdf",
        "aff": "University of California Santa Barbara; ETH Z ¨urich, Switzerland; University of California Santa Barbara; University of California Santa Barbara; ETH Z ¨urich, Switzerland",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 705192,
        "gs_citation": 165,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12237090988622500564&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff_domain": "cs.ucsb.edu;inf.ethz.ch;cs.ucsb.edu;cs.ucsb.edu;inf.ethz.ch",
        "email": "cs.ucsb.edu;inf.ethz.ch;cs.ucsb.edu;cs.ucsb.edu;inf.ethz.ch",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Sweeney_Optimizing_the_Viewing_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;0;0;1",
        "aff_unique_norm": "University of California, Santa Barbara;ETH Zürich",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ucsb.edu;https://www.ethz.ch",
        "aff_unique_abbr": "UCSB;ETHZ",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Santa Barbara;",
        "aff_country_unique_index": "0;1;0;0;1",
        "aff_country_unique": "United States;Switzerland",
        "bibtex": "@InProceedings{Sweeney_2015_ICCV,\n    \n    author = {\n    Sweeney,\n    Chris and Sattler,\n    Torsten and Hollerer,\n    Tobias and Turk,\n    Matthew and Pollefeys,\n    Marc\n},\n    title = {\n    Optimizing the Viewing Graph for Structure-From-Motion\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "e18e5f0562",
        "title": "Oriented Light-Field Windows for Scene Flow",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Pratul P. Srinivasan, Michael W. Tao, Ren Ng, Ravi Ramamoorthi",
        "author": "Pratul P. Srinivasan; Michael W. Tao; Ren Ng; Ravi Ramamoorthi",
        "abstract": "2D spatial image windows are used for comparing pixel values in computer vision applications such as correspondence for optical flow and 3D reconstruction, bilateral filtering, and image segmentation. However, pixel window comparisons can suffer from varying defocus blur and perspective at different depths, and can also lead to a loss of precision. In this paper, we leverage the recent use of light-field cameras to propose alternative - oriented light-field windows that enable more robust and accurate pixel comparisons. For Lambertian surfaces focused to the correct depth, the 2D distribution of angular rays from a pixel remains consistent. We build on this idea to develop an oriented 4D light-field window that accounts for shearing (depth), translation (matching), and windowing. Our main application is to scene flow, a generalization of optical flow to the 3D vector field describing the motion of each point in the scene. We show significant benefits of oriented light-field windows over standard 2D spatial windows. We also demonstrate additional applications of oriented light-field windows for bilateral filtering and image segmentation.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Srinivasan_Oriented_Light-Field_Windows_ICCV_2015_paper.pdf",
        "aff": "University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of California, San Diego",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 813034,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16111449342790022165&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu;cs.ucsd.edu",
        "email": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu;cs.ucsd.edu",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Srinivasan_Oriented_Light-Field_Windows_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "University of California, Berkeley;University of California, San Diego",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.berkeley.edu;https://www.ucsd.edu",
        "aff_unique_abbr": "UC Berkeley;UCSD",
        "aff_campus_unique_index": "0;0;0;1",
        "aff_campus_unique": "Berkeley;San Diego",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Srinivasan_2015_ICCV,\n    \n    author = {\n    Srinivasan,\n    Pratul P. and Tao,\n    Michael W. and Ng,\n    Ren and Ramamoorthi,\n    Ravi\n},\n    title = {\n    Oriented Light-Field Windows for Scene Flow\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "5d1ff3d03b",
        "title": "Oriented Object Proposals",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Shengfeng He, Rynson W.H. Lau",
        "author": "Shengfeng He; Rynson W.H. Lau",
        "abstract": "In this paper, we propose a new approach to generate oriented object proposals (OOPs) to reduce the detection error caused by various orientations of the object. To this end, we propose to efficiently locate object regions according to pixelwise object probability, rather than measuring the objectness from a set of sampled windows. We formulate the proposal generation problem as a generative probabilistic model such that object proposals of different shapes (i.e., sizes and orientations) can be produced by locating the local maximum likelihoods. The new approach has three main advantages. First, it helps the object detector handle objects of different orientations. Second, as the shapes of the proposals may vary to fit the objects, the resulting proposals are tighter than the sampling windows with fixed sizes. Third, it avoids massive window sampling, and thereby reducing the number of proposals while maintaining a high recall. Experiments on the PASCAL VOC 2007 dataset show that the proposed OOP outperforms the state-of-the-art fast methods. Further experiments show that the rotation invariant property helps a class-specific object detector achieve better performance than the state-of-the-art proposal generation methods in either object rotation scenarios or general scenarios. Generating OOPs is very fast and takes only 0.5s per image.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/He_Oriented_Object_Proposals_ICCV_2015_paper.pdf",
        "aff": "City University of Hong Kong; City University of Hong Kong",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2266788,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7667886440900419936&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "yahoo.com;cityu.edu.hk",
        "email": "yahoo.com;cityu.edu.hk",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/He_Oriented_Object_Proposals_ICCV_2015_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "City University of Hong Kong",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cityu.edu.hk",
        "aff_unique_abbr": "CityU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{He_2015_ICCV,\n    \n    author = {\n    He,\n    Shengfeng and Lau,\n    Rynson W.H.\n},\n    title = {\n    Oriented Object Proposals\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "428d299df1",
        "title": "P-CNN: Pose-Based CNN Features for Action Recognition",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Guilhem Chéron, Ivan Laptev, Cordelia Schmid",
        "author": "Guilhem Cheron; Ivan Laptev; Cordelia Schmid",
        "abstract": "This work targets human action recognition in video. While recent methods typically represent actions by statistics of local video features, here we argue for the importance of a representation derived from human pose. To this end we propose a new Pose-based Convolutional Neural Network descriptor (P-CNN) for action recognition. The descriptor aggregates motion and appearance information along tracks of human body parts. We investigate different schemes of temporal aggregation and experiment with P-CNN features obtained both for automatically estimated and manually annotated human poses. We evaluate our method on the recent and challenging JHMDB and MPII Cooking datasets. For both datasets our method shows consistent improvement over the state of the art.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Cheron_P-CNN_Pose-Based_CNN_ICCV_2015_paper.pdf",
        "aff": "WILLOW project-team, Departement d’Informatique de l’Ecole Normale Superieure, ENS/Inria/CNRS UMR 8548, Paris, France; WILLOW project-team, Departement d’Informatique de l’Ecole Normale Superieure, ENS/Inria/CNRS UMR 8548, Paris, France; LEAR project-team, Inria Grenoble Rhone-Alpes, Laboratoire Jean Kuntzmann, CNRS, Univ. Grenoble Alpes, France",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3094607,
        "gs_citation": 788,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10794121515679536826&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 35,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Cheron_P-CNN_Pose-Based_CNN_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Ecole Normale Superieure;Inria Grenoble Rhone-Alpes",
        "aff_unique_dep": "Departement d’Informatique;Laboratoire Jean Kuntzmann",
        "aff_unique_url": "https://www.ens.fr;https://www.inria.fr/grenoble",
        "aff_unique_abbr": "ENS;Inria",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Paris;Grenoble",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "France",
        "bibtex": "@InProceedings{Cheron_2015_ICCV,\n    \n    author = {\n    Cheron,\n    Guilhem and Laptev,\n    Ivan and Schmid,\n    Cordelia\n},\n    title = {\n    P-CNN: Pose-Based CNN Features for Action Recognition\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "aeac96b340",
        "title": "PIEFA: Personalized Incremental and Ensemble Face Alignment",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Xi Peng, Shaoting Zhang, Yu Yang, Dimitris N. Metaxas",
        "author": "Xi Peng; Shaoting Zhang; Yu Yang; Dimitris N. Metaxas",
        "abstract": "Face alignment, especially on real-time or large-scale sequential images, is a challenging task with broad applications. Both generic and joint alignment approaches have been proposed with varying degrees of success. However, many generic methods are heavily sensitive to initializations and usually rely on offline-trained static models, which limit their performance on sequential images with extensive variations. On the other hand, joint methods are restricted to offline applications, since they require all frames to conduct batch alignment. To address these limitations, we propose to exploit incremental learning for personalized ensemble alignment. We sample multiple initial shapes to achieve image congealing within one frame, which enables us to incrementally conduct ensemble alignment by group-sparse regularized rank minimization. At the same time, personalized modeling is obtained by subspace adaptation under the same incremental framework, while correction strategy is used to alleviate model drifting. Experimental results on multiple controlled and in-the-wild databases demonstrate the superior performance of our approach compared with state-of-the-arts in terms of fitting accuracy and efficiency.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Peng_PIEFA_Personalized_Incremental_ICCV_2015_paper.pdf",
        "aff": "Rutgers University; The University of North Carolina at Charlotte + Rutgers University; Rutgers University; Rutgers University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1191601,
        "gs_citation": 56,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14104271924551992572&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "cs.rutgers.edu;uncc.edu;cs.rutgers.edu;cs.rutgers.edu",
        "email": "cs.rutgers.edu;uncc.edu;cs.rutgers.edu;cs.rutgers.edu",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Peng_PIEFA_Personalized_Incremental_ICCV_2015_paper.html",
        "aff_unique_index": "0;1+0;0;0",
        "aff_unique_norm": "Rutgers University;University of North Carolina at Charlotte",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.rutgers.edu;https://www.uncc.edu",
        "aff_unique_abbr": "Rutgers;UNCC",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Charlotte",
        "aff_country_unique_index": "0;0+0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Peng_2015_ICCV,\n    \n    author = {\n    Peng,\n    Xi and Zhang,\n    Shaoting and Yang,\n    Yu and Metaxas,\n    Dimitris N.\n},\n    title = {\n    PIEFA: Personalized Incremental and Ensemble Face Alignment\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "924a707d47",
        "title": "POP Image Fusion - Derivative Domain Image Fusion Without Reintegration",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Graham D. Finlayson, Alex E. Hayes",
        "author": "Graham D. Finlayson; Alex E. Hayes",
        "abstract": "There are many applications where multiple images are fused to form a single summary greyscale or colour output, including computational photography (e.g. RGB-NIR), diffusion tensor imaging (medical), and remote sensing. Often, and intuitively, image fusion is carried out in the derivative domain. Here, a new composite fused derivative is found that best accounts for the detail across all images and then the resulting gradient field is reintegrated. However, the reintegration step generally hallucinates new detail (not appearing in any of the input  image bands) including halo and bending artifacts. In this paper we avoid these hallucinated details by avoiding the reintegration step.  Our work builds directly on the work of Socolinsky and Wolff who derive their equivalent gradient field from the per-pixel Di Zenzo structure tensor which is defined as the inner product of the image Jacobian. We show that the x- and y- derivatives of the projection of the original image onto the Principal characteristic vector of the Outer Product (POP) of the Jacobian generates the same equivalent gradient field. In so doing, we have derived a fused image that has the derivative structure we seek. Of course, this projection will be meaningful only where the Jacobian has non-zero derivatives, so we diffuse the projection directions using a bilateral filter before we calculate the fused image. The resulting POP fused image has maximal fused detail but avoids hallucinated artifacts. Experiments demonstrate our method delivers state of the art image fusion performance.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Finlayson_POP_Image_Fusion_ICCV_2015_paper.pdf",
        "aff": "University of East Anglia; University of East Anglia",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1406103,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6893981895853484892&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "uea.ac.uk;uea.ac.uk",
        "email": "uea.ac.uk;uea.ac.uk",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Finlayson_POP_Image_Fusion_ICCV_2015_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of East Anglia",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uea.ac.uk",
        "aff_unique_abbr": "UEA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom",
        "bibtex": "@InProceedings{Finlayson_2015_ICCV,\n    \n    author = {\n    Finlayson,\n    Graham D. and Hayes,\n    Alex E.\n},\n    title = {\n    POP Image Fusion - Derivative Domain Image Fusion Without Reintegration\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "d0a27f524e",
        "title": "PQTable: Fast Exact Asymmetric Distance Neighbor Search for Product Quantization Using Hash Tables",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Yusuke Matsui, Toshihiko Yamasaki, Kiyoharu Aizawa",
        "author": "Yusuke Matsui; Toshihiko Yamasaki; Kiyoharu Aizawa",
        "abstract": "We propose the product quantization table (PQTable), a product quantization-based hash table that is fast and requires neither parameter tuning nor training steps. The PQTable produces exactly the same results as a linear PQ search, and is 10^2 to 10^5 times faster when tested on the SIFT1B data. In addition, although state-of-the-art performance can be achieved by previous inverted-indexing-based approaches, such methods do require manually designed parameter setting and much training, whereas our method is free from them. Therefore, PQTable offers a practical and useful solution for real-world problems.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Matsui_PQTable_Fast_Exact_ICCV_2015_paper.pdf",
        "aff": "The University of Tokyo, Japan; The University of Tokyo, Japan; The University of Tokyo, Japan",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1069972,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4983359530815467025&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "hal.t.u-tokyo.ac.jp;hal.t.u-tokyo.ac.jp;hal.t.u-tokyo.ac.jp",
        "email": "hal.t.u-tokyo.ac.jp;hal.t.u-tokyo.ac.jp;hal.t.u-tokyo.ac.jp",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Matsui_PQTable_Fast_Exact_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "The University of Tokyo",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.u-tokyo.ac.jp",
        "aff_unique_abbr": "UTokyo",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Japan",
        "bibtex": "@InProceedings{Matsui_2015_ICCV,\n    \n    author = {\n    Matsui,\n    Yusuke and Yamasaki,\n    Toshihiko and Aizawa,\n    Kiyoharu\n},\n    title = {\n    PQTable: Fast Exact Asymmetric Distance Neighbor Search for Product Quantization Using Hash Tables\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "217eb287c6",
        "title": "Pairwise Conditional Random Forests for Facial Expression Recognition",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Arnaud Dapogny, Kevin Bailly, Séverine Dubuisson",
        "author": "Arnaud Dapogny; Kevin Bailly; Severine Dubuisson",
        "abstract": "Facial expression can be seen as the dynamic variation of one's appearance over time. Successful recognition thus involves finding representations of high-dimensional spatiotemporal patterns that can be generalized to unseen facial morphologies and variations of the expression dynamics. In this paper, we propose to learn Random Forests from heterogeneous derivative features (e.g. facial fiducial point movements or texture variations) upon pairs of images. Those forests are conditioned on the expression label of the first frame to reduce the variability of the ongoing expression transitions. When testing on a specific frame of a video, pairs are created between this frame and the previous ones. Predictions for each previous frame are used to draw trees from Pairwise Conditional Random Forests (PCRF) whose pairwise outputs are averaged over time to produce robust estimates. As such, PCRF appears as a natural extension of Random Forests to learn spatio-temporal patterns, that leads to significant improvements over standard Random Forests as well as state-of-the-art approaches on several facial expression benchmarks.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Dapogny_Pairwise_Conditional_Random_ICCV_2015_paper.pdf",
        "aff": "Sorbonne Universit ´es, UPMC Univ Paris 06, CNRS, ISIR UMR 7222, 4 place Jussieu 75005 Paris; Sorbonne Universit ´es, UPMC Univ Paris 06, CNRS, ISIR UMR 7222, 4 place Jussieu 75005 Paris; Sorbonne Universit ´es, UPMC Univ Paris 06, CNRS, ISIR UMR 7222, 4 place Jussieu 75005 Paris",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 5274432,
        "gs_citation": 80,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10844706510717091213&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff_domain": "isir.upmc.fr;isir.upmc.fr;isir.upmc.fr",
        "email": "isir.upmc.fr;isir.upmc.fr;isir.upmc.fr",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Dapogny_Pairwise_Conditional_Random_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Sorbonne Universités",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.sorbonne-universite.fr",
        "aff_unique_abbr": "Sorbonne",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Paris",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "France",
        "bibtex": "@InProceedings{Dapogny_2015_ICCV,\n    \n    author = {\n    Dapogny,\n    Arnaud and Bailly,\n    Kevin and Dubuisson,\n    Severine\n},\n    title = {\n    Pairwise Conditional Random Forests for Facial Expression Recognition\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "eca827a228",
        "title": "Pan-Sharpening With a Hyper-Laplacian Penalty",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Yiyong Jiang, Xinghao Ding, Delu Zeng, Yue Huang, John Paisley",
        "author": "Yiyong Jiang; Xinghao Ding; Delu Zeng; Yue Huang; John Paisley",
        "abstract": "Pan-sharpening is the task of fusing spectral information in low resolution multispectral images with spatial information in a corresponding high resolution panchromatic image. In such approaches, there is a trade-off between spectral and spatial quality, as well as computational efficiency. We present a method for pan-sharpening in which a sparsity-promoting objective function preserves both spatial and spectral content, and is efficient to optimize. Our objective incorporates the L1/2-norm in a way that can leverage recent computationally efficient methods, and L1 for which the alternating direction method of multipliers can be used. Additionally, our objective penalizes image gradients to enforce high resolution fidelity, and exploits the Fourier domain for further computational efficiency. Visual quality metrics demonstrate that our proposed objective function can achieve higher spatial and spectral resolution than several previous well-known methods with competitive computational efficiency.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Jiang_Pan-Sharpening_With_a_ICCV_2015_paper.pdf",
        "aff": "Fujian Key Laboratory of Sensing and Computing for Smart City, Xiamen University; Fujian Key Laboratory of Sensing and Computing for Smart City, Xiamen University; Fujian Key Laboratory of Sensing and Computing for Smart City, Xiamen University; Fujian Key Laboratory of Sensing and Computing for Smart City, Xiamen University; Department of Electrical Engineering, Columbia University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2369596,
        "gs_citation": 59,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16823406454668447259&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Jiang_Pan-Sharpening_With_a_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0;1",
        "aff_unique_norm": "Xiamen University;Columbia University",
        "aff_unique_dep": "Fujian Key Laboratory of Sensing and Computing for Smart City;Department of Electrical Engineering",
        "aff_unique_url": "https://www.xmu.edu.cn;https://www.columbia.edu",
        "aff_unique_abbr": ";Columbia",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;1",
        "aff_country_unique": "China;United States",
        "bibtex": "@InProceedings{Jiang_2015_ICCV,\n    \n    author = {\n    Jiang,\n    Yiyong and Ding,\n    Xinghao and Zeng,\n    Delu and Huang,\n    Yue and Paisley,\n    John\n},\n    title = {\n    Pan-Sharpening With a Hyper-Laplacian Penalty\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "a2a8e99abc",
        "title": "Panoptic Studio: A Massively Multiview System for Social Motion Capture",
        "session": "vision and people",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "Hanbyul Joo, Hao Liu, Lei Tan, Lin Gui, Bart Nabbe, Iain Matthews, Takeo Kanade, Shohei Nobuhara, Yaser Sheikh",
        "author": "Hanbyul Joo; Hao Liu; Lei Tan; Lin Gui; Bart Nabbe; Iain Matthews; Takeo Kanade; Shohei Nobuhara; Yaser Sheikh",
        "abstract": "We present an approach to capture the 3D structure and motion of a group of people engaged in a social interaction. The core challenges in capturing social interactions are: (1) occlusion is functional and frequent; (2) subtle motion needs to be measured over a space large enough to host a social group; and (3) human appearance and configuration variation is immense. The Panoptic Studio is a system organized around the thesis that social interactions should be measured through the perceptual integration of a large variety of view points. We present a modularized system designed around this principle, consisting of integrated structural, hardware, and software innovations. The system takes, as input, 480 synchronized video streams of multiple people engaged in social activities, and produces, as output, the labeled time-varying 3D structure of anatomical landmarks on individuals in the space. The algorithmic contributions include a hierarchical approach for generating skeletal trajectory proposals, and an optimization framework for skeletal reconstruction with trajectory re-association.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Joo_Panoptic_Studio_A_ICCV_2015_paper.pdf",
        "aff": "The Robotics Institute, Carnegie Mellon University; Ocean University of China; Hunan University; Ocean University of China; The Robotics Institute, Carnegie Mellon University; Disney Research Pittsburgh; The Robotics Institute, Carnegie Mellon University; Kyoto University; The Robotics Institute, Carnegie Mellon University",
        "project": "http://www.cs.cmu.edu/~hanbyulj/panoptic-studio",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1481764,
        "gs_citation": 1063,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3051990792701342480&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff_domain": "cs.cmu.edu;ouc.edu.cn;hnu.edu.cn;ouc.edu.cn;cs.cmu.edu;disneyresearch.com;cs.cmu.edu;i.kyoto-u.ac.jp;cs.cmu.edu",
        "email": "cs.cmu.edu;ouc.edu.cn;hnu.edu.cn;ouc.edu.cn;cs.cmu.edu;disneyresearch.com;cs.cmu.edu;i.kyoto-u.ac.jp;cs.cmu.edu",
        "author_num": 9,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Joo_Panoptic_Studio_A_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;2;1;0;3;0;4;0",
        "aff_unique_norm": "Carnegie Mellon University;Ocean University of China;Hunan University;Disney Research;Kyoto University",
        "aff_unique_dep": "The Robotics Institute;;;;",
        "aff_unique_url": "https://www.cmu.edu;http://www.ouc.edu.cn;http://www.hunu.edu.cn/;https://www.disneyresearch.com/pittsburgh;https://www.kyoto-u.ac.jp",
        "aff_unique_abbr": "CMU;OUC;HNU;Disney Research;Kyoto U",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Pittsburgh",
        "aff_country_unique_index": "0;1;1;1;0;0;0;2;0",
        "aff_country_unique": "United States;China;Japan",
        "bibtex": "@InProceedings{Joo_2015_ICCV,\n    \n    author = {\n    Joo,\n    Hanbyul and Liu,\n    Hao and Tan,\n    Lei and Gui,\n    Lin and Nabbe,\n    Bart and Matthews,\n    Iain and Kanade,\n    Takeo and Nobuhara,\n    Shohei and Sheikh,\n    Yaser\n},\n    title = {\n    Panoptic Studio: A Massively Multiview System for Social Motion Capture\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "85029a0f1f",
        "title": "Parsimonious Labeling",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Puneet K. Dokania, M. Pawan Kumar",
        "author": "Puneet K. Dokania; M. Pawan Kumar",
        "abstract": "We propose a new family of discrete energy minimization problems, which we call parsimonious labeling. Our energy function consists of unary potentials and high-order clique potentials. While the unary potentials are arbitrary, the clique potentials are proportional to the diversity of the set of unique labels assigned to the clique. Intuitively, our energy function encourages the labeling to be parsimonious, that is, use as few labels as possible. This in turn allows us to capture useful cues for important computer vision applications such as stereo correspondence and image denoising. Furthermore, we propose an efficient graph-cuts based algorithm for the parsimonious labeling problem that provides strong theoretical guarantees on the quality of the solution. Our algorithm consists of three steps. First, we approximate a given diversity using a mixture of a novel hierarchical Pn Potts model. Second, we use a divide-and-conquer approach for each mixture component, where each subproblem is solved using an efficient alpha-expansion algorithm. This provides us with a small number of putative labelings, one for each mixture component. Third, we choose the best putative labeling in terms of the energy value. Using both synthetic and standard real datasets, we show that our algorithm significantly outperforms other graph-cuts based approaches.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Dokania_Parsimonious_Labeling_ICCV_2015_paper.pdf",
        "aff": "CentraleSupélec and INRIA Saclay; CentraleSupélec and INRIA Saclay",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1031546,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5824462776362543375&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff_domain": "inria.fr;ecp.fr",
        "email": "inria.fr;ecp.fr",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Dokania_Parsimonious_Labeling_ICCV_2015_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "CentraleSupélec",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.centralesupelec.fr",
        "aff_unique_abbr": "CentraleSupélec",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "France",
        "bibtex": "@InProceedings{Dokania_2015_ICCV,\n    \n    author = {\n    Dokania,\n    Puneet K. and Kumar,\n    M. Pawan\n},\n    title = {\n    Parsimonious Labeling\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "83affaa839",
        "title": "Partial Person Re-Identification",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Wei-Shi Zheng, Xiang Li, Tao Xiang, Shengcai Liao, Jianhuang Lai, Shaogang Gong",
        "author": "Wei-Shi Zheng; Xiang Li; Tao Xiang; Shengcai Liao; Jianhuang Lai; Shaogang Gong",
        "abstract": "We address a new partial person re-identification (re-id) problem, where only a partial observation of a person is available for matching across different non-overlapping camera views. This differs significantly from the conventional person re-id setting where it is assumed that the full body of a person is detected and aligned. To solve this more challenging and realistic re-id problem without the implicit assumption of manual body-parts alignment, we propose a matching framework consisting of 1) a local patch-level matching model based on a novel sparse representation classification formulation with explicit patch ambiguity modelling, and 2) a global part-based matching model providing complementary spatial layout information. Our framework is evaluated on a new partial person re-id dataset as well as two existing datasets modified to include partial person images. The results show that the proposed method outperforms significantly existing re-id methods as well as other partial visual matching methods.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Zheng_Partial_Person_Re-Identification_ICCV_2015_paper.pdf",
        "aff": "School of Information Science and Technology, Sun Yat-sen University, China; School of Information Science and Technology, Sun Yat-sen University, China; School of Electronic Engineering and Computer Science, Queen Mary University of London, UK; NLPR, Institute of Automation, Chinese Academy of Sciences, China; School of Information Science and Technology, Sun Yat-sen University, China; School of Electronic Engineering and Computer Science, Queen Mary University of London, UK",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1448620,
        "gs_citation": 388,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17554356806824168534&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "ieee.org;gmail.com;qmul.ac.uk;nlpr.ia.ac.cn;mail.sysu.edu.cn;qmul.ac.uk",
        "email": "ieee.org;gmail.com;qmul.ac.uk;nlpr.ia.ac.cn;mail.sysu.edu.cn;qmul.ac.uk",
        "author_num": 6,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Zheng_Partial_Person_Re-Identification_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;1;2;0;1",
        "aff_unique_norm": "Sun Yat-sen University;Queen Mary University of London;Chinese Academy of Sciences",
        "aff_unique_dep": "School of Information Science and Technology;School of Electronic Engineering and Computer Science;Institute of Automation",
        "aff_unique_url": "http://www.sysu.edu.cn/;https://www.qmul.ac.uk;http://www.ia.cas.cn",
        "aff_unique_abbr": "SYSU;QMUL;CAS",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";London",
        "aff_country_unique_index": "0;0;1;0;0;1",
        "aff_country_unique": "China;United Kingdom",
        "bibtex": "@InProceedings{Zheng_2015_ICCV,\n    \n    author = {\n    Zheng,\n    Wei-Shi and Li,\n    Xiang and Xiang,\n    Tao and Liao,\n    Shengcai and Lai,\n    Jianhuang and Gong,\n    Shaogang\n},\n    title = {\n    Partial Person Re-Identification\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "7fdaa54b0c",
        "title": "Patch Group Based Nonlocal Self-Similarity Prior Learning for Image Denoising",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Jun Xu, Lei Zhang, Wangmeng Zuo, David Zhang, Xiangchu Feng",
        "author": "Jun Xu; Lei Zhang; Wangmeng Zuo; David Zhang; Xiangchu Feng",
        "abstract": "Patch based image modeling has achieved a great success in low level vision such as image denoising. In particular, the use of image nonlocal self-similarity (NSS) prior, which refers to the fact that a local patch often has many nonlocal similar patches to it across the image, has significantly enhanced the denoising performance. However, in most existing methods only the NSS of input degraded image is exploited, while how to utilize the NSS of clean natural images is still an open problem. In this paper, we propose a patch group (PG) based NSS prior learning scheme to learn explicit NSS models from natural images for high performance denoising. PGs are extracted from training images by putting nonlocal similar patches into groups, and a PG based Gaussian Mixture Model (PG-GMM) learning algorithm is developed to learn the NSS prior. We demonstrate that, owe to the learned PG-GMM, a simple weighted sparse coding model, which has a closed-form solution, can be used to perform image denoising effectively, resulting in high PSNR measure, fast speed, and particularly the best visual quality among all competing methods.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Xu_Patch_Group_Based_ICCV_2015_paper.pdf",
        "aff": "Dept. of Computing, The Hong Kong Polytechnic University, Hong Kong, China; Dept. of Computing, The Hong Kong Polytechnic University, Hong Kong, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; Dept. of Computing, The Hong Kong Polytechnic University, Hong Kong, China; School of Mathematics and Statistics, Xidian University, Xi’an, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3655398,
        "gs_citation": 472,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2901845607314763425&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "comp.polyu.edu.hk;comp.polyu.edu.hk;hit.edu.cn;comp.polyu.edu.hk;mail.xidian.edu.cn",
        "email": "comp.polyu.edu.hk;comp.polyu.edu.hk;hit.edu.cn;comp.polyu.edu.hk;mail.xidian.edu.cn",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Xu_Patch_Group_Based_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;1;0;2",
        "aff_unique_norm": "The Hong Kong Polytechnic University;Harbin Institute of Technology;Xidian University",
        "aff_unique_dep": "Dept. of Computing;School of Computer Science and Technology;School of Mathematics and Statistics",
        "aff_unique_url": "https://www.polyu.edu.hk;http://www.hit.edu.cn/;http://www.xidian.edu.cn/",
        "aff_unique_abbr": "PolyU;HIT;Xidian",
        "aff_campus_unique_index": "0;0;1;0;2",
        "aff_campus_unique": "Hong Kong;Harbin;Xi'an",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Xu_2015_ICCV,\n    \n    author = {\n    Xu,\n    Jun and Zhang,\n    Lei and Zuo,\n    Wangmeng and Zhang,\n    David and Feng,\n    Xiangchu\n},\n    title = {\n    Patch Group Based Nonlocal Self-Similarity Prior Learning for Image Denoising\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "977b22d000",
        "title": "PatchMatch-Based Automatic Lattice Detection for Near-Regular Textures",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Siying Liu, Tian-Tsong Ng, Kalyan Sunkavalli, Minh N. Do, Eli Shechtman, Nathan Carr",
        "author": "Siying Liu; Tian-Tsong Ng; Kalyan Sunkavalli; Minh N. Do; Eli Shechtman; Nathan Carr",
        "abstract": "In this work, we investigate the problem of automatically inferring the lattice structure of near-regular textures (NRT) in real-world images. Our technique leverages the PatchMatch algorithm for finding k-nearest-neighbor (kNN) correspondences in an image. We use these kNNs to recover an initial estimate of the 2D wallpaper basis vectors, and seed vertices of the texture lattice. We iteratively expand this lattice by solving an MRF optimization problem. We show that we can discretize the space of good solutions for the MRF using the kNNs, allowing us to efficiently and accurately optimize the MRF energy function using the Particle Belief Propagation algorithm. We demonstrate our technique on a benchmark NRT dataset containing a wide range of images with geometric and photometric variations, and show that our method clearly outperforms the state of the art in terms of both texel detection rate and texel localization score.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Liu_PatchMatch-Based_Automatic_Lattice_ICCV_2015_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3016377,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=871728931085045266&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Liu_PatchMatch-Based_Automatic_Lattice_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Liu_2015_ICCV,\n    \n    author = {\n    Liu,\n    Siying and Ng,\n    Tian-Tsong and Sunkavalli,\n    Kalyan and Do,\n    Minh N. and Shechtman,\n    Eli and Carr,\n    Nathan\n},\n    title = {\n    PatchMatch-Based Automatic Lattice Detection for Near-Regular Textures\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "4ff7e595c9",
        "title": "Pedestrian Travel Time Estimation in Crowded Scenes",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Shuai Yi, Hongsheng Li, Xiaogang Wang",
        "author": "Shuai Yi; Hongsheng Li; Xiaogang Wang",
        "abstract": "In this paper, we target on the problem of estimating the statistic of pedestrian travel time within a period from an entrance to a destination in a crowded scene. Such estimation is based on the global distributions of crowd densities and velocities instead of complete trajectories of pedestrians, which cannot be obtained in crowded scenes. The proposed method is motivated by our statistical investigation into the correlations between travel time and global properties of crowded scenes. Active regions are created for each source-destination pair to model the probable walking regions over the corresponding source-destination traffic flow. Two sets of scene features are specially designed for modeling moving and stationary persons inside the active regions and their influences on pedestrian travel time. The estimation of pedestrian travel time provides valuable  information for both crowd scene understanding and pedestrian behavior analysis, but was not sufficiently studied in literature. The effectiveness of the proposed pedestrian travel time estimation model is demonstrated through several surveillance applications, including dynamic scene monitoring,  localization of regions blocking traffics, and detection of abnormal pedestrian behaviors. Many more valuable applications based on our method are to be explored in the future.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Yi_Pedestrian_Travel_Time_ICCV_2015_paper.pdf",
        "aff": "Department of Electronic Engineering, The Chinese University of Hong Kong + Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences; Department of Electronic Engineering, The Chinese University of Hong Kong + Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences; Department of Electronic Engineering, The Chinese University of Hong Kong + Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences",
        "project": "http://www.ee.cuhk.edu.hk/~syi/",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1412531,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6658429668814273633&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "ee.cuhk.edu.hk;ee.cuhk.edu.hk;ee.cuhk.edu.hk",
        "email": "ee.cuhk.edu.hk;ee.cuhk.edu.hk;ee.cuhk.edu.hk",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Yi_Pedestrian_Travel_Time_ICCV_2015_paper.html",
        "aff_unique_index": "0+1;0+1;0+1",
        "aff_unique_norm": "The Chinese University of Hong Kong;Chinese Academy of Sciences",
        "aff_unique_dep": "Department of Electronic Engineering;Shenzhen Institutes of Advanced Technology",
        "aff_unique_url": "https://www.cuhk.edu.hk;http://www.siat.cas.cn",
        "aff_unique_abbr": "CUHK;SIAT",
        "aff_campus_unique_index": "0+1;0+1;0+1",
        "aff_campus_unique": "Hong Kong SAR;Shenzhen",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Yi_2015_ICCV,\n    \n    author = {\n    Yi,\n    Shuai and Li,\n    Hongsheng and Wang,\n    Xiaogang\n},\n    title = {\n    Pedestrian Travel Time Estimation in Crowded Scenes\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "8b589cb64f",
        "title": "Peeking Template Matching for Depth Extension",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Simon Korman, Eyal Ofek, Shai Avidan",
        "author": "Simon Korman; Eyal Ofek; Shai Avidan",
        "abstract": "We propose a method that extends a given depth image into regions in 3D that are not visible from the point of view of the camera. The algorithm detects repeated 3D structures in the visible scene and suggests a set of 3D extension hypotheses, which are then combined together through a global 3D MRF discrete optimization. The recovered global 3D surface is consistent with both the input depth map and the hypotheses.  A key component of this work is a novel 3D template matcher that is used to detect repeated 3D structure in the scene and to suggest the hypotheses. A unique property of this matcher is that it can handle depth uncertainty. This is crucial because the matcher is required to ``peek around the corner'', as it operates at the boundaries of the visible 3D scene where depth information is missing. The proposed matcher is fast and is guaranteed to find an approximation to the globally optimal solution.  We demonstrate on real-world data that our algorithm is capable of completing a full 3D scene from a single depth image and can synthesize a full depth map from a novel viewpoint of the scene. In addition, we report results on an extensive synthetic set of 3D shapes, which allows us to evaluate the method both qualitatively and quantitatively.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Korman_Peeking_Template_Matching_ICCV_2015_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3171409,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5276262010828846326&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Korman_Peeking_Template_Matching_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Korman_2015_ICCV,\n    \n    author = {\n    Korman,\n    Simon and Ofek,\n    Eyal and Avidan,\n    Shai\n},\n    title = {\n    Peeking Template Matching for Depth Extension\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "68e5c5d92a",
        "title": "Per-Sample Kernel Adaptation for Visual Recognition and Grouping",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Borislav Antic, Björn Ommer",
        "author": "Borislav Antic; Bjorn Ommer",
        "abstract": "Object, action, or scene representations that are corrupted by noise significantly impair the performance of visual recognition. Typically, partial occlusion, clutter, or excessive articulation affects only a subset of all feature dimensions and, most importantly, different dimensions are corrupted in different samples. Nevertheless, the common approach to this problem in feature selection and kernel methods is to down-weight or eliminate entire training samples or the same dimensions of all samples. Thus, valuable signal is lost, resulting in suboptimal classification.  Our goal is, therefore, to adjust the contribution of individual feature dimensions when comparing any two samples and computing their similarity. Consequently, per-sample selection of informative dimensions is directly integrated into kernel computation. The interrelated problems of learning the parameters of a kernel classifier and determining the informative components of each sample are then addressed in a joint objective function.  The approach can be integrated into the learning stage of any kernel-based visual recognition problem and it does not affect the computational performance in the retrieval phase. Experiments on diverse challenges of action recognition in videos and indoor scene classification show the general applicability of the approach and its ability to improve learning of visual representations.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Antic_Per-Sample_Kernel_Adaptation_ICCV_2015_paper.pdf",
        "aff": "Heidelberg Collaboratory for Image Processing, IWR, Heidelberg University, Germany; Heidelberg Collaboratory for Image Processing, IWR, Heidelberg University, Germany",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1656382,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5842447801233214265&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "iwr.uni-heidelberg.de;uni-heidelberg.de",
        "email": "iwr.uni-heidelberg.de;uni-heidelberg.de",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Antic_Per-Sample_Kernel_Adaptation_ICCV_2015_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Heidelberg University",
        "aff_unique_dep": "Institute for Computer Science (IWR)",
        "aff_unique_url": "https://www.uni-heidelberg.de",
        "aff_unique_abbr": "Uni HD",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Heidelberg",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany",
        "bibtex": "@InProceedings{Antic_2015_ICCV,\n    \n    author = {\n    Antic,\n    Borislav and Ommer,\n    Bjorn\n},\n    title = {\n    Per-Sample Kernel Adaptation for Visual Recognition and Grouping\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "f67f1f558a",
        "title": "Person Re-Identification Ranking Optimisation by Discriminant Context Information Analysis",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Jorge García, Niki Martinel, Christian Micheloni, Alfredo Gardel",
        "author": "Jorge Garcia; Niki Martinel; Christian Micheloni; Alfredo Gardel",
        "abstract": "Person re-identification is an open and challenging problem in computer vision. Existing re-identification approaches focus on optimal methods for features matching (e.g., metric learning approaches) or study the inter-camera transformations of such features. These methods hardly ever pay attention to the problem of visual ambiguities shared between the first ranks. In this paper, we focus on such a problem and introduce an unsupervised ranking optimization approach based on discriminant context information analysis. The proposed approach refines a given initial ranking by removing the visual ambiguities common to first ranks. This is achieved by analyzing their content and context information.  Extensive experiments on three publicly available benchmark datasets and different baseline methods have been conducted. Results demonstrate a remarkable improvement in the first positions of the ranking. Regardless of the selected dataset, state-of-the-art methods are strongly outperformed by our method.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Garcia_Person_Re-Identification_Ranking_ICCV_2015_paper.pdf",
        "aff": "Department of Electronics, University of Alcala; Department of Mathematics and Computer Science, University of Udine; Department of Mathematics and Computer Science, University of Udine; Department of Electronics, University of Alcala",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2800248,
        "gs_citation": 131,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3924214180861405277&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "depeca.uah.es;uniud.it;uniud.it;depeca.uah.es",
        "email": "depeca.uah.es;uniud.it;uniud.it;depeca.uah.es",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Garcia_Person_Re-Identification_Ranking_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "University of Alcala;University of Udine",
        "aff_unique_dep": "Department of Electronics;Department of Mathematics and Computer Science",
        "aff_unique_url": "https://www.uah.es;https://www.uniud.it",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;0",
        "aff_country_unique": "Spain;Italy",
        "bibtex": "@InProceedings{Garcia_2015_ICCV,\n    \n    author = {\n    Garcia,\n    Jorge and Martinel,\n    Niki and Micheloni,\n    Christian and Gardel,\n    Alfredo\n},\n    title = {\n    Person Re-Identification Ranking Optimisation by Discriminant Context Information Analysis\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "6b6f65c8f2",
        "title": "Person Re-Identification With Correspondence Structure Learning",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Yang Shen, Weiyao Lin, Junchi Yan, Mingliang Xu, Jianxin Wu, Jingdong Wang",
        "author": "Yang Shen; Weiyao Lin; Junchi Yan; Mingliang Xu; Jianxin Wu; Jingdong Wang",
        "abstract": "This paper addresses the problem of handling spatial misalignments due to camera-view changes or human-pose variations in person re-identification. We first introduce a boosting-based approach to learn a correspondence structure which indicates the patch-wise matching probabilities between images from a target camera pair. The learned correspondence structure can not only capture the spatial correspondence pattern between cameras but also handle the viewpoint or human-pose variation in individual images. We further introduce a global-based matching process. It integrates a global matching constraint over the learned correspondence structure to exclude cross-view misalignments during the image patch matching process, hence achieving a more reliable matching score between images. Experimental results on various datasets demonstrate the effectiveness of our approach.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Shen_Person_Re-Identification_With_ICCV_2015_paper.pdf",
        "aff": "Dept. of Electronic Engineering, Shanghai Jiao Tong University, China; Dept. of Electronic Engineering, Shanghai Jiao Tong University, China; Dept. of Electronic Engineering, Shanghai Jiao Tong University, China; School of Information Engineering, Zhengzhou University, China; National Key Lab for Novel Software Technology, Nanjing University, China; Microsoft Research, Beijing, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2417453,
        "gs_citation": 205,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=168687705279921928&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff_domain": "sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;zzu.edu.cn;nju.edu.cn;microsoft.com",
        "email": "sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;zzu.edu.cn;nju.edu.cn;microsoft.com",
        "author_num": 6,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Shen_Person_Re-Identification_With_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;1;2;3",
        "aff_unique_norm": "Shanghai Jiao Tong University;Zhengzhou University;Nanjing University;Microsoft Research",
        "aff_unique_dep": "Dept. of Electronic Engineering;School of Information Engineering;National Key Lab for Novel Software Technology;",
        "aff_unique_url": "https://www.sjtu.edu.cn;http://www.zzu.edu.cn;http://www.nju.edu.cn;https://www.microsoft.com/en-us/research/group/microsoft-research-asia",
        "aff_unique_abbr": "SJTU;;Nanjing U;MSR",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Shen_2015_ICCV,\n    \n    author = {\n    Shen,\n    Yang and Lin,\n    Weiyao and Yan,\n    Junchi and Xu,\n    Mingliang and Wu,\n    Jianxin and Wang,\n    Jingdong\n},\n    title = {\n    Person Re-Identification With Correspondence Structure Learning\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "7cfd5f5541",
        "title": "Person Re-Identification With Discriminatively Trained Viewpoint Invariant Dictionaries",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Srikrishna Karanam, Yang Li, Richard J. Radke",
        "author": "Srikrishna Karanam; Yang Li; Richard J. Radke",
        "abstract": "This paper introduces a new approach to address the person re-identification problem in cameras with non-overlapping fields of view. Unlike previous approaches that learn Mahalanobis-like distance metrics in some transformed feature space, we propose to learn a dictionary that is capable of discriminatively and sparsely encoding features representing different people.  Our approach directly addresses two key challenges in person re-identification: viewpoint variations and discriminability. First, to tackle viewpoint and associated appearance changes, we learn a single dictionary to represent both gallery and probe images in the training phase. We then discriminatively train the dictionary by enforcing explicit constraints on the associated sparse representations of the feature vectors. In the testing phase, we re-identify a probe image by simply determining the gallery image that has the closest sparse representation to that of the probe image in the Euclidean sense.  Extensive performance evaluations on three publicly available multi-shot re-identification datasets demonstrate the advantages of our algorithm over several state-of-the-art dictionary learning, temporal sequence matching, and spatial appearance and metric learning based techniques.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Karanam_Person_Re-Identification_With_ICCV_2015_paper.pdf",
        "aff": "Department of Electrical, Computer, and Systems Engineering, Rensselaer Polytechnic Institute, Troy, NY 12180; Department of Electrical, Computer, and Systems Engineering, Rensselaer Polytechnic Institute, Troy, NY 12180; Department of Electrical, Computer, and Systems Engineering, Rensselaer Polytechnic Institute, Troy, NY 12180",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 693893,
        "gs_citation": 259,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8171033890082431181&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "rpi.edu;gmail.com;ecse.rpi.edu",
        "email": "rpi.edu;gmail.com;ecse.rpi.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Karanam_Person_Re-Identification_With_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Rensselaer Polytechnic Institute",
        "aff_unique_dep": "Department of Electrical, Computer, and Systems Engineering",
        "aff_unique_url": "https://www.rpi.edu",
        "aff_unique_abbr": "RPI",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Troy",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Karanam_2015_ICCV,\n    \n    author = {\n    Karanam,\n    Srikrishna and Li,\n    Yang and Radke,\n    Richard J.\n},\n    title = {\n    Person Re-Identification With Discriminatively Trained Viewpoint Invariant Dictionaries\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "c7fc80a709",
        "title": "Person Recognition in Personal Photo Collections",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Seong Joon Oh, Rodrigo Benenson, Mario Fritz, Bernt Schiele",
        "author": "Seong Joon Oh; Rodrigo Benenson; Mario Fritz; Bernt Schiele",
        "abstract": "Recognising persons in everyday photos presents major challenges (occluded faces, different clothing, locations, etc.) for machine vision. We propose a convnet based person recognition system on which we provide an in-depth analysis of informativeness of different body cues, impact of training data, and the common failure modes of the system. In addition, we discuss the limitations of existing benchmarks and propose more challenging ones. Our method is simple and is built on open source and open data, yet it improves the state of the art results on a large dataset of social media photos (PIPA).",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Oh_Person_Recognition_in_ICCV_2015_paper.pdf",
        "aff": "Max Planck Institute for Informatics; Max Planck Institute for Informatics; Max Planck Institute for Informatics; Max Planck Institute for Informatics",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1509691,
        "gs_citation": 78,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6335337526402909838&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 22,
        "aff_domain": "mpi-inf.mpg.de;mpi-inf.mpg.de;mpi-inf.mpg.de;mpi-inf.mpg.de",
        "email": "mpi-inf.mpg.de;mpi-inf.mpg.de;mpi-inf.mpg.de;mpi-inf.mpg.de",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Oh_Person_Recognition_in_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Max Planck Institute for Informatics",
        "aff_unique_dep": "",
        "aff_unique_url": "https://mpi-inf.mpg.de",
        "aff_unique_abbr": "MPII",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Germany",
        "bibtex": "@InProceedings{Oh_2015_ICCV,\n    \n    author = {\n    Oh,\n    Seong Joon and Benenson,\n    Rodrigo and Fritz,\n    Mario and Schiele,\n    Bernt\n},\n    title = {\n    Person Recognition in Personal Photo Collections\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "88f7aafe28",
        "title": "Personalized Age Progression With Aging Dictionary",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Xiangbo Shu, Jinhui Tang, Hanjiang Lai, Luoqi Liu, Shuicheng Yan",
        "author": "Xiangbo Shu; Jinhui Tang; Hanjiang Lai; Luoqi Liu; Shuicheng Yan",
        "abstract": "In this paper, we aim to automatically render aging faces in a personalized way. Basically, a set of age-group specific dictionaries are learned, where the dictionary bases corresponding to the same index yet from different dictionaries form a particular aging process pattern cross different age groups, and a linear combination of these patterns expresses a particular personalized aging process. Moreover, two factors are taken into consideration in the dictionary learning process. First, beyond the aging dictionaries, each subject may have extra personalized facial characteristics, e.g. mole, which are invariant in the aging process. Second, it is challenging or even impossible to collect faces of all age groups for a particular subject, yet much easier and more practical to get face pairs from neighboring age groups. Thus a personality-aware coupled reconstruction loss is utilized to learn the dictionaries based on face pairs from neighboring age groups. Extensive experiments well demonstrate the advantages of our proposed solution over other state-of-the-arts  in term of personalized aging progression, as well as the performance gain for cross-age face verification by synthesizing aging faces.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Shu_Personalized_Age_Progression_ICCV_2015_paper.pdf",
        "aff": "School of Computer Science and Engineering, Nanjing University of Science and Technology‡; School of Computer Science and Engineering, Nanjing University of Science and Technology‡; Department of Electrical and Computer Engineering, National University of Singapore§; Department of Electrical and Computer Engineering, National University of Singapore§; Department of Electrical and Computer Engineering, National University of Singapore§",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2126207,
        "gs_citation": 178,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5955604685008289183&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "gmail.com;njust.edu.cn;gmail.com;nus.edu.sg;nus.edu.sg",
        "email": "gmail.com;njust.edu.cn;gmail.com;nus.edu.sg;nus.edu.sg",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Shu_Personalized_Age_Progression_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;1;1;1",
        "aff_unique_norm": "Nanjing University of Science and Technology;National University of Singapore",
        "aff_unique_dep": "School of Computer Science and Engineering;Department of Electrical and Computer Engineering",
        "aff_unique_url": "http://www.nust.edu.cn;https://www.nus.edu.sg",
        "aff_unique_abbr": "NUST;NUS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;1;1",
        "aff_country_unique": "China;Singapore",
        "bibtex": "@InProceedings{Shu_2015_ICCV,\n    \n    author = {\n    Shu,\n    Xiangbo and Tang,\n    Jinhui and Lai,\n    Hanjiang and Liu,\n    Luoqi and Yan,\n    Shuicheng\n},\n    title = {\n    Personalized Age Progression With Aging Dictionary\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "ec75fc9098",
        "title": "Photogeometric Scene Flow for High-Detail Dynamic 3D Reconstruction",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Paulo F. U. Gotardo, Tomas Simon, Yaser Sheikh, Iain Matthews",
        "author": "Paulo F. U. Gotardo; Tomas Simon; Yaser Sheikh; Iain Matthews",
        "abstract": "Photometric stereo (PS) is an established technique for high-detail reconstruction of 3D geometry and appearance. To correct for surface integration errors, PS is often combined with multiview stereo (MVS). With dynamic objects, PS reconstruction also faces the problem of computing optical flow (OF) for image alignment under rapid changes in illumination. Current PS methods typically compute optical flow and MVS as independent stages, each one with its own limitations and errors introduced by early regularization. In contrast, scene flow methods estimate geometry and motion, but lack the fine detail from PS. This paper proposes photogeometric scene flow (PGSF) for high-quality dynamic 3D reconstruction. PGSF performs PS, OF, and MVS simultaneously. It is based on two key observations: (i) while image alignment improves PS, PS allows for surfaces to be relit to improve alignment; (ii) PS provides surface gradients that render the smoothness term in MVS unnecessary, leading to truly data-driven, continuous depth estimates. This synergy is demonstrated in the quality of the resulting RGB appearance, 3D geometry, and 3D motion.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Gotardo_Photogeometric_Scene_Flow_ICCV_2015_paper.pdf",
        "aff": "Disney Research; Carnegie Mellon University; Carnegie Mellon University; Disney Research+Carnegie Mellon University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3491164,
        "gs_citation": 61,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16494922912759445635&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "disneyresearch.com;cs.cmu.edu;cs.cmu.edu;disneyresearch.com",
        "email": "disneyresearch.com;cs.cmu.edu;cs.cmu.edu;disneyresearch.com",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Gotardo_Photogeometric_Scene_Flow_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;1;0+1",
        "aff_unique_norm": "Disney Research;Carnegie Mellon University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://research.disney.com;https://www.cmu.edu",
        "aff_unique_abbr": "Disney Research;CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0+0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Gotardo_2015_ICCV,\n    \n    author = {\n    Gotardo,\n    Paulo F. U. and Simon,\n    Tomas and Sheikh,\n    Yaser and Matthews,\n    Iain\n},\n    title = {\n    Photogeometric Scene Flow for High-Detail Dynamic 3D Reconstruction\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "9e425fe294",
        "title": "Photometric Stereo With Small Angular Variations",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Jian Wang, Yasuyuki Matsushita, Boxin Shi, Aswin C. Sankaranarayanan",
        "author": "Jian Wang; Yasuyuki Matsushita; Boxin Shi; Aswin C. Sankaranarayanan",
        "abstract": "Most existing successful photometric stereo setups require large angular variations in illumination directions, which results in acquisition rigs that have large spatial extent. For many applications, especially involving mobile devices, it is important that the device be spatially compact. This naturally implies smaller angular variations in the illumination directions. This paper studies the effect of small angular variations in illumination directions to photometric stereo. We explore both theoretical justification and practical issues in the design of a compact and portable photometric stereo device on which a camera is surrounded by a ring of point light sources. We first derive the relationship between the estimation error of surface normal and the baseline of the point light sources. Armed with this theoretical insight, we develop a small baseline photometric stereo prototype to experimentally examine the theory and its practicality.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Wang_Photometric_Stereo_With_ICCV_2015_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3329911,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4377049011936882304&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Wang_Photometric_Stereo_With_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Wang_2015_ICCV,\n    \n    author = {\n    Wang,\n    Jian and Matsushita,\n    Yasuyuki and Shi,\n    Boxin and Sankaranarayanan,\n    Aswin C.\n},\n    title = {\n    Photometric Stereo With Small Angular Variations\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "5380d7fd03",
        "title": "Photometric Stereo in a Scattering Medium",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Zak Murez, Tali Treibitz, Ravi Ramamoorthi, David Kriegman",
        "author": "Zak Murez; Tali Treibitz; Ravi Ramamoorthi; David Kriegman",
        "abstract": "Photometric stereo is widely used for 3D reconstruction. However, its use in scattering media such as water, biological tissue and fog has been limited until now, because of forward scattered light from both the source and object, as well as light scattered back from the medium (backscatter). Here we make three contributions to address the key modes of light propagation, under the common single scattering assumption for dilute media. First, we show through extensive simulations that single-scattered light from a source can be approximated by a point light source with a single direction. This alleviates the need to handle light source blur explicitly. Next, we model the blur due to scattering of light from the object. We measure the object point-spread function and introduce a simple deconvolution method. Finally, we show how imaging fluorescence emission where available, eliminates the backscatter component and increases the signal-to-noise ratio. Experimental results in a water tank, with different concentrations of scattering media added, show that deconvolution produces higher-quality 3D reconstructions than previous techniques, and that when combined with fluorescence, can produce results similar to that in clear water even for highly turbid media.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Murez_Photometric_Stereo_in_ICCV_2015_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2950115,
        "gs_citation": 115,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15794439655094069866&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 22,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Murez_Photometric_Stereo_in_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Murez_2015_ICCV,\n    \n    author = {\n    Murez,\n    Zak and Treibitz,\n    Tali and Ramamoorthi,\n    Ravi and Kriegman,\n    David\n},\n    title = {\n    Photometric Stereo in a Scattering Medium\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "b7ad69a3e6",
        "title": "Piecewise Flat Embedding for Image Segmentation",
        "session": "segmentation, edges and saliency",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "Yizhou Yu, Chaowei Fang, Zicheng Liao",
        "author": "Yizhou Yu; Chaowei Fang; Zicheng Liao",
        "abstract": "Image segmentation is a critical step in many computer vision tasks, including high-level visual recognition and scene understanding as well as low-level photo and video processing. In this paper, we propose a new nonlinear embedding, called piecewise flat embedding, for image segmentation. Based on the theory of sparse signal recovery, piecewise flat embedding attempts to identify segment boundaries while significantly suppressing variations within segments. We adopt an L1-regularized energy term in the formulation to promote sparse solutions. We further devise an effective two-stage numerical algorithm based on Bregman iterations to solve the proposed embedding. Piecewise flat embedding can be easily integrated into existing image segmentation frameworks, including segmentation based on spectral clustering and hierarchical segmentation based on contour detection. Experiments on BSDS500 indicate that segmentation algorithms incorporating this embedding can achieve significantly improved results in both frameworks.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Yu_Piecewise_Flat_Embedding_ICCV_2015_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 922113,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11816961983246047406&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 19,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Yu_Piecewise_Flat_Embedding_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Yu_2015_ICCV,\n    \n    author = {\n    Yu,\n    Yizhou and Fang,\n    Chaowei and Liao,\n    Zicheng\n},\n    title = {\n    Piecewise Flat Embedding for Image Segmentation\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "cc89e144d7",
        "title": "Point Triangulation Through Polyhedron Collapse Using the l[?] Norm",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Simon Donné, Bart Goossens, Wilfried Philips",
        "author": "Simon Donne; Bart Goossens; Wilfried Philips",
        "abstract": "Multi-camera triangulation of feature points based on a minimisation of the overall L2 reprojection error can get stuck in suboptimal local minima or require slow global optimisation. For this reason, researchers have proposed optimising the L-infinity norm of the L2 single view reprojection errors, which avoids the problem of local minima entirely. In this paper we present a novel method for L-infinity triangulation that minimizes the L-infinity norm of the L-infinity reprojection errors: this apparently small difference leads to a much faster but equally accurate solution which is related to the MLE under the assumption of uniform noise. The proposed method adopts a new optimisation strategy based on solving simple quadratic equations. This stands in contrast with the fastest existing methods, which solve a sequence of more complex auxiliary Linear Programming or Second Order Cone Problems. The proposed algorithm performs well: for triangulation, it achieves the same accuracy as existing techniques while executing faster and being straightforward to implement.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Donne_Point_Triangulation_Through_ICCV_2015_paper.pdf",
        "aff": "Ghent University; Ghent University; Ghent University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 716268,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9834552226439958752&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "telin.ugent.be;telin.ugent.be;telin.ugent.be",
        "email": "telin.ugent.be;telin.ugent.be;telin.ugent.be",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Donne_Point_Triangulation_Through_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Ghent University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ugent.be/en",
        "aff_unique_abbr": "UGent",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Belgium",
        "bibtex": "@InProceedings{Donne_2015_ICCV,\n    \n    author = {\n    Donne,\n    Simon and Goossens,\n    Bart and Philips,\n    Wilfried\n},\n    title = {\n    Point Triangulation Through Polyhedron Collapse Using the l[?] Norm\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "0c8a76fa3e",
        "title": "Polarized 3D: High-Quality Depth Sensing With Polarization Cues",
        "session": "computational photography and image enhancement",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "Achuta Kadambi, Vage Taamazyan, Boxin Shi, Ramesh Raskar",
        "author": "Achuta Kadambi; Vage Taamazyan; Boxin Shi; Ramesh Raskar",
        "abstract": "Coarse depth maps can be enhanced by using the shape information from polarization cues. We propose a framework to combine surface normals from polarization (hereafter polarization normals) with an aligned depth map. Polarization normals have not been used for depth enhancement before. This is because polarization normals suffer from physics-based artifacts, such as azimuthal ambiguity, refractive distortion and fronto-parallel signal degradation. We propose a framework to overcome these key challenges, allowing the benefits of polarization to be used to enhance depth maps. Our results demonstrate improvement with respect to state-of-the-art 3D reconstruction techniques.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Kadambi_Polarized_3D_High-Quality_ICCV_2015_paper.pdf",
        "aff": "MIT Media Lab; Skoltech+MIT Media Lab; SUTD+MIT Media Lab; MIT Media Lab",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1820113,
        "gs_citation": 302,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9195777869856462861&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "mit.edu; ; ; ",
        "email": "mit.edu; ; ; ",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Kadambi_Polarized_3D_High-Quality_ICCV_2015_paper.html",
        "aff_unique_index": "0;1+0;2+0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology;Skolkovo Institute of Science and Technology;Singapore University of Technology and Design",
        "aff_unique_dep": "Media Lab;;",
        "aff_unique_url": "http://www.media.mit.edu/;https://www.skoltech.ru;https://www.sutd.edu.sg",
        "aff_unique_abbr": "MIT;Skoltech;SUTD",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;1+0;2+0;0",
        "aff_country_unique": "United States;Russia;Singapore",
        "bibtex": "@InProceedings{Kadambi_2015_ICCV,\n    \n    author = {\n    Kadambi,\n    Achuta and Taamazyan,\n    Vage and Shi,\n    Boxin and Raskar,\n    Ramesh\n},\n    title = {\n    Polarized 3D: High-Quality Depth Sensing With Polarization Cues\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "03d5e3d500",
        "title": "Pose Induction for Novel Object Categories",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Shubham Tulsiani, João Carreira, Jitendra Malik",
        "author": "Shubham Tulsiani; Joao Carreira; Jitendra Malik",
        "abstract": "We address the task of predicting pose for objects of unannotated object categories from a small seed set of annotated object classes. We present a generalized classifier that can reliably induce pose given a single instance of a novel category. In case of availability of a large collection of novel instances, our approach then jointly reasons over all instances to improve the initial estimates. We empirically validate the various components of our algorithm and quantitatively show that our method produces reliable pose estimates. We also show qualitative results on a diverse set of classes and further demonstrate the applicability of our system for learning shape models of novel object classes.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Tulsiani_Pose_Induction_for_ICCV_2015_paper.pdf",
        "aff": "University of California, Berkeley; University of California, Berkeley; University of California, Berkeley",
        "project": "",
        "github": "https://github.com/shubhtuls/poseInduction",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1117759,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14182382523126711930&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu",
        "email": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Tulsiani_Pose_Induction_for_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Tulsiani_2015_ICCV,\n    \n    author = {\n    Tulsiani,\n    Shubham and Carreira,\n    Joao and Malik,\n    Jitendra\n},\n    title = {\n    Pose Induction for Novel Object Categories\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "4a521c6968",
        "title": "Pose-Invariant 3D Face Alignment",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Amin Jourabloo, Xiaoming Liu",
        "author": "Amin Jourabloo; Xiaoming Liu",
        "abstract": "Face alignment aims to estimate the locations of a set of landmarks for a given image.This problem has received much attention as evidenced by the recent advancement in both the methodology and performance. However, most of the existing works neither explicitly handle face images with arbitrary poses, nor perform large-scale experiments on non-frontal and profile face images. In order to address these limitations, this paper proposes a novel face alignment algorithm that estimates both 2D and 3D landmarks and their 2D visibilities for a face image with an arbitrary pose. By integrating a 3D point distribution model, a cascaded coupled-regressor approach is designed to estimate both the camera projection matrix and the 3D landmarks.  Furthermore, the 3D model also allows us to automatically estimate the 2D landmark visibilities via surface normal. We use a substantially larger collection of all-pose face images to evaluate our algorithm and demonstrate superior performances than the state-of-the-art methods.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Jourabloo_Pose-Invariant_3D_Face_ICCV_2015_paper.pdf",
        "aff": "Department of Computer Science and Engineering, Michigan State University, East Lansing MI 48824; Department of Computer Science and Engineering, Michigan State University, East Lansing MI 48824",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 937843,
        "gs_citation": 227,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2528785491925529646&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff_domain": "msu.edu;msu.edu",
        "email": "msu.edu;msu.edu",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Jourabloo_Pose-Invariant_3D_Face_ICCV_2015_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Michigan State University",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.msu.edu",
        "aff_unique_abbr": "MSU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "East Lansing",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Jourabloo_2015_ICCV,\n    \n    author = {\n    Jourabloo,\n    Amin and Liu,\n    Xiaoming\n},\n    title = {\n    Pose-Invariant 3D Face Alignment\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "c018776bcd",
        "title": "PoseNet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Alex Kendall, Matthew Grimes, Roberto Cipolla",
        "author": "Alex Kendall; Matthew Grimes; Roberto Cipolla",
        "abstract": "We present a robust and real-time monocular six degree of freedom relocalization system. Our system trains a convolutional neural network to regress the 6-DOF camera pose from a single RGB image in an end-to-end manner with no need of additional engineering or graph optimisation. The algorithm can operate indoors and outdoors in real time, taking 5ms per frame to compute. It obtains approximately 2m and 3 degrees accuracy for large scale outdoor scenes and 0.5m and 5 degrees accuracy indoors. This is achieved using an efficient 23 layer deep convnet, demonstrating that convnets can be used to solve complicated out of image plane regression problems. This was made possible by leveraging transfer learning from large scale classification data. We show that the PoseNet localizes from high level features and is robust to difficult lighting, motion blur and different camera intrinsics where point based SIFT registration fails. Furthermore we show how the pose feature that is produced generalizes to other scenes allowing us to regress pose with only a few dozen training examples.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Kendall_PoseNet_A_Convolutional_ICCV_2015_paper.pdf",
        "aff": "University of Cambridge; University of Cambridge; King’s College Old Hospital Shop Fac ¸ade St Mary’s Church",
        "project": "mi.eng.cam.ac.uk/projects/relocalisation/",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3227498,
        "gs_citation": 2991,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7791289761831755523&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 20,
        "aff_domain": "cam.ac.uk;cam.ac.uk;cam.ac.uk",
        "email": "cam.ac.uk;cam.ac.uk;cam.ac.uk",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Kendall_PoseNet_A_Convolutional_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of Cambridge;King's College",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cam.ac.uk;https://www.kcl.ac.uk",
        "aff_unique_abbr": "Cambridge;KCL",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom",
        "bibtex": "@InProceedings{Kendall_2015_ICCV,\n    \n    author = {\n    Kendall,\n    Alex and Grimes,\n    Matthew and Cipolla,\n    Roberto\n},\n    title = {\n    PoseNet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "74f5ad4f4e",
        "title": "Predicting Deep Zero-Shot Convolutional Neural Networks Using Textual Descriptions",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Jimmy Lei Ba, Kevin Swersky, Sanja Fidler, Ruslan salakhutdinov",
        "author": "Jimmy Lei Ba; Kevin Swersky; Sanja Fidler; Ruslan salakhutdinov",
        "abstract": "One of the main challenges in Zero-Shot Learning of visual categories is gathering semantic attributes to accompany images. Recent work has shown that learning from textual descriptions, such as Wikipedia articles, avoids the problem of having to explicitly define these attributes. We present a new model that can classify unseen categories from their textual description. Specifically, we use text features to predict the output weights of both the convolutional and the fully connected layers in a deep convolutional neural network (CNN). We take advantage of the architecture of CNNs and learn features at different layers, rather than just learning an embedding space for both modalities, as is common with existing approaches. The proposed model also allows us to automatically generate a list of pseudo-attributes for each visual category consisting of words from Wikipedia articles. We train our models end-to-end using the Caltech-UCSD bird and flower datasets and evaluate both ROC and Precision-Recall curves.  Our empirical results show that the proposed model significantly outperforms previous methods.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Ba_Predicting_Deep_Zero-Shot_ICCV_2015_paper.pdf",
        "aff": "University of Toronto; University of Toronto; University of Toronto; University of Toronto",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1010559,
        "gs_citation": 527,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2724062316492775050&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "cs.toronto.edu;cs.toronto.edu;cs.toronto.edu;cs.toronto.edu",
        "email": "cs.toronto.edu;cs.toronto.edu;cs.toronto.edu;cs.toronto.edu",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Ba_Predicting_Deep_Zero-Shot_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Toronto",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.utoronto.ca",
        "aff_unique_abbr": "U of T",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Canada",
        "bibtex": "@InProceedings{Ba_2015_ICCV,\n    \n    author = {\n    Ba,\n    Jimmy Lei and Swersky,\n    Kevin and Fidler,\n    Sanja and salakhutdinov,\n    Ruslan\n},\n    title = {\n    Predicting Deep Zero-Shot Convolutional Neural Networks Using Textual Descriptions\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "433dc639b6",
        "title": "Predicting Depth, Surface Normals and Semantic Labels With a Common Multi-Scale Convolutional Architecture",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "David Eigen, Rob Fergus",
        "author": "David Eigen; Rob Fergus",
        "abstract": "In this paper we address three different computer vision tasks using a single basic architecture:  depth prediction, surface normal estimation, and semantic labeling.  We use a multiscale convolutional network that is able to adapt easily to each task using only small modifications, regressing from the input image to the output map directly.  Our method progressively refines predictions using a sequence of scales, and captures many image details without any superpixels or low-level segmentation.  We achieve state-of-the-art performance on benchmarks for all three tasks.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Eigen_Predicting_Depth_Surface_ICCV_2015_paper.pdf",
        "aff": "Dept. of Computer Science, Courant Institute, New York University; Dept. of Computer Science, Courant Institute, New York University + Facebook AI Research",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1328207,
        "gs_citation": 3530,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12162254258128213973&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff_domain": "cs.nyu.edu;cs.nyu.edu",
        "email": "cs.nyu.edu;cs.nyu.edu",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Eigen_Predicting_Depth_Surface_ICCV_2015_paper.html",
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "New York University;Facebook",
        "aff_unique_dep": "Dept. of Computer Science;Facebook AI Research",
        "aff_unique_url": "https://www.nyu.edu;https://research.facebook.com",
        "aff_unique_abbr": "NYU;FAIR",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "New York;",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Eigen_2015_ICCV,\n    \n    author = {\n    Eigen,\n    David and Fergus,\n    Rob\n},\n    title = {\n    Predicting Depth,\n    Surface Normals and Semantic Labels With a Common Multi-Scale Convolutional Architecture\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "e73bb4b6b2",
        "title": "Predicting Good Features for Image Geo-Localization Using Per-Bundle VLAD",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Hyo Jin Kim, Enrique Dunn, Jan-Michael Frahm",
        "author": "Hyo Jin Kim; Enrique Dunn; Jan-Michael Frahm",
        "abstract": "We address the problem of recognizing a place depicted in a query image by using a large database of geo-tagged images at a city-scale. In particular, we discover features that are useful for recognizing a place in a data-driven manner, and use this knowledge to predict useful features in a query image prior to the geo-localization process. This allows us to achieve better performance while reducing the number of features. Also, for both learning to predict features and retrieving geo-tagged images from the database, we propose per-bundle vector of locally aggregated descriptors (PBVLAD), where each maximally stable region is described by a vector of locally aggregated descriptors (VLAD) on multiple scale-invariant features detected within the region. Experimental results show the proposed approach achieves a significant improvement over other baseline methods.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Kim_Predicting_Good_Features_ICCV_2015_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 75,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4945837750953164718&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Kim_Predicting_Good_Features_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Kim_2015_ICCV,\n    \n    author = {\n    Kim,\n    Hyo Jin and Dunn,\n    Enrique and Frahm,\n    Jan-Michael\n},\n    title = {\n    Predicting Good Features for Image Geo-Localization Using Per-Bundle VLAD\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "8d0f8d205e",
        "title": "Predicting Multiple Structured Visual Interpretations",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Debadeepta Dey, Varun Ramakrishna, Martial Hebert, J. Andrew Bagnell",
        "author": "Debadeepta Dey; Varun Ramakrishna; Martial Hebert; J. Andrew Bagnell",
        "abstract": "We present a simple approach for producing a small number of structured visual outputs which have high recall, for a variety of tasks including monocular pose estimation and semantic scene segmentation. Current state-of-the-art approaches learn a single model and modify inference procedures to produce a small number of diverse predictions. We take the alternate route of modifying the learning procedure to directly optimize for good, high recall sequences of structured-output predictors. Our approach introduces no new parameters, naturally learns diverse predictions and is not tied to any specific structured learning or inference procedure. We leverage recent advances in the contextual submodular maximization literature to learn a sequence of predictors and empirically demonstrate the simplicity and performance of our approach on multiple challenging vision tasks including achieving state-of-the-art results on multiple predictions for monocular pose-estimation and image foreground/background segmentation.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Dey_Predicting_Multiple_Structured_ICCV_2015_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1577538,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14893922335485685432&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Dey_Predicting_Multiple_Structured_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Dey_2015_ICCV,\n    \n    author = {\n    Dey,\n    Debadeepta and Ramakrishna,\n    Varun and Hebert,\n    Martial and Bagnell,\n    J. Andrew\n},\n    title = {\n    Predicting Multiple Structured Visual Interpretations\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "965b9b7cad",
        "title": "Probabilistic Appearance Models for Segmentation and Classification",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Julia Krüger, Jan Ehrhardt, Heinz Handels",
        "author": "Julia Kruger; Jan Ehrhardt; Heinz Handels",
        "abstract": "Statistical shape and appearance models are often based on the accurate identification of one-to-one correspondences in a training data set. At the same time, the determination of these corresponding landmarks is the most challenging part of such methods. Hufnagel etal developed an alternative method using correspondence probabilities for a statistical shape model.  We propose the use of probabilistic correspondences for statistical appearance models by incorporating appearance information into the framework. A point-based representation is employed representing the image by a set of vectors assembling position and appearances. Using probabilistic correspondences between these multi-dimensional feature vectors eliminates the need for extensive preprocessing to find corresponding landmarks and reduces the dependence of the generated model on the landmark positions. Then, a maximum a-posteriori approach is used to derive a single global optimization criterion with respect to model parameters and observation dependent parameters, that directly affects shape and appearance information of the considered structures. Model generation and fitting can be expressed by optimizing the same criterion.  The developed framework describes the modeling process in a concise and flexible mathematical way and allows for additional constraints as topological regularity in the modeling process. Furthermore, it eliminates the demand for costly correspondence determination.  We apply the model for segmentation and landmark identification in hand X-ray images, where segmentation information is modeled as further features in the vectorial image representation. The results demonstrate the feasibility of the model to reconstruct contours and landmarks for unseen test images. Furthermore, we apply the model for tissue classification, where a model is generated for healthy brain tissue using 2D MRI slices. Applying the model to images of stroke patients the probabilistic correspondences are used to classify between healthy and pathological structures. The results demonstrate the ability of the probabilistic model to recognize healthy and pathological tissue automatically.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Kruger_Probabilistic_Appearance_Models_ICCV_2015_paper.pdf",
        "aff": "Institute of Medical Informatics, University of Lubeck, Germany; Institute of Medical Informatics, University of Lubeck, Germany; Institute of Medical Informatics, University of Lubeck, Germany",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1734570,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3918784013624470370&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "imi.uni-luebeck.de;imi.uni-luebeck.de;imi.uni-luebeck.de",
        "email": "imi.uni-luebeck.de;imi.uni-luebeck.de;imi.uni-luebeck.de",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Kruger_Probabilistic_Appearance_Models_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Lubeck",
        "aff_unique_dep": "Institute of Medical Informatics",
        "aff_unique_url": "https://www.uni-luebeck.de",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany",
        "bibtex": "@InProceedings{Kruger_2015_ICCV,\n    \n    author = {\n    Kruger,\n    Julia and Ehrhardt,\n    Jan and Handels,\n    Heinz\n},\n    title = {\n    Probabilistic Appearance Models for Segmentation and Classification\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "d0994d48c2",
        "title": "Probabilistic Label Relation Graphs With Ising Models",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Nan Ding, Jia Deng, Kevin P. Murphy, Hartmut Neven",
        "author": "Nan Ding; Jia Deng; Kevin P. Murphy; Hartmut Neven",
        "abstract": "We consider classification problems in which the label space has structure. A common example is hierarchical label spaces, corresponding to the case where one label subsumes another (e.g., animal subsumes dog). But labels can also be mutually exclusive (e.g., dog vs cat) or unrelated (e.g., furry, carnivore). To jointly model hierarchy and exclusion relations, the notion of a HEX (hierarchy and exclusion) graph was introduced. This combined a conditional random field (CRF) with a deep neural network (DNN), resulting in state of the art results when applied to visual object classification problems where the training labels were drawn from different levels of the ImageNet hierarchy (e.g., an image might be labeled with the basic level category \"dog\", rather than the more specific label \"husky\"). In this paper, we extend the HEX model to allow for soft or probabilistic relations between labels, which is useful when there is uncertainty about the relationship between two labels (e.g., an antelope is \"sort of\" furry, but not to the same degree as a grizzly bear). We call our new model pHEX, for probabilistic HEX. We show that the pHEX graph can be converted to an Ising model, which allows us to use existing off-the-shelf inference methods (in contrast to the HEX method, which needed specialized inference algorithms). Experimental results show significant improvements in a number of large-scale visual object classification tasks, outperforming the previous HEX model.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Ding_Probabilistic_Label_Relation_ICCV_2015_paper.pdf",
        "aff": "Google Inc.; University of Michigan; Google Inc.; Google Inc.",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1387881,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10050502831043293360&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "google.com;umich.edu;google.com;google.com",
        "email": "google.com;umich.edu;google.com;google.com",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Ding_Probabilistic_Label_Relation_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Google;University of Michigan",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.google.com;https://www.umich.edu",
        "aff_unique_abbr": "Google;UM",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Mountain View;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Ding_2015_ICCV,\n    \n    author = {\n    Ding,\n    Nan and Deng,\n    Jia and Murphy,\n    Kevin P. and Neven,\n    Hartmut\n},\n    title = {\n    Probabilistic Label Relation Graphs With Ising Models\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "606ebe6bb4",
        "title": "Procedural Editing of 3D Building Point Clouds",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "İlke Demir, Daniel G. Aliaga, Bedrich Benes",
        "author": "Ilke Demir; Daniel G. Aliaga; Bedrich Benes",
        "abstract": "Thanks to the recent advances in computational photography and remote sensing, point clouds of buildings are becoming increasingly available, yet their processing poses various challenges. In our work, we tackle the problem of point cloud completion and editing and we approach it via inverse procedural modeling. Contrary to the previous work, our approach operates directly on the point cloud without an intermediate triangulation. Our approach consists of 1) semi-automatic segmentation of the input point cloud with segment comparison and template matching to detect repeating structures, 2) a consensus-based voting schema and a pattern extraction algorithm to discover completed terminal geometry and their patterns of usage, all encoded into a context-free grammar, and 3) an interactive editing tool where the user can create new point clouds by using procedural copy and paste operations, and smart resizing. We demonstrate our approach on editing of building models with up to 1.8M points. In our implementation, preprocessing takes up to several minutes and a single editing operation needs from one second to one minute depending on the model size and the operation type.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Demir_Procedural_Editing_of_ICCV_2015_paper.pdf",
        "aff": "Purdue University; Purdue University; Purdue University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1876019,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13405070275411911202&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "purdue.edu;purdue.edu;purdue.edu",
        "email": "purdue.edu;purdue.edu;purdue.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Demir_Procedural_Editing_of_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Purdue University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.purdue.edu",
        "aff_unique_abbr": "Purdue",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Demir_2015_ICCV,\n    \n    author = {\n    Demir,\n    Ilke and Aliaga,\n    Daniel G. and Benes,\n    Bedrich\n},\n    title = {\n    Procedural Editing of 3D Building Point Clouds\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "1425686ae5",
        "title": "Projection Bank: From High-Dimensional Data to Medium-Length Binary Codes",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Li Liu, Mengyang Yu, Ling Shao",
        "author": "Li Liu; Mengyang Yu; Ling Shao",
        "abstract": "Recently, very high-dimensional feature representations, e.g., Fisher Vector, have achieved excellent performance for visual recognition and retrieval. However, these lengthy representations always cause extremely heavy computational and storage costs and even become unfeasible in some large-scale applications. A few existing techniques can transfer very high-dimensional data into binary codes, but they still require the reduced code length to be relatively long to maintain acceptable accuracies. To target a better balance between computational efficiency and accuracies, in this paper, we propose a novel embedding method called Binary Projection Bank (BPB), which can effectively reduce the very high-dimensional representations to medium-dimensional binary codes without sacrificing accuracies. Instead of using conventional single linear or bilinear projections, the proposed method learns a bank of small projections via the max-margin constraint to optimally preserve the intrinsic data similarity. We have systematically evaluated the proposed method on three datasets: Flickr 1M, ILSVR2010 and UCF101, showing competitive retrieval and recognition accuracies compared with state-of-the-art approaches, but with a significantly smaller memory footprint and lower coding complexity.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Liu_Projection_Bank_From_ICCV_2015_paper.pdf",
        "aff": "Department of Computer Science and Digital Technologies, Northumbria University, Newcastle upon Tyne, NE1 8ST, UK; Department of Computer Science and Digital Technologies, Northumbria University, Newcastle upon Tyne, NE1 8ST, UK; Department of Computer Science and Digital Technologies, Northumbria University, Newcastle upon Tyne, NE1 8ST, UK",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3060193,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6977153581408883825&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff_domain": "northumbria.ac.uk;ieee.org;ieee.org",
        "email": "northumbria.ac.uk;ieee.org;ieee.org",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Liu_Projection_Bank_From_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Northumbria University",
        "aff_unique_dep": "Department of Computer Science and Digital Technologies",
        "aff_unique_url": "https://www.northumbria.ac.uk",
        "aff_unique_abbr": "Northumbria",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Newcastle upon Tyne",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom",
        "bibtex": "@InProceedings{Liu_2015_ICCV,\n    \n    author = {\n    Liu,\n    Li and Yu,\n    Mengyang and Shao,\n    Ling\n},\n    title = {\n    Projection Bank: From High-Dimensional Data to Medium-Length Binary Codes\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "449b687146",
        "title": "Projection Onto the Manifold of Elongated Structures for Accurate Extraction",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Amos Sironi, Vincent Lepetit, Pascal Fua",
        "author": "Amos Sironi; Vincent Lepetit; Pascal Fua",
        "abstract": "Detection of elongated structures in 2D images and 3D image stacks is a critical prerequisite  in many  applications and  Machine Learning-based  approaches have recently been  shown to  deliver superior  performance.  However,  these methods essentially classify individual locations and do not explicitly model the strong relationship  that exists  between  neighboring ones.   As  a result,  isolated erroneous responses, discontinuities, and topological  errors are present in the resulting score maps.  We solve this  problem by projecting patches  of the score map  to their nearest neighbors in  a set  of ground  truth training  patches.  Our  algorithm induces global spatial consistency on the classifier  score map and returns results that are  provably geometrically consistent.   We apply  our  algorithm to  challenging datasets  in four  different domains  and show  that  it compares favorably to  state-of-the-art methods.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Sironi_Projection_Onto_the_ICCV_2015_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1297639,
        "gs_citation": 47,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16523678685587807684&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 20,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Sironi_Projection_Onto_the_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Sironi_2015_ICCV,\n    \n    author = {\n    Sironi,\n    Amos and Lepetit,\n    Vincent and Fua,\n    Pascal\n},\n    title = {\n    Projection Onto the Manifold of Elongated Structures for Accurate Extraction\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "6dd3aa9dad",
        "title": "Query Adaptive Similarity Measure for RGB-D Object Recognition",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Yanhua Cheng, Rui Cai, Chi Zhang, Zhiwei Li, Xin Zhao, Kaiqi Huang, Yong Rui",
        "author": "Yanhua Cheng; Rui Cai; Chi Zhang; Zhiwei Li; Xin Zhao; Kaiqi Huang; Yong Rui",
        "abstract": "This paper studies the problem of improving the top-1 accuracy of RGB-D object recognition. Despite of the impressive top-5 accuracies achieved by existing methods, their top-1 accuracies are not very satisfactory. The reasons are in two-fold: (1) existing similarity measures are sensitive to object pose and scale changes, as well as intra-class variations; and (2) effectively fusing RGB and depth cues is still an open problem. To address these problems, this paper first proposes a new similarity measure based on dense matching, through which objects in comparison are warped and aligned, to better tolerate variations. Towards RGB and depth fusion, we argue that a constant and golden weight doesn't exist. The two modalities have varying contributions when comparing objects from different categories. To capture such a dynamic characteristic, a group of matchers equipped with various fusion weights is constructed, to explore the responses of dense matching under different fusion configurations. All the response scores are finally merged following a learning-to-combination way, which provides quite good generalization ability in practice. The proposed approach win the best results on several public benchmarks, e.g., achieves 92.7% top-1 test accuracy on the Washington RGB-D object dataset, with a 5.1% improvement over the state-of-the-art.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Cheng_Query_Adaptive_Similarity_ICCV_2015_paper.pdf",
        "aff": "CRIPAC&NLPR, CASIA; Microsoft Research; Sun Yat-Sen University; Microsoft Research; CRIPAC&NLPR, CASIA; CAS Center for Excellence in Brain Science and Intelligence Technology; Microsoft Research",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1205772,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17541747370184664563&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "nlpr.ia.ac.cn;microsoft.com;microsoft.com;microsoft.com;nlpr.ia.ac.cn;nlpr.ia.ac.cn;microsoft.com",
        "email": "nlpr.ia.ac.cn;microsoft.com;microsoft.com;microsoft.com;nlpr.ia.ac.cn;nlpr.ia.ac.cn;microsoft.com",
        "author_num": 7,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Cheng_Query_Adaptive_Similarity_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;2;1;0;3;1",
        "aff_unique_norm": "Chinese Academy of Sciences Institute of Automation;Microsoft Corporation;Sun Yat-Sen University;Chinese Academy of Sciences",
        "aff_unique_dep": "CRIPAC (Computational Intelligence & Pattern Analysis Group) & NLPR (National Laboratory of Pattern Recognition);Microsoft Research;;Center for Excellence in Brain Science and Intelligence Technology",
        "aff_unique_url": "http://www.ia.cas.cn;https://www.microsoft.com/en-us/research;http://www.sysu.edu.cn/;http://www.cas.cn/",
        "aff_unique_abbr": "CASIA;MSR;SYSU;CAS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;1;0;0;1",
        "aff_country_unique": "China;United States",
        "bibtex": "@InProceedings{Cheng_2015_ICCV,\n    \n    author = {\n    Cheng,\n    Yanhua and Cai,\n    Rui and Zhang,\n    Chi and Li,\n    Zhiwei and Zhao,\n    Xin and Huang,\n    Kaiqi and Rui,\n    Yong\n},\n    title = {\n    Query Adaptive Similarity Measure for RGB-D Object Recognition\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "a58aa5a0fc",
        "title": "RGB-Guided Hyperspectral Image Upsampling",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Hyeokhyen Kwon, Yu-Wing Tai",
        "author": "Hyeokhyen Kwon; Yu-Wing Tai",
        "abstract": "Hyperspectral imaging usually lack of spatial resolution due to limitations of hardware design of imaging sensors. On the contrary, latest imaging sensors capture a RGB image with resolution of multiple times larger than a hyperspectral image. In this paper, we present an algorithm to enhance and upsample the resolution of hyperspectral images. Our algorithm consists of two stages: spatial upsampling stage and spectrum substitution stage. The spatial upsampling stage is guided by a high resolution RGB image of the same scene, and the spectrum substitution stage utilizes sparse coding to locally refine the upsampled hyperspectral image through dictionary substitution. Experiments show that our algorithm is highly effective and has outperformed state-of-the-art matrix factorization based approaches.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Kwon_RGB-Guided_Hyperspectral_Image_ICCV_2015_paper.pdf",
        "aff": "KAIST; SenseTime Group Limited",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2510278,
        "gs_citation": 75,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14137166801573093370&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "kaist.ac.kr;gmail.com",
        "email": "kaist.ac.kr;gmail.com",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Kwon_RGB-Guided_Hyperspectral_Image_ICCV_2015_paper.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Korea Advanced Institute of Science and Technology;SenseTime Group Limited",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.kaist.ac.kr;https://www.sensetime.com",
        "aff_unique_abbr": "KAIST;SenseTime",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "South Korea;China",
        "bibtex": "@InProceedings{Kwon_2015_ICCV,\n    \n    author = {\n    Kwon,\n    Hyeokhyen and Tai,\n    Yu-Wing\n},\n    title = {\n    RGB-Guided Hyperspectral Image Upsampling\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "2d417000ff",
        "title": "RGB-W: When Vision Meets Wireless",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Alexandre Alahi, Albert Haque, Li Fei-Fei",
        "author": "Alexandre Alahi; Albert Haque; Li Fei-Fei",
        "abstract": "Inspired by the recent success of RGB-D cameras, we propose the enrichment of RGB data with an additional \"quasi-free\" modality, namely, the wireless signal (e.g., wifi or Bluetooth) emitted by individuals' cell phones, referred to as RGB-W. The received signal strength acts as a rough proxy for depth and a reliable cue on their identity. Although the measured signals are highly noisy (more than 2m average localization error), we demonstrate that the combination of visual and wireless data significantly improves the localization accuracy.   We introduce a novel image-driven representation of wireless data which embeds all received signals onto a single image. We then indicate the ability of this additional data to (i) locate persons within a sparsity-driven framework and to (ii) track individuals with a new confidence measure on the data association problem. Our solution outperforms existing localization methods by a significant margin. It can be applied to the millions of currently installed RGB cameras to better analyze human behavior and offer the next generation of high-accuracy location-based services.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Alahi_RGB-W_When_Vision_ICCV_2015_paper.pdf",
        "aff": "Computer Science Department, Stanford University; Computer Science Department, Stanford University; Computer Science Department, Stanford University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1283991,
        "gs_citation": 79,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12707938803466875351&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff_domain": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Alahi_RGB-W_When_Vision_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Alahi_2015_ICCV,\n    \n    author = {\n    Alahi,\n    Alexandre and Haque,\n    Albert and Fei-Fei,\n    Li\n},\n    title = {\n    RGB-W: When Vision Meets Wireless\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "8e4d034a8d",
        "title": "RIDE: Reversal Invariant Descriptor Enhancement",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Lingxi Xie, Jingdong Wang, Weiyao Lin, Bo Zhang, Qi Tian",
        "author": "Lingxi Xie; Jingdong Wang; Weiyao Lin; Bo Zhang; Qi Tian",
        "abstract": "In many fine-grained object recognition datasets, image orientation (left/right) might vary from sample to sample. Since handcrafted descriptors such as SIFT are not reversal invariant, the stability of image representation based on them is consequently limited. A popular solution is to augment the datasets by adding a left-right reversed copy for each original image. This strategy improves recognition accuracy to some extent, but also brings the price of almost doubled time and memory consumptions.  In this paper, we present RIDE (Reversal Invariant Descriptor Enhancement) for fine-grained object recognition. RIDE is a generalized algorithm which cancels out the impact of image reversal by estimating the orientation of local descriptors, and guarantees to produce the identical representation for an image and its left-right reversed copy. Experimental results reveal the consistent accuracy gain of RIDE with various types of descriptors. We also provide insightful discussions on the working mechanism of RIDE and its generalization to other applications.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Xie_RIDE_Reversal_Invariant_ICCV_2015_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5278670673850746632&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Xie_RIDE_Reversal_Invariant_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Xie_2015_ICCV,\n    \n    author = {\n    Xie,\n    Lingxi and Wang,\n    Jingdong and Lin,\n    Weiyao and Zhang,\n    Bo and Tian,\n    Qi\n},\n    title = {\n    RIDE: Reversal Invariant Descriptor Enhancement\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "6483bddb9a",
        "title": "Real-Time Pose Estimation Piggybacked on Object Detection",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Roman Juránek, Adam Herout, Markéta Dubská, Pavel Zemčík",
        "author": "Roman Juranek; Adam Herout; Marketa Dubska; Pavel Zemcik",
        "abstract": "We present an object detector coupled with pose estimation directly in a single compact and simple model, where the detector shares extracted image features with the pose estimator. The output of the classification of each candidate window consists of both object score and likelihood map of poses. This extension introduces negligible overhead during detection so that the detector is still capable of real time operation. We evaluated the proposed approach on the problem of vehicle detection. We used existing datasets with viewpoint/pose annotation (WCVP, 3D objects, KITTI).  Besides that, we collected a new traffic surveillance dataset COD20k which fills certain gaps of the existing datasets and we make it public. The experimental results show that the proposed approach is comparable with state-of-the-art approaches in terms of accuracy, but it is considerably faster - easily operating in real time (Matlab with C++ code). The source codes and the collected COD20k dataset are made public along with the paper.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Juranek_Real-Time_Pose_Estimation_ICCV_2015_paper.pdf",
        "aff": "Graph@FIT, Brno University of Technology; Graph@FIT, Brno University of Technology; Graph@FIT, Brno University of Technology; Graph@FIT, Brno University of Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2106438,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14840488460539801221&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "fit.vutbr.cz;fit.vutbr.cz;fit.vutbr.cz;fit.vutbr.cz",
        "email": "fit.vutbr.cz;fit.vutbr.cz;fit.vutbr.cz;fit.vutbr.cz",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Juranek_Real-Time_Pose_Estimation_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Brno University of Technology",
        "aff_unique_dep": "Faculty of Information Technology",
        "aff_unique_url": "https://www.fit.vutbr.cz",
        "aff_unique_abbr": "Brno University of Technology",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Brno",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Czech Republic",
        "bibtex": "@InProceedings{Juranek_2015_ICCV,\n    \n    author = {\n    Juranek,\n    Roman and Herout,\n    Adam and Dubska,\n    Marketa and Zemcik,\n    Pavel\n},\n    title = {\n    Real-Time Pose Estimation Piggybacked on Object Detection\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "03b1771f3a",
        "title": "Realtime Edge-Based Visual Odometry for a Monocular Camera",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Juan José Tarrio, Sol Pedre",
        "author": "Juan Jose Tarrio; Sol Pedre",
        "abstract": "In this work we present a novel algorithm for realtime visual odometry for a monocular camera. The main idea is to develop an approach between classical feature-based visual odometry systems and modern direct dense/semi-dense methods, trying to benefit from the best attributes of both. Similar to feature-based systems, we extract information from the images, instead of working with raw image intensities as direct methods. In particular, the information extracted are the edges present in the image, while the rest of the algorithm is designed to take advantage of the structural information provided when pixels are treated as edges. Edge extraction is an efficient and higly parallelizable operation. The edge depth information extracted is dense enough to allow acceptable surface fitting, similar to modern semi-dense methods. This is a valuable attribute that feature-based odometry lacks. Experimental results show that the proposed method has similar drift than state of the art feature-based and direct methods, and is a simple algorithm that runs at realtime and can be parallelized. Finally, we have also developed an inertial aided version that successfully stabilizes an unmanned air vehicle in complex indoor environments using only a frontal camera, while running the complete solution in the embedded hardware on board the vehicle.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Tarrio_Realtime_Edge-Based_Visual_ICCV_2015_paper.pdf",
        "aff": ";",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 676137,
        "gs_citation": 109,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15817183037433660696&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Tarrio_Realtime_Edge-Based_Visual_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Tarrio_2015_ICCV,\n    \n    author = {\n    Tarrio,\n    Juan Jose and Pedre,\n    Sol\n},\n    title = {\n    Realtime Edge-Based Visual Odometry for a Monocular Camera\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "fb8261057d",
        "title": "Recurrent Network Models for Human Dynamics",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Katerina Fragkiadaki, Sergey Levine, Panna Felsen, Jitendra Malik",
        "author": "Katerina Fragkiadaki; Sergey Levine; Panna Felsen; Jitendra Malik",
        "abstract": "We propose the Encoder-Recurrent-Decoder (ERD) model for recognition and prediction of human body pose in videos and motion capture. The ERD model is a  recurrent neural network that incorporates nonlinear encoder and decoder networks before and after  recurrent layers.  We test instantiations of ERD architectures in the tasks of motion capture (mocap) generation, body pose labeling and body pose forecasting in videos. Our model handles mocap training data across multiple subjects and activity domains, and  synthesizes novel motions while avoiding drifting for long periods of time.     For human pose labeling, ERD outperforms a per frame body part detector by resolving left-right body part confusions.  For video pose forecasting, ERD  predicts body joint displacements across a temporal horizon of 400ms and outperforms a first order motion model based on optical flow.  ERDs extend previous  Long Short Term Memory (LSTM) models  in the literature to jointly learn representations and their dynamics. Our experiments show such representation learning is crucial for both labeling and prediction in space-time. We find this is a distinguishing feature between the spatio-temporal visual domain in comparison to  1D text, speech or  handwriting, where straightforward hard coded representations have shown excellent results when directly combined with recurrent units.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Fragkiadaki_Recurrent_Network_Models_ICCV_2015_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 1203,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4892488825670482291&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Fragkiadaki_Recurrent_Network_Models_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Fragkiadaki_2015_ICCV,\n    \n    author = {\n    Fragkiadaki,\n    Katerina and Levine,\n    Sergey and Felsen,\n    Panna and Malik,\n    Jitendra\n},\n    title = {\n    Recurrent Network Models for Human Dynamics\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "7d203768d4",
        "title": "Recursive Frechet Mean Computation on the Grassmannian and its Applications to Computer Vision",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Rudrasis Chakraborty, Baba C. Vemuri",
        "author": "Rudrasis Chakraborty; Baba C. Vemuri",
        "abstract": "In the past decade, Grassmann manifolds (Grassmannian) have been commonly used in mathematical formulations of many Computer Vision tasks. Averaging points on a Grassmann manifold is a very common operation in many applications including but not limited to, tracking, action recognition, video-face recognition, face recognition, etc. Computing the intrinsic/Frechet mean (FM) of a set of points on the Grassmann can be cast as finding the global optimum (if it exists) of the sum of squared geodesic distances cost function. A common approach to solve this problem involves the use of the gradient descent method.  An alternative way to compute the FM is to develop a recursive/inductive definition that does not involve optimizing the aforementioned cost function.  In this paper, we propose one such computationally efficient algorithm called the it Grassmann   inductive Frechet mean estimator (GiFME). In developing the recursive solution to find the FM of the given set of points, GiFME exploits the fact that there is a closed form solution to find the FM of two points on the Grassmann.  In the limit as the number of samples tends to infinity, we prove that GiFME converges to the FM (this is called the weak consistency result on the Grassmann manifold).  Further, for the finite sample case, in the limit as the number of sample paths (trials) goes to infinity, we show that GiFME converges to the finite sample FM. Moreover, we present a bound on the geodesic distance between the estimate from GiFME and the true FM. We present several experiments on synthetic and real data sets to demonstrate the performance of GiFME in comparison to the gradient descent based (batch mode) technique. Our goal in these applications is to demonstrate the computational advantage and achieve comparable accuracy to the state-of-the-art.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Chakraborty_Recursive_Frechet_Mean_ICCV_2015_paper.pdf",
        "aff": "Department of CISE, University of Florida; Department of CISE, University of Florida",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 596326,
        "gs_citation": 50,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8172977743186129814&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "cise.ufl.edu;cise.ufl.edu",
        "email": "cise.ufl.edu;cise.ufl.edu",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Chakraborty_Recursive_Frechet_Mean_ICCV_2015_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Florida",
        "aff_unique_dep": "Department of CISE",
        "aff_unique_url": "https://www.ufl.edu",
        "aff_unique_abbr": "UF",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Chakraborty_2015_ICCV,\n    \n    author = {\n    Chakraborty,\n    Rudrasis and Vemuri,\n    Baba C.\n},\n    title = {\n    Recursive Frechet Mean Computation on the Grassmannian and its Applications to Computer Vision\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "a055dee0df",
        "title": "Reflection Modeling for Passive Stereo",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Rahul Nair, Andrew Fitzgibbon, Daniel Kondermann, Carsten Rother",
        "author": "Rahul Nair; Andrew Fitzgibbon; Daniel Kondermann; Carsten Rother",
        "abstract": "Stereo reconstruction in presence of reality faces many challenges that still need to be addressed. This paper considers reflections, which introduce incorrect matches due to the observation violating the diffuse-world assumption underlying the majority of stereo techniques. Unlike most existing work, which employ regularization or robust data terms to suppress such errors,  we derive two least squares models from first principles that generalize diffuse world stereo  and explicitly take reflections into account.   These models are parametrized by depth, orientation and material properties, resulting in a total of up to 5 parameters per pixel that have to be estimated.  Additionally large non-local interactions between viewed and reflected surface have to be taken into account.  These two properties make inference of the model appear prohibitive, but we present evidence that inference is actually possible using a variant of patch match stereo.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Nair_Reflection_Modeling_for_ICCV_2015_paper.pdf",
        "aff": "Heidelberg Collaboratory for Image Processing, Heidelberg University, Germany; Microsoft Research, Cambridge, UK; Heidelberg Collaboratory for Image Processing, Heidelberg University, Germany + Computer Vision Lab Dresden, Technical University Dresden, Germany; Computer Vision Lab Dresden, Technical University Dresden, Germany",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1245538,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11515535039375766640&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "iwr.uni-heidelberg.de;microsoft.com;iwr.uni-heidelberg.de;tu-dresden.de",
        "email": "iwr.uni-heidelberg.de;microsoft.com;iwr.uni-heidelberg.de;tu-dresden.de",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Nair_Reflection_Modeling_for_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;0+2;2",
        "aff_unique_norm": "Heidelberg University;Microsoft Research;Technical University Dresden",
        "aff_unique_dep": "Heidelberg Collaboratory for Image Processing;;Computer Vision Lab",
        "aff_unique_url": "https://www.uni-heidelberg.de;https://www.microsoft.com/en-us/research;https://tu-dresden.de",
        "aff_unique_abbr": "Uni Heidelberg;MSR;TUD",
        "aff_campus_unique_index": "0;1;0+2;2",
        "aff_campus_unique": "Heidelberg;Cambridge;Dresden",
        "aff_country_unique_index": "0;1;0+0;0",
        "aff_country_unique": "Germany;United Kingdom",
        "bibtex": "@InProceedings{Nair_2015_ICCV,\n    \n    author = {\n    Nair,\n    Rahul and Fitzgibbon,\n    Andrew and Kondermann,\n    Daniel and Rother,\n    Carsten\n},\n    title = {\n    Reflection Modeling for Passive Stereo\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "316aa88058",
        "title": "Registering Images to Untextured Geometry Using Average Shading Gradients",
        "session": "registration, alignment and stereo",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "Tobias Plötz, Stefan Roth",
        "author": "Tobias Plotz; Stefan Roth",
        "abstract": "Many existing approaches for image-to-geometry registration assume that either a textured 3D model or a good initial guess of the 3D pose is available to bootstrap the registration process. In this paper we consider the registration of photographs to 3D models even when no texture information is available. This is very challenging as we cannot rely on texture gradients, and even shading gradients are hard to estimate since the lighting conditions are unknown. To that end, we propose average shading gradients, a rendering technique that estimates the average gradient magnitude over all lighting directions under Lambertian shading. We use this gradient representation as the building block of a registration pipeline based on matching sparse features. To cope with inevitable false matches due to the missing texture information and to increase robustness, the pose of the 3D model is estimated in two stages. Coarse pose hypotheses are first obtained from a single correct match each, subsequently refined using SIFT flow, and finally verified. We apply our algorithm to registering images of real-world objects to untextured 3D meshes of limited accuracy.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Plotz_Registering_Images_to_ICCV_2015_paper.pdf",
        "aff": "Department of Computer Science, TU Darmstadt; Department of Computer Science, TU Darmstadt",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3381572,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16873202640309663658&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Plotz_Registering_Images_to_ICCV_2015_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Technische Universität Darmstadt",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.tu-darmstadt.de",
        "aff_unique_abbr": "TU Darmstadt",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Darmstadt",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany",
        "bibtex": "@InProceedings{Plotz_2015_ICCV,\n    \n    author = {\n    Plotz,\n    Tobias and Roth,\n    Stefan\n},\n    title = {\n    Registering Images to Untextured Geometry Using Average Shading Gradients\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "24e796c07d",
        "title": "Regressing a 3D Face Shape From a Single Image",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Sergey Tulyakov, Nicu Sebe",
        "author": "Sergey Tulyakov; Nicu Sebe",
        "abstract": "In this work we present a method to estimate a 3D face shape from a single image. Our method is based on a cascade regression framework that directly estimates face landmarks locations in 3D. We include the knowledge that a face is a 3D object into the learning pipeline and show how this information decreases localization errors while keeping the computational time low. We predict the actual positions of the landmarks even if they are occluded due to face rotation. To support the ability of our method to reliably reconstruct 3D shapes, we introduce a simple method for head pose estimation using a single image that reaches higher accuracy than the state of the art. Comparison of 3D face landmarks localization with the available state of the art further supports the feasibility of a single-step face shape estimation. The code, trained models and our 3D annotations will be made available to the research community.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Tulyakov_Regressing_a_3D_ICCV_2015_paper.pdf",
        "aff": "University of Trento, Italy; University of Trento, Italy",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 18649851,
        "gs_citation": 74,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12420431884215954636&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "unitn.it;disi.unitn.it",
        "email": "unitn.it;disi.unitn.it",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Tulyakov_Regressing_a_3D_ICCV_2015_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Trento",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.unitn.it",
        "aff_unique_abbr": "UniTN",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Italy",
        "bibtex": "@InProceedings{Tulyakov_2015_ICCV,\n    \n    author = {\n    Tulyakov,\n    Sergey and Sebe,\n    Nicu\n},\n    title = {\n    Regressing a 3D Face Shape From a Single Image\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "f66954db66",
        "title": "Regressive Tree Structured Model for Facial Landmark Localization",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Gee-Sern Hsu, Kai-Hsiang Chang, Shih-Chieh Huang",
        "author": "Gee-Sern Hsu; Kai-Hsiang Chang; Shih-Chieh Huang",
        "abstract": "Although the Tree Structured Model (TSM) is proven effective for solving face detection, pose estimation and landmark localization in an unified model, its sluggish run time makes it unfavorable in practical applications, especially when dealing with cases of multiple faces. We propose the Regressive Tree Structure Model (RTSM) to improve the run-time speed and localization accuracy. The RTSM is composed of two component TSMs, the coarse TSM (c-TSM) and the refined TSM (r-TSM), and a Bilateral Support Vector Regressor (BSVR). The c-TSM is built on the low-resolution octaves of samples so that it provides coarse but fast face detection. The r-TSM is built on the mid-resolution octaves so that it can locate the landmarks on the face candidates given by the c-TSM and improve precision. The r-TSM based landmarks are used in the forward BSVR as references to locate the dense set of landmarks, which are then used in the backward BSVR to relocate the landmarks with large localization errors. The forward and backward regression goes on iteratively until convergence. The performance of the RTSM is validated on three benchmark databases, the Multi-PIE, LFPW and AFW, and compared with the latest TSM to demonstrate its efficacy.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Hsu_Regressive_Tree_Structured_ICCV_2015_paper.pdf",
        "aff": "Artiﬁcial Vision Lab., Dept Mechanical Engineering, National Taiwan University of Science and Technology; Artiﬁcial Vision Lab., Dept Mechanical Engineering, National Taiwan University of Science and Technology; Artiﬁcial Vision Lab., Dept Mechanical Engineering, National Taiwan University of Science and Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 982543,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13554810913428558068&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "mail.ntust.edu.tw; ; ",
        "email": "mail.ntust.edu.tw; ; ",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Hsu_Regressive_Tree_Structured_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "National Taiwan University of Science and Technology",
        "aff_unique_dep": "Dept Mechanical Engineering",
        "aff_unique_url": "https://www.ntust.edu.tw",
        "aff_unique_abbr": "NTUST",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Taiwan",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Hsu_2015_ICCV,\n    \n    author = {\n    Hsu,\n    Gee-Sern and Chang,\n    Kai-Hsiang and Huang,\n    Shih-Chieh\n},\n    title = {\n    Regressive Tree Structured Model for Facial Landmark Localization\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "1bc98e5134",
        "title": "Relaxed Multiple-Instance SVM With Application to Object Discovery",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Xinggang Wang, Zhuotun Zhu, Cong Yao, Xiang Bai",
        "author": "Xinggang Wang; Zhuotun Zhu; Cong Yao; Xiang Bai",
        "abstract": "Multiple-instance learning (MIL) has served as an important tool for a wide range of vision applications, for instance, image classification, object detection, and visual tracking. In this paper, we propose a novel method to solve the classical MIL problem, named relaxed multiple-instance SVM (RMI-SVM). We treat the positiveness of instance as a continuous variable, use Noisy-OR model to enforce the MIL constraints, and optimize them jointly in a unified framework. The optimization problem can be efficiently solved using stochastic gradient decent. The extensive experiments demonstrate that RMI-SVM consistently achieves superior performance on various benchmarks for MIL. Moreover, we simply applied RMI-SVM to a challenging vision task, common object discovery. The state-of-the arts results of object discovery on PASCAL VOC datasets further confirm the advantages of the proposed method.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Wang_Relaxed_Multiple-Instance_SVM_ICCV_2015_paper.pdf",
        "aff": "School of Electronic Information and Communications, Huazhong University of Science and Technology; School of Electronic Information and Communications, Huazhong University of Science and Technology; ; School of Electronic Information and Communications, Huazhong University of Science and Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3954350,
        "gs_citation": 111,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15392142402604047545&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "hust.edu.cn;hust.edu.cn;gmail.com;hust.edu.cn",
        "email": "hust.edu.cn;hust.edu.cn;gmail.com;hust.edu.cn",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Wang_Relaxed_Multiple-Instance_SVM_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Huazhong University of Science and Technology",
        "aff_unique_dep": "School of Electronic Information and Communications",
        "aff_unique_url": "http://www.hust.edu.cn",
        "aff_unique_abbr": "HUST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Wang_2015_ICCV,\n    \n    author = {\n    Wang,\n    Xinggang and Zhu,\n    Zhuotun and Yao,\n    Cong and Bai,\n    Xiang\n},\n    title = {\n    Relaxed Multiple-Instance SVM With Application to Object Discovery\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "f52c84ca48",
        "title": "Relaxing From Vocabulary: Robust Weakly-Supervised Deep Learning for Vocabulary-Free Image Tagging",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Jianlong Fu, Yue Wu, Tao Mei, Jinqiao Wang, Hanqing Lu, Yong Rui",
        "author": "Jianlong Fu; Yue Wu; Tao Mei; Jinqiao Wang; Hanqing Lu; Yong Rui",
        "abstract": "The development of deep learning has empowered machines with comparable capability of recognizing limited image categories to human beings. However, most existing approaches heavily rely on human-curated training data, which hinders the scalability to large and unlabeled vocabularies in image tagging. In this paper, we propose a weakly-supervised deep learning model which can be trained from the readily available Web images to relax the dependence on human labors and scale up to arbitrary tags (categories). Specifically, based on the assumption that features of true samples in a category tend to be similar and noises tend to be variant, we embed the feature map of the last deep layer into a new affinity representation, and further minimize the discrepancy between the affinity representation and its low-rank approximation. The discrepancy is finally transformed into the objective function to give relevance feedback to back propagation. Experiments show that we can achieve a performance gain of 14.0% in terms of a semantic-based relevance metric in image tagging with 63,043 tags from the WordNet, against the typical deep model trained on the ImageNet 1,000 vocabulary set.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Fu_Relaxing_From_Vocabulary_ICCV_2015_paper.pdf",
        "aff": "National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China + Microsoft Research, Beijing, China; University of Science and Technology of China, Hefei, China; Microsoft Research, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Microsoft Research, Beijing, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 819206,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16827464848735369824&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "microsoft.com;mail.ustc.edu.cn;microsoft.com;nlpr.ia.ac.cn;nlpr.ia.ac.cn;microsoft.com",
        "email": "microsoft.com;mail.ustc.edu.cn;microsoft.com;nlpr.ia.ac.cn;nlpr.ia.ac.cn;microsoft.com",
        "author_num": 6,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Fu_Relaxing_From_Vocabulary_ICCV_2015_paper.html",
        "aff_unique_index": "0+1;2;1;0;0;1",
        "aff_unique_norm": "Chinese Academy of Sciences;Microsoft Research;University of Science and Technology of China",
        "aff_unique_dep": "Institute of Automation;;",
        "aff_unique_url": "http://www.ia.cas.cn;https://www.microsoft.com/en-us/research/group/microsoft-research-asia;http://www.ustc.edu.cn",
        "aff_unique_abbr": "CAS;MSR;USTC",
        "aff_campus_unique_index": "0+0;1;0;0;0;0",
        "aff_campus_unique": "Beijing;Hefei",
        "aff_country_unique_index": "0+0;0;0;0;0;0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Fu_2015_ICCV,\n    \n    author = {\n    Fu,\n    Jianlong and Wu,\n    Yue and Mei,\n    Tao and Wang,\n    Jinqiao and Lu,\n    Hanqing and Rui,\n    Yong\n},\n    title = {\n    Relaxing From Vocabulary: Robust Weakly-Supervised Deep Learning for Vocabulary-Free Image Tagging\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "a89940b2c0",
        "title": "Removing Rain From a Single Image via Discriminative Sparse Coding",
        "session": "computational photography and image enhancement",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "Yu Luo, Yong Xu, Hui Ji",
        "author": "Yu Luo; Yong Xu; Hui Ji",
        "abstract": "Visual distortions on images caused by bad weather conditions can have a negative impact on the performance of many outdoor vision systems. One often seen bad weather is rain which causes  significant yet complex local  intensity fluctuations in images. The paper aims at developing an effective algorithm to remove visual effects of rain from a single rainy image, i.e. separate  the rain layer and the de-rained image layer from an rainy image.  Built upon a non-linear generative model of rainy image, namely screen blend mode, we proposed a dictionary learning based algorithm for single image de-raining. The basic idea is to sparsely approximate the patches of two layers by very high discriminative codes over a learned dictionary with strong mutual exclusivity property. Such discriminative sparse codes lead to accurate separation of two layers from their non-linear composite. The experiments showed that the proposed method outperformed the existing single image de-raining methods on tested rain images.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Luo_Removing_Rain_From_ICCV_2015_paper.pdf",
        "aff": "School of Computer Science & Engineering, South China University of Technology, Guangzhou 510006, China + Department of Mathematics, National University of Singapore, Singapore 119076; School of Computer Science & Engineering, South China University of Technology, Guangzhou 510006, China; Department of Mathematics, National University of Singapore, Singapore 119076",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2036733,
        "gs_citation": 945,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9023302150417163363&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "mail.scut.edu.cn;scut.edu.cn;nus.edu.sg",
        "email": "mail.scut.edu.cn;scut.edu.cn;nus.edu.sg",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Luo_Removing_Rain_From_ICCV_2015_paper.html",
        "aff_unique_index": "0+1;0;1",
        "aff_unique_norm": "South China University of Technology;National University of Singapore",
        "aff_unique_dep": "School of Computer Science & Engineering;Department of Mathematics",
        "aff_unique_url": "http://www.scut.edu.cn;https://www.nus.edu.sg",
        "aff_unique_abbr": "SCUT;NUS",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Guangzhou;",
        "aff_country_unique_index": "0+1;0;1",
        "aff_country_unique": "China;Singapore",
        "bibtex": "@InProceedings{Luo_2015_ICCV,\n    \n    author = {\n    Luo,\n    Yu and Xu,\n    Yong and Ji,\n    Hui\n},\n    title = {\n    Removing Rain From a Single Image via Discriminative Sparse Coding\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "4c92c449e8",
        "title": "Render for CNN: Viewpoint Estimation in Images Using CNNs Trained With Rendered 3D Model Views",
        "session": "3d representations for recognition and localization",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "Hao Su, Charles R. Qi, Yangyan Li, Leonidas J. Guibas",
        "author": "Hao Su; Charles R. Qi; Yangyan Li; Leonidas J. Guibas",
        "abstract": "Object viewpoint estimation from 2D images is an essential task in computer vision. However, two issues hinder its progress: scarcity of training data with viewpoint annotations, and a lack of powerful features. Inspired by the growing availability of 3D models, we propose a framework to address both issues by combining render-based image synthesis and CNNs (Convolutional Neural Networks). We believe that 3D models have the potential in generating a large number of images of high variation, which can be well exploited by deep CNN with a high learning capacity. Towards this goal, we propose a scalable and overfit-resistant image synthesis pipeline, together with a novel CNN specifically tailored for the viewpoint estimation task. Experimentally, we show that the viewpoint estimation from our pipeline can significantly outperform state-of-the-art methods on PASCAL 3D+ benchmark.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Su_Render_for_CNN_ICCV_2015_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 5551357,
        "gs_citation": 966,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1209553997502402606&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Su_Render_for_CNN_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Su_2015_ICCV,\n    \n    author = {\n    Su,\n    Hao and Qi,\n    Charles R. and Li,\n    Yangyan and Guibas,\n    Leonidas J.\n},\n    title = {\n    Render for CNN: Viewpoint Estimation in Images Using CNNs Trained With Rendered 3D Model Views\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "d48886cfe6",
        "title": "Rendering of Eyes for Eye-Shape Registration and Gaze Estimation",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Erroll Wood, Tadas Baltrušaitis, Xucong Zhang, Yusuke Sugano, Peter Robinson, Andreas Bulling",
        "author": "Erroll Wood; Tadas Baltrusaitis; Xucong Zhang; Yusuke Sugano; Peter Robinson; Andreas Bulling",
        "abstract": "Images of the eye are key in several computer vision problems, such as shape registration and gaze estimation. Recent large-scale supervised methods for these problems require time-consuming data collection and manual annotation, which can be unreliable. We propose synthesizing perfectly labelled photo-realistic training data in a fraction of the time. We used computer graphics techniques to build a collection of dynamic eye-region models from head scan geometry. These were randomly posed to synthesize close-up eye images for a wide range of head poses, gaze directions, and illumination conditions. We used our model's controllability to verify the importance of realistic illumination and shape variations in eye-region training data. Finally, we demonstrate the benefits of our synthesized training data (SynthesEyes) by out-performing state-of-the-art methods for eye-shape registration as well as cross-dataset appearance-based gaze estimation in the wild.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Wood_Rendering_of_Eyes_ICCV_2015_paper.pdf",
        "aff": "University of Cambridge, United Kingdom; University of Cambridge, United Kingdom; Max Planck Institute for Informatics, Germany; Max Planck Institute for Informatics, Germany; University of Cambridge, United Kingdom; Max Planck Institute for Informatics, Germany",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 9547485,
        "gs_citation": 422,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=964871139004991733&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 21,
        "aff_domain": "cam.ac.uk;cam.ac.uk;mpi-inf.mpg.de;mpi-inf.mpg.de;cam.ac.uk;mpi-inf.mpg.de",
        "email": "cam.ac.uk;cam.ac.uk;mpi-inf.mpg.de;mpi-inf.mpg.de;cam.ac.uk;mpi-inf.mpg.de",
        "author_num": 6,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Wood_Rendering_of_Eyes_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;1;1;0;1",
        "aff_unique_norm": "University of Cambridge;Max Planck Institute for Informatics",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cam.ac.uk;https://mpi-inf.mpg.de",
        "aff_unique_abbr": "Cambridge;MPII",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;0;1;1;0;1",
        "aff_country_unique": "United Kingdom;Germany",
        "bibtex": "@InProceedings{Wood_2015_ICCV,\n    \n    author = {\n    Wood,\n    Erroll and Baltrusaitis,\n    Tadas and Zhang,\n    Xucong and Sugano,\n    Yusuke and Robinson,\n    Peter and Bulling,\n    Andreas\n},\n    title = {\n    Rendering of Eyes for Eye-Shape Registration and Gaze Estimation\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "fd0d575af0",
        "title": "Resolving Scale Ambiguity Via XSlit Aspect Ratio Analysis",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Wei Yang, Haiting Lin, Sing Bing Kang, Jingyi Yu",
        "author": "Wei Yang; Haiting Lin; Sing Bing Kang; Jingyi Yu",
        "abstract": "In perspective cameras, images of a frontal-parallel 3D object preserve its aspect ratio invariant to its depth. Such an invariance is useful in photography but is unique to perspective projection. In this paper, we show that alternative non-perspective cameras such as the crossed-slit or XSlit cameras exhibit a different depth-dependent aspect ratio (DDAR) property that can be used to 3D recovery. We first conduct a comprehensive analysis to characterize DDAR, infer object depth from its AR, and model recoverable depth range, sensitivity, and error. We show that repeated shape patterns in real Manhattan World scenes can be used for 3D reconstruction using a single XSlit image. We also extend our analysis to model slopes of lines. Specifically, parallel 3D lines exhibit depth-dependent slopes (DDS) on their images which can also be used to infer their depths. We validate our analyses using real XSlit cameras, XSlit panoramas, and catadioptric mirrors. Experiments show that DDAR and DDS provide important depth cues and enable effective single-image scene reconstruction.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Yang_Resolving_Scale_Ambiguity_ICCV_2015_paper.pdf",
        "aff": "University of Delaware; University of Delaware; Microsoft Research; University of Delaware + ShanghaiTech University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 15111256,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7478891939104240603&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "udel.edu;udel.edu;microsoft.com;eecis.udel.edu",
        "email": "udel.edu;udel.edu;microsoft.com;eecis.udel.edu",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Yang_Resolving_Scale_Ambiguity_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;1;0+2",
        "aff_unique_norm": "University of Delaware;Microsoft Corporation;ShanghaiTech University",
        "aff_unique_dep": ";Microsoft Research;",
        "aff_unique_url": "https://www.udel.edu;https://www.microsoft.com/en-us/research;https://www.shanghaitech.edu.cn",
        "aff_unique_abbr": "UD;MSR;ShanghaiTech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0+1",
        "aff_country_unique": "United States;China",
        "bibtex": "@InProceedings{Yang_2015_ICCV,\n    \n    author = {\n    Yang,\n    Wei and Lin,\n    Haiting and Kang,\n    Sing Bing and Yu,\n    Jingyi\n},\n    title = {\n    Resolving Scale Ambiguity Via XSlit Aspect Ratio Analysis\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "8d9e55ac95",
        "title": "Robust Facial Landmark Detection Under Significant Head Poses and Occlusion",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Yue Wu, Qiang Ji",
        "author": "Yue Wu; Qiang Ji",
        "abstract": "There have been tremendous improvements for facial landmark detection on general \"in-the-wild\" images. However, it is still challenging to detect the facial landmarks on images with severe occlusion and images with large head poses (e.g. profile face). In fact, the existing algorithms usually can only handle one of them. In this work, we propose a unified robust cascade regression framework that can handle both images with severe occlusion and images with large head poses. Specifically, the method iteratively predicts the landmark occlusions and the landmark locations. For occlusion estimation, instead of directly predicting the binary occlusion vectors, we introduce a supervised regression method that gradually updates the landmark visibility probabilities in each iteration to achieve robustness. In addition, we explicitly add occlusion pattern as a constraint to improve the performance of occlusion prediction. For landmark detection, we combine the landmark visibility probabilities, the local appearances, and the local shapes to iteratively update their positions. The experimental results show that the proposed method is significantly better than state-of-the-art works on images with severe occlusion and images with large head poses. It is also comparable to other methods on general \"in-the-wild\" images.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Wu_Robust_Facial_Landmark_ICCV_2015_paper.pdf",
        "aff": ";",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 873400,
        "gs_citation": 132,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13980376868304145730&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Wu_Robust_Facial_Landmark_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Wu_2015_ICCV,\n    \n    author = {\n    Wu,\n    Yue and Ji,\n    Qiang\n},\n    title = {\n    Robust Facial Landmark Detection Under Significant Head Poses and Occlusion\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "7f43d55761",
        "title": "Robust Heart Rate Measurement From Video Using Select Random Patches",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Antony Lam, Yoshinori Kuno",
        "author": "Antony Lam; Yoshinori Kuno",
        "abstract": "The ability to remotely measure heart rate from videos without requiring any special setup is beneficial to many applications. In recent years, a number of papers on heart rate (HR) measurement from videos have been proposed. However, these methods typically require the human subject to be stationary and for the illumination to be controlled. For methods that do take into account motion and illumination changes, strong assumptions are still made about the environment (e.g. background can be used for illumination rectification). In this paper, we propose an HR measurement method that is robust to motion, illumination changes, and does not require use of an environment's background. We present conditions under which cardiac activity extraction from local regions of the face can be treated as a linear Blind Source Separation problem and propose a simple but robust algorithm for selecting good local regions. The independent HR estimates from multiple local regions are then combined in a majority voting scheme that robustly recovers the HR. We validate our algorithm on a large database of challenging videos.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Lam_Robust_Heart_Rate_ICCV_2015_paper.pdf",
        "aff": "Graduate School of Science and Engineering; Graduate School of Science and Engineering",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 885333,
        "gs_citation": 224,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5156811663956381651&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "cv.ics.saitama-u.ac.jp;cv.ics.saitama-u.ac.jp",
        "email": "cv.ics.saitama-u.ac.jp;cv.ics.saitama-u.ac.jp",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Lam_Robust_Heart_Rate_ICCV_2015_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Graduate School of Science and Engineering",
        "aff_unique_dep": "Science and Engineering",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": "",
        "bibtex": "@InProceedings{Lam_2015_ICCV,\n    \n    author = {\n    Lam,\n    Antony and Kuno,\n    Yoshinori\n},\n    title = {\n    Robust Heart Rate Measurement From Video Using Select Random Patches\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "40784309ca",
        "title": "Robust Image Segmentation Using Contour-Guided Color Palettes",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Xiang Fu, Chien-Yi Wang, Chen Chen, Changhu Wang, C.-C. Jay Kuo",
        "author": "Xiang Fu; Chien-Yi Wang; Chen Chen; Changhu Wang; C.-C. Jay Kuo",
        "abstract": "The contour-guided color palette (CCP) is proposed for robust image segmentation. It efficiently integrates contour and color cues of an image. To find representative colors of an image, color samples along long contours between regions, similar in spirit to machine learning methodology that focus on samples near decision boundaries, are collected followed by the mean-shift (MS) algorithm in the sampled color space to achieve an image-dependent color palette. This color palette provides a preliminary segmentation in the spatial domain, which is further fine-tuned by post-processing techniques such as leakage avoidance, fake boundary removal, and small region mergence. Segmentation performances of CCP and MS are compared and analyzed. While CCP offers an acceptable standalone segmentation result, it can be further integrated into the framework of layered spectral segmentation to produce a more robust segmentation. The superior performance of CCP-based segmentation algorithm is demonstrated by experiments on the Berkeley Segmentation Dataset.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Fu_Robust_Image_Segmentation_ICCV_2015_paper.pdf",
        "aff": "University of Southern California; University of Southern California; University of Southern California; Microsoft Research; University of Southern California",
        "project": "",
        "github": "https://github.com/fuxiang87/MCL_CCP",
        "supp": "",
        "arxiv": "",
        "pdf_size": 790649,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3567176760908911951&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "usc.edu;usc.edu;usc.edu;microsoft.com;sipi.usc.edu",
        "email": "usc.edu;usc.edu;usc.edu;microsoft.com;sipi.usc.edu",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Fu_Robust_Image_Segmentation_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "University of Southern California;Microsoft Corporation",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.usc.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "USC;MSR",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Fu_2015_ICCV,\n    \n    author = {\n    Fu,\n    Xiang and Wang,\n    Chien-Yi and Chen,\n    Chen and Wang,\n    Changhu and Kuo,\n    C.-C. Jay\n},\n    title = {\n    Robust Image Segmentation Using Contour-Guided Color Palettes\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "75b7451cec",
        "title": "Robust Model-Based 3D Head Pose Estimation",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Gregory P. Meyer, Shalini Gupta, Iuri Frosio, Dikpal Reddy, Jan Kautz",
        "author": "Gregory P. Meyer; Shalini Gupta; Iuri Frosio; Dikpal Reddy; Jan Kautz",
        "abstract": "We introduce a method for accurate three dimensional head pose estimation using a commodity depth camera. We perform pose estimation by registering a morphable face model to the measured depth data, using a combination of particle swarm optimization (PSO) and the iterative closest point (ICP) algorithm, which minimizes a cost function that includes a 3D registration and a 2D overlap term. The pose is estimated on the fly without requiring an explicit initialization or training phase. Our method handles large pose angles and partial occlusions by dynamically adapting to the reliable visible parts of the face. It is robust and generalizes to different depth sensors without modification. On the Biwi Kinect dataset, we achieve best-in-class performance, with average angular errors of 2.1, 2.1 and 2.4 degrees for yaw, pitch, and roll, respectively, and an average translational error of 5.9 mm, while running at 6 fps on a graphics processing unit.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Meyer_Robust_Model-Based_3D_ICCV_2015_paper.pdf",
        "aff": "University of Illinois Urbana-Champaign+NVIDIA; NVIDIA; NVIDIA; NVIDIA; NVIDIA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1612521,
        "gs_citation": 139,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8954062560719794677&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Meyer_Robust_Model-Based_3D_ICCV_2015_paper.html",
        "aff_unique_index": "0+1;1;1;1;1",
        "aff_unique_norm": "University of Illinois at Urbana-Champaign;NVIDIA Corporation",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://illinois.edu;https://www.nvidia.com",
        "aff_unique_abbr": "UIUC;NVIDIA",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Urbana-Champaign;",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Meyer_2015_ICCV,\n    \n    author = {\n    Meyer,\n    Gregory P. and Gupta,\n    Shalini and Frosio,\n    Iuri and Reddy,\n    Dikpal and Kautz,\n    Jan\n},\n    title = {\n    Robust Model-Based 3D Head Pose Estimation\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "f3021f3d94",
        "title": "Robust Non-Rigid Motion Tracking and Surface Reconstruction Using L0 Regularization",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Kaiwen Guo, Feng Xu, Yangang Wang, Yebin Liu, Qionghai Dai",
        "author": "Kaiwen Guo; Feng Xu; Yangang Wang; Yebin Liu; Qionghai Dai",
        "abstract": "We present a new motion tracking method to robustly reconstruct non-rigid geometries and motions from single view depth inputs captured by a consumer depth sensor. The idea comes from the observation of the existence of intrinsic articulated subspace in most of non-rigid motions. To take advantage of this characteristic, we propose a novel L0 based motion regularizer with an iterative optimization solver that can implicitly constrain local deformation only on joints with articulated motions, leading to reduced solution space and physical plausible deformations. The L0 strategy is integrated into the available non-rigid motion tracking pipeline, forming the proposed L0-L2 non-rigid motion tracking method that can adaptively stop the tracking error propagation. Extensive experiments over complex human body motions with occlusions, face and hand motions demonstrate that our approach substantially improves tracking robustness and surface reconstruction accuracy.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Guo_Robust_Non-Rigid_Motion_ICCV_2015_paper.pdf",
        "aff": "Tsinghua National Laboratory for Information Science and Technology, Beijing, China+Department of Automation, Tsinghua University, Beijing, China; Tsinghua National Laboratory for Information Science and Technology, Beijing, China+School of Software, Tsinghua University, Beijing, China; Microsoft Research; Tsinghua National Laboratory for Information Science and Technology, Beijing, China+Department of Automation, Tsinghua University, Beijing, China; Tsinghua National Laboratory for Information Science and Technology, Beijing, China+Department of Automation, Tsinghua University, Beijing, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3756729,
        "gs_citation": 146,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13851937792276573393&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "tsinghua.edu.cn;tsinghua.edu.cn;microsoft.com;tsinghua.edu.cn;tsinghua.edu.cn",
        "email": "tsinghua.edu.cn;tsinghua.edu.cn;microsoft.com;tsinghua.edu.cn;tsinghua.edu.cn",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Guo_Robust_Non-Rigid_Motion_ICCV_2015_paper.html",
        "aff_unique_index": "0+0;0+0;1;0+0;0+0",
        "aff_unique_norm": "Tsinghua University;Microsoft Corporation",
        "aff_unique_dep": "National Laboratory for Information Science and Technology;Microsoft Research",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "THU;MSR",
        "aff_campus_unique_index": "0+0;0+0;0+0;0+0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0+0;0+0;1;0+0;0+0",
        "aff_country_unique": "China;United States",
        "bibtex": "@InProceedings{Guo_2015_ICCV,\n    \n    author = {\n    Guo,\n    Kaiwen and Xu,\n    Feng and Wang,\n    Yangang and Liu,\n    Yebin and Dai,\n    Qionghai\n},\n    title = {\n    Robust Non-Rigid Motion Tracking and Surface Reconstruction Using L0 Regularization\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "0daa6d42c1",
        "title": "Robust Nonrigid Registration by Convex Optimization",
        "session": "registration, alignment and stereo",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "Qifeng Chen, Vladlen Koltun",
        "author": "Qifeng Chen; Vladlen Koltun",
        "abstract": "We present an approach to nonrigid registration of 3D surfaces. We cast isometric embedding as MRF optimization and apply efficient global optimization algorithms based on linear programming relaxations. The Markov random field perspective suggests a natural connection with robust statistics and motivates robust forms of the intrinsic distortion functional. Our approach outperforms a large body of prior work by a significant margin, increasing registration precision on real data by a factor of 3.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Chen_Robust_Nonrigid_Registration_ICCV_2015_paper.pdf",
        "aff": ";",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 138,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1870291533851122751&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Chen_Robust_Nonrigid_Registration_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Chen_2015_ICCV,\n    \n    author = {\n    Chen,\n    Qifeng and Koltun,\n    Vladlen\n},\n    title = {\n    Robust Nonrigid Registration by Convex Optimization\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "da71981258",
        "title": "Robust Optimization for Deep Regression",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Vasileios Belagiannis, Christian Rupprecht, Gustavo Carneiro, Nassir Navab",
        "author": "Vasileios Belagiannis; Christian Rupprecht; Gustavo Carneiro; Nassir Navab",
        "abstract": "Convolutional Neural Networks (ConvNets) have successfully contributed to improve the accuracy of regression-based methods for computer vision tasks such as human pose estimation, landmark localization, and object detection. The network optimization has been usually performed with L2 loss and without considering the impact of outliers on the training process, where an outlier in this context is defined by a sample estimation that lies at an abnormal distance from the other training sample estimations in the objective space. In this work, we propose a regression model with ConvNets that achieves robustness to such outliers by minimizing Tukey's biweight function, an M-estimator robust to outliers, as the loss function for the ConvNet. In addition to the robust loss, we introduce a coarse-to-fine model, which processes input images of progressively higher resolutions for improving the accuracy of the regressed values. In our experiments, we demonstrate faster convergence and better generalization of our robust loss function for the tasks of human pose estimation and age estimation from face images. We also show that the combination of the robust loss function with the coarse-to-fine model produces comparable or better results than current state-of-the-art approaches in four publicly available human pose estimation datasets.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Belagiannis_Robust_Optimization_for_ICCV_2015_paper.pdf",
        "aff": "Computer Aided Medical Procedures, Technische Universität München + Visual Geometry Group, Department of Engineering Science, University of Oxford; Computer Aided Medical Procedures, Technische Universität München + Johns Hopkins University; Australian Centre for Visual Technologies, University of Adelaide; Computer Aided Medical Procedures, Technische Universität München + Johns Hopkins University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 8239198,
        "gs_citation": 238,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3603931825932761333&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff_domain": "robots.ox.ac.uk;in.tum.de;in.tum.de;adelaide.edu.au",
        "email": "robots.ox.ac.uk;in.tum.de;in.tum.de;adelaide.edu.au",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Belagiannis_Robust_Optimization_for_ICCV_2015_paper.html",
        "aff_unique_index": "0+1;0+2;3;0+2",
        "aff_unique_norm": "Technische Universität München;University of Oxford;Johns Hopkins University;University of Adelaide",
        "aff_unique_dep": "Computer Aided Medical Procedures;Department of Engineering Science;;Australian Centre for Visual Technologies",
        "aff_unique_url": "https://www.tum.de;https://www.ox.ac.uk;https://www.jhu.edu;https://www.adelaide.edu.au",
        "aff_unique_abbr": "TUM;Oxford;JHU;Adelaide",
        "aff_campus_unique_index": "1;;2;",
        "aff_campus_unique": ";Oxford;Adelaide",
        "aff_country_unique_index": "0+1;0+2;3;0+2",
        "aff_country_unique": "Germany;United Kingdom;United States;Australia",
        "bibtex": "@InProceedings{Belagiannis_2015_ICCV,\n    \n    author = {\n    Belagiannis,\n    Vasileios and Rupprecht,\n    Christian and Carneiro,\n    Gustavo and Navab,\n    Nassir\n},\n    title = {\n    Robust Optimization for Deep Regression\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "a4a99611d4",
        "title": "Robust Principal Component Analysis on Graphs",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Nauman Shahid, Vassilis Kalofolias, Xavier Bresson, Michael Bronstein, Pierre Vandergheynst",
        "author": "Nauman Shahid; Vassilis Kalofolias; Xavier Bresson; Michael Bronstein; Pierre Vandergheynst",
        "abstract": "Principal Component Analysis (PCA) is the most widely used tool for linear dimensionality reduction and clustering. Still it is highly sensitive to outliers and does not scale well with respect to the number of data samples. Robust PCA solves the first issue with a sparse penalty term. The second issue can be handled with the matrix factorization model, which is however non-convex. Besides, PCA based clustering can also be enhanced by using a graph of data similarity. In this article, we introduce a new model called 'Robust PCA on Graphs' which incorporates spectral graph regularization into the Robust PCA framework. Our proposed model benefits from 1) the robustness of principal components to occlusions and missing values, 2) enhanced low-rank recovery, 3) improved clustering property due to the graph smoothness assumption on the low-rank matrix, and 4) convexity of the resulting optimization problem. Extensive experiments on 8 benchmark, 3 video and 2 artificial datasets with  corruptions clearly reveal that our model outperforms 10 other state-of-the-art models in its clustering and low-rank recovery tasks.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Shahid_Robust_Principal_Component_ICCV_2015_paper.pdf",
        "aff": "Signal Processing Laboratory (LTS2), EPFL, Switzerland; Signal Processing Laboratory (LTS2), EPFL, Switzerland; Signal Processing Laboratory (LTS2), EPFL, Switzerland; Universita della Svizzera Italiana, Switzerland; Signal Processing Laboratory (LTS2), EPFL, Switzerland",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1775140,
        "gs_citation": 158,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3846528303176965495&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": "epfl.ch;epfl.ch;epfl.ch;usi.ch;epfl.ch",
        "email": "epfl.ch;epfl.ch;epfl.ch;usi.ch;epfl.ch",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Shahid_Robust_Principal_Component_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "École Polytechnique Fédérale de Lausanne;Universita della Svizzera Italiana",
        "aff_unique_dep": "Signal Processing Laboratory (LTS2);",
        "aff_unique_url": "https://www.epfl.ch;https://www.usi.ch",
        "aff_unique_abbr": "EPFL;USI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Switzerland",
        "bibtex": "@InProceedings{Shahid_2015_ICCV,\n    \n    author = {\n    Shahid,\n    Nauman and Kalofolias,\n    Vassilis and Bresson,\n    Xavier and Bronstein,\n    Michael and Vandergheynst,\n    Pierre\n},\n    title = {\n    Robust Principal Component Analysis on Graphs\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "80e117b36e",
        "title": "Robust RGB-D Odometry Using Point and Line Features",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Yan Lu, Dezhen Song",
        "author": "Yan Lu; Dezhen Song",
        "abstract": "Lighting variation and uneven feature distribution are main challenges for indoor RGB-D visual odometry where color information is often combined with depth information. To meet the challenges, we fuse point and line features to form a robust odometry algorithm. Line features are abundant indoors and less sensitive to lighting change than points. We extract 3D points and lines from RGB-D data, analyze their measurement uncertainties, and compute camera motion using maximum likelihood estimation. We prove that fusing points and lines produces smaller motion estimate uncertainty than using either feature type alone. In experiments we compare our method with state-of-the-art methods including a keypoint-based approach and a dense visual odometry  algorithm. Our method outperforms the counterparts under both constant and varying lighting conditions. Specifically, our method achieves an average translational error that is 34.9% smaller than the counterparts, when tested using public datasets.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Lu_Robust_RGB-D_Odometry_ICCV_2015_paper.pdf",
        "aff": "Texas A&M University, College Station, TX, USA; Texas A&M University, College Station, TX, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 690014,
        "gs_citation": 132,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4105307219942937570&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "cs.tamu.edu;cs.tamu.edu",
        "email": "cs.tamu.edu;cs.tamu.edu",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Lu_Robust_RGB-D_Odometry_ICCV_2015_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Texas A&M University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.tamu.edu",
        "aff_unique_abbr": "TAMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "College Station",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Lu_2015_ICCV,\n    \n    author = {\n    Lu,\n    Yan and Song,\n    Dezhen\n},\n    title = {\n    Robust RGB-D Odometry Using Point and Line Features\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "24f2c974ab",
        "title": "Robust Statistical Face Frontalization",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Christos Sagonas, Yannis Panagakis, Stefanos Zafeiriou, Maja Pantic",
        "author": "Christos Sagonas; Yannis Panagakis; Stefanos Zafeiriou; Maja Pantic",
        "abstract": "Recently, it has been shown that excellent results can be achieved in both facial landmark localization and pose-invariant face recognition. These breakthroughs are attributed to the efforts of the community to manually annotate facial images in many different poses and to collect 3D facial data. In this paper, we propose a novel method for joint frontal view reconstruction and landmark localization using a small set of frontal images only. By observing that the frontal facial image is the one having the minimum rank of all different poses, an appropriate model which is able to jointly recover the frontalized version of the face as well as the facial landmarks is devised. To this end, a suitable optimization problem, involving the minimization of the nuclear norm and the matrix l1 norm is solved. The proposed method is assessed in frontal face reconstruction, face landmark localization, pose-invariant face recognition, and face verification in unconstrained conditions. The relevant experiments have been conducted on 8 databases. The experimental results demonstrate the effectiveness of the proposed method in comparison to the state-of-the-art methods for the target problems.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Sagonas_Robust_Statistical_Face_ICCV_2015_paper.pdf",
        "aff": "Department of Computing, Imperial College London; Department of Computing, Imperial College London; Department of Computing, Imperial College London; Department of Computing, Imperial College London + EEMCS, University of Twente",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3949715,
        "gs_citation": 136,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13841686304281668916&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "imperial.ac.uk;imperial.ac.uk;imperial.ac.uk;imperial.ac.uk",
        "email": "imperial.ac.uk;imperial.ac.uk;imperial.ac.uk;imperial.ac.uk",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Sagonas_Robust_Statistical_Face_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0+1",
        "aff_unique_norm": "Imperial College London;University of Twente",
        "aff_unique_dep": "Department of Computing;EEMCS",
        "aff_unique_url": "https://www.imperial.ac.uk;https://www.utwente.nl",
        "aff_unique_abbr": "Imperial;UT",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "London;",
        "aff_country_unique_index": "0;0;0;0+1",
        "aff_country_unique": "United Kingdom;Netherlands",
        "bibtex": "@InProceedings{Sagonas_2015_ICCV,\n    \n    author = {\n    Sagonas,\n    Christos and Panagakis,\n    Yannis and Zafeiriou,\n    Stefanos and Pantic,\n    Maja\n},\n    title = {\n    Robust Statistical Face Frontalization\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "eaa7b3e8ab",
        "title": "Robust and Optimal Sum-of-Squares-Based Point-to-Plane Registration of Image Sets and Structured Scenes",
        "session": "registration, alignment and stereo",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "Danda Pani Paudel, Adlane Habed, Cédric Demonceaux, Pascal Vasseur",
        "author": "Danda Pani Paudel; Adlane Habed; Cedric Demonceaux; Pascal Vasseur",
        "abstract": "This paper deals with the problem of registering a known structured 3D scene and its metric Structure-from-Motion (SfM) counterpart. The proposed work relies on a prior plane segmentation of the 3D scene and aligns the data obtained from both modalities by solving the point-to-plane assignment problem. An inliers-maximization approach within a Branch-and-Bound (BnB) search scheme is adopted. For the first time in this paper, a Sum-of-Squares optimization theory framework is employed for identifying point-to-plane mismatches (i.e. outliers) with certainty. This allows us to iteratively build potential inliers sets and converge to the solution satisfied by the largest number of point-to-plane assignments. Furthermore, our approach is boosted by new plane visibility conditions which are also introduced in this paper. Using this framework, we solve the registration problem in two cases: (i) a set of putative point-to-plane correspondences (with possibly overwhelmingly many outliers) is given as input and (ii) no initial correspondences are given. In both cases, our approach yields outstanding results in terms of robustness and optimality.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Paudel_Robust_and_Optimal_ICCV_2015_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1002891,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14676643990753880957&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Paudel_Robust_and_Optimal_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Paudel_2015_ICCV,\n    \n    author = {\n    Paudel,\n    Danda Pani and Habed,\n    Adlane and Demonceaux,\n    Cedric and Vasseur,\n    Pascal\n},\n    title = {\n    Robust and Optimal Sum-of-Squares-Based Point-to-Plane Registration of Image Sets and Structured Scenes\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "e4b829973e",
        "title": "Rolling Shutter Super-Resolution",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Abhijith Punnappurath, Vijay Rengarajan, A.N. Rajagopalan",
        "author": "Abhijith Punnappurath; Vijay Rengarajan; A.N. Rajagopalan",
        "abstract": "Classical multi-image super-resolution (SR) algorithms, designed for CCD cameras, assume that the motion among the images is global. But CMOS sensors that have increasingly started to replace their more expensive CCD counterparts in many applications do not respect this assumption if there is a motion of the camera relative to the scene during the exposure duration of an image because of the row-wise acquisition mechanism. In this paper, we study the hitherto unexplored topic of multi-image SR in CMOS cameras. We initially develop an SR observation model that accounts for the row-wise distortions called the ``rolling shutter'' (RS) effect observed in images captured using non-stationary CMOS cameras. We then propose a unified RS-SR framework to obtain an RS-free high-resolution image (and the row-wise motion) from distorted low-resolution images. We demonstrate the efficacy of the proposed scheme using synthetic data as well as real images captured using a hand-held CMOS camera. Quantitative and qualitative assessments reveal that our method significantly advances the state-of-the-art.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Punnappurath_Rolling_Shutter_Super-Resolution_ICCV_2015_paper.pdf",
        "aff": "Department of Electrical Engineering, Indian Institute of Technology Madras, Chennai, India; Department of Electrical Engineering, Indian Institute of Technology Madras, Chennai, India; Department of Electrical Engineering, Indian Institute of Technology Madras, Chennai, India",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1689809,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3518547764849945655&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "gmail.com;ee.iitm.ac.in;ee.iitm.ac.in",
        "email": "gmail.com;ee.iitm.ac.in;ee.iitm.ac.in",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Punnappurath_Rolling_Shutter_Super-Resolution_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Indian Institute of Technology Madras",
        "aff_unique_dep": "Department of Electrical Engineering",
        "aff_unique_url": "https://www.iitm.ac.in",
        "aff_unique_abbr": "IIT Madras",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Chennai",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India",
        "bibtex": "@InProceedings{Punnappurath_2015_ICCV,\n    \n    author = {\n    Punnappurath,\n    Abhijith and Rengarajan,\n    Vijay and Rajagopalan,\n    A.N.\n},\n    title = {\n    Rolling Shutter Super-Resolution\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "fee80fd955",
        "title": "SALICON: Reducing the Semantic Gap in Saliency Prediction by Adapting Deep Neural Networks",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Xun Huang, Chengyao Shen, Xavier Boix, Qi Zhao",
        "author": "Xun Huang; Chengyao Shen; Xavier Boix; Qi Zhao",
        "abstract": "Saliency in Context (SALICON) is an ongoing effort that aims at understanding and predicting visual attention. Conventional saliency models typically rely on low-level image statistics to predict human fixations. While these models perform significantly better than chance, there is still a large gap between model prediction and human behavior. This gap is largely due to the limited capability of models in predicting eye fixations with strong semantic content, the so-called semantic gap. This paper presents a focused study to narrow the semantic gap with an architecture based on Deep Neural Network (DNN). It leverages the representational power of high-level semantics encoded in DNNs pretrained for object recognition. Two key components are fine-tuning the DNNs fully convolutionally with an objective function based on the saliency evaluation metrics, and integrating information at different image scales. We compare our method with 14 saliency models on 6 public eye tracking benchmark datasets. Results demonstrate that our DNNs can automatically learn features particularly for saliency prediction that surpass by a big margin the state-of-the-art. In addition, our model ranks top to date under all seven metrics on the MIT300 challenge set.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Huang_SALICON_Reducing_the_ICCV_2015_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1238000,
        "gs_citation": 729,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9477125556514146246&as_sdt=40005&sciodt=0,10&hl=en",
        "gs_version_total": 13,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Huang_SALICON_Reducing_the_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Huang_2015_ICCV,\n    \n    author = {\n    Huang,\n    Xun and Shen,\n    Chengyao and Boix,\n    Xavier and Zhao,\n    Qi\n},\n    title = {\n    SALICON: Reducing the Semantic Gap in Saliency Prediction by Adapting Deep Neural Networks\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "510a4a8443",
        "title": "SOWP: Spatially Ordered and Weighted Patch Descriptor for Visual Tracking",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Han-Ul Kim, Dae-Youn Lee, Jae-Young Sim, Chang-Su Kim",
        "author": "Han-Ul Kim; Dae-Youn Lee; Jae-Young Sim; Chang-Su Kim",
        "abstract": "A simple yet effective object descriptor for visual tracking is proposed in this paper. We first decompose the bounding box of a target object into multiple patches, which are described by color and gradient histograms. Then, we concatenate the features of the spatially ordered patches to represent the object appearance. Moreover, to alleviate the impacts of background information possibly included in the bounding box, we determine patch weights using random walk with restart (RWR) simulations. The patch weights represent the importance of each patch in the description of foreground information, and are used to construct an object descriptor, called spatially ordered and weighted patch (SOWP) descriptor. We incorporate the proposed SOWP descriptor into the structured output tracking framework. Experimental results demonstrate that the proposed algorithm yields significantly better performance than the state-of-the-art trackers on a recent benchmark dataset, and also excels in another recent benchmark dataset.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Kim_SOWP_Spatially_Ordered_ICCV_2015_paper.pdf",
        "aff": "School of Electrical Engineering, Korea University; Samsung Electronics Co., Ltd.; School of Electrical and Computer Engineering, Ulsan National Institute of Science and Technology; School of Electrical Engineering, Korea University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4109381,
        "gs_citation": 141,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6697447846377424471&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "mcl.korea.ac.kr;mcl.korea.ac.kr;unist.ac.kr;korea.ac.kr",
        "email": "mcl.korea.ac.kr;mcl.korea.ac.kr;unist.ac.kr;korea.ac.kr",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Kim_SOWP_Spatially_Ordered_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Korea University;Samsung Electronics;Ulsan National Institute of Science and Technology",
        "aff_unique_dep": "School of Electrical Engineering;;School of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.korea.ac.kr;https://www.samsung.com;https://www.unist.ac.kr",
        "aff_unique_abbr": "KU;Samsung;UNIST",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Ulsan",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "South Korea",
        "bibtex": "@InProceedings{Kim_2015_ICCV,\n    \n    author = {\n    Kim,\n    Han-Ul and Lee,\n    Dae-Youn and Sim,\n    Jae-Young and Kim,\n    Chang-Su\n},\n    title = {\n    SOWP: Spatially Ordered and Weighted Patch Descriptor for Visual Tracking\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "e0447c2e0f",
        "title": "SPM-BP: Sped-up PatchMatch Belief Propagation for Continuous MRFs",
        "session": "motion and correspondence",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "Yu Li, Dongbo Min, Michael S. Brown, Minh N. Do, Jiangbo Lu",
        "author": "Yu Li; Dongbo Min; Michael S. Brown; Minh N. Do; Jiangbo Lu",
        "abstract": "Markov random fields are widely used to model many computer vision problems that can be cast in an energy minimization framework composed of unary and pairwise potentials. While computationally tractable discrete optimizers such as Graph Cuts and belief propagation (BP) exist for multi-label discrete problems, they still face prohibitively high computational challenges when the labels reside in a huge or very densely sampled space. Integrating key ideas from PatchMatch of effective particle propagation and resampling, PatchMatch belief propagation (PMBP) has been demonstrated to have good performance in addressing continuous labeling problems and runs orders of magnitude faster than Particle BP (PBP). However, the quality of the PMBP solution is tightly coupled with the local window size, over which the raw data cost is aggregated to mitigate ambiguity in the data constraint. This dependency heavily influences the overall complexity, increasing linearly with the window size. This paper proposes a novel algorithm called sped-up PMBP (SPM-BP) to tackle this critical computational bottleneck and speeds up PMBP by 50-100 times. The crux of SPM-BP is on unifying efficient filter-based cost aggregation and message passing with PatchMatch-based particle generation in a highly effective way. Though simple in its formulation, SPM-BP achieves superior performance for sub-pixel accurate stereo and optical-flow on benchmark datasets when compared with more complex and task-specific approaches.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Li_SPM-BP_Sped-up_PatchMatch_ICCV_2015_paper.pdf",
        "aff": "Advanced Digital Sciences Center, Singapore + National University of Singapore, Singapore; Chungnam National University, Korea; National University of Singapore, Singapore; University of Illinois at Urbana-Champaign, US; Advanced Digital Sciences Center, Singapore",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2180119,
        "gs_citation": 111,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1016838212814420704&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff_domain": "nus.edu.sg;cnu.ac.kr;nus.edu.sg;illinois.edu;nus.edu.sg",
        "email": "nus.edu.sg;cnu.ac.kr;nus.edu.sg;illinois.edu;nus.edu.sg",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Li_SPM-BP_Sped-up_PatchMatch_ICCV_2015_paper.html",
        "aff_unique_index": "0+1;2;1;3;0",
        "aff_unique_norm": "Advanced Digital Sciences Center;National University of Singapore;Chungnam National University;University of Illinois at Urbana-Champaign",
        "aff_unique_dep": ";;;",
        "aff_unique_url": ";https://www.nus.edu.sg;http://www.cnu.ac.kr;https://illinois.edu",
        "aff_unique_abbr": ";NUS;CNU;UIUC",
        "aff_campus_unique_index": ";1",
        "aff_campus_unique": ";Urbana-Champaign",
        "aff_country_unique_index": "0+0;1;0;2;0",
        "aff_country_unique": "Singapore;South Korea;United States",
        "bibtex": "@InProceedings{Li_2015_ICCV,\n    \n    author = {\n    Li,\n    Yu and Min,\n    Dongbo and Brown,\n    Michael S. and Do,\n    Minh N. and Lu,\n    Jiangbo\n},\n    title = {\n    SPM-BP: Sped-up PatchMatch Belief Propagation for Continuous MRFs\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "9beb735c05",
        "title": "Scalable Nonlinear Embeddings for Semantic Category-Based Image Retrieval",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Gaurav Sharma, Bernt Schiele",
        "author": "Gaurav Sharma; Bernt Schiele",
        "abstract": "We propose a novel algorithm for the task of supervised discriminative distance learning by nonlinearly embedding vectors into a low dimensional Euclidean space. We work in the challenging setting where supervision is with constraints on similar and dissimilar pairs while training. The proposed method is derived by an approximate kernelization of a linear Mahalanobis-like distance metric learning algorithm and can also be seen as a kernel neural network. The number of model parameters and test time evaluation complexity of the proposed method are O(dD) where D is the dimensionality of the input features and d is the dimension of the projection space -- this is in contrast to the usual kernelization methods as, unlike them, the complexity does not scale linearly with the number of training examples. We propose a stochastic gradient based learning algorithm which makes the method scalable (w.r.t. the number of training examples), while being nonlinear. We train the method with up to half a million training pairs of 4096 dimensional CNN features. We give empirical comparisons with relevant baselines on seven challenging datasets for the task of low dimensional semantic category based image retrieval.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Sharma_Scalable_Nonlinear_Embeddings_ICCV_2015_paper.pdf",
        "aff": "Max Planck Institute for Informatics, Germany; Max Planck Institute for Informatics, Germany",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 845623,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=168379289328903480&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "mpi-inf.mpg.de;mpi-inf.mpg.de",
        "email": "mpi-inf.mpg.de;mpi-inf.mpg.de",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Sharma_Scalable_Nonlinear_Embeddings_ICCV_2015_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Max Planck Institute for Informatics",
        "aff_unique_dep": "",
        "aff_unique_url": "https://mpi-inf.mpg.de",
        "aff_unique_abbr": "MPII",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany",
        "bibtex": "@InProceedings{Sharma_2015_ICCV,\n    \n    author = {\n    Sharma,\n    Gaurav and Schiele,\n    Bernt\n},\n    title = {\n    Scalable Nonlinear Embeddings for Semantic Category-Based Image Retrieval\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "bdbcd8cb25",
        "title": "Scalable Person Re-Identification: A Benchmark",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jingdong Wang, Qi Tian",
        "author": "Liang Zheng; Liyue Shen; Lu Tian; Shengjin Wang; Jingdong Wang; Qi Tian",
        "abstract": "This paper contributes a new high quality dataset for person re-identification, named \"Market-1501\". Generally, current datasets: 1) are limited in scale; 2) consist of hand-drawn bboxes, which are unavailable under realistic settings; 3) have only one ground truth and one query image for each identity (close environment). To tackle these problems, the proposed Market-1501 dataset is featured in three aspects. First, it contains over 32,000 annotated bboxes, plus a distractor set of over 500K images, making it the largest person re-id dataset to date. Second, images in Market-1501 dataset are produced using the Deformable Part Model (DPM) as pedestrian detector. Third, our dataset is collected in an open system, where each identity has multiple images under each camera.  As a minor contribution, inspired by recent advances in large-scale image search, this paper proposes an unsupervised Bag-of-Words descriptor. We view person re-identification as a special task of image search. In experiment, we show that the proposed descriptor yields competitive accuracy on VIPeR, CUHK03, and Market-1501 datasets, and is scalable on the large-scale 500k dataset.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Zheng_Scalable_Person_Re-Identification_ICCV_2015_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1261659,
        "gs_citation": 5508,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11794457944740724244&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Zheng_Scalable_Person_Re-Identification_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Zheng_2015_ICCV,\n    \n    author = {\n    Zheng,\n    Liang and Shen,\n    Liyue and Tian,\n    Lu and Wang,\n    Shengjin and Wang,\n    Jingdong and Tian,\n    Qi\n},\n    title = {\n    Scalable Person Re-Identification: A Benchmark\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "57ca70fd31",
        "title": "Scene-Domain Active Part Models for Object Representation",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Zhou Ren, Chaohui Wang, Alan L. Yuille",
        "author": "Zhou Ren; Chaohui Wang; Alan L. Yuille",
        "abstract": "In this paper, we are interested in enhancing the expressivity and robustness of part-based models for object representation, in the common scenario where the training data are based on 2D images. To this end, we propose scene-domain active part models (SDAPM), which reconstruct and characterize the 3D geometric statistics between object's parts in 3D scene-domain by using 2D training data in the image-domain alone. And on top of this, we explicitly model and handle occlusions in SDAPM. Together with the developed learning and inference algorithms, such a model provides rich object descriptions, including 2D object and parts localization, 3D landmark shape and camera viewpoint, which offers an effective representation to various image understanding tasks, such as object and parts detection, 3D landmark shape and viewpoint estimation from images. Experiments on the above tasks show that SDAPM outperforms previous part-based models, and thus demonstrates the potential of the proposed technique.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Ren_Scene-Domain_Active_Part_ICCV_2015_paper.pdf",
        "aff": "University of California, Los Angeles; Universit ´e Paris-Est, LIGM - CNRS UMR 8049; University of California, Los Angeles",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1546692,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16503720683138665156&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff_domain": "cs.ucla.edu;u-pem.fr;stat.ucla.edu",
        "email": "cs.ucla.edu;u-pem.fr;stat.ucla.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Ren_Scene-Domain_Active_Part_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of California, Los Angeles;Université Paris-Est",
        "aff_unique_dep": ";LIGM - CNRS UMR 8049",
        "aff_unique_url": "https://www.ucla.edu;https://www.univ-Paris12.fr",
        "aff_unique_abbr": "UCLA;UPE",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;France",
        "bibtex": "@InProceedings{Ren_2015_ICCV,\n    \n    author = {\n    Ren,\n    Zhou and Wang,\n    Chaohui and Yuille,\n    Alan L.\n},\n    title = {\n    Scene-Domain Active Part Models for Object Representation\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "f6de649196",
        "title": "Secrets of GrabCut and Kernel K-Means",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Meng Tang, Ismail Ben Ayed, Dmitrii Marin, Yuri Boykov",
        "author": "Meng Tang; Ismail Ben Ayed; Dmitrii Marin; Yuri Boykov",
        "abstract": "The log-likelihood energy term in popular model-fitting segmentation methods, e.g. Zhu&Yuille, Chan-Vese, GrabCut, is presented as a generalized \"probabilistic K-means\" energy for color space clustering. This interpretation reveals some limitations,  e.g. over-fitting. We propose an alternative approach to color clustering using kernel K-means energy with well-known properties such as non-linear separation and scalability to higher-dimensional feature spaces. Our bound formulation for kernel K-means allows to combine general pair-wise feature clustering methods with image grid regularization using graph cuts, similarly to standard color model fitting techniques for segmentation. Unlike histogram or GMM fitting, our approach is closely related to average association and normalized cut. But, in contrast to previous pairwise clustering algorithms, our approach can incorporate any standard geometric regularization in the image domain. We analyze extreme cases for kernel bandwidth (e.g. Gini bias) and demonstrate effectiveness of KNN-based adaptive bandwidth strategies. Our kernel K-means approach to segmentation benefits from higher-dimensional features where standard model fitting fails.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Tang_Secrets_of_GrabCut_ICCV_2015_paper.pdf",
        "aff": "Computer Science Department, University of Western Ontario, Canada; ´Ecole de Technologie Supérieure, University of Quebec, Canada; Computer Science Department, University of Western Ontario, Canada; Computer Science Department, University of Western Ontario, Canada",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1974706,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18301045343236275789&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "csd.uwo.ca;etsmtl.ca;gmail.com;csd.uwo.ca",
        "email": "csd.uwo.ca;etsmtl.ca;gmail.com;csd.uwo.ca",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Tang_Secrets_of_GrabCut_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "University of Western Ontario;University of Quebec",
        "aff_unique_dep": "Computer Science Department;",
        "aff_unique_url": "https://www.uwo.ca;https://www.etsmtl.ca",
        "aff_unique_abbr": "UWO;ETS",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Ecole de Technologie Supérieure",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Canada",
        "bibtex": "@InProceedings{Tang_2015_ICCV,\n    \n    author = {\n    Tang,\n    Meng and Ben Ayed,\n    Ismail and Marin,\n    Dmitrii and Boykov,\n    Yuri\n},\n    title = {\n    Secrets of GrabCut and Kernel K-Means\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "b1f3a5219a",
        "title": "Secrets of Matrix Factorization: Approximations, Numerics, Manifold Optimization and Random Restarts",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Je Hyeong Hong, Andrew Fitzgibbon",
        "author": "Je Hyeong Hong; Andrew Fitzgibbon",
        "abstract": "Matrix factorization (or low-rank matrix completion) with missing data is a key computation in many computer vision and machine learning tasks, and is also related to a broader class of nonlinear optimization problems such as bundle adjustment. The problem has received much attention recently, with renewed interest in variable-projection approaches, yielding dramatic improvements in reliability and speed. However, on a wide class of problems, no one approach dominates, and because the various approaches have been derived in a multitude of different ways, it has been difficult to unify them. This paper provides a unified derivation of a number of recent approaches, so that similarities and differences are easily observed. We also present a simple meta-algorithm which wraps any existing algorithm, yielding 100% success rate on many standard datasets. Given 100% success, the focus of evaluation must turn to speed, as 100% success is trivially achieved if we do not care about speed. Again our unification allows a number of generic improvements applicable to all members of the family to be isolated, yielding a unified algorithm that outperforms our re-implementation of existing algorithms, which in some cases already outperform the original authors' publicly available codes.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Hong_Secrets_of_Matrix_ICCV_2015_paper.pdf",
        "aff": "University of Cambridge; Microsoft, Cambridge, UK",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2300894,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14766162297049023692&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "cam.ac.uk;microsoft.com",
        "email": "cam.ac.uk;microsoft.com",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Hong_Secrets_of_Matrix_ICCV_2015_paper.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Cambridge;Microsoft",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cam.ac.uk;https://www.microsoft.com",
        "aff_unique_abbr": "Cambridge;MSFT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom",
        "bibtex": "@InProceedings{Hong_2015_ICCV,\n    \n    author = {\n    Hong,\n    Je Hyeong and Fitzgibbon,\n    Andrew\n},\n    title = {\n    Secrets of Matrix Factorization: Approximations,\n    Numerics,\n    Manifold Optimization and Random Restarts\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "0caab4e385",
        "title": "See the Difference: Direct Pre-Image Reconstruction and Pose Estimation by Differentiating HOG",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Wei-Chen Chiu, Mario Fritz",
        "author": "Wei-Chen Chiu; Mario Fritz",
        "abstract": "The Histogram of Oriented Gradient (HOG) descriptor has led to many advances in computer vision over the last decade and is still part of many state of the art approaches. We realize that the associated feature computation is piecewise differentiable and therefore many pipelines which build on HOG can be made differentiable. This lends to advanced introspection as well as opportunities for end-to-end optimization. We present our implementation of [?]HOG based on the auto-differentiation toolbox Chumpy and show applications to pre-image visualization and pose estimation which extends the existing differentiable renderer OpenDR pipeline. Both applications improve on the respective state-of-the-art HOG approaches.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Chiu_See_the_Difference_ICCV_2015_paper.pdf",
        "aff": "Max Planck Institute for Informatics, Saarbr ¨ucken, Germany; Max Planck Institute for Informatics, Saarbr ¨ucken, Germany",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1591994,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15451923786614400682&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff_domain": "mpi-inf.mpg.de;mpi-inf.mpg.de",
        "email": "mpi-inf.mpg.de;mpi-inf.mpg.de",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Chiu_See_the_Difference_ICCV_2015_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Max Planck Institute for Informatics",
        "aff_unique_dep": "",
        "aff_unique_url": "https://mpi-inf.mpg.de",
        "aff_unique_abbr": "MPII",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Saarbrücken",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany",
        "bibtex": "@InProceedings{Chiu_2015_ICCV,\n    \n    author = {\n    Chiu,\n    Wei-Chen and Fritz,\n    Mario\n},\n    title = {\n    See the Difference: Direct Pre-Image Reconstruction and Pose Estimation by Differentiating HOG\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "b03a24335c",
        "title": "Segment Graph Based Image Filtering: Fast Structure-Preserving Smoothing",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Feihu Zhang, Longquan Dai, Shiming Xiang, Xiaopeng Zhang",
        "author": "Feihu Zhang; Longquan Dai; Shiming Xiang; Xiaopeng Zhang",
        "abstract": "In this paper, we design a new edge-aware structure, named segment graph, to represent the image and we further develop a novel double weighted average image filter (SGF) based on the segment graph. In our SGF, we use the tree distance on the segment graph to define the internal weight function of the filtering kernel, which enables the filter to smooth out high-contrast details and textures while preserving major image structures very well. While for the external weight function, we introduce a user specified smoothing window to balance the smoothing effects from each node of the segment graph. Moreover, we also set a threshold to adjust the edge-preserving performance. These advantages make the SGF more flexible in various applications and overcome the \"halo\" and \"leak\" problems appearing in most of the state-of-the-art approaches. Finally and importantly, we develop a linear algorithm for the implementation of our SGF, which has an O(N) time complexity for both gray-scale and high dimensional images, regardless of the kernel size and the intensity range. Typically, as one of the fastest edge-preserving filters, our CPU implementation achieves 0.15s per megapixel when performing filtering for 3-channel color images. The strength of the proposed filter is demonstrated by various applications, including stereo matching, optical flow, joint depth map upsampling, edge-preserving smoothing, edges detection, image abstraction and texture editing.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Zhang_Segment_Graph_Based_ICCV_2015_paper.pdf",
        "aff": "School of Computer Science, Northwestern Polytechnical University, Xi’an, China+NLPR-LIAMA, Institute of Automation, Chinese Academy of Sciences, Beijing, China; NLPR-LIAMA, Institute of Automation, Chinese Academy of Sciences, Beijing, China; NLPR-LIAMA, Institute of Automation, Chinese Academy of Sciences, Beijing, China; NLPR-LIAMA, Institute of Automation, Chinese Academy of Sciences, Beijing, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3519283,
        "gs_citation": 70,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9903708617887988712&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "gmail.com;foxmail.com;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "email": "gmail.com;foxmail.com;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Zhang_Segment_Graph_Based_ICCV_2015_paper.html",
        "aff_unique_index": "0+1;1;1;1",
        "aff_unique_norm": "Northwestern Polytechnical University;Chinese Academy of Sciences",
        "aff_unique_dep": "School of Computer Science;Institute of Automation",
        "aff_unique_url": "https://www.nwpu.edu.cn;http://www.ia.cas.cn",
        "aff_unique_abbr": "NPU;CAS",
        "aff_campus_unique_index": "0+1;1;1;1",
        "aff_campus_unique": "Xi'an;Beijing",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Zhang_2015_ICCV,\n    \n    author = {\n    Zhang,\n    Feihu and Dai,\n    Longquan and Xiang,\n    Shiming and Zhang,\n    Xiaopeng\n},\n    title = {\n    Segment Graph Based Image Filtering: Fast Structure-Preserving Smoothing\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "05e7fde4d7",
        "title": "Segment-Phrase Table for Semantic Segmentation, Visual Entailment and Paraphrasing",
        "session": "vision and language",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "Hamid Izadinia, Fereshteh Sadeghi, Santosh K. Divvala, Hannaneh Hajishirzi, Yejin Choi, Ali Farhadi",
        "author": "Hamid Izadinia; Fereshteh Sadeghi; Santosh K. Divvala; Hannaneh Hajishirzi; Yejin Choi; Ali Farhadi",
        "abstract": "We introduce Segment-Phrase Table (SPT), a large collection of bijective associations between textual phrases and their corresponding segmentations. Leveraging recent progress in object recognition and natural language semantics, we show how we can successfully build a high-quality segment-phrase table using minimal human supervision. More importantly, we demonstrate the unique value unleashed by this rich bimodal resource, for both vision as well as natural language understanding. First, we show that fine-grained textual labels facilitate contextual reasoning that helps in satisfying semantic constraints across image segments. This feature enables us to achieve state-of-the-art segmentation results on benchmark datasets. Next, we show that the association of high-quality segmentations to textual phrases aids in richer semantic understanding and reasoning of these textual phrases. Leveraging this feature, we motivate the problem of visual entailment and visual paraphrasing,  and demonstrate its utility on a large dataset.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Izadinia_Segment-Phrase_Table_for_ICCV_2015_paper.pdf",
        "aff": "University of Washington†; University of Washington†; University of Washington‡,†; University of Washington†; University of Washington†; The Allen Institute for AI‡,†",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1109888,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3404586012216221577&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff_domain": "cs.uw.edu;cs.uw.edu;allenai.org;uw.edu;cs.uw.edu;allenai.org",
        "email": "cs.uw.edu;cs.uw.edu;allenai.org;uw.edu;cs.uw.edu;allenai.org",
        "author_num": 6,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Izadinia_Segment-Phrase_Table_for_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0;0;1",
        "aff_unique_norm": "University of Washington;Allen Institute for AI",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.washington.edu;https://allenai.org",
        "aff_unique_abbr": "UW;AI2",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Izadinia_2015_ICCV,\n    \n    author = {\n    Izadinia,\n    Hamid and Sadeghi,\n    Fereshteh and Divvala,\n    Santosh K. and Hajishirzi,\n    Hannaneh and Choi,\n    Yejin and Farhadi,\n    Ali\n},\n    title = {\n    Segment-Phrase Table for Semantic Segmentation,\n    Visual Entailment and Paraphrasing\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "a43c3368bb",
        "title": "Selecting Relevant Web Trained Concepts for Automated Event Retrieval",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Bharat Singh, Xintong Han, Zhe Wu, Vlad I. Morariu, Larry S. Davis",
        "author": "Bharat Singh; Xintong Han; Zhe Wu; Vlad I. Morariu; Larry S. Davis",
        "abstract": "Complex event retrieval is a challenging research problem, especially when no training videos are available. An alternative to collecting training videos is to train a large semantic concept bank a priori. Given a text description of an event, event retrieval is performed by selecting concepts linguistically related to the event description and fusing the concept responses on unseen videos. However, defining an exhaustive concept lexicon and pre-training it requires vast computational resources. Therefore, recent approaches automate concept discovery and training by leveraging large amounts of weakly annotated web data. Compact visually salient concepts are automatically obtained by the use of concept pairs or, more generally, n-grams. However, not all visually salient n-grams are necessarily useful for an event query--some combinations of concepts may be visually compact but irrelevant--and this drastically affects performance. We propose an event retrieval algorithm that constructs pairs of automatically discovered concepts and then prunes those concepts that are unlikely to be helpful for retrieval. Pruning depends both on the query and on the specific video instance being evaluated. Our approach also addresses calibration and domain adaptation issues that arise when applying concept detectors to unseen videos. We demonstrate large improvements over other vision based systems on the TRECVID MED 13 dataset.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Singh_Selecting_Relevant_Web_ICCV_2015_paper.pdf",
        "aff": "Center For Automation Research, University of Maryland, College Park; Center For Automation Research, University of Maryland, College Park; Center For Automation Research, University of Maryland, College Park; Center For Automation Research, University of Maryland, College Park; Center For Automation Research, University of Maryland, College Park",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2672889,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10289355972455269749&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "umiacs.umd.edu;umiacs.umd.edu;umiacs.umd.edu;umiacs.umd.edu;umiacs.umd.edu",
        "email": "umiacs.umd.edu;umiacs.umd.edu;umiacs.umd.edu;umiacs.umd.edu;umiacs.umd.edu",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Singh_Selecting_Relevant_Web_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Maryland",
        "aff_unique_dep": "Center For Automation Research",
        "aff_unique_url": "https://www.umd.edu",
        "aff_unique_abbr": "UMD",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "College Park",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Singh_2015_ICCV,\n    \n    author = {\n    Singh,\n    Bharat and Han,\n    Xintong and Wu,\n    Zhe and Morariu,\n    Vlad I. and Davis,\n    Larry S.\n},\n    title = {\n    Selecting Relevant Web Trained Concepts for Automated Event Retrieval\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "1cd861ed20",
        "title": "Selective Encoding for Recognizing Unreliably Localized Faces",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Ang Li, Vlad Morariu, Larry S. Davis",
        "author": "Ang Li; Vlad Morariu; Larry S. Davis",
        "abstract": "Most existing face verification systems rely on precise face detection and registration. However, these two components are fallible under unconstrained scenarios (e.g., mobile face authentication) due to partial occlusions, pose variations, lighting conditions and limited view-angle coverage of mobile cameras. We address the unconstrained face verification problem by encoding face images directly without any explicit models of detection or registration. We propose a selective encoding framework which injects relevance information (e.g., foreground/background probabilities) into each cluster of a descriptor codebook. An additional selector component also discards distractive image patches and improves spatial robustness. We evaluate our framework using Gaussian mixture models and Fisher vectors on challenging face verification datasets. We apply selective encoding to Fisher vector features, which in our experiments degrade quickly with inaccurate face localization; our framework improves robustness with no extra test time computation. We also apply our approach to mobile based active face authentication task, demonstrating its utility in real scenarios.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Li_Selective_Encoding_for_ICCV_2015_paper.pdf",
        "aff": "University of Maryland, College Park; University of Maryland, College Park; University of Maryland, College Park",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 866657,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12882656405109021662&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "umiacs.umd.edu;umiacs.umd.edu;umiacs.umd.edu",
        "email": "umiacs.umd.edu;umiacs.umd.edu;umiacs.umd.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Li_Selective_Encoding_for_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Maryland",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www/umd.edu",
        "aff_unique_abbr": "UMD",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "College Park",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Li_2015_ICCV,\n    \n    author = {\n    Li,\n    Ang and Morariu,\n    Vlad and Davis,\n    Larry S.\n},\n    title = {\n    Selective Encoding for Recognizing Unreliably Localized Faces\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "fb6d2f45fb",
        "title": "Self-Calibration of Optical Lenses",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Michael Hirsch, Bernhard Schölkopf",
        "author": "Michael Hirsch; Bernhard Scholkopf",
        "abstract": "Even high-quality lenses suffer from optical aberrations, especially when used at full aperture. Furthermore, there are significant lens-to-lens deviations due to manufacturing tolerances, often rendering current software solutions like DxO, Lightroom, and PTLens insufficient as they don't adapt and only include generic lens blur models.  We propose a method that enables the self-calibration of lenses from a natural image, or a set of such images. To this end we develop a machine learning framework that is able to exploit several recorded images and distills the available information into an accurate model of the considered lens.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Hirsch_Self-Calibration_of_Optical_ICCV_2015_paper.pdf",
        "aff": "Department of Empirical Inference, Max Planck Institute for Intelligent Systems; Department of Empirical Inference, Max Planck Institute for Intelligent Systems",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3249903,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10332910630559937575&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "tue.mpg.de;tue.mpg.de",
        "email": "tue.mpg.de;tue.mpg.de",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Hirsch_Self-Calibration_of_Optical_ICCV_2015_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Max Planck Institute for Intelligent Systems",
        "aff_unique_dep": "Department of Empirical Inference",
        "aff_unique_url": "https://www.mpituebingen.mpg.de",
        "aff_unique_abbr": "MPI-IS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany",
        "bibtex": "@InProceedings{Hirsch_2015_ICCV,\n    \n    author = {\n    Hirsch,\n    Michael and Scholkopf,\n    Bernhard\n},\n    title = {\n    Self-Calibration of Optical Lenses\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "c2b58e550e",
        "title": "Self-Occlusions and Disocclusions in Causal Video Object Segmentation",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Yanchao Yang, Ganesh Sundaramoorthi, Stefano Soatto",
        "author": "Yanchao Yang; Ganesh Sundaramoorthi; Stefano Soatto",
        "abstract": "We propose a method to detect disocclusion in video sequences of three-dimensional scenes and to partition the disoccluded regions into objects, defined by coherent deformation corresponding to surfaces in the scene. Our method infers deformation fields that are piecewise smooth by construction without the need for an explicit regularizer and the associated choice of weight. It then partitions the disoccluded region and groups its components with objects by leveraging on the complementarity of motion and appearance cues: Where appearance changes within an object, motion can usually be reliably inferred and used for grouping. Where appearance is close to constant, it can be used for grouping directly. We integrate both cues in an energy minimization framework, incorporate prior assumptions explicitly into the energy, and propose a numerical scheme.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Yang_Self-Occlusions_and_Disocclusions_ICCV_2015_paper.pdf",
        "aff": "University of California, Los Angeles, USA+King Abdullah University of Science & Technology (KAUST), Saudi Arabia; King Abdullah University of Science & Technology (KAUST), Saudi Arabia; University of California, Los Angeles, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2693452,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17303501789773224381&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "g.ucla.edu;kaust.edu.sa;ucla.edu",
        "email": "g.ucla.edu;kaust.edu.sa;ucla.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Yang_Self-Occlusions_and_Disocclusions_ICCV_2015_paper.html",
        "aff_unique_index": "0+1;1;0",
        "aff_unique_norm": "University of California, Los Angeles;King Abdullah University of Science & Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ucla.edu;https://www.kaust.edu.sa",
        "aff_unique_abbr": "UCLA;KAUST",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0+1;1;0",
        "aff_country_unique": "United States;Saudi Arabia",
        "bibtex": "@InProceedings{Yang_2015_ICCV,\n    \n    author = {\n    Yang,\n    Yanchao and Sundaramoorthi,\n    Ganesh and Soatto,\n    Stefano\n},\n    title = {\n    Self-Occlusions and Disocclusions in Causal Video Object Segmentation\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "127b9fd718",
        "title": "Semantic Component Analysis",
        "session": "statistical methods and learning",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "Calvin Murdock, Fernando De la Torre",
        "author": "Calvin Murdock; Fernando De la Torre",
        "abstract": "Unsupervised and weakly-supervised visual learning in large image collections are critical in order to avoid the time-consuming and error-prone process of manual labeling. Standard approaches rely on methods like multiple-instance learning or graphical models, which can be computationally intensive and sensitive to initialization. On the other hand, simpler component analysis or clustering methods usually cannot achieve meaningful invariances or semantic interpretability. To address the issues of previous work, we present a simple but effective method called Semantic Component Analysis (SCA), which provides a decomposition of images into semantic components.   Unsupervised SCA decomposes additive image representations into spatially-meaningful visual components that naturally correspond to object categories. Using an overcomplete representation that allows for rich instance-level constraints and spatial priors, SCA gives improved results and more interpretable components in comparison to traditional matrix factorization techniques. If weakly-supervised information is available in the form of image-level tags, SCA factorizes a set of images into semantic groups of superpixels. We also provide qualitative connections to traditional methods for component analysis (e.g. Grassmann averages, PCA, and NMF). The effectiveness of our approach is validated through synthetic data and on the MSRC2 and Sift Flow datasets, demonstrating competitive results in unsupervised and weakly-supervised semantic segmentation.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Murdock_Semantic_Component_Analysis_ICCV_2015_paper.pdf",
        "aff": "Machine Learning Department, Carnegie Mellon University; The Robotics Institute, Carnegie Mellon University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2226208,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_domain": "cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Murdock_Semantic_Component_Analysis_ICCV_2015_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Machine Learning Department",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Murdock_2015_ICCV,\n    \n    author = {\n    Murdock,\n    Calvin and De la Torre,\n    Fernando\n},\n    title = {\n    Semantic Component Analysis\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "d943c88855",
        "title": "Semantic Image Segmentation via Deep Parsing Network",
        "session": "segmentation, edges and saliency",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "Ziwei Liu, Xiaoxiao Li, Ping Luo, Chen-Change Loy, Xiaoou Tang",
        "author": "Ziwei Liu; Xiaoxiao Li; Ping Luo; Chen-Change Loy; Xiaoou Tang",
        "abstract": "This paper addresses semantic image segmentation by incorporating rich information into Markov Random Field (MRF), including high-order relations and mixture of label contexts. Unlike previous works that optimized MRFs using iterative algorithm, we solve MRF by proposing a Convolutional Neural Network (CNN), namely Deep Parsing Network (DPN), which enables deterministic end-to-end computation in a single forward pass. Specifically, DPN extends a contemporary CNN architecture to model unary terms and additional layers are carefully devised to approximate the mean field algorithm (MF) for pairwise terms. It has several appealing properties. First, different from the recent works that combined CNN and MRF, where many iterations of MF were required for each training image during back-propagation, DPN is able to achieve high performance by approximating one iteration of MF. Second, DPN represents various types of pairwise terms, making many existing works as its special cases. Third, DPN makes MF easier to be parallelized and speeded up in Graphical Processing Unit (GPU). DPN is thoroughly evaluated on the PASCAL VOC 2012 dataset, where a single DPN model yields a new state-of-the-art segmentation accuracy of 77.5%.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Liu_Semantic_Image_Segmentation_ICCV_2015_paper.pdf",
        "aff": "Department of Information Engineering, The Chinese University of Hong Kong; Department of Information Engineering, The Chinese University of Hong Kong; Department of Information Engineering, The Chinese University of Hong Kong; Department of Information Engineering, The Chinese University of Hong Kong; Department of Information Engineering, The Chinese University of Hong Kong",
        "project": "http://personal.ie.cuhk.edu.hk/~lz013/projects/DPN.html",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 684676,
        "gs_citation": 887,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18281955767933637624&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "ie.cuhk.edu.hk;ie.cuhk.edu.hk;ie.cuhk.edu.hk;ie.cuhk.edu.hk;ie.cuhk.edu.hk",
        "email": "ie.cuhk.edu.hk;ie.cuhk.edu.hk;ie.cuhk.edu.hk;ie.cuhk.edu.hk;ie.cuhk.edu.hk",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Liu_Semantic_Image_Segmentation_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "The Chinese University of Hong Kong",
        "aff_unique_dep": "Department of Information Engineering",
        "aff_unique_url": "https://www.cuhk.edu.hk",
        "aff_unique_abbr": "CUHK",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Liu_2015_ICCV,\n    \n    author = {\n    Liu,\n    Ziwei and Li,\n    Xiaoxiao and Luo,\n    Ping and Loy,\n    Chen-Change and Tang,\n    Xiaoou\n},\n    title = {\n    Semantic Image Segmentation via Deep Parsing Network\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "76b07edd8e",
        "title": "Semantic Pose Using Deep Networks Trained on Synthetic RGB-D",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Jeremie Papon, Markus Schoeler",
        "author": "Jeremie Papon; Markus Schoeler",
        "abstract": "In this work we address the problem of indoor scene understanding from RGB-D images. Specifically, we propose to find instances of common furniture classes, their spatial extent, and their pose with respect to generalized class models. To accomplish this, we use a deep, wide, multi-output convolutional neural network (CNN) that predicts class, pose, and location of possible objects simultaneously. To overcome the lack of large annotated RGB-D training sets (especially those with pose), we use an on-the-fly rendering pipeline that generates realistic cluttered room scenes in parallel to training. We then perform transfer learning on the relatively small amount of publicly available annotated RGB-D data, and find that our model is able to successfully annotate even highly challenging real scenes. Importantly, our trained network is able to understand noisy and sparse observations of highly cluttered scenes with a remarkable degree of accuracy, inferring class and pose from a very limited set of cues. Additionally, our neural network is only moderately deep and computes class, pose and position in tandem, so the overall run-time is significantly faster than existing methods, estimating all output parameters simultaneously in parallel.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Papon_Semantic_Pose_Using_ICCV_2015_paper.pdf",
        "aff": "Bernstein Center for Computational Neuroscience (BCCN); III. Physikalisches Institut - Biophysik, Georg-August University of Göttingen",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1632376,
        "gs_citation": 95,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7396722370568555317&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "gmail.com;gwdg.de",
        "email": "gmail.com;gwdg.de",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Papon_Semantic_Pose_Using_ICCV_2015_paper.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Bernstein Center for Computational Neuroscience;Georg-August University of Göttingen",
        "aff_unique_dep": "Computational Neuroscience;III. Physikalisches Institut - Biophysik",
        "aff_unique_url": ";https://www.uni-goettingen.de",
        "aff_unique_abbr": "BCCN;Uni Göttingen",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany",
        "bibtex": "@InProceedings{Papon_2015_ICCV,\n    \n    author = {\n    Papon,\n    Jeremie and Schoeler,\n    Markus\n},\n    title = {\n    Semantic Pose Using Deep Networks Trained on Synthetic RGB-D\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "b12c8053f1",
        "title": "Semantic Segmentation With Object Clique Potential",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Xiaojuan Qi, Jianping Shi, Shu Liu, Renjie Liao, Jiaya Jia",
        "author": "Xiaojuan Qi; Jianping Shi; Shu Liu; Renjie Liao; Jiaya Jia",
        "abstract": "In this paper, we propose an object clique potential for semantic segmentation. Our object clique potential addresses the misclassified object-part issues arising in solutions based on fully-connected networks. Our object clique set, compared to that yielded from segment-proposal-based approaches, is with a significantly small size, making our method consume notably less computation. Regarding system design and model formation, our object clique potential can be regarded as a functionally complement to local-appearance-based CRF models and works in synergy with these effective approaches for further performance improvement. Extensive experiments verify our method.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Qi_Semantic_Segmentation_With_ICCV_2015_paper.pdf",
        "aff": "The Chinese University of Hong Kong; The Chinese University of Hong Kong; The Chinese University of Hong Kong; The Chinese University of Hong Kong; The Chinese University of Hong Kong",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1026571,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4828310216707140726&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "cse.cuhk.edu.hk;cse.cuhk.edu.hk;cse.cuhk.edu.hk;cse.cuhk.edu.hk;cse.cuhk.edu.hk",
        "email": "cse.cuhk.edu.hk;cse.cuhk.edu.hk;cse.cuhk.edu.hk;cse.cuhk.edu.hk;cse.cuhk.edu.hk",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Qi_Semantic_Segmentation_With_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "The Chinese University of Hong Kong",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cuhk.edu.hk",
        "aff_unique_abbr": "CUHK",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Qi_2015_ICCV,\n    \n    author = {\n    Qi,\n    Xiaojuan and Shi,\n    Jianping and Liu,\n    Shu and Liao,\n    Renjie and Jia,\n    Jiaya\n},\n    title = {\n    Semantic Segmentation With Object Clique Potential\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "7b5d06a84b",
        "title": "Semantic Segmentation of RGBD Images With Mutex Constraints",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Zhuo Deng, Sinisa Todorovic, Longin Jan Latecki",
        "author": "Zhuo Deng; Sinisa Todorovic; Longin Jan Latecki",
        "abstract": "In this paper, we address the problem of semantic scene segmentation of RGB-D images of indoor scenes. We propose a novel image region labeling method which augments CRF formulation with hard mutual exclusion (mutex) constraints. This way our approach can make use of rich and accurate 3D geometric structure coming from Kinect in a principled manner. The final labeling result must satisfy all mutex constraints, which allows us to eliminate configurations that violate common sense physics laws like placing a floor above a night stand. Three classes of mutex constraints are proposed: global object co-occurrence constraint, relative height relationship constraint, and local support relationship constraint. We evaluate our approach on the NYU-Depth V2 dataset, which consists of 1449 cluttered indoor scenes, and also test generalization of our model trained on NYU-Depth V2 dataset directly on a recent SUN3D dataset without any new training. The experimental results show that we significantly outperform the state-of-the-art methods in scene labeling on both datasets.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Deng_Semantic_Segmentation_of_ICCV_2015_paper.pdf",
        "aff": "Temple University; Oregon State University; Temple University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 724243,
        "gs_citation": 113,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13442061503028009662&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "temple.edu;eecs.oregonstate.edu;temple.edu",
        "email": "temple.edu;eecs.oregonstate.edu;temple.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Deng_Semantic_Segmentation_of_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Temple University;Oregon State University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.temple.edu;https://oregonstate.edu",
        "aff_unique_abbr": "Temple;OSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Deng_2015_ICCV,\n    \n    author = {\n    Deng,\n    Zhuo and Todorovic,\n    Sinisa and Latecki,\n    Longin Jan\n},\n    title = {\n    Semantic Segmentation of RGBD Images With Mutex Constraints\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "6200429a5a",
        "title": "Semantic Video Entity Linking Based on Visual Content and Metadata",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Yuncheng Li, Xitong Yang, Jiebo Luo",
        "author": "Yuncheng Li; Xitong Yang; Jiebo Luo",
        "abstract": "Video entity linking, which connects online videos to the related entities in a semantic knowledge base, can enable a wide variety of video based applications including video retrieval and video recommendation. Most existing systems for video entity linking rely on video metadata. In this paper, we propose to exploit video visual content to improve video entity linking. In the proposed framework, videos are first linked to entity candidates using a text-based method. Next, the entity candidates are verified and reranked according to visual content. In order to properly handle large variations in visual content matching, we propose to use Multiple Instance Metric Learning to learn a \"set to sequence'' metric for this specific matching problem. To evaluate the proposed framework, we collect and annotate 1912 videos crawled from the YouTube open API. Experiment results have shown consistent gains by the proposed framework over several strong baselines.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Li_Semantic_Video_Entity_ICCV_2015_paper.pdf",
        "aff": "University of Rochester; University of Rochester; University of Rochester",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 935852,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1777927703913605189&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "cs.rochester.edu;cs.rochester.edu;cs.rochester.edu",
        "email": "cs.rochester.edu;cs.rochester.edu;cs.rochester.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Li_Semantic_Video_Entity_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Rochester",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.rochester.edu",
        "aff_unique_abbr": "U of R",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Li_2015_ICCV,\n    \n    author = {\n    Li,\n    Yuncheng and Yang,\n    Xitong and Luo,\n    Jiebo\n},\n    title = {\n    Semantic Video Entity Linking Based on Visual Content and Metadata\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "6d03fcbf44",
        "title": "Semantically-Aware Aerial Reconstruction From Multi-Modal Data",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Randi Cabezas, Julian Straub, John W. Fisher III",
        "author": "Randi Cabezas; Julian Straub; John W. Fisher III",
        "abstract": "We consider a methodology for integrating multiple sensors along with semantic information to enhance scene representations.  We propose a probabilistic generative model for inferring semantically-informed aerial reconstructions from multi-modal data within a consistent mathematical framework. The approach, called Semantically- Aware Aerial Reconstruction (SAAR), not only exploits inferred scene geometry, appearance, and semantic observations to obtain a meaningful categorization of the data, but also extends previously proposed methods by imposing structure on the prior over geometry, appearance, and semantic labels.  This leads to more accurate reconstructions and the ability to fill in missing contextual labels via joint sensor and semantic information. We introduce a new multi-modal synthetic dataset in order to provide quantitative performance analysis.  Additionally, we apply the model to real-world data and exploit OpenStreetMap as a source of semantic observations. We show quantitative improvements in reconstruction accuracy of large-scale urban scenes from the combination of LiDAR, aerial photography, and semantic data.  Furthermore, we demonstrate the model's ability to fill in for missing sensed data, leading to more interpretable reconstructions.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Cabezas_Semantically-Aware_Aerial_Reconstruction_ICCV_2015_paper.pdf",
        "aff": "Massachusetts Institute of Technology; Massachusetts Institute of Technology; Massachusetts Institute of Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 12288053,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3420349684041176976&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "csail.mit.edu;csail.mit.edu;csail.mit.edu",
        "email": "csail.mit.edu;csail.mit.edu;csail.mit.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Cabezas_Semantically-Aware_Aerial_Reconstruction_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://web.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Cabezas_2015_ICCV,\n    \n    author = {\n    Cabezas,\n    Randi and Straub,\n    Julian and Fisher,\n    III,\n    John W.\n},\n    title = {\n    Semantically-Aware Aerial Reconstruction From Multi-Modal Data\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "f37aab6ccf",
        "title": "Semi-Supervised Normalized Cuts for Image Segmentation",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Selene E. Chew, Nathan D. Cahill",
        "author": "Selene E. Chew; Nathan D. Cahill",
        "abstract": "Since its introduction as a  powerful graph-based method for image segmentation, the Normalized Cuts (NCuts) algorithm has been generalized to incorporate expert knowledge about how certain pixels or regions should be grouped, or how the resulting segmentation should be biased to be correlated with priors. Previous approaches incorporate hard must-link constraints on how certain pixels should be grouped as well as hard cannot-link constraints on how other pixels should be separated into different groups. In this paper, we reformulate NCuts to allow both sets of constraints to be handled in a soft manner, enabling the user to tune the degree to which the constraints are satisfied. An approximate spectral solution to the reformulated problem exists without requiring explicit construction of a large, dense matrix; hence, computation time is comparable to that of unconstrained NCuts. Using synthetic data and real imagery, we show that soft handling of constraints yields better results than unconstrained NCuts and enables more robust clustering and segmentation than is possible when the constraints are strictly enforced.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Chew_Semi-Supervised_Normalized_Cuts_ICCV_2015_paper.pdf",
        "aff": "School of Mathematical Sciences, Rochester Institute of Technology; School of Mathematical Sciences, Rochester Institute of Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2240605,
        "gs_citation": 47,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9351680263732732535&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "rit.edu;rit.edu",
        "email": "rit.edu;rit.edu",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Chew_Semi-Supervised_Normalized_Cuts_ICCV_2015_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Rochester Institute of Technology",
        "aff_unique_dep": "School of Mathematical Sciences",
        "aff_unique_url": "https://www.rit.edu",
        "aff_unique_abbr": "RIT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Rochester",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Chew_2015_ICCV,\n    \n    author = {\n    Chew,\n    Selene E. and Cahill,\n    Nathan D.\n},\n    title = {\n    Semi-Supervised Normalized Cuts for Image Segmentation\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "3f449aba89",
        "title": "Semi-Supervised Zero-Shot Classification With Label Representation Learning",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Xin Li, Yuhong Guo, Dale Schuurmans",
        "author": "Xin Li; Yuhong Guo; Dale Schuurmans",
        "abstract": "Given the challenge of gathering labeled training data, zero-shot classification, which transfers information from observed classes to recognize unseen classes, has become increasingly popular in the computer vision community. Most existing zero-shot learning methods require a user to first provide a set of semantic visual attributes for each class as side information before applying a two-step prediction procedure that introduces an intermediate attribute prediction problem. In this paper, we propose a novel zero-shot classification approach that automatically learns label embeddings from the input data in a semi-supervised large-margin learning framework. The proposed framework jointly considers multi-class classification over all classes (observed and unseen) and tackles the target prediction problem directly without introducing intermediate prediction problems. It also has the capacity to incorporate semantic label information from different sources when available. To evaluate the proposed approach, we conduct experiments on standard zero-shot data sets. The empirical results show the proposed approach outperforms existing state-of-the-art zero-shot learning methods.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Li_Semi-Supervised_Zero-Shot_Classification_ICCV_2015_paper.pdf",
        "aff": "Department of Computer and Information Sciences, Temple University; Department of Computer and Information Sciences, Temple University; Department of Computing Science, University of Alberta",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 619593,
        "gs_citation": 141,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12815020160359463701&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "temple.edu;temple.edu;ualberta.ca",
        "email": "temple.edu;temple.edu;ualberta.ca",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Li_Semi-Supervised_Zero-Shot_Classification_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Temple University;University of Alberta",
        "aff_unique_dep": "Department of Computer and Information Sciences;Department of Computing Science",
        "aff_unique_url": "https://www.temple.edu;https://www.ualberta.ca",
        "aff_unique_abbr": "Temple;UAlberta",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "United States;Canada",
        "bibtex": "@InProceedings{Li_2015_ICCV,\n    \n    author = {\n    Li,\n    Xin and Guo,\n    Yuhong and Schuurmans,\n    Dale\n},\n    title = {\n    Semi-Supervised Zero-Shot Classification With Label Representation Learning\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "2b92bbfa44",
        "title": "Separating Fluorescent and Reflective Components by Using a Single Hyperspectral Image",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Yinqiang Zheng, Ying Fu, Antony Lam, Imari Sato, Yoichi Sato",
        "author": "Yinqiang Zheng; Ying Fu; Antony Lam; Imari Sato; Yoichi Sato",
        "abstract": "This paper introduces a novel method to separate fluorescent and reflective components in the spectral domain. In contrast to existing methods, which require to capture two or more images under varying illuminations, we aim to achieve this separation task by using a single hyperspectral image. After identifying the critical hurdle in single-image component separation, we mathematically design the optimal illumination spectrum, which is shown to contain substantial high-frequency components in the frequency domain. This observation, in turn, leads us to recognize a key difference between reflectance and fluorescence in response to the frequency modulation effect of illumination, which fundamentally explains the feasibility of our method. On the practical side, we successfully find an off-the-shelf lamp as the light source, which is strong in irradiance intensity and cheap in cost. A fast linear separation algorithm is developed as well. Experiments using both synthetic data and real images have confirmed the validity of the selected illuminant and the accuracy of our separation algorithm.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Zheng_Separating_Fluorescent_and_ICCV_2015_paper.pdf",
        "aff": "National Institute of Informatics; The University of Tokyo; Saitama University; National Institute of Informatics; The University of Tokyo",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1799421,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6518818992749282997&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "nii.ac.jp;iis.u-tokyo.ac.jp;cv.ics.saitama-u.ac.jp;nii.ac.jp;iis.u-tokyo.ac.jp",
        "email": "nii.ac.jp;iis.u-tokyo.ac.jp;cv.ics.saitama-u.ac.jp;nii.ac.jp;iis.u-tokyo.ac.jp",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Zheng_Separating_Fluorescent_and_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;2;0;1",
        "aff_unique_norm": "National Institute of Informatics;University of Tokyo;Saitama University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.nii.ac.jp/;https://www.u-tokyo.ac.jp;https://www.saitama-u.ac.jp",
        "aff_unique_abbr": "NII;UTokyo;Saitama U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Japan",
        "bibtex": "@InProceedings{Zheng_2015_ICCV,\n    \n    author = {\n    Zheng,\n    Yinqiang and Fu,\n    Ying and Lam,\n    Antony and Sato,\n    Imari and Sato,\n    Yoichi\n},\n    title = {\n    Separating Fluorescent and Reflective Components by Using a Single Hyperspectral Image\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "f6df1f45cb",
        "title": "Sequence to Sequence - Video to Text",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Subhashini Venugopalan, Marcus Rohrbach, Jeffrey Donahue, Raymond Mooney, Trevor Darrell, Kate Saenko",
        "author": "Subhashini Venugopalan; Marcus Rohrbach; Jeffrey Donahue; Raymond Mooney; Trevor Darrell; Kate Saenko",
        "abstract": "Real-world videos often have complex dynamics; methods for generating open-domain video descriptions should be senstive to temporal structure and allow both input (sequence of frames) and output (sequence of words) of variable length. To approach this problem we propose a novel end-to-end sequence-to-sequence model to generate captions for videos. For this we exploit recurrent neural networks, specifically LSTMs, which have demonstrated state-of-the-art performance in image caption generation. Our LSTM model is trained on video-sentence pairs and learns to associate a sequence of video frames to a sequence of words in order to generate a description of the event in the video clip. Our model naturally is able to learn the temporal structure of the sequence of frames as well as the sequence model of the generated sentences, i.e. a language model. We evaluate several variants of our model that exploit different visual features on a standard set of YouTube videos and two movie description datasets (M-VAD and MPII-MD).",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Venugopalan_Sequence_to_Sequence_ICCV_2015_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1504243,
        "gs_citation": 1877,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2341786742021115670&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Venugopalan_Sequence_to_Sequence_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Venugopalan_2015_ICCV,\n    \n    author = {\n    Venugopalan,\n    Subhashini and Rohrbach,\n    Marcus and Donahue,\n    Jeffrey and Mooney,\n    Raymond and Darrell,\n    Trevor and Saenko,\n    Kate\n},\n    title = {\n    Sequence to Sequence - Video to Text\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "3ed4b39fcb",
        "title": "Shape Interaction Matrix Revisited and Robustified: Efficient Subspace Clustering With Corrupted and Incomplete Data",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Pan Ji, Mathieu Salzmann, Hongdong Li",
        "author": "Pan Ji; Mathieu Salzmann; Hongdong Li",
        "abstract": "The Shape Interaction Matrix (SIM) is one of the earliest approaches to performing subspace clustering (i.e., separating points drawn from a union of subspaces). In this paper, we revisit the SIM and reveal its connections to several recent subspace clustering methods. Our analysis lets us derive a simple, yet effective algorithm to robustify the SIM and make it applicable to realistic scenarios where the data is corrupted by noise. We justify our method by intuitive examples and the matrix perturbation theory. We then show how this approach can be extended to handle missing data, thus yielding an efficient and general subspace clustering algorithm. We demonstrate the benefits of our approach over state-of-the-art subspace clustering methods on several challenging motion segmentation and face clustering problems, where the data includes corruptions and missing measurements.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Ji_Shape_Interaction_Matrix_ICCV_2015_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1184684,
        "gs_citation": 90,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4747043013209012986&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Ji_Shape_Interaction_Matrix_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Ji_2015_ICCV,\n    \n    author = {\n    Ji,\n    Pan and Salzmann,\n    Mathieu and Li,\n    Hongdong\n},\n    title = {\n    Shape Interaction Matrix Revisited and Robustified: Efficient Subspace Clustering With Corrupted and Incomplete Data\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "70ea233ec3",
        "title": "Shell PCA: Statistical Shape Modelling in Shell Space",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Chao Zhang, Behrend Heeren, Martin Rumpf, William A. P. Smith",
        "author": "Chao Zhang; Behrend Heeren; Martin Rumpf; William A. P. Smith",
        "abstract": "In this paper we describe how to perform Principal Components Analysis in \"shell space\". Thin shells are a physical model for surfaces with non-zero thickness whose deformation dissipates elastic energy. Thin shells, or their discrete counterparts, can be considered to reside in a shell space in which the notion of distance is given by the elastic energy required to deform one shape into another. It is in this setting that we show how to perform statistical analysis of a set of shapes (meshes in dense correspondence), providing a hybrid between physical and statistical shape modelling. The resulting models are better able to capture non-linear deformations, for example resulting from articulated motion, even when training data is very sparse compared to the dimensionality of the observation space.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Zhang_Shell_PCA_Statistical_ICCV_2015_paper.pdf",
        "aff": "Department of Computer Science, The University of York, UK; Institute for Numerical Simulation, University of Bonn, Germany; Institute for Numerical Simulation, University of Bonn, Germany; Department of Computer Science, The University of York, UK",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2171704,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15518409673409893471&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "york.ac.uk;york.ac.uk;ins.uni-bonn.de;ins.uni-bonn.de",
        "email": "york.ac.uk;york.ac.uk;ins.uni-bonn.de;ins.uni-bonn.de",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Zhang_Shell_PCA_Statistical_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "The University of York;University of Bonn",
        "aff_unique_dep": "Department of Computer Science;Institute for Numerical Simulation",
        "aff_unique_url": "https://www.york.ac.uk;https://www.uni-bonn.de",
        "aff_unique_abbr": "York;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;0",
        "aff_country_unique": "United Kingdom;Germany",
        "bibtex": "@InProceedings{Zhang_2015_ICCV,\n    \n    author = {\n    Zhang,\n    Chao and Heeren,\n    Behrend and Rumpf,\n    Martin and Smith,\n    William A. P.\n},\n    title = {\n    Shell PCA: Statistical Shape Modelling in Shell Space\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "40d4fdb94c",
        "title": "Similarity Gaussian Process Latent Variable Model for Multi-Modal Data Analysis",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Guoli Song, Shuhui Wang, Qingming Huang, Qi Tian",
        "author": "Guoli Song; Shuhui Wang; Qingming Huang; Qi Tian",
        "abstract": "Data from real applications involve multiple modalities representing content with the same semantics and deliver rich information from complementary aspects. However, relations among heterogeneous modalities are simply treated as observation-to-fit by existing work, and the parameterized cross-modal mapping functions lack flexibility in directly adapting to the content divergence and semantic complicacy of multi-modal data. In this paper, we build our work based on Gaussian process latent variable model (GPLVM) to learn the non-linear non-parametric mapping functions and transform heterogeneous data into a shared latent space. We propose multi-modal Similarity Gaussian Process latent variable model (m-SimGP), which learns the nonlinear mapping functions between the intra-modal similarities and latent representation. We further propose multi-modal regularized similarity GPLVM (m-RSimGP) by encouraging similar/dissimilar points to be similar/dissimilar in the output space. The overall objective functions are solved by simple and scalable gradient decent techniques. The proposed models are robust to content divergence and high-dimensionality in multi-modal representation. They can be applied to various tasks to discover the non-linear correlations and obtain the comparable low-dimensional representation for heterogeneous modalities. On two widely used real-world datasets, we outperform previous approaches for cross-modal content retrieval and cross-modal classification.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Song_Similarity_Gaussian_Process_ICCV_2015_paper.pdf",
        "aff": "Key Lab of Big Data Mining and Knowledge Management, University of Chinese Academy of Sciences, Beijing, China+Key Lab of Intell. Info. Process., Inst. of Comput. Tech, Chinese Academy of Sciences, Beijing, China; Key Lab of Intell. Info. Process., Inst. of Comput. Tech, Chinese Academy of Sciences, Beijing, China; Key Lab of Big Data Mining and Knowledge Management, University of Chinese Academy of Sciences, Beijing, China+Key Lab of Intell. Info. Process., Inst. of Comput. Tech, Chinese Academy of Sciences, Beijing, China; Department of Computer Science, University of Texas at San Antonio, TX, 78249, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 512263,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8676962707546678049&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "vipl.ict.ac.cn;ict.ac.cn;jdl.ac.cn;cs.utsa.edu",
        "email": "vipl.ict.ac.cn;ict.ac.cn;jdl.ac.cn;cs.utsa.edu",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Song_Similarity_Gaussian_Process_ICCV_2015_paper.html",
        "aff_unique_index": "0+1;1;0+1;2",
        "aff_unique_norm": "University of Chinese Academy of Sciences;Chinese Academy of Sciences;University of Texas at San Antonio",
        "aff_unique_dep": "Key Lab of Big Data Mining and Knowledge Management;Key Lab of Intell. Info. Process., Inst. of Comput. Tech;Department of Computer Science",
        "aff_unique_url": "http://www.ucas.ac.cn;http://www.ict.cas.cn;https://www.utsa.edu",
        "aff_unique_abbr": "UCAS;CAS;UTSA",
        "aff_campus_unique_index": "0+0;0;0+0;1",
        "aff_campus_unique": "Beijing;San Antonio",
        "aff_country_unique_index": "0+0;0;0+0;1",
        "aff_country_unique": "China;United States",
        "bibtex": "@InProceedings{Song_2015_ICCV,\n    \n    author = {\n    Song,\n    Guoli and Wang,\n    Shuhui and Huang,\n    Qingming and Tian,\n    Qi\n},\n    title = {\n    Similarity Gaussian Process Latent Variable Model for Multi-Modal Data Analysis\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "286a0d3905",
        "title": "Simpler Non-Parametric Methods Provide as Good or Better Results to Multiple-Instance Learning",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Ragav Venkatesan, Parag Chandakkar, Baoxin Li",
        "author": "Ragav Venkatesan; Parag Chandakkar; Baoxin Li",
        "abstract": "Multiple-instance learning (MIL) is a unique learning problem in which training data labels are available only for collections of objects (called bags) instead of individual objects (called instances). A plethora of approaches have been developed to solve this problem in the past years. Popular methods include the diverse density, MILIS and DD-SVM. While having been widely used, these methods, particularly those in computer vision have attempted fairly sophisticated solutions to solve certain unique and particular configurations of the MIL space.      In this paper, we analyze the MIL feature space using modified versions of traditional non-parametric techniques like the Parzen window and k-nearest-neighbour, and develop a learning approach employing distances to k-nearest neighbours of a point in the feature space. We show that these methods work as well, if not better than most recently published methods on benchmark datasets. We compare and contrast our analysis with the well-established diverse-density approach and its variants in recent literature, using benchmark datasets including the Musk, Andrews' and Corel datasets, along with a diabetic retinopathy pathology diagnosis dataset. Experimental results demonstrate that, while enjoying an intuitive interpretation and supporting fast learning, these method have the potential of delivering improved performance even for complex data arising from real-world applications.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Venkatesan_Simpler_Non-Parametric_Methods_ICCV_2015_paper.pdf",
        "aff": "Arizona State University, Tempe, AZ, USA; Arizona State University, Tempe, AZ, USA; Arizona State University, Tempe, AZ, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4669777,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9893593471237955035&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "asu.edu;asu.edu;asu.edu",
        "email": "asu.edu;asu.edu;asu.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Venkatesan_Simpler_Non-Parametric_Methods_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Arizona State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.asu.edu",
        "aff_unique_abbr": "ASU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Tempe",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Venkatesan_2015_ICCV,\n    \n    author = {\n    Venkatesan,\n    Ragav and Chandakkar,\n    Parag and Li,\n    Baoxin\n},\n    title = {\n    Simpler Non-Parametric Methods Provide as Good or Better Results to Multiple-Instance Learning\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "69b0587e89",
        "title": "Simultaneous Deep Transfer Across Domains and Tasks",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Eric Tzeng, Judy Hoffman, Trevor Darrell, Kate Saenko",
        "author": "Eric Tzeng; Judy Hoffman; Trevor Darrell; Kate Saenko",
        "abstract": "Recent reports suggest that a generic supervised deep CNN model trained on a large-scale dataset reduces, but does not remove, dataset bias. Fine-tuning deep models in a new domain can require a significant amount of labeled data, which for many applications is simply not available. We propose a new CNN architecture to exploit unlabeled and sparsely labeled target domain data. Our approach simultaneously optimizes for domain invariance to facilitate domain transfer and uses a soft label distribution matching loss to transfer information between tasks. Our proposed adaptation method offers empirical performance which exceeds previously published results on two standard benchmark visual domain adaptation tasks, evaluated across supervised and semi-supervised adaptation settings.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Tzeng_Simultaneous_Deep_Transfer_ICCV_2015_paper.pdf",
        "aff": "UC Berkeley, EECS & ICSI; UC Berkeley, EECS & ICSI; UC Berkeley, EECS & ICSI; UMass Lowell, CS",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2077240,
        "gs_citation": 1666,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2973992380342580480&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff_domain": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu;cs.uml.edu",
        "email": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu;cs.uml.edu",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Tzeng_Simultaneous_Deep_Transfer_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "University of California, Berkeley;University of Massachusetts Lowell",
        "aff_unique_dep": "Electrical Engineering and Computer Sciences;Computer Science",
        "aff_unique_url": "https://www.berkeley.edu;https://www.uml.edu",
        "aff_unique_abbr": "UC Berkeley;UMass Lowell",
        "aff_campus_unique_index": "0;0;0;1",
        "aff_campus_unique": "Berkeley;Lowell",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Tzeng_2015_ICCV,\n    \n    author = {\n    Tzeng,\n    Eric and Hoffman,\n    Judy and Darrell,\n    Trevor and Saenko,\n    Kate\n},\n    title = {\n    Simultaneous Deep Transfer Across Domains and Tasks\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "1edde6be22",
        "title": "Simultaneous Foreground Detection and Classification With Hybrid Features",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Jaemyun Kim, Adín Ramírez Rivera, Byungyong Ryu, Oksam Chae",
        "author": "Jaemyun Kim; Adin Ramirez Rivera; Byungyong Ryu; Oksam Chae",
        "abstract": "In this paper, we propose a hybrid background model that relies on edge and non-edge features of the image to produce the model. We encode these features into a coding scheme, that we called Local Hybrid Pattern (LHP), that selectively models edges and non-edges features of each pixel. Furthermore, we model each pixel with an adaptive code dictionary to represent the background dynamism, and update it by adding stable codes and discarding unstable ones. We weight each code in the dictionary to enhance its description of the pixel it models. The foreground is detected as the incoming codes that deviate from the dictionary. We can detect (as foreground or background) and classify (as edge or inner region) each pixel simultaneously. We tested our proposed method in existing databases with promising results.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Kim_Simultaneous_Foreground_Detection_ICCV_2015_paper.pdf",
        "aff": "Dept. of Computer Engineering, Kyung Hee University, Gyeonggi-do, South Korea; Escuela Informática y Telecomunicaciones, Universidad Diego Portales, Santiago, Chile; Dept. of Computer Engineering, Kyung Hee University, Gyeonggi-do, South Korea; Dept. of Computer Engineering, Kyung Hee University, Gyeonggi-do, South Korea",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1874451,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16662536765449490791&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 8,
        "aff_domain": "khu.ac.kr;mail.udp.cl;khu.ac.kr;khu.ac.kr",
        "email": "khu.ac.kr;mail.udp.cl;khu.ac.kr;khu.ac.kr",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Kim_Simultaneous_Foreground_Detection_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Kyung Hee University;Universidad Diego Portales",
        "aff_unique_dep": "Dept. of Computer Engineering;Escuela Informática y Telecomunicaciones",
        "aff_unique_url": "http://www.khu.ac.kr;https://www.udp.cl",
        "aff_unique_abbr": "KHU;",
        "aff_campus_unique_index": "0;1;0;0",
        "aff_campus_unique": "Gyeonggi-do;Santiago",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "South Korea;Chile",
        "bibtex": "@InProceedings{Kim_2015_ICCV,\n    \n    author = {\n    Kim,\n    Jaemyun and Rivera,\n    Adin Ramirez and Ryu,\n    Byungyong and Chae,\n    Oksam\n},\n    title = {\n    Simultaneous Foreground Detection and Classification With Hybrid Features\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "a3ebaf111d",
        "title": "Simultaneous Local Binary Feature Learning and Encoding for Face Recognition",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Jiwen Lu, Venice Erin Liong, Jie Zhou",
        "author": "Jiwen Lu; Venice Erin Liong; Jie Zhou",
        "abstract": "In this paper, we propose a simultaneous local binary feature learning and encoding (SLBFLE) method for face recognition. Different from existing hand-crafted face descriptors such as local binary pattern (LBP) and Gabor features which require strong prior knowledge, our SLBFLE is an unsupervised feature learning approach which is automatically learned from raw pixels. Unlike existing binary face descriptors such as the LBP and discriminant face descriptor (DFD) which use a two-stage feature extraction approach, our SLBFLE jointly learns binary codes for local face patches and the codebook for feature encoding so that discriminative information from raw pixels can be simultaneously learned with a one-stage procedure. Experimental results on four widely used face datasets including LFW, YouTube Face (YTF), FERET and PaSC clearly demonstrate the effectiveness of the proposed method.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Lu_Simultaneous_Local_Binary_ICCV_2015_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16156138329588200720&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Lu_Simultaneous_Local_Binary_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Lu_2015_ICCV,\n    \n    author = {\n    Lu,\n    Jiwen and Liong,\n    Venice Erin and Zhou,\n    Jie\n},\n    title = {\n    Simultaneous Local Binary Feature Learning and Encoding for Face Recognition\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "bf82e8fcd2",
        "title": "Single Image 3D Without a Single 3D Image",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "David F. Fouhey, Wajahat Hussain, Abhinav Gupta, Martial Hebert",
        "author": "David F. Fouhey; Wajahat Hussain; Abhinav Gupta; Martial Hebert",
        "abstract": "Do we really need 3D labels in order to learn how to predict 3D? In this paper, we show that one can learn a mapping from appearance to 3D properties without ever seeing a single explicit 3D label. Rather than use explicit supervision, we use the regularity of indoor scenes to learn the mapping in a completely unsupervised manner. We demonstrate this on both a standard 3D scene understanding dataset as well as Internet images for which 3D is unavailable, precluding supervised learning. Despite never seeing a 3D label, our method produces competitive results.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Fouhey_Single_Image_3D_ICCV_2015_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2551317,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5677852021039492347&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Fouhey_Single_Image_3D_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Fouhey_2015_ICCV,\n    \n    author = {\n    Fouhey,\n    David F. and Hussain,\n    Wajahat and Gupta,\n    Abhinav and Hebert,\n    Martial\n},\n    title = {\n    Single Image 3D Without a Single 3D Image\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "124f1cb55a",
        "title": "Single Image Pop-Up From Discriminatively Learned Parts",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Menglong Zhu, Xiaowei Zhou, Kostas Daniilidis",
        "author": "Menglong Zhu; Xiaowei Zhou; Kostas Daniilidis",
        "abstract": "We introduce a new approach for estimating a fine grained 3D shape and continuous pose of an object from a single image. Given a training set of view exemplars, we learn and select appearance-based discriminative parts which are mapped onto the 3D model through a facility location optimization. The training set of 3D models is summarized into a set of basis shapes from which we can generalize by linear combination. Given a test image, we detect hypotheses for each part. The main challenge is to select from these hypotheses and compute the 3D pose and shape coefficients at the same time. To achieve this, we optimize a function that considers simultaneously the appearance matching of the parts as well as the geometric reprojection error. We apply the alternating direction method of multipliers (ADMM) to minimize the resulting convex function. Our main and novel contribution is the simultaneous solution for part localization and detailed 3D geometry estimation by maximizing both appearance and geometric compatibility with convex relaxation.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Zhu_Single_Image_Pop-Up_ICCV_2015_paper.pdf",
        "aff": "Computer and Information Science, University of Pennsylvania; Computer and Information Science, University of Pennsylvania; Computer and Information Science, University of Pennsylvania",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1643813,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6124670112741590244&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "cis.upenn.edu;cis.upenn.edu;cis.upenn.edu",
        "email": "cis.upenn.edu;cis.upenn.edu;cis.upenn.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Zhu_Single_Image_Pop-Up_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Pennsylvania",
        "aff_unique_dep": "Computer and Information Science",
        "aff_unique_url": "https://www.upenn.edu",
        "aff_unique_abbr": "UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Zhu_2015_ICCV,\n    \n    author = {\n    Zhu,\n    Menglong and Zhou,\n    Xiaowei and Daniilidis,\n    Kostas\n},\n    title = {\n    Single Image Pop-Up From Discriminatively Learned Parts\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "2c7512d740",
        "title": "Single-Shot Specular Surface Reconstruction With Gonio-Plenoptic Imaging",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Lingfei Meng, Liyang Lu, Noah Bedard, Kathrin Berkner",
        "author": "Lingfei Meng; Liyang Lu; Noah Bedard; Kathrin Berkner",
        "abstract": "We present a gonio-plenoptic imaging system that realizes a single-shot shape measurement for specular surfaces. The system is comprised of a collimated illumination source and a plenoptic camera. Unlike a conventional plenoptic camera, our system captures the BRDF variation of the object surface in a single image in addition to the light field information from the scene, which allows us to recover very fine 3D structures of the surface. The shape of the surface is reconstructed based on the reflectance property of the material rather than the parallax between different views. Since only a single-shot is required to reconstruct the whole surface, our system is able to capture dynamic surface deformation in a video mode. We also describe a novel calibration technique that maps the light field viewing directions from the object space to subpixels on the sensor plane. The proposed system is evaluated using a concave mirror with known curvature, and is compared to a parabolic mirror scanning system as well as a multi-illumination photometric stereo approach based on simulations and experiments.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Meng_Single-Shot_Specular_Surface_ICCV_2015_paper.pdf",
        "aff": "Ricoh Innovations, Corp.; Rice University + Ricoh Innovations, Corp.; Ricoh Innovations, Corp.; Ricoh Innovations, Corp.",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2150476,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5487352107746674885&as_sdt=5,47&sciodt=0,47&hl=en",
        "gs_version_total": 5,
        "aff_domain": "ric.ricoh.com;rice.edu;ric.ricoh.com;ric.ricoh.com",
        "email": "ric.ricoh.com;rice.edu;ric.ricoh.com;ric.ricoh.com",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Meng_Single-Shot_Specular_Surface_ICCV_2015_paper.html",
        "aff_unique_index": "0;1+0;0;0",
        "aff_unique_norm": "Ricoh Innovations;Rice University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ricoh-innovations.com;https://www.rice.edu",
        "aff_unique_abbr": "RIC;Rice",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Meng_2015_ICCV,\n    \n    author = {\n    Meng,\n    Lingfei and Lu,\n    Liyang and Bedard,\n    Noah and Berkner,\n    Kathrin\n},\n    title = {\n    Single-Shot Specular Surface Reconstruction With Gonio-Plenoptic Imaging\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "9ea3e32df2",
        "title": "Sparse Dynamic 3D Reconstruction From Unsynchronized Videos",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Enliang Zheng, Dinghuang Ji, Enrique Dunn, Jan-Michael Frahm",
        "author": "Enliang Zheng; Dinghuang Ji; Enrique Dunn; Jan-Michael Frahm",
        "abstract": "We target the sparse 3D reconstruction of dynamic objects observed by multiple unsynchronized video cameras with unknown temporal overlap. To this end, we develop a framework to recover the unknown structure without sequencing information across video sequences. Our proposed compressed sensing framework poses the estimation of 3D structure as the problem of dictionary learning. Moreover, we define our dictionary as the temporally varying 3D structure, while we define local sequencing information in terms of the sparse coefficients describing a locally linear 3D structural interpolation. Our formulation optimizes a biconvex cost function that leverages a compressed sensing formulation and enforces both structural dependency coherence across video streams, as well as motion smoothness across estimates from common video sources. Experimental results demonstrate the effectiveness of our approach in both synthetic data and captured imagery.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Zheng_Sparse_Dynamic_3D_ICCV_2015_paper.pdf",
        "aff": "The University of North Carolina at Chapel Hill; The University of North Carolina at Chapel Hill; The University of North Carolina at Chapel Hill; The University of North Carolina at Chapel Hill",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 936219,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12512503125296040254&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "cs.unc.edu;cs.unc.edu;cs.unc.edu;cs.unc.edu",
        "email": "cs.unc.edu;cs.unc.edu;cs.unc.edu;cs.unc.edu",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Zheng_Sparse_Dynamic_3D_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of North Carolina at Chapel Hill",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.unc.edu",
        "aff_unique_abbr": "UNC Chapel Hill",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Chapel Hill",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Zheng_2015_ICCV,\n    \n    author = {\n    Zheng,\n    Enliang and Ji,\n    Dinghuang and Dunn,\n    Enrique and Frahm,\n    Jan-Michael\n},\n    title = {\n    Sparse Dynamic 3D Reconstruction From Unsynchronized Videos\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "301bd39333",
        "title": "Spatial Semantic Regularisation for Large Scale Object Detection",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Damian Mrowca, Marcus Rohrbach, Judy Hoffman, Ronghang Hu, Kate Saenko, Trevor Darrell",
        "author": "Damian Mrowca; Marcus Rohrbach; Judy Hoffman; Ronghang Hu; Kate Saenko; Trevor Darrell",
        "abstract": "Large scale object detection with thousands of classes introduces the problem of many contradicting false positive detections, which have to be suppressed. Class-independent non-maximum suppression has traditionally been used for this step, but it does not scale well as the number of classes grows. Traditional non-maximum suppression does not consider label- and instance-level relationships nor does it allow an exploitation of the spatial layout of detection proposals. We propose a new multi-class spatial semantic regularisation method based on affinity propagation clustering, which simultaneously optimises across all categories and all proposed locations in the image, to improve both the localisation and categorisation of selected detection proposals.  Constraints are shared across the labels through the semantic WordNet hierarchy. Our approach proves to be especially useful in large scale settings with thousands of classes, where spatial and semantic interactions are very frequent and only weakly supervised detectors can be built due to a lack of bounding box annotations. Detection experiments are conducted on the ImageNet and COCO dataset, and in settings with thousands of detected categories. Our method provides a significant precision improvement by reducing false positives, while simultaneously improving the recall.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Mrowca_Spatial_Semantic_Regularisation_ICCV_2015_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3654405,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5308845824293019585&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Mrowca_Spatial_Semantic_Regularisation_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Mrowca_2015_ICCV,\n    \n    author = {\n    Mrowca,\n    Damian and Rohrbach,\n    Marcus and Hoffman,\n    Judy and Hu,\n    Ronghang and Saenko,\n    Kate and Darrell,\n    Trevor\n},\n    title = {\n    Spatial Semantic Regularisation for Large Scale Object Detection\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "e0c220435a",
        "title": "SpeDo: 6 DOF Ego-Motion Sensor Using Speckle Defocus Imaging",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Kensei Jo, Mohit Gupta, Shree K. Nayar",
        "author": "Kensei Jo; Mohit Gupta; Shree K. Nayar",
        "abstract": "Sensors that measure their motion with respect to the surrounding environment (ego-motion sensors) can be broadly classified into two categories. First is inertial sensors such as accelerometers. In order to estimate position and velocity, these sensors integrate the measured acceleration, which often results in accumulation of large errors over time. Second, camera-based approaches such as SLAM that can measure position directly, but their performance depends on the surrounding scene's properties. These approaches cannot function reliably if the scene has low frequency textures or small depth variations. We present a novel ego-motion sensor called SpeDo that addresses these fundamental limitations. SpeDo is based on using coherent light sources and cameras with large defocus. Coherent light, on interacting with a scene, creates a high frequency interferometric pattern in the captured images, called speckle. We develop a theoretical model for speckle flow (motion of speckle as a function of sensor motion), and show that it is quasi-invariant to surrounding scene's properties. As a result, SpeDo can measure ego-motion (not derivative of motion) simply by estimating optical flow at a few image locations. We have built a low-cost and compact hardware prototype of SpeDo and demonstrated high precision 6 DOF ego-motion estimation for complex trajectories in scenarios where the scene properties are challenging (e.g., repeating or no texture) as well as unknown.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Jo_SpeDo_6_DOF_ICCV_2015_paper.pdf",
        "aff": "Columbia University; Columbia University; Columbia University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2646069,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15336284078981572635&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "cs.columbia.edu;cs.columbia.edu;cs.columbia.edu",
        "email": "cs.columbia.edu;cs.columbia.edu;cs.columbia.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Jo_SpeDo_6_DOF_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Columbia University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.columbia.edu",
        "aff_unique_abbr": "Columbia",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Jo_2015_ICCV,\n    \n    author = {\n    Jo,\n    Kensei and Gupta,\n    Mohit and Nayar,\n    Shree K.\n},\n    title = {\n    SpeDo: 6 DOF Ego-Motion Sensor Using Speckle Defocus Imaging\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "a78b7d4b85",
        "title": "Square Localization for Efficient and Accurate Object Detection",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Cewu Lu, Yongyi Lu, Hao Chen, Chi-Keung Tang",
        "author": "Cewu Lu; Yongyi Lu; Hao Chen; Chi-Keung Tang",
        "abstract": "The key contribution of this paper is the compact square object localization, which relaxes the exhaustive sliding window from testing all windows of different combinations of aspect ratios. Square object localization is category scalable. By using a binary search strategy, the number of scales to test is further reduced empirically to only O(log(minfH;Wg)) rounds of sliding CNNs, where H and W are respectively the image height and width. In the training phase, square CNN models and object co-presence priors are learned. In the testing phase, sliding CNN models are applied which produces a set of response maps that can be effectively filtered by the learned co-presence prior to output the final bounding boxes for localizing an object. We performed extensive experimental evaluation on the VOC 2007 and 2012 datasets to demonstrate that while efficient,square localization can output precise bounding boxes to improve the final detection result.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Lu_Square_Localization_for_ICCV_2015_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1886167,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2990698492900310186&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Lu_Square_Localization_for_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Lu_2015_ICCV,\n    \n    author = {\n    Lu,\n    Cewu and Lu,\n    Yongyi and Chen,\n    Hao and Tang,\n    Chi-Keung\n},\n    title = {\n    Square Localization for Efficient and Accurate Object Detection\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "52b5022b76",
        "title": "StereoSnakes: Contour Based Consistent Object Extraction For Stereo Images",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Ran Ju, Tongwei Ren, Gangshan Wu",
        "author": "Ran Ju; Tongwei Ren; Gangshan Wu",
        "abstract": "Consistent object extraction plays an essential role for stereo image editing with the population of stereoscopic 3D media. Most previous methods perform segmentation on entire images for both views using dense stereo correspondence constraints. We find that for such kind of methods the computation is highly redundant since the two views are near-duplicate. Besides, the consistency may be violated due to the imperfectness of current stereo matching algorithms. In this paper, we propose a contour based method which searches for consistent object contours instead of regions. It integrates both stereo correspondence and object boundary constraints into an energy minimization framework. The proposed method has several advantages compared to previous works. First, the searching space is restricted in object boundaries thus the efficiency significantly improved. Second, the discriminative power of object contours results in a more consistent segmentation. Furthermore, the proposed method can effortlessly extend existing single-image segmentation methods to work in stereo scenarios. The experiment on the Adobe benchmark shows superior extraction accuracy and significant improvement of efficiency of our method to state-of-the-art. We also demonstrate in a few applications how our method can be used as a basic tool for stereo image editing.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Ju_StereoSnakes_Contour_Based_ICCV_2015_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10614492892384272435&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Ju_StereoSnakes_Contour_Based_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Ju_2015_ICCV,\n    \n    author = {\n    Ju,\n    Ran and Ren,\n    Tongwei and Wu,\n    Gangshan\n},\n    title = {\n    StereoSnakes: Contour Based Consistent Object Extraction For Stereo Images\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "ae9874be22",
        "title": "Storyline Representation of Egocentric Videos With an Applications to Story-Based Search",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Bo Xiong, Gunhee Kim, Leonid Sigal",
        "author": "Bo Xiong; Gunhee Kim; Leonid Sigal",
        "abstract": "Egocentric videos are a valuable source of information as a daily log of our lives. However, large fraction of egocentric video content is typically irrelevant and boring to re-watch. It is an agonizing task, for example, to manually search for the moment when your daughter first met Mickey Mouse from hours-long egocentric videos taken at Disneyland. Although many summarization methods have been successfully proposed to create concise representations of videos, in practice, the value of the subshots to users may change according to their immediate preference/mood; thus summaries with fixed criteria may not fully satisfy users' various search intents. To address this, we propose a storyline representation that expresses an egocentric video as a set of jointly inferred, through MRF inference, story elements comprising of actors, locations, supporting objects and events, depicted on a timeline. We construct such a storyline with very limited annotation data (a list of map locations and weak knowledge of what events may be possible at each location), by bootstrapping the process with data obtained through focused Web image and video searches. Our representation promotes story-based search with queries in the form of AND-OR graphs, which span any subset of story elements and their spatio-temporal composition. We show effectiveness of our approach on a set of unconstrained YouTube egocentric videos of visits to Disneyland.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Xiong_Storyline_Representation_of_ICCV_2015_paper.pdf",
        "aff": "The University of Texas at Austin; Seoul National University; Disney Research",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1653270,
        "gs_citation": 63,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7681022357414246174&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "cs.utexas.edu;snu.ac.kr;disneyresearch.com",
        "email": "cs.utexas.edu;snu.ac.kr;disneyresearch.com",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Xiong_Storyline_Representation_of_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University of Texas at Austin;Seoul National University;Disney Research",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.utexas.edu;https://www.snu.ac.kr;https://research.disney.com",
        "aff_unique_abbr": "UT Austin;SNU;Disney Research",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Austin;",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;South Korea",
        "bibtex": "@InProceedings{Xiong_2015_ICCV,\n    \n    author = {\n    Xiong,\n    Bo and Kim,\n    Gunhee and Sigal,\n    Leonid\n},\n    title = {\n    Storyline Representation of Egocentric Videos With an Applications to Story-Based Search\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "de8d4d9868",
        "title": "Structural Kernel Learning for Large Scale Multiclass Object Co-Detection",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Zeeshan Hayder, Xuming He, Mathieu Salzmann",
        "author": "Zeeshan Hayder; Xuming He; Mathieu Salzmann",
        "abstract": "Exploiting contextual relationships across images has recently proven key to improve object detection. The resulting object co-detection algorithms, however, fail to exploit the correlations between multiple classes and, for scalability reasons are limited to modeling object instance similarity with relatively low-dimensional hand-crafted features. Here, we address the problem of multiclass object co-detection for large scale datasets. To this end, we formulate co-detection as the joint multiclass labeling of object candidates obtained in a class-independent manner. To exploit the correlations between objects, we build a fully-connected CRF on the candidates, which explicitly incorporates both geometric layout relations across object classes and similarity relations across multiple images. We then introduce a structural boosting algorithm that lets us exploits rich, high-dimensional deep network features to learn object similarity within our fully-connected CRF. Our experiments on PASCAL VOC 2007 and 2012 evidences the benefits of our approach over object detection with RCNN, single-image CRF methods and state-of-the-art co-detection algorithms.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Hayder_Structural_Kernel_Learning_ICCV_2015_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 56213269,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15546655224296411533&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Hayder_Structural_Kernel_Learning_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Hayder_2015_ICCV,\n    \n    author = {\n    Hayder,\n    Zeeshan and He,\n    Xuming and Salzmann,\n    Mathieu\n},\n    title = {\n    Structural Kernel Learning for Large Scale Multiclass Object Co-Detection\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "e8e88ecb15",
        "title": "Structure From Motion Using Structure-Less Resection",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Enliang Zheng, Changchang Wu",
        "author": "Enliang Zheng; Changchang Wu",
        "abstract": "This paper proposes a new incremental structure from motion (SfM) algorithm based on a novel structure-less camera resection technique. Traditional methods rely on 2D-3D correspondences to compute the pose of candidate cameras using PnP.  In this work, we take the collection of already reconstructed cameras as a generalized camera, and  determine the absolute pose of a candidate pinhole camera from pure 2D correspondences, which we call it  semi-generalized camera pose problem. We present the minimal solvers of the new problem for both calibrated and partially calibrated (unknown focal length) pinhole cameras. By integrating these new algorithms in an incremental SfM system, we go beyond the state-of-art methods with the capability of reconstructing cameras without 2D-3D correspondences.  Large-scale real image experiments show that our new SfM system significantly improves the completeness of 3D reconstruction over the standard approach.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Zheng_Structure_From_Motion_ICCV_2015_paper.pdf",
        "aff": "The University of North Carolina at Chapel Hill; Google",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2153716,
        "gs_citation": 101,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6877742287206850444&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "cs.unc.edu;google.com",
        "email": "cs.unc.edu;google.com",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Zheng_Structure_From_Motion_ICCV_2015_paper.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of North Carolina at Chapel Hill;Google",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.unc.edu;https://www.google.com",
        "aff_unique_abbr": "UNC Chapel Hill;Google",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Chapel Hill;Mountain View",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Zheng_2015_ICCV,\n    \n    author = {\n    Zheng,\n    Enliang and Wu,\n    Changchang\n},\n    title = {\n    Structure From Motion Using Structure-Less Resection\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "3cd388e835",
        "title": "Structured Feature Selection",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Tian Gao, Ziheng Wang, Qiang Ji",
        "author": "Tian Gao; Ziheng Wang; Qiang Ji",
        "abstract": "Feature dimensionality reduction has been widely used in various computer vision tasks. We explore feature selection as the dimensionality reduction technique and propose to use a structured approach, based on the Markov Blanket (MB), to select features. We first introduce a new MB discovery algorithm, Simultaneous Markov Blanket (STMB) discovery,  that improves the efficiency of state-of-the-art algorithms. Then we theoretically justify three advantages of structured feature selection over traditional feature selection methods. Specifically, we show that the Markov Blanket is the minimum feature set that retains the maximal mutual information and also gives the lowest Bayes classification error. Then we apply structured feature selection to two applications: 1) We introduce a new method that enables STMB to scale up and show the competitive performance of our algorithms on large-scale image classification tasks. 2) We propose a method for structured feature selection to handle hierarchical features and show the proposed method can lead to big performance gain  in facial expression and action unit (AU) recognition tasks.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Gao_Structured_Feature_Selection_ICCV_2015_paper.pdf",
        "aff": "Department of ECSE, Rensselaer Polytechnic Institute, USA; Department of ECSE, Rensselaer Polytechnic Institute, USA; Department of ECSE, Rensselaer Polytechnic Institute, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 630641,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6547726116049871509&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "rpi.edu;rpi.edu;rpi.edu",
        "email": "rpi.edu;rpi.edu;rpi.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Gao_Structured_Feature_Selection_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Rensselaer Polytechnic Institute",
        "aff_unique_dep": "Department of ECSE",
        "aff_unique_url": "https://www.rpi.edu",
        "aff_unique_abbr": "RPI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Gao_2015_ICCV,\n    \n    author = {\n    Gao,\n    Tian and Wang,\n    Ziheng and Ji,\n    Qiang\n},\n    title = {\n    Structured Feature Selection\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "a64d4388cb",
        "title": "Structured Indoor Modeling",
        "session": "3d vision",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "Satoshi Ikehata, Hang Yang, Yasutaka Furukawa",
        "author": "Satoshi Ikehata; Hang Yang; Yasutaka Furukawa",
        "abstract": "This paper presents a novel 3D modeling framework that reconstructs an indoor scene as a structured model from panorama RGBD images. A scene geometry is represented as a graph, where nodes correspond to structural elements such as rooms, walls, and objects. The approach devises a structure grammar that defines how a scene graph can be manipulated. The grammar then drives a principled new reconstruction algorithm, where the grammar rules are sequentially applied to recover a structured model.  The paper also proposes a new room segmentation algorithm and an offset-map reconstruction algorithm that are used in the framework and can enforce architectural shape priors far beyond existing state-of-the-art. The structured scene representation enables a variety of novel applications, ranging from indoor scene visualization, automated floorplan generation, Inverse-CAD, and more.  We have tested our framework and algorithms on six synthetic and five real datasets with qualitative and quantitative evaluations.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Ikehata_Structured_Indoor_Modeling_ICCV_2015_paper.pdf",
        "aff": "Washington University in St. Louis; Washington University in St. Louis;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4276482,
        "gs_citation": 182,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10797876504155140637&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Ikehata_Structured_Indoor_Modeling_ICCV_2015_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Washington University in St. Louis",
        "aff_unique_dep": "",
        "aff_unique_url": "https://wustl.edu",
        "aff_unique_abbr": "WashU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "St. Louis",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Ikehata_2015_ICCV,\n    \n    author = {\n    Ikehata,\n    Satoshi and Yang,\n    Hang and Furukawa,\n    Yasutaka\n},\n    title = {\n    Structured Indoor Modeling\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "564c85a989",
        "title": "Synthesizing Illumination Mosaics From Internet Photo-Collections",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Dinghuang Ji, Enrique Dunn, Jan-Michael Frahm",
        "author": "Dinghuang Ji; Enrique Dunn; Jan-Michael Frahm",
        "abstract": "We propose a framework for the automatic creation of time-lapse mosaics of a given scene. We achieve this by leveraging the illumination variations captured in Internet photo-collections. In order to depict and characterize the illumination spectrum of a scene, our method relies on building discrete representations of the image appearance space through connectivity graphs defined over a pairwise image distance function. The smooth appearance transitions are found as the shortest path in the similarity graph among images, and robust image alignment is achieved by leveraging scene semantics, multi-view geometry, and image warping techniques. The attained results present an insightful and compact visualization of the scene illuminations captured in crowd-sourced imagery.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Ji_Synthesizing_Illumination_Mosaics_ICCV_2015_paper.pdf",
        "aff": "Department of Computer Science, The University of North Carolina at Chapel Hill; Department of Computer Science, The University of North Carolina at Chapel Hill; Department of Computer Science, The University of North Carolina at Chapel Hill",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2107390,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18034432411563012130&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "cs.unc.edu;cs.unc.edu;cs.unc.edu",
        "email": "cs.unc.edu;cs.unc.edu;cs.unc.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Ji_Synthesizing_Illumination_Mosaics_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "The University of North Carolina at Chapel Hill",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.unc.edu",
        "aff_unique_abbr": "UNC Chapel Hill",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Chapel Hill",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Ji_2015_ICCV,\n    \n    author = {\n    Ji,\n    Dinghuang and Dunn,\n    Enrique and Frahm,\n    Jan-Michael\n},\n    title = {\n    Synthesizing Illumination Mosaics From Internet Photo-Collections\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "4f62729e56",
        "title": "TRIC-track: Tracking by Regression With Incrementally Learned Cascades",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Xiaomeng Wang, Michel Valstar, Brais Martinez, Muhammad Haris Khan, Tony Pridmore",
        "author": "Xiaomeng Wang; Michel Valstar; Brais Martinez; Muhammad Haris Khan; Tony Pridmore",
        "abstract": "This paper proposes a novel approach to part-based tracking by replacing local matching of an appearance model by direct prediction of the displacement between local image patches and part locations. We propose to use cascaded regression with incremental learning to track generic objects without any prior knowledge of an object's structure or appearance. We exploit the spatial constraints between parts by implicitly learning the shape and deformation parameters of the object in an online fashion. We integrate a multiple temporal scale motion model to initialise our cascaded regression search close to the target and to allow it to cope with occlusions. Experimental results show that our tracker ranks first on the CVPR 2013 Benchmark.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Wang_TRIC-track_Tracking_by_ICCV_2015_paper.pdf",
        "aff": "Computer Vision Laboratory, University of Nottingham, Nottingham, UK; Computer Vision Laboratory, University of Nottingham, Nottingham, UK; Computer Vision Laboratory, University of Nottingham, Nottingham, UK; Computer Vision Laboratory, University of Nottingham, Nottingham, UK; Computer Vision Laboratory, University of Nottingham, Nottingham, UK",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 742122,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=676351822307762864&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff_domain": "nottingham.ac.uk;nottingham.ac.uk;nottingham.ac.uk;nottingham.ac.uk;nottingham.ac.uk",
        "email": "nottingham.ac.uk;nottingham.ac.uk;nottingham.ac.uk;nottingham.ac.uk;nottingham.ac.uk",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Wang_TRIC-track_Tracking_by_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Nottingham",
        "aff_unique_dep": "Computer Vision Laboratory",
        "aff_unique_url": "https://www.nottingham.ac.uk",
        "aff_unique_abbr": "UoN",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Nottingham",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United Kingdom",
        "bibtex": "@InProceedings{Wang_2015_ICCV,\n    \n    author = {\n    Wang,\n    Xiaomeng and Valstar,\n    Michel and Martinez,\n    Brais and Khan,\n    Muhammad Haris and Pridmore,\n    Tony\n},\n    title = {\n    TRIC-track: Tracking by Regression With Incrementally Learned Cascades\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "d5337243f1",
        "title": "Task-Driven Feature Pooling for Image Classification",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Guo-Sen Xie, Xu-Yao Zhang, Xiangbo Shu, Shuicheng Yan, Cheng-Lin Liu",
        "author": "Guo-Sen Xie; Xu-Yao Zhang; Xiangbo Shu; Shuicheng Yan; Cheng-Lin Liu",
        "abstract": "Feature pooling is an important strategy to achieve high performance in image classification. However, most pooling methods are unsupervised and heuristic. In this paper, we propose a novel  task-driven pooling (TDP) model to directly learn the pooled representation from data in a discriminative manner. Different from the traditional methods (e.g., average and max pooling), TDP is an implicit pooling method which elegantly integrates the learning of representations into the given classification task. The optimization of TDP can equalize the similarities between the descriptors and the learned representation, and maximize the classification accuracy. TDP can be combined with the traditional BoW models (coding vectors) or the recent state-of-the-art CNN models (feature maps) to achieve a much better pooled representation. Furthermore, a self-training mechanism is used to generate the TDP representation for a new test image. A multi-task extension of TDP is also proposed to further improve the performance. Experiments on three databases (Flower-17, Indoor-67 and Caltech-101) well validate the effectiveness of our models.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Xie_Task-Driven_Feature_Pooling_ICCV_2015_paper.pdf",
        "aff": "NLPR, Institute of Automation, Chinese Academy of Sciences, China; NLPR, Institute of Automation, Chinese Academy of Sciences, China; Nanjing University of Science and Technology, China; National University of Singapore, Singapore; NLPR, Institute of Automation, Chinese Academy of Sciences, China + CAS Center for Excellence in Brain Science and Intelligence Technology, CAS, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1293870,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9820009934639743013&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "nlpr.ia.ac.cn;nlpr.ia.ac.cn; ;nus.edu.sg;nlpr.ia.ac.cn",
        "email": "nlpr.ia.ac.cn;nlpr.ia.ac.cn; ;nus.edu.sg;nlpr.ia.ac.cn",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Xie_Task-Driven_Feature_Pooling_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;1;2;0+0",
        "aff_unique_norm": "Chinese Academy of Sciences;Nanjing University of Science and Technology;National University of Singapore",
        "aff_unique_dep": "Institute of Automation;;",
        "aff_unique_url": "http://www.ia.cas.cn;http://www.nust.edu.cn/;https://www.nus.edu.sg",
        "aff_unique_abbr": "CAS;NUST;NUS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1;0+0",
        "aff_country_unique": "China;Singapore",
        "bibtex": "@InProceedings{Xie_2015_ICCV,\n    \n    author = {\n    Xie,\n    Guo-Sen and Zhang,\n    Xu-Yao and Shu,\n    Xiangbo and Yan,\n    Shuicheng and Liu,\n    Cheng-Lin\n},\n    title = {\n    Task-Driven Feature Pooling for Image Classification\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "0f8dcdd70f",
        "title": "Temporal Perception and Prediction in Ego-Centric Video",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Yipin Zhou, Tamara L. Berg",
        "author": "Yipin Zhou; Tamara L. Berg",
        "abstract": "Given a video of an activity, can we predict what will happen next? In this paper we explore two simple tasks related to temporal prediction in egocentric videos of everyday activities. We provide both human experiments to understand how well people can perform on these tasks and computational models for prediction. Experiments indicate that humans and computers can do well on temporal prediction and that personalization to a particular individual or environment provides significantly increased performance. Developing methods for temporal prediction could have far reaching benefits for robots or intelligent agents to anticipate what a person will do, before they do it.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Zhou_Temporal_Perception_and_ICCV_2015_paper.pdf",
        "aff": "University of North Carolina at Chapel Hill; University of North Carolina at Chapel Hill",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1130204,
        "gs_citation": 78,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7085431075086935571&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "cs.unc.edu;cs.unc.edu",
        "email": "cs.unc.edu;cs.unc.edu",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Zhou_Temporal_Perception_and_ICCV_2015_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of North Carolina",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.unc.edu",
        "aff_unique_abbr": "UNC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Chapel Hill",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Zhou_2015_ICCV,\n    \n    author = {\n    Zhou,\n    Yipin and Berg,\n    Tamara L.\n},\n    title = {\n    Temporal Perception and Prediction in Ego-Centric Video\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "c6ae70348e",
        "title": "Temporal Subspace Clustering for Human Motion Segmentation",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Sheng Li, Kang Li, Yun Fu",
        "author": "Sheng Li; Kang Li; Yun Fu",
        "abstract": "Subspace clustering is an effective technique for segmenting data drawn from multiple subspaces. However, for time series data (e.g., human motion), exploiting temporal information is still a challenging problem. We propose a novel temporal subspace clustering (TSC) approach in this paper. We improve the subspace clustering technique from two aspects. First, a temporal Laplacian regularization is designed, which encodes the sequential relationships in time series data. Second, to obtain expressive codings, we learn a non-negative dictionary from data. An efficient optimization algorithm is presented to jointly learn the representation codings and dictionary. After constructing an affinity graph using the codings, multiple temporal segments can be grouped via spectral clustering. Experimental results on three action and gesture datasets demonstrate the effectiveness of our approach. In particular, TSC significantly improves the clustering accuracy, compared to the state-of-the-art subspace clustering methods.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Li_Temporal_Subspace_Clustering_ICCV_2015_paper.pdf",
        "aff": "Department of Electrical and Computer Engineering, College of Engineering; Department of Electrical and Computer Engineering, College of Engineering; Department of Electrical and Computer Engineering, College of Engineering + College of Computer and Information Science",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 880179,
        "gs_citation": 129,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13400792564721951552&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "ece.neu.edu;husky.neu.edu;ece.neu.edu",
        "email": "ece.neu.edu;husky.neu.edu;ece.neu.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Li_Temporal_Subspace_Clustering_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0+1",
        "aff_unique_norm": "College of Engineering;Northeastern University",
        "aff_unique_dep": "Department of Electrical and Computer Engineering;College of Computer and Information Science",
        "aff_unique_url": ";https://ccis.northeastern.edu/",
        "aff_unique_abbr": ";CCIS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United States",
        "bibtex": "@InProceedings{Li_2015_ICCV,\n    \n    author = {\n    Li,\n    Sheng and Li,\n    Kang and Fu,\n    Yun\n},\n    title = {\n    Temporal Subspace Clustering for Human Motion Segmentation\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "c66514de6f",
        "title": "Text Flow: A Unified Text Detection System in Natural Scene Images",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Shangxuan Tian, Yifeng Pan, Chang Huang, Shijian Lu, Kai Yu, Chew Lim Tan",
        "author": "Shangxuan Tian; Yifeng Pan; Chang Huang; Shijian Lu; Kai Yu; Chew Lim Tan",
        "abstract": "The prevalent scene text detection approach follows four sequential steps comprising character candidate detection, false character candidate removal, text line extraction, and  text line verification. However, errors occur and accumulate throughout each of these sequential steps which often lead to low detection performance. To address these issues, we propose a unified scene text detection system, namely Text Flow, by utilizing the minimum cost (min-cost) flow network model. With character candidates detected by cascade boosting, the min-cost flow network model integrates the last three sequential steps into a single process which solves the error accumulation problem at both character level and text line level effectively. The proposed technique has been tested on three public datasets, i.e, ICDAR2011 dataset, ICDAR2013 dataset and a multilingual dataset and it outperforms the state-of-the-art methods on all three datasets with much higher recall and F-score. The good performance on the multilingual dataset shows that the proposed technique can be used for the detection of texts in different languages.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Tian_Text_Flow_A_ICCV_2015_paper.pdf",
        "aff": "School of Computing, National University of Singapore, Singapore; Institute of Deep Learning, Baidu Research, China; Institute of Deep Learning, Baidu Research, China; Visual Computing Department, Institute for Infocomm Research, Singapore; Institute of Deep Learning, Baidu Research, China; School of Computing, National University of Singapore, Singapore",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2458654,
        "gs_citation": 295,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10949109774637130903&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "u.nus.edu;baidu.com;baidu.com;i2r.a-star.edu.sg;baidu.com;comp.nus.edu.sg",
        "email": "u.nus.edu;baidu.com;baidu.com;i2r.a-star.edu.sg;baidu.com;comp.nus.edu.sg",
        "author_num": 6,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Tian_Text_Flow_A_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;1;2;1;0",
        "aff_unique_norm": "National University of Singapore;Baidu Research;Institute for Infocomm Research",
        "aff_unique_dep": "School of Computing;Institute of Deep Learning;Visual Computing Department",
        "aff_unique_url": "https://www.nus.edu.sg;https://baidu.com;https://www.i2r.a-star.edu.sg",
        "aff_unique_abbr": "NUS;Baidu;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;0;1;0",
        "aff_country_unique": "Singapore;China",
        "bibtex": "@InProceedings{Tian_2015_ICCV,\n    \n    author = {\n    Tian,\n    Shangxuan and Pan,\n    Yifeng and Huang,\n    Chang and Lu,\n    Shijian and Yu,\n    Kai and Tan,\n    Chew Lim\n},\n    title = {\n    Text Flow: A Unified Text Detection System in Natural Scene Images\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "199aeb12e1",
        "title": "The HCI Stereo Metrics: Geometry-Aware Performance Analysis of Stereo Algorithms",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Katrin Honauer, Lena Maier-Hein, Daniel Kondermann",
        "author": "Katrin Honauer; Lena Maier-Hein; Daniel Kondermann",
        "abstract": "Performance characterization of stereo methods is mandatory to decide which algorithm is useful for which application. Prevalent benchmarks mainly use the root mean squared error (RMS) with respect to ground truth disparity maps to quantify algorithm performance.  We show that the RMS is of limited expressiveness for algorithm selection and introduce the HCI Stereo Metrics.  These metrics assess stereo results by harnessing three semantic cues: depth discontinuities, planar surfaces, and fine geometric structures. For each cue, we extract the relevant set of pixels from existing ground truth. We then apply our evaluation functions to quantify characteristics such as edge fattening and surface smoothness.  We demonstrate that our approach supports practitioners in selecting the most suitable algorithm for their application. Using the new Middlebury dataset, we show that rankings based on our metrics reveal specific algorithm strengths and weaknesses which are not quantified by existing metrics. We finally show how stacked bar charts and radar charts visually support multidimensional performance evaluation.  An interactive stereo benchmark based on the proposed metrics and visualizations is available at:  http://hci.iwr.uni-heidelberg.de/stereometrics",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Honauer_The_HCI_Stereo_ICCV_2015_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7094525986175417838&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Honauer_The_HCI_Stereo_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Honauer_2015_ICCV,\n    \n    author = {\n    Honauer,\n    Katrin and Maier-Hein,\n    Lena and Kondermann,\n    Daniel\n},\n    title = {\n    The HCI Stereo Metrics: Geometry-Aware Performance Analysis of Stereo Algorithms\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "f39c49aef4",
        "title": "The Joint Image Handbook",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Matthew Trager, Martial Hebert, Jean Ponce",
        "author": "Matthew Trager; Martial Hebert; Jean Ponce",
        "abstract": "Given multiple perspective photographs, point correspondences form the \"joint image\", effectively a replica of three dimensional space distributed across its two-dimensional projections. This set can be characterized by multilinear equations over image coordinates, such as epipolar and trifocal constraints. We revisit in this paper the geometric and algebraic properties of the joint image, and address fundamental questions such as how many and which multilinearities are necessary and/or sufficient to determine camera geometry and/or image correspondences. The new theoretical results in this paper answer these questions in a very general setting and, in turn, are intended to serve as a \"handbook\" reference about multilinearities for practitioners.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Trager_The_Joint_Image_ICCV_2015_paper.pdf",
        "aff": "Inria; Carnegie Mellon University; ´Ecole Normale Supérieure / PSL Research University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 638179,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2561296221744046220&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "inria.fr;cmu.edu;ens.fr",
        "email": "inria.fr;cmu.edu;ens.fr",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Trager_The_Joint_Image_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Inria;Carnegie Mellon University;Ecole Normale Supérieure",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.inria.fr;https://www.cmu.edu;https://www.ens.fr",
        "aff_unique_abbr": "Inria;CMU;ENS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "France;United States",
        "bibtex": "@InProceedings{Trager_2015_ICCV,\n    \n    author = {\n    Trager,\n    Matthew and Hebert,\n    Martial and Ponce,\n    Jean\n},\n    title = {\n    The Joint Image Handbook\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "1b30237320",
        "title": "The Likelihood-Ratio Test and Efficient Robust Estimation",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Andrea Cohen, Christopher Zach",
        "author": "Andrea Cohen; Christopher Zach",
        "abstract": "Robust estimation of model parameters in the presence of outliers is a key problem in computer vision. RANSAC inspired techniques are widely used in this context, although their application might be limited due to the need of a priori knowledge on the inlier noise level. We propose a new approach for jointly optimizing over model parameters and the inlier noise level based on the likelihood ratio test. This allows control over the type I error incurred. We also propose an early bailout strategy for efficiency. Tests on both synthetic and real data show that our method outperforms the state-of-the-art in a fraction of the time.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Cohen_The_Likelihood-Ratio_Test_ICCV_2015_paper.pdf",
        "aff": "Department of Computer Science, ETH Zurich, Switzerland; Toshiba Research, Cambridge, UK",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1401946,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4313041822920970868&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "inf.ethz.ch;crl.toshiba.uk",
        "email": "inf.ethz.ch;crl.toshiba.uk",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Cohen_The_Likelihood-Ratio_Test_ICCV_2015_paper.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "ETH Zurich;Toshiba Research Europe Limited",
        "aff_unique_dep": "Department of Computer Science;",
        "aff_unique_url": "https://www.ethz.ch;https://www.toshiba-research.eu",
        "aff_unique_abbr": "ETHZ;TREL",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Switzerland;United Kingdom",
        "bibtex": "@InProceedings{Cohen_2015_ICCV,\n    \n    author = {\n    Cohen,\n    Andrea and Zach,\n    Christopher\n},\n    title = {\n    The Likelihood-Ratio Test and Efficient Robust Estimation\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "0e07e8f67a",
        "title": "The Middle Child Problem: Revisiting Parametric Min-Cut and Seeds for Object Proposals",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Ahmad Humayun, Fuxin Li, James M. Rehg",
        "author": "Ahmad Humayun; Fuxin Li; James M. Rehg",
        "abstract": "Object proposals have recently fueled the progress in detection performance. These proposals aim to provide category-agnostic localizations for all objects in an image. One way to generate proposals is to perform parametric min-cuts over seed locations. This paper demonstrates that standard parametric-cut models are ineffective in obtaining medium-sized objects, which we refer to as the middle child problem. We propose a new energy minimization framework incorporating geodesic distances between segments which solves this problem. In addition, we introduce a new superpixel merging algorithm which can generate a small set of seeds that reliably cover a large number of objects of all sizes. We call our method POISE--- \"Proposals for Objects from Improved Seeds and Energies.\" POISE enables parametric min-cuts to reach their full potential. On PASCAL VOC it generates  2,640 segments with an average overlap of 0.81, whereas the closest competing methods require more than 4,200 proposals to reach the same accuracy. We show detailed quantitative comparisons against 5 state-of-the-art methods on PASCAL VOC and Microsoft COCO segmentation challenges.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Humayun_The_Middle_Child_ICCV_2015_paper.pdf",
        "aff": "Georgia Institute of Technology; Oregon State University + Georgia Institute of Technology; Georgia Institute of Technology",
        "project": "http://cpl.cc.gatech.edu/projects/POISE/",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1154703,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10682772155813189653&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Humayun_The_Middle_Child_ICCV_2015_paper.html",
        "aff_unique_index": "0;1+0;0",
        "aff_unique_norm": "Georgia Institute of Technology;Oregon State University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.gatech.edu;https://oregonstate.edu",
        "aff_unique_abbr": "Georgia Tech;OSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Humayun_2015_ICCV,\n    \n    author = {\n    Humayun,\n    Ahmad and Li,\n    Fuxin and Rehg,\n    James M.\n},\n    title = {\n    The Middle Child Problem: Revisiting Parametric Min-Cut and Seeds for Object Proposals\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "d6260bb4a7",
        "title": "The One Triangle Three Parallelograms Sampling Strategy and Its Application in Shape Regression",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Mikael Nilsson",
        "author": "Mikael Nilsson",
        "abstract": "The purpose of this paper is threefold. Firstly, the paper introduces the One Triangle Three Parallelograms (OTTP) sampling strategy, which can be viewed as a way to index pixels from a given shape and image. Secondly, a framework for cascaded shape regression, including the OTTP sampling, is presented. In short, this framework involves binary pixel tests for appearance features combined with shape features followed by a large linear system for each regression stage in the cascade. The proposed solution is found to produce state-of-the-art results on the task of facial landmark estimation. Thirdly, the dependence of accuracy of the landmark predictions and the placement of the mean shape within the detection box is discussed and a method to visualize it is presented.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Nilsson_The_One_Triangle_ICCV_2015_paper.pdf",
        "aff": "Centre of Mathematical Sciences, Lund University, Lund, Sweden",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4892622,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:ejg48FJtgEQJ:scholar.google.com/&scioq=The+One+Triangle+Three+Parallelograms+Sampling+Strategy+and+Its+Application+in+Shape+Regression&hl=en&as_sdt=0,5",
        "gs_version_total": 7,
        "aff_domain": "math.lth.se",
        "email": "math.lth.se",
        "author_num": 1,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Nilsson_The_One_Triangle_ICCV_2015_paper.html",
        "aff_unique_index": "0",
        "aff_unique_norm": "Lund University",
        "aff_unique_dep": "Centre of Mathematical Sciences",
        "aff_unique_url": "https://www.lunduniversity.lu.se",
        "aff_unique_abbr": "LU",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Lund",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Sweden",
        "bibtex": "@InProceedings{Nilsson_2015_ICCV,\n    \n    author = {\n    Nilsson,\n    Mikael\n},\n    title = {\n    The One Triangle Three Parallelograms Sampling Strategy and Its Application in Shape Regression\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "81ccce51f7",
        "title": "Thin Structure Estimation With Curvature Regularization",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Dmitrii Marin, Yuchen Zhong, Maria Drangova, Yuri Boykov",
        "author": "Dmitrii Marin; Yuchen Zhong; Maria Drangova; Yuri Boykov",
        "abstract": "Many applications in vision require estimation of thin structures such as boundary edges, surfaces, roads, blood vessels, neurons, etc. Unlike most previous approaches, we simultaneously detect and delineate thin structures with sub-pixel localization and real-valued orientation estimation. This is an ill-posed problem that requires regularization. We propose an objective function combining detection likelihoods with a prior minimizing curvature of the center-lines or surfaces. Unlike simple block-coordinate descent, we develop a novel algorithm that is able to perform joint optimization of location and detection variables more effectively. Our lower bound optimization algorithm applies to quadratic or absolute curvature. The proposed early vision framework is sufficiently general and it can be used in many higher-level applications. We illustrate the advantage of our approach on a range of 2D and 3D examples.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Marin_Thin_Structure_Estimation_ICCV_2015_paper.pdf",
        "aff": "University of Western Ontario, Canada; University of Western Ontario, Canada; University of Western Ontario, Canada + Robarts Research Institute, Canada; University of Western Ontario, Canada",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1179642,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2818616836636815086&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff_domain": "gmail.com;gmail.com;robarts.ca;csd.uwo.ca",
        "email": "gmail.com;gmail.com;robarts.ca;csd.uwo.ca",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Marin_Thin_Structure_Estimation_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0+1;0",
        "aff_unique_norm": "University of Western Ontario;Robarts Research Institute",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uwo.ca;https://www.robarts.ca",
        "aff_unique_abbr": "UWO;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+0;0",
        "aff_country_unique": "Canada",
        "bibtex": "@InProceedings{Marin_2015_ICCV,\n    \n    author = {\n    Marin,\n    Dmitrii and Zhong,\n    Yuchen and Drangova,\n    Maria and Boykov,\n    Yuri\n},\n    title = {\n    Thin Structure Estimation With Curvature Regularization\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "b6ccce36f1",
        "title": "Top Rank Supervised Binary Coding for Visual Search",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Dongjin Song, Wei Liu, Rongrong Ji, David A. Meyer, John R. Smith",
        "author": "Dongjin Song; Wei Liu; Rongrong Ji; David A. Meyer; John R. Smith",
        "abstract": "In recent years, binary coding techniques are becoming increasingly popular because of their high efficiency in handling large-scale computer vision applications. It has been demonstrated that supervised binary coding techniques that leverage supervised information can significantly enhance the coding quality, and hence greatly benefit visual search tasks. Typically, a modern binary coding method seeks to learn a group of coding functions which compress data samples into binary codes. However, few methods pursued the coding functions such that the precision at the top of a ranking list according to Hamming distances of the generated binary codes is optimized. In this paper, we propose a novel supervised binary coding approach, namely Top Rank Supervised Binary Coding (Top-RSBC), which explicitly focuses on optimizing the precision of top positions in a Hamming-distance ranking list towards preserving the supervision information. The core idea is to train the disciplined coding functions, by which the mistakes at the top of a Hamming-distance ranking list are penalized more than those at the bottom. To solve such coding functions, we relax the original discrete optimization objective with a continuous surrogate, and derive a stochastic gradient descent to optimize the surrogate objective. To further reduce the training time cost, we also design an online learning algorithm to optimize the surrogate objective more efficiently. Empirical studies based upon three benchmark image datasets demonstrate that the proposed binary coding approach achieves superior image search accuracy over the state-of-the-arts.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Song_Top_Rank_Supervised_ICCV_2015_paper.pdf",
        "aff": "Department of ECE, UC San Diego; School of Electronic Engineering, Xidian University + IBM Research; School of Information Science and Engineering, Xiamen University; Department of Mathematics, UC San Diego; IBM T. J. Watson Research Center",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 733489,
        "gs_citation": 83,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13206149515566411882&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "ucsd.edu;ee.columbia.edu;xmu.edu.cn;math.ucsd.edu;us.ibm.com",
        "email": "ucsd.edu;ee.columbia.edu;xmu.edu.cn;math.ucsd.edu;us.ibm.com",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Song_Top_Rank_Supervised_ICCV_2015_paper.html",
        "aff_unique_index": "0;1+2;3;0;2",
        "aff_unique_norm": "University of California, San Diego;Xidian University;IBM;Xiamen University",
        "aff_unique_dep": "Department of Electrical and Computer Engineering;School of Electronic Engineering;IBM Research;School of Information Science and Engineering",
        "aff_unique_url": "https://www.ucsd.edu;http://www.xidian.edu.cn/;https://www.ibm.com/research;https://www.xmu.edu.cn",
        "aff_unique_abbr": "UCSD;Xidian;IBM;XMU",
        "aff_campus_unique_index": "0;;0;2",
        "aff_campus_unique": "San Diego;;T. J. Watson",
        "aff_country_unique_index": "0;1+0;1;0;0",
        "aff_country_unique": "United States;China",
        "bibtex": "@InProceedings{Song_2015_ICCV,\n    \n    author = {\n    Song,\n    Dongjin and Liu,\n    Wei and Ji,\n    Rongrong and Meyer,\n    David A. and Smith,\n    John R.\n},\n    title = {\n    Top Rank Supervised Binary Coding for Visual Search\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "3c3cca3e2e",
        "title": "Towards Computational Baby Learning: A Weakly-Supervised Approach for Object Detection",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Xiaodan Liang, Si Liu, Yunchao Wei, Luoqi Liu, Liang Lin, Shuicheng Yan",
        "author": "Xiaodan Liang; Si Liu; Yunchao Wei; Luoqi Liu; Liang Lin; Shuicheng Yan",
        "abstract": "Intuitive observations show that a baby may inherently possess the capability of recognizing a new visual concept (e.g., chair, dog) by learning from only very few positive instances taught by parent(s) or others, and this recognition capability can be gradually further improved by exploring and/or interacting with the real instances in the physical world. Inspired by these observations, we propose a computational model for weakly-supervised object detection, based on prior knowledge modelling, exemplar learning and learning with video contexts. The prior knowledge is modeled with a pre-trained Convolutional Neural Network (CNN). When very few instances of a new concept are given, an initial concept detector is built by exemplar learning over the deep features the pre-trained CNN. The well-designed tracking solution is then used to discover more diverse instances from the massive online weakly labeled videos. Once a positive instance is detected/identified with high score in each video, more instances possibly from different view-angles and/or different distances are tracked and accumulated.  Then the concept detector can be fine-tuned based on these new instances. This process can be repeated again and again till we obtain a very mature concept detector. Extensive experiments on Pascal VOC-07/10/12 object detection datasets well demonstrate the effectiveness of our framework. It can beat the state-of-the-art full-training based performances by learning from very few samples for each object category, along with about 20,000 weakly labeled videos.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Liang_Towards_Computational_Baby_ICCV_2015_paper.pdf",
        "aff": "National University of Singapore; Sun Yat-sen University; Beijing Jiaotong university; National University of Singapore; Sun Yat-sen University; National University of Singapore + State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1260927,
        "gs_citation": 114,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14587544309241832401&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Liang_Towards_Computational_Baby_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;2;0;1;0+3",
        "aff_unique_norm": "National University of Singapore;Sun Yat-sen University;Beijing Jiaotong University;Chinese Academy of Sciences",
        "aff_unique_dep": ";;;State Key Laboratory of Information Security, Institute of Information Engineering",
        "aff_unique_url": "https://www.nus.edu.sg;http://www.sysu.edu.cn/;http://www.bjtu.edu.cn;http://www.cas.cn",
        "aff_unique_abbr": "NUS;SYSU;BJTU;CAS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;0;1;0+1",
        "aff_country_unique": "Singapore;China",
        "bibtex": "@InProceedings{Liang_2015_ICCV,\n    \n    author = {\n    Liang,\n    Xiaodan and Liu,\n    Si and Wei,\n    Yunchao and Liu,\n    Luoqi and Lin,\n    Liang and Yan,\n    Shuicheng\n},\n    title = {\n    Towards Computational Baby Learning: A Weakly-Supervised Approach for Object Detection\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "c6acd3c564",
        "title": "Towards Pointless Structure From Motion: 3D Reconstruction and Camera Parameters From General 3D Curves",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Irina Nurutdinova, Andrew Fitzgibbon",
        "author": "Irina Nurutdinova; Andrew Fitzgibbon",
        "abstract": "Modern structure from motion (SfM) remains dependent on point features to recover camera positions, meaning that reconstruction is severely hampered in low-texture environments, for example scanning a plain coffee cup on an uncluttered table.  We show how 3D curves can be used to refine camera position estimation in challenging low-texture scenes.  In contrast to previous work, we allow the curves to be partially observed in all images, meaning that for the first time, curve-based SfM can be demonstrated in realistic scenes.  The algorithm is based on bundle adjustment, so needs an initial estimate, but even a poor estimate from a few point correspondences can be substantially improved by including curves, suggesting that this method would benefit many existing systems.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Nurutdinova_Towards_Pointless_Structure_ICCV_2015_paper.pdf",
        "aff": "Technische Universit ¨at Berlin; Microsoft, Cambridge, UK",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2045306,
        "gs_citation": 61,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12948724232621317368&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "tu-berlin.de;microsoft.com",
        "email": "tu-berlin.de;microsoft.com",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Nurutdinova_Towards_Pointless_Structure_ICCV_2015_paper.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Technische Universität Berlin;Microsoft",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tu-berlin.de;https://www.microsoft.com",
        "aff_unique_abbr": "TU Berlin;MSFT",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Germany;United Kingdom",
        "bibtex": "@InProceedings{Nurutdinova_2015_ICCV,\n    \n    author = {\n    Nurutdinova,\n    Irina and Fitzgibbon,\n    Andrew\n},\n    title = {\n    Towards Pointless Structure From Motion: 3D Reconstruction and Camera Parameters From General 3D Curves\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "0e934f5ad5",
        "title": "Tracking-by-Segmentation With Online Gradient Boosting Decision Tree",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Jeany Son, Ilchae Jung, Kayoung Park, Bohyung Han",
        "author": "Jeany Son; Ilchae Jung; Kayoung Park; Bohyung Han",
        "abstract": "We propose an online tracking algorithm that adaptively models target appearances based on an online gradient boosting decision tree. Our algorithm is particularly useful for non-rigid and/or articulated objects since it handles various deformations of the target effectively by integrating a classifier operating on individual patches and provides segmentation masks of the target as final results. The posterior of the target state is propagated over time by particle filtering, where the likelihood is computed based mainly on patch-level confidence map associated with a latent target state corresponding to each sample. Once tracking is completed in each frame, our gradient boosting decision tree is updated to adapt new data in a recursive manner. For effective evaluation of segmentation-based tracking algorithms, we construct a new ground-truth that contains pixel-level annotation of segmentation mask. We evaluate the performance of our tracking algorithm based on the measures for segmentation masks, where our algorithm illustrates superior accuracy compared to the state-of-the-art segmentation-based tracking methods.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Son_Tracking-by-Segmentation_With_Online_ICCV_2015_paper.pdf",
        "aff": "Dept. of Computer Science and Engineering, POSTECH, Korea + Computer Vision Lab., POSTECH, Korea; Dept. of Computer Science and Engineering, POSTECH, Korea + Computer Vision Lab., POSTECH, Korea; Computer Vision Lab., POSTECH, Korea; Dept. of Computer Science and Engineering, POSTECH, Korea + Computer Vision Lab., POSTECH, Korea",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2874709,
        "gs_citation": 160,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1905464106895052228&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "postech.ac.kr;postech.ac.kr;postech.ac.kr;postech.ac.kr",
        "email": "postech.ac.kr;postech.ac.kr;postech.ac.kr;postech.ac.kr",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Son_Tracking-by-Segmentation_With_Online_ICCV_2015_paper.html",
        "aff_unique_index": "0+0;0+0;0;0+0",
        "aff_unique_norm": "POSTECH",
        "aff_unique_dep": "Dept. of Computer Science and Engineering",
        "aff_unique_url": "https://www.postech.ac.kr",
        "aff_unique_abbr": "POSTECH",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0;0+0",
        "aff_country_unique": "South Korea",
        "bibtex": "@InProceedings{Son_2015_ICCV,\n    \n    author = {\n    Son,\n    Jeany and Jung,\n    Ilchae and Park,\n    Kayoung and Han,\n    Bohyung\n},\n    title = {\n    Tracking-by-Segmentation With Online Gradient Boosting Decision Tree\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "cb022188b4",
        "title": "Training a Feedback Loop for Hand Pose Estimation",
        "session": "vision and people",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "Markus Oberweger, Paul Wohlhart, Vincent Lepetit",
        "author": "Markus Oberweger; Paul Wohlhart; Vincent Lepetit",
        "abstract": "We propose an entirely data-driven approach to estimating the 3D pose of a hand given a depth image. We show that we can correct the mistakes made by a Convolutional Neural Network trained to predict an estimate of the 3D pose by using a feedback loop. The components of this feedback loop are also Deep Networks, optimized using training data. They remove the need for fitting a 3D model to the input data, which requires both a carefully designed fitting function and algorithm. We show that our approach outperforms state-of-the-art methods, and is efficient as our implementation runs at over 400 fps on a single GPU.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Oberweger_Training_a_Feedback_ICCV_2015_paper.pdf",
        "aff": "Institute for Computer Graphics and Vision, Graz University of Technology, Austria; Institute for Computer Graphics and Vision, Graz University of Technology, Austria; Institute for Computer Graphics and Vision, Graz University of Technology, Austria",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1082594,
        "gs_citation": 349,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5562345415917778971&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 20,
        "aff_domain": "icg.tugraz.at;icg.tugraz.at;icg.tugraz.at",
        "email": "icg.tugraz.at;icg.tugraz.at;icg.tugraz.at",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Oberweger_Training_a_Feedback_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Graz University of Technology",
        "aff_unique_dep": "Institute for Computer Graphics and Vision",
        "aff_unique_url": "https://www.tugraz.at",
        "aff_unique_abbr": "TU Graz",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Graz",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Austria",
        "bibtex": "@InProceedings{Oberweger_2015_ICCV,\n    \n    author = {\n    Oberweger,\n    Markus and Wohlhart,\n    Paul and Lepetit,\n    Vincent\n},\n    title = {\n    Training a Feedback Loop for Hand Pose Estimation\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "1c09a645f8",
        "title": "TransCut: Transparent Object Segmentation From a Light-Field Image",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Yichao Xu, Hajime Nagahara, Atsushi Shimada, Rin-ichiro Taniguchi",
        "author": "Yichao Xu; Hajime Nagahara; Atsushi Shimada; Rin-ichiro Taniguchi",
        "abstract": "The segmentation of transparent objects can be very useful in computer vision applications. However, because they borrow texture from their background and have a similar appearance to their surroundings, transparent objects are not handled well by regular image segmentation methods. We propose a method that overcomes these problems using the consistency and distortion properties of a light-field image. Graph-cut optimization is applied for the pixel labeling problem. The light-field linearity is used to estimate the likelihood of a pixel belonging to the transparent object or Lambertian background, and the occlusion detector is used to find the occlusion boundary.  We acquire a light field dataset for the transparent object, and use this dataset to evaluate our method. The results demonstrate that the proposed method successfully segments transparent objects from the background.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Xu_TransCut_Transparent_Object_ICCV_2015_paper.pdf",
        "aff": "Kyushu University, Japan; Kyushu University, Japan; Kyushu University, Japan; Kyushu University, Japan",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 8136544,
        "gs_citation": 139,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=869529272903375357&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 21,
        "aff_domain": "limu.ait.kyushu-u.ac.jp;limu.ait.kyushu-u.ac.jp;limu.ait.kyushu-u.ac.jp;limu.ait.kyushu-u.ac.jp",
        "email": "limu.ait.kyushu-u.ac.jp;limu.ait.kyushu-u.ac.jp;limu.ait.kyushu-u.ac.jp;limu.ait.kyushu-u.ac.jp",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Xu_TransCut_Transparent_Object_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Kyushu University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.kyushu-u.ac.jp",
        "aff_unique_abbr": "Kyushu U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Japan",
        "bibtex": "@InProceedings{Xu_2015_ICCV,\n    \n    author = {\n    Xu,\n    Yichao and Nagahara,\n    Hajime and Shimada,\n    Atsushi and Taniguchi,\n    Rin-ichiro\n},\n    title = {\n    TransCut: Transparent Object Segmentation From a Light-Field Image\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "a3412ca085",
        "title": "Two Birds, One Stone: Jointly Learning Binary Code for Large-Scale Face Image Retrieval and Attributes Prediction",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Yan Li, Ruiping Wang, Haomiao Liu, Huajie Jiang, Shiguang Shan, Xilin Chen",
        "author": "Yan Li; Ruiping Wang; Haomiao Liu; Huajie Jiang; Shiguang Shan; Xilin Chen",
        "abstract": "We address the challenging large-scale content-based face image retrieval problem, intended as searching images based on the presence of specific subject, given one face image of him/her. To this end, one natural demand is a supervised binary code learning method. While the learned codes might be discriminating, people often have a further expectation that whether some semantic message (e.g., visual attributes) can be read from the human-incomprehensible codes. For this purpose, we propose a novel binary code learning framework by jointly encoding identity discriminability and a number of facial attributes into unified binary code. In this way, the learned binary codes can be applied to not only fine-grained face image retrieval, but also facial attributes prediction, which is the very innovation of this work, just like killing two birds with one stone. To evaluate the effectiveness of the proposed method, extensive experiments are conducted on a new purified large-scale web celebrity database, named CFW 60K, with abundant manual identity and attributes annotation, and experimental results exhibit the superiority of our method over state-of-the-art.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Li_Two_Birds_One_ICCV_2015_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 65,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13821043325332428975&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Li_Two_Birds_One_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Li_2015_ICCV,\n    \n    author = {\n    Li,\n    Yan and Wang,\n    Ruiping and Liu,\n    Haomiao and Jiang,\n    Huajie and Shan,\n    Shiguang and Chen,\n    Xilin\n},\n    title = {\n    Two Birds,\n    One Stone: Jointly Learning Binary Code for Large-Scale Face Image Retrieval and Attributes Prediction\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "9e591e1403",
        "title": "Uncovering Interactions and Interactors: Joint Estimation of Head, Body Orientation and F-Formations From Surveillance Videos",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Elisa Ricci, Jagannadan Varadarajan, Ramanathan Subramanian, Samuel Rota Bulò, Narendra Ahuja, Oswald Lanz",
        "author": "Elisa Ricci; Jagannadan Varadarajan; Ramanathan Subramanian; Samuel Rota Bulo; Narendra Ahuja; Oswald Lanz",
        "abstract": "We present a novel approach for jointly estimating tar- gets' head, body orientations and conversational groups called F-formations from a distant social scene (e.g., a cocktail party captured by surveillance cameras). Differing from related works that have (i) coupled head and body pose learning by exploiting the limited range of orientations that the two can jointly take, or (ii) determined F-formations based on the mutual head (but not body) orientations of in- teractors, we present a unified framework to jointly infer both (i) and (ii). Apart from exploiting spatial and orien- tation relationships, we also integrate cues pertaining to temporal consistency and occlusions, which are beneficial while handling low-resolution data under surveillance set- tings. Efficacy of the joint inference framework reflects via increased head, body pose and F-formation estimation ac- curacy over the state-of-the-art, as confirmed by extensive experiments on two social datasets.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Ricci_Uncovering_Interactions_and_ICCV_2015_paper.pdf",
        "aff": "Fondazione Bruno Kessler, Trento, Italy + Department of Engineering, University of Perugia, Italy; Advanced Digital Sciences Center, University of Illinois at Urbana-Champaign, Singapore; Advanced Digital Sciences Center, University of Illinois at Urbana-Champaign, Singapore; Fondazione Bruno Kessler, Trento, Italy; Advanced Digital Sciences Center, University of Illinois at Urbana-Champaign, Singapore + University of Illinois at Urbana-Champaign, IL USA; Fondazione Bruno Kessler, Trento, Italy",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 861996,
        "gs_citation": 85,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=80509551107285581&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff_domain": "fbk.eu;adsc.com.sg;adsc.com.sg;fbk.eu;illinois.edu;fbk.eu",
        "email": "fbk.eu;adsc.com.sg;adsc.com.sg;fbk.eu;illinois.edu;fbk.eu",
        "author_num": 6,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Ricci_Uncovering_Interactions_and_ICCV_2015_paper.html",
        "aff_unique_index": "0+1;2;2;0;2+2;0",
        "aff_unique_norm": "Fondazione Bruno Kessler;University of Perugia;University of Illinois at Urbana-Champaign",
        "aff_unique_dep": ";Department of Engineering;Advanced Digital Sciences Center",
        "aff_unique_url": "https://www.fbk.eu;https://www.unipg.it;https://illinois.edu",
        "aff_unique_abbr": "FBK;;UIUC",
        "aff_campus_unique_index": "0;2;2;0;2+2;0",
        "aff_campus_unique": "Trento;;Urbana-Champaign",
        "aff_country_unique_index": "0+0;1;1;0;1+1;0",
        "aff_country_unique": "Italy;United States",
        "bibtex": "@InProceedings{Ricci_2015_ICCV,\n    \n    author = {\n    Ricci,\n    Elisa and Varadarajan,\n    Jagannadan and Subramanian,\n    Ramanathan and Bulo,\n    Samuel Rota and Ahuja,\n    Narendra and Lanz,\n    Oswald\n},\n    title = {\n    Uncovering Interactions and Interactors: Joint Estimation of Head,\n    Body Orientation and F-Formations From Surveillance Videos\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "89642743cf",
        "title": "Understanding Deep Features With Computer-Generated Imagery",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Mathieu Aubry, Bryan C. Russell",
        "author": "Mathieu Aubry; Bryan C. Russell",
        "abstract": "We introduce an approach for analyzing the variation of features generated  by convolutional neural networks (CNNs) trained on large image datasets with respect to scene factors that occur in natural images. Such factors may include object style, 3D viewpoint, color, and scene lighting configuration. Our approach analyzes CNN feature responses with respect to different scene factors by controlling for them via rendering using a large database of 3D CAD models. The rendered images are presented to a trained CNN and responses for different layers are studied with respect to the input scene factors.  We perform a linear decomposition of the responses based on knowledge of the input scene factors and analyze the resulting components. In particular, we quantify their relative importance in the CNN responses and visualize them using principal component analysis. We show qualitative and quantitative results of our study on three trained CNNs: AlexNet [??], Places [??], and Oxford VGG [??]. We observe important differences across the different networks and CNN layers with respect to  different scene factors and object categories. Finally, we demonstrate that our analysis based on computer-generated imagery translates to the network representation of natural images.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Aubry_Understanding_Deep_Features_ICCV_2015_paper.pdf",
        "aff": "UC Berkeley Universit ´e Paris-Est, LIGM (UMR CNRS 8049), ENPC; Adobe Research",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2311686,
        "gs_citation": 182,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17433684573736614822&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff_domain": "imagine.enpc.fr;adobe.com",
        "email": "imagine.enpc.fr;adobe.com",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Aubry_Understanding_Deep_Features_ICCV_2015_paper.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of California, Berkeley;Adobe",
        "aff_unique_dep": ";Adobe Research",
        "aff_unique_url": "https://www.berkeley.edu;https://research.adobe.com",
        "aff_unique_abbr": "UC Berkeley;Adobe",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Aubry_2015_ICCV,\n    \n    author = {\n    Aubry,\n    Mathieu and Russell,\n    Bryan C.\n},\n    title = {\n    Understanding Deep Features With Computer-Generated Imagery\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "df4a482e36",
        "title": "Understanding Everyday Hands in Action From RGB-D Images",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Grégory Rogez, James S. Supančič III, Deva Ramanan",
        "author": "Gregory Rogez; James S. Supancic III; Deva Ramanan",
        "abstract": "We analyze functional manipulations of handheld objects, formalizing the problem as one of fine-grained grasp classification. To do so, we make use of a recently developed fine-grained taxonomy of human-object grasps. We introduce a large dataset of 12000 RGB-D images covering 71 everyday grasps in natural interactions. Our dataset is different from past work (typically addressed from a robotics perspective) in terms of its scale, diversity, and combination of RGB and depth data. From a computer-vision perspective, our dataset allows for exploration of contact and force prediction (crucial concepts in functional grasp analysis) from perceptual cues. We present extensive experimental results with state-of-the-art baselines, illustrating the role of segmentation, object context, and 3D-understanding in functional grasp analysis. We demonstrate a near 2X improvement over prior work and a naive deep baseline, while pointing out important directions for improvement.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Rogez_Understanding_Everyday_Hands_ICCV_2015_paper.pdf",
        "aff": "Inria Rhˆone-Alpes; University of California, Irvine; Carnegie Mellon University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2347120,
        "gs_citation": 179,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3486450508966631342&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 24,
        "aff_domain": "inria.fr;ics.uci.edu;cs.cmu.edu",
        "email": "inria.fr;ics.uci.edu;cs.cmu.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Rogez_Understanding_Everyday_Hands_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Inria;University of California, Irvine;Carnegie Mellon University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.inria.fr;https://www.uci.edu;https://www.cmu.edu",
        "aff_unique_abbr": "Inria;UCI;CMU",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Rhˆne-Alpes;Irvine;",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "France;United States",
        "bibtex": "@InProceedings{Rogez_2015_ICCV,\n    \n    author = {\n    Rogez,\n    Gregory and Supancic,\n    III,\n    James S. and Ramanan,\n    Deva\n},\n    title = {\n    Understanding Everyday Hands in Action From RGB-D Images\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "6e30a39f92",
        "title": "Understanding and Diagnosing Visual Tracking Systems",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Naiyan Wang, Jianping Shi, Dit-Yan Yeung, Jiaya Jia",
        "author": "Naiyan Wang; Jianping Shi; Dit-Yan Yeung; Jiaya Jia",
        "abstract": "Several benchmark datasets for visual tracking research have been created in recent years.  Despite their usefulness, whether they are sufficient for understanding and diagnosing the strengths and weaknesses of different trackers remains questionable.  To address this issue, we propose a framework by breaking a tracker down into five constituent parts, namely, motion model, feature extractor, observation model, model updater, and ensemble post-processor.  We then conduct ablative experiments on each component to study how it affects the overall result.  Surprisingly, our findings are discrepant with some common beliefs in the visual tracking research community.  We find that the feature extractor plays the most important role in a tracker.  On the other hand, although the observation model is the focus of many studies, we find that it often brings no significant improvement.  Moreover, the motion model and model updater contain many details that could affect the result.  Also, the ensemble post-processor can improve the result substantially when the constituent trackers have high diversity.  Based on our findings, we put together some very elementary building blocks to give a basic tracker which is competitive in performance to the state-of-the-art trackers.  We believe our framework can provide a solid baseline when conducting controlled experiments for visual tracking research.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Wang_Understanding_and_Diagnosing_ICCV_2015_paper.pdf",
        "aff": "Hong Kong University of Science and Technology; Linkface Group Limited + Chinese University of Hong Kong; Hong Kong University of Science and Technology; Chinese University of Hong Kong",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 900252,
        "gs_citation": 465,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=368796726239543774&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff_domain": "gmail.com;cse.cuhk.edu.hk;cse.ust.hk;cse.cuhk.edu.hk",
        "email": "gmail.com;cse.cuhk.edu.hk;cse.ust.hk;cse.cuhk.edu.hk",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Wang_Understanding_and_Diagnosing_ICCV_2015_paper.html",
        "aff_unique_index": "0;1+2;0;2",
        "aff_unique_norm": "Hong Kong University of Science and Technology;Linkface Group Limited;Chinese University of Hong Kong",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.ust.hk;;https://www.cuhk.edu.hk",
        "aff_unique_abbr": "HKUST;;CUHK",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0;0+0;0;0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Wang_2015_ICCV,\n    \n    author = {\n    Wang,\n    Naiyan and Shi,\n    Jianping and Yeung,\n    Dit-Yan and Jia,\n    Jiaya\n},\n    title = {\n    Understanding and Diagnosing Visual Tracking Systems\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "6979fb1108",
        "title": "Understanding and Predicting Image Memorability at a Large Scale",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Aditya Khosla, Akhil S. Raju, Antonio Torralba, Aude Oliva",
        "author": "Aditya Khosla; Akhil S. Raju; Antonio Torralba; Aude Oliva",
        "abstract": "Progress in estimating visual memorability has been limited by the small scale and lack of variety of benchmark data. Here, we introduce a novel experimental procedure to objectively measure human memory, building the largest annotated image memorability dataset to date (with 60,000 labeled images from a diverse array of sources). Using Convolutional Neural Networks (CNNs), we show that fine-tuned deep features outperform all other features by a large margin, reaching a rank correlation of 0.64, near human consistency (0.68). Analysis of the responses of the high-level CNN layers shows which objects and regions are positively, and negatively, correlated with memorability, allowing us to create memorability maps for each image and provide a concrete method to perform image memorability manipulation. This work demonstrates that one can now robustly estimate the memorability of images from many different classes, positioning memorability and deep memorability features as prime candidates to estimate the utility of information for cognitive systems.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Khosla_Understanding_and_Predicting_ICCV_2015_paper.pdf",
        "aff": "MIT; MIT; MIT; MIT",
        "project": "http://memorability.csail.mit.edu",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3961164,
        "gs_citation": 435,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4151583339195604249&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "mit.edu;mit.edu;mit.edu;mit.edu",
        "email": "mit.edu;mit.edu;mit.edu;mit.edu",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Khosla_Understanding_and_Predicting_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://web.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Khosla_2015_ICCV,\n    \n    author = {\n    Khosla,\n    Aditya and Raju,\n    Akhil S. and Torralba,\n    Antonio and Oliva,\n    Aude\n},\n    title = {\n    Understanding and Predicting Image Memorability at a Large Scale\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "5de38943a3",
        "title": "Unsupervised Cross-Modal Synthesis of Subject-Specific Scans",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Raviteja Vemulapalli, Hien Van Nguyen, Shaohua Kevin Zhou",
        "author": "Raviteja Vemulapalli; Hien Van Nguyen; Shaohua Kevin Zhou",
        "abstract": "Recently, cross-modal synthesis of subject-specific scans has been receiving significant attention from the medical imaging community. Though various synthesis approaches have been introduced in the recent past, most of them are either tailored to a specific application or proposed for the supervised setting, i.e., they assume the availability of training data from the same set of subjects in both source and target modalities. But, collecting multiple scans from each subject is undesirable. Hence, to address this issue, we propose a general unsupervised cross-modal medical image synthesis approach that works without paired training data. Given a source modality image of a subject, we first generate multiple target modality candidate values for each voxel independently using cross-modal nearest neighbor search. Then, we select the best candidate values jointly for all the voxels by simultaneously maximizing a global mutual information cost function and a local spatial consistency cost function. Finally, we use coupled sparse representation for further refinement of synthesized images. Our experiments on generating T1-MRI brain scans from T2-MRI and vice versa demonstrate that the synthesis capability of the proposed unsupervised approach is comparable to various state-of-the-art supervised approaches in the literature.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Vemulapalli_Unsupervised_Cross-Modal_Synthesis_ICCV_2015_paper.pdf",
        "aff": "Center for Automation Research, UMIACS, University of Maryland, College Park; Siemens Healthcare Technology Center, Princeton, New Jersey; Siemens Healthcare Technology Center, Princeton, New Jersey",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 829728,
        "gs_citation": 81,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=299602863031397824&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "umd.edu;siemens.com;siemens.com",
        "email": "umd.edu;siemens.com;siemens.com",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Vemulapalli_Unsupervised_Cross-Modal_Synthesis_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of Maryland, College Park;Siemens Healthcare",
        "aff_unique_dep": "Center for Automation Research, UMIACS;Technology Center",
        "aff_unique_url": "https://www.umd.edu;https://www.siemens-healthineers.com",
        "aff_unique_abbr": "UMD;Siemens",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "College Park;Princeton",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Vemulapalli_2015_ICCV,\n    \n    author = {\n    Vemulapalli,\n    Raviteja and Van Nguyen,\n    Hien and Zhou,\n    Shaohua Kevin\n},\n    title = {\n    Unsupervised Cross-Modal Synthesis of Subject-Specific Scans\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "ca1563e61c",
        "title": "Unsupervised Domain Adaptation With Imbalanced Cross-Domain Data",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Tzu Ming Harry Hsu, Wei Yu Chen, Cheng-An Hou, Yao-Hung Hubert Tsai, Yi-Ren Yeh, Yu-Chiang Frank Wang",
        "author": "Tzu Ming Harry Hsu; Wei Yu Chen; Cheng-An Hou; Yao-Hung Hubert Tsai; Yi-Ren Yeh; Yu-Chiang Frank Wang",
        "abstract": "We address a challenging unsupervised domain adaptation problem with imbalanced cross-domain data. For standard unsupervised domain adaptation, one typically obtains labeled data in the source domain and only observes unlabeled data in the target domain. However, most existing works do not consider the scenarios in which either the label numbers across domains are different, or the data in the source and/or target domains might be collected from multiple datasets. To address the aforementioned settings of imbalanced cross-domain data, we propose Closest Common Space Learning (CCSL) for associating such data with the capability of preserving label and structural information within and across domains. Experiments on multiple cross-domain visual classification tasks confirm that our method performs favorably against state-of-the-art approaches, especially when imbalanced cross-domain data are presented.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Hsu_Unsupervised_Domain_Adaptation_ICCV_2015_paper.pdf",
        "aff": "Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; The Robotics Institute, Carnegie Mellon University, Pittsburgh, PA 15213, USA; Research Center for IT Innovation, Academia Sinica, Taipei, Taiwan; Department of Mathematics, National Kaohsiung Normal University, Kaohsiung, Taiwan; Research Center for IT Innovation, Academia Sinica, Taipei, Taiwan",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2391579,
        "gs_citation": 90,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13961084661402053165&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "gmail.com;gmail.com;gmail.com;gmail.com;nknu.edu.tw;citi.sinica.edu.tw",
        "email": "gmail.com;gmail.com;gmail.com;gmail.com;nknu.edu.tw;citi.sinica.edu.tw",
        "author_num": 6,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Hsu_Unsupervised_Domain_Adaptation_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;1;2;3;2",
        "aff_unique_norm": "National Taiwan University;Carnegie Mellon University;Academia Sinica;National Kaohsiung Normal University",
        "aff_unique_dep": "Department of Electrical Engineering;The Robotics Institute;Research Center for IT Innovation;Department of Mathematics",
        "aff_unique_url": "https://www.ntu.edu.tw;https://www.cmu.edu;https://www.sinica.edu.tw;https://www.nknu.edu.tw",
        "aff_unique_abbr": "NTU;CMU;AS;NKNU",
        "aff_campus_unique_index": "0;0;1;0;0;0",
        "aff_campus_unique": "Taiwan;Pittsburgh",
        "aff_country_unique_index": "0;0;1;0;0;0",
        "aff_country_unique": "China;United States",
        "bibtex": "@InProceedings{Hsu_2015_ICCV,\n    \n    author = {\n    Hsu,\n    Tzu Ming Harry and Chen,\n    Wei Yu and Hou,\n    Cheng-An and Tsai,\n    Yao-Hung Hubert and Yeh,\n    Yi-Ren and Wang,\n    Yu-Chiang Frank\n},\n    title = {\n    Unsupervised Domain Adaptation With Imbalanced Cross-Domain Data\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "0a8a8262e8",
        "title": "Unsupervised Domain Adaptation for Zero-Shot Learning",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Elyor Kodirov, Tao Xiang, Zhenyong Fu, Shaogang Gong",
        "author": "Elyor Kodirov; Tao Xiang; Zhenyong Fu; Shaogang Gong",
        "abstract": "Zero-shot learning (ZSL) can be considered as a special case of transfer learning where the source and target domains have different tasks/label spaces and the target domain is unlabelled, providing little guidance for the knowledge transfer. A ZSL method typically assumes that the two domains share a common semantic representation space, where a visual feature vector extracted from an image/video can be  projected/embedded using a projection function. Existing approaches learn the projection function from the source domain and apply it without adaptation to the target domain. They are thus based on naive knowledge transfer and the learned projections are prone to the domain shift problem. In this paper a novel ZSL method is proposed based on unsupervised domain adaptation. Specifically, we formulate a novel regularised sparse coding framework which uses the target domain class labels' projections in the semantic space  to regularise the learned target domain projection thus effectively overcoming the projection domain shift problem. Extensive experiments on four object and action recognition benchmark datasets show that the proposed ZSL method significantly outperforms the state-of-the-arts.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Kodirov_Unsupervised_Domain_Adaptation_ICCV_2015_paper.pdf",
        "aff": "Queen Mary University of London, London E1 4NS, UK; Queen Mary University of London, London E1 4NS, UK; Queen Mary University of London, London E1 4NS, UK; Queen Mary University of London, London E1 4NS, UK",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 581663,
        "gs_citation": 480,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12896396517895949011&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "qmul.ac.uk;qmul.ac.uk;qmul.ac.uk;qmul.ac.uk",
        "email": "qmul.ac.uk;qmul.ac.uk;qmul.ac.uk;qmul.ac.uk",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Kodirov_Unsupervised_Domain_Adaptation_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Queen Mary University of London",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.qmul.ac.uk",
        "aff_unique_abbr": "QMUL",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "London",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United Kingdom",
        "bibtex": "@InProceedings{Kodirov_2015_ICCV,\n    \n    author = {\n    Kodirov,\n    Elyor and Xiang,\n    Tao and Fu,\n    Zhenyong and Gong,\n    Shaogang\n},\n    title = {\n    Unsupervised Domain Adaptation for Zero-Shot Learning\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "73828eceaf",
        "title": "Unsupervised Extraction of Video Highlights Via Robust Recurrent Auto-Encoders",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Huan Yang, Baoyuan Wang, Stephen Lin, David Wipf, Minyi Guo, Baining Guo",
        "author": "Huan Yang; Baoyuan Wang; Stephen Lin; David Wipf; Minyi Guo; Baining Guo",
        "abstract": "With the growing popularity of short-form video sharing platforms such as Instagram and Vine, there has been an increasing need for techniques that automatically extract highlights from video. Whereas prior works have approached this problem with heuristic rules or supervised learning, we present an unsupervised learning approach that takes advantage of the abundance of user-edited videos on social media websites such as YouTube. Based on the idea that the most significant sub-events within a video class are commonly present among edited videos while less interesting ones appear less frequently, we identify the significant sub-events via a robust recurrent auto-encoder trained on a collection of user-edited videos queried for each particular class of interest. The auto-encoder is trained using a proposed shrinking exponential loss function that makes it robust to noise in the web-crawled training data, and is configured with bidirectional long short term memory (LSTM) cells to better model the temporal structure of highlight segments. Different from supervised techniques, our method can infer highlights using only a set of downloaded edited videos, without also needing their pre-edited counterparts which are rarely available online. Extensive experiments indicate the promise of our proposed solution in this challenging unsupervised setting.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Yang_Unsupervised_Extraction_of_ICCV_2015_paper.pdf",
        "aff": "Shanghai Jiao Tong University; Microsoft Technology & Research; Microsoft Research; Microsoft Research; Shanghai Jiao Tong University; Microsoft Research",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 833461,
        "gs_citation": 221,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13385857189976884567&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "sjtu.edu.cn;microsoft.com;microsoft.com;microsoft.com;sjtu.edu.cn;microsoft.com",
        "email": "sjtu.edu.cn;microsoft.com;microsoft.com;microsoft.com;sjtu.edu.cn;microsoft.com",
        "author_num": 6,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Yang_Unsupervised_Extraction_of_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;2;2;0;2",
        "aff_unique_norm": "Shanghai Jiao Tong University;Microsoft;Microsoft Corporation",
        "aff_unique_dep": ";Technology & Research;Microsoft Research",
        "aff_unique_url": "https://www.sjtu.edu.cn;https://www.microsoft.com;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "SJTU;Microsoft;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;1;0;1",
        "aff_country_unique": "China;United States",
        "bibtex": "@InProceedings{Yang_2015_ICCV,\n    \n    author = {\n    Yang,\n    Huan and Wang,\n    Baoyuan and Lin,\n    Stephen and Wipf,\n    David and Guo,\n    Minyi and Guo,\n    Baining\n},\n    title = {\n    Unsupervised Extraction of Video Highlights Via Robust Recurrent Auto-Encoders\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "517e7804f1",
        "title": "Unsupervised Generation of a Viewpoint Annotated Car Dataset From Videos",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Nima Sedaghat, Thomas Brox",
        "author": "Nima Sedaghat; Thomas Brox",
        "abstract": "Object recognition approaches have recently been extended to yield, aside of the object class output, also viewpoint or pose. Training such approaches typically requires additional viewpoint or keypoint annotation in the training data or, alternatively, synthetic CAD models. In this paper,we present an approach that creates a dataset of images annotated with bounding boxes and viewpoint labels in a fully automated manner from videos. We assume that the scene is static in order to reconstruct 3D surfaces via structure from motion. We automatically detect when the reconstruction fails and normalize for the viewpoint of the 3D models by aligning the reconstructed point clouds. Exemplarily for cars we show that we can expand a large dataset of annotated single images and obtain improved performance when training a viewpoint regressor on this joined dataset.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Sedaghat_Unsupervised_Generation_of_ICCV_2015_paper.pdf",
        "aff": "Computer Vision Group, University of Freiburg, Germany; Computer Vision Group, University of Freiburg, Germany",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1164705,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3835006623448893017&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "cs.uni-freiburg.de;cs.uni-freiburg.de",
        "email": "cs.uni-freiburg.de;cs.uni-freiburg.de",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Sedaghat_Unsupervised_Generation_of_ICCV_2015_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Freiburg",
        "aff_unique_dep": "Computer Vision Group",
        "aff_unique_url": "https://www.uni-freiburg.de",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Freiburg",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany",
        "bibtex": "@InProceedings{Sedaghat_2015_ICCV,\n    \n    author = {\n    Sedaghat,\n    Nima and Brox,\n    Thomas\n},\n    title = {\n    Unsupervised Generation of a Viewpoint Annotated Car Dataset From Videos\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "cf0740538f",
        "title": "Unsupervised Learning of Spatiotemporally Coherent Metrics",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Ross Goroshin, Joan Bruna, Jonathan Tompson, David Eigen, Yann LeCun",
        "author": "Ross Goroshin; Joan Bruna; Jonathan Tompson; David Eigen; Yann LeCun",
        "abstract": "Current state-of-the-art classification and detection algorithms train deep convolutional networks using labeled data. In this work we study unsupervised feature learning with convolutional networks in the context of temporally coherent unlabeled data. We focus on feature learning from unlabeled video data, using the assumption that adjacent video frames contain semantically similar information. This assumption is exploited to train a convolutional pooling auto-encoder regularized by slowness and sparsity. We establish a connection between slow feature learning and metric learning. Using this connection we define \"temporal coherence\"--a criterion which can be used to select hyper-parameters automatically. In a transfer learning experiment, we show that the resulting encoder can be used to define a more semantically coherent metric without the use of labeled data.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Goroshin_Unsupervised_Learning_of_ICCV_2015_paper.pdf",
        "aff": "Courant Institute, NYU; University of California, Berkeley; Google Inc.; Courant Institute, NYU; Courant Institute, NYU & Facebook AI Research",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4366030,
        "gs_citation": 192,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6959318222283158379&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff_domain": "cs.nyu.edu;berkeley.edu;google.com;cs.nyu.edu;cs.nyu.edu",
        "email": "cs.nyu.edu;berkeley.edu;google.com;cs.nyu.edu;cs.nyu.edu",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Goroshin_Unsupervised_Learning_of_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;2;0;3",
        "aff_unique_norm": "New York University;University of California, Berkeley;Google;Courant Institute of Mathematical Sciences, New York University",
        "aff_unique_dep": "Courant Institute;;;Department of Mathematics",
        "aff_unique_url": "https://www.courant.nyu.edu;https://www.berkeley.edu;https://www.google.com;https://www.courant.nyu.edu",
        "aff_unique_abbr": "NYU;UC Berkeley;Google;Courant",
        "aff_campus_unique_index": "0;1;2;0;0",
        "aff_campus_unique": "New York;Berkeley;Mountain View",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Goroshin_2015_ICCV,\n    \n    author = {\n    Goroshin,\n    Ross and Bruna,\n    Joan and Tompson,\n    Jonathan and Eigen,\n    David and LeCun,\n    Yann\n},\n    title = {\n    Unsupervised Learning of Spatiotemporally Coherent Metrics\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "7f700a105b",
        "title": "Unsupervised Learning of Visual Representations Using Videos",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Xiaolong Wang, Abhinav Gupta",
        "author": "Xiaolong Wang; Abhinav Gupta",
        "abstract": "Is strong supervision necessary for learning a good visual representation? Do we really need millions of semantically-labeled images to train a Convolutional Neural Network (CNN)? In this paper, we present a simple yet surprisingly powerful approach for unsupervised learning of CNN. Specifically, we use hundreds of thousands of unlabeled videos from the web to learn visual representations. Our key idea is that visual tracking provides the supervision. That is, two patches connected by a track should have similar visual representation in deep feature space since they probably belong to same object or object part. We design a Siamese-triplet network with a ranking loss function to train this CNN representation. Without using a single image from ImageNet, just using 100K unlabeled videos and the VOC 2012 dataset, we train an ensemble of unsupervised networks that achieves 52% mAP (no bounding box regression). This performance comes tantalizingly close to its ImageNet-supervised counterpart, an ensemble which achieves a mAP of 54.4%. We also show that our unsupervised network can perform competitively in other tasks such as surface-normal estimation.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Wang_Unsupervised_Learning_of_ICCV_2015_paper.pdf",
        "aff": ";",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 1374,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15788792053693868696&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Wang_Unsupervised_Learning_of_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Wang_2015_ICCV,\n    \n    author = {\n    Wang,\n    Xiaolong and Gupta,\n    Abhinav\n},\n    title = {\n    Unsupervised Learning of Visual Representations Using Videos\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "61d2e67ce2",
        "title": "Unsupervised Object Discovery and Tracking in Video Collections",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Suha Kwak, Minsu Cho, Ivan Laptev, Jean Ponce, Cordelia Schmid",
        "author": "Suha Kwak; Minsu Cho; Ivan Laptev; Jean Ponce; Cordelia Schmid",
        "abstract": "This paper addresses the problem of automatically localizing dominant objects as spatio-temporal tubes in a noisy collection of videos with minimal or even no supervision. We formulate the problem as a combination of two complementary processes: discovery and tracking. The first one establishes correspondences between prominent regions across videos, and the second one associates similar object regions within the same video. Interestingly, our algorithm also discovers the implicit topology of frames associated with instances of the same object class across different videos, a role normally left to supervisory information in the form of class labels in conventional image and video understanding methods. Indeed, as demonstrated by our experiments, our method can handle video collections featuring multiple object classes, and substantially outperforms the state of the art in colocalization, even though it tackles a broader problem with much less supervision.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Kwak_Unsupervised_Object_Discovery_ICCV_2015_paper.pdf",
        "aff": "Inria; Inria; Inria; Ecole Normale Supérieure / PSL Research University; Inria+Inria Grenoble Rhône-Alpes, Laboratoire Jean Kuntzmann, CNRS, Univ. Grenoble Alpes, France",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1832598,
        "gs_citation": 153,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12203500835590333101&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 42,
        "aff_domain": "; ; ; ; ",
        "email": "; ; ; ; ",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Kwak_Unsupervised_Object_Discovery_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;1;0+2",
        "aff_unique_norm": "Inria;Ecole Normale Supérieure;Inria Grenoble Rhône-Alpes",
        "aff_unique_dep": ";;Laboratoire Jean Kuntzmann",
        "aff_unique_url": "https://www.inria.fr;https://www.ens.fr;https://www.inria.fr/grenoble",
        "aff_unique_abbr": "Inria;ENS;Inria",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Grenoble",
        "aff_country_unique_index": "0;0;0;0;0+0",
        "aff_country_unique": "France",
        "bibtex": "@InProceedings{Kwak_2015_ICCV,\n    \n    author = {\n    Kwak,\n    Suha and Cho,\n    Minsu and Laptev,\n    Ivan and Ponce,\n    Jean and Schmid,\n    Cordelia\n},\n    title = {\n    Unsupervised Object Discovery and Tracking in Video Collections\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "01a788d1a7",
        "title": "Unsupervised Semantic Parsing of Video Collections",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Ozan Sener, Amir R. Zamir, Silvio Savarese, Ashutosh Saxena",
        "author": "Ozan Sener; Amir R. Zamir; Silvio Savarese; Ashutosh Saxena",
        "abstract": "Human communication typically has an underlying structure. This is reflected in the fact that in many user generated videos, a starting point, ending, and certain objective steps between these two can be identified. In this paper, we propose a method for parsing a video into such semantic steps in an unsupervised way. The proposed method is capable of providing a semantic ``storyline'' of the video composed of its objective steps. We accomplish this utilizing both visual and language cues in a joint generative model. The proposed method can also provide a textual description for each of identified semantic steps and video segments. We evaluate this method on a large number of complex YouTube videos and show results of unprecedented quality for this new and impactful problem.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Sener_Unsupervised_Semantic_Parsing_ICCV_2015_paper.pdf",
        "aff": "Stanford University; Stanford University; Stanford University; Cornell University + Brain of Things Inc.",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2953060,
        "gs_citation": 123,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5734916927818018329&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Sener_Unsupervised_Semantic_Parsing_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;1+2",
        "aff_unique_norm": "Stanford University;Cornell University;Brain of Things",
        "aff_unique_dep": ";;Inc.",
        "aff_unique_url": "https://www.stanford.edu;https://www.cornell.edu;https://www.brainofthings.com",
        "aff_unique_abbr": "Stanford;Cornell;BOT",
        "aff_campus_unique_index": "0;0;0;",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;0;0;0+0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Sener_2015_ICCV,\n    \n    author = {\n    Sener,\n    Ozan and Zamir,\n    Amir R. and Savarese,\n    Silvio and Saxena,\n    Ashutosh\n},\n    title = {\n    Unsupervised Semantic Parsing of Video Collections\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "61d0ad21b9",
        "title": "Unsupervised Synchrony Discovery in Human Interaction",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Wen-Sheng Chu, Jiabei Zeng, Fernando De la Torre, Jeffrey F. Cohn, Daniel S. Messinger",
        "author": "Wen-Sheng Chu; Jiabei Zeng; Fernando De la Torre; Jeffrey F. Cohn; Daniel S. Messinger",
        "abstract": "People are inherently social. Social interaction plays an important and natural role in human behavior. Most computational methods focus on individuals alone rather than in social context. They also require labelled training data. We present an unsupervised approach to discover interpersonal synchrony, referred as to two or more persons preforming common actions in overlapping video frames or segments. For computational efficiency, we develop a branch-and-bound (B&B) approach that affords exhaustive search while guaranteeing a globally optimal solution. The proposed method is entirely general. It takes from two or more videos any multi-dimensional signal that can be represented as a histogram. We derive three novel bounding functions and provide efficient extensions, including multi-synchrony detection and accelerated search, using a warm-start strategy and parallelism. We evaluate the effectiveness of our approach in multiple databases, including human actions using the CMU Mocap dataset, spontaneous facial behaviors using group-formation task dataset and parent-infant interaction dataset.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Chu_Unsupervised_Synchrony_Discovery_ICCV_2015_paper.pdf",
        "aff": "Robotics Institute, Carnegie Mellon University + University of Pittsburgh, USA; Beihang University, Beijing, China; Robotics Institute, Carnegie Mellon University; University of Pittsburgh, USA; University of Miami, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2358385,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8850965957965802781&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Chu_Unsupervised_Synchrony_Discovery_ICCV_2015_paper.html",
        "aff_unique_index": "0+1;2;0;1;3",
        "aff_unique_norm": "Carnegie Mellon University;University of Pittsburgh;Beihang University;University of Miami",
        "aff_unique_dep": "Robotics Institute;;;",
        "aff_unique_url": "https://www.cmu.edu;https://www.pitt.edu;http://www.buaa.edu.cn/;https://www.miami.edu",
        "aff_unique_abbr": "CMU;Pitt;BUAA;UM",
        "aff_campus_unique_index": "0;2;0",
        "aff_campus_unique": "Pittsburgh;;Beijing",
        "aff_country_unique_index": "0+0;1;0;0;0",
        "aff_country_unique": "United States;China",
        "bibtex": "@InProceedings{Chu_2015_ICCV,\n    \n    author = {\n    Chu,\n    Wen-Sheng and Zeng,\n    Jiabei and De la Torre,\n    Fernando and Cohn,\n    Jeffrey F. and Messinger,\n    Daniel S.\n},\n    title = {\n    Unsupervised Synchrony Discovery in Human Interaction\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "c281d557de",
        "title": "Unsupervised Trajectory Clustering via Adaptive Multi-Kernel-Based Shrinkage",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Hongteng Xu, Yang Zhou, Weiyao Lin, Hongyuan Zha",
        "author": "Hongteng Xu; Yang Zhou; Weiyao Lin; Hongyuan Zha",
        "abstract": "This paper proposes a shrinkage-based framework for unsupervised trajectory clustering. Facing to the challenges of trajectory clustering, e.g., large variations within a cluster and ambiguities across clusters, we first introduce an adaptive multi-kernel-based estimation process to estimate the `shrunk' positions and speeds of trajectories' points. This kernel-based estimation effectively leverages both multiple structural information within a trajectory and the local motion patterns across multiple trajectories, such that the discrimination of the shrunk point can be properly increased. We further introduce a speed-regularized optimization process, which utilizes the estimated speeds to regularize the optimal shrunk points, so as to guarantee the smoothness and the discriminative pattern of the final shrunk trajectory. Using our approach, the variations among similar trajectories can be reduced while the boundaries between different clusters are enlarged. Experimental results demonstrate that our approach is superior to the state-of-art approaches on both clustering accuracy and robustness. Besides, additional experiments further reveal the effectiveness of our approach when applied to trajectory analysis applications such as anomaly detection.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Xu_Unsupervised_Trajectory_Clustering_ICCV_2015_paper.pdf",
        "aff": "College of Computing, Georgia Tech + School of EE, Georgia Tech; Department of EE, Shanghai Jiao Tong University; Department of EE, Shanghai Jiao Tong University; College of Computing, Georgia Tech",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 821034,
        "gs_citation": 96,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12463284232718591482&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "gatech.edu;sjtu.edu.cn;sjtu.edu.cn;cc.gatech.edu",
        "email": "gatech.edu;sjtu.edu.cn;sjtu.edu.cn;cc.gatech.edu",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Xu_Unsupervised_Trajectory_Clustering_ICCV_2015_paper.html",
        "aff_unique_index": "0+0;1;1;0",
        "aff_unique_norm": "Georgia Institute of Technology;Shanghai Jiao Tong University",
        "aff_unique_dep": "College of Computing;Department of EE",
        "aff_unique_url": "https://www.gatech.edu;https://www.sjtu.edu.cn",
        "aff_unique_abbr": "Georgia Tech;SJTU",
        "aff_campus_unique_index": "0+0;0",
        "aff_campus_unique": "Atlanta;",
        "aff_country_unique_index": "0+0;1;1;0",
        "aff_country_unique": "United States;China",
        "bibtex": "@InProceedings{Xu_2015_ICCV,\n    \n    author = {\n    Xu,\n    Hongteng and Zhou,\n    Yang and Lin,\n    Weiyao and Zha,\n    Hongyuan\n},\n    title = {\n    Unsupervised Trajectory Clustering via Adaptive Multi-Kernel-Based Shrinkage\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "0eb4798595",
        "title": "Unsupervised Tube Extraction Using Transductive Learning and Dense Trajectories",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Mihai Marian Puscas, Enver Sangineto, Dubravko Culibrk, Nicu Sebe",
        "author": "Mihai Marian Puscas; Enver Sangineto; Dubravko Culibrk; Nicu Sebe",
        "abstract": "We address the problem of automatic extraction of foreground objects from videos. The goal is to provide a method for unsupervised collection of samples  which can be further used for object detection training without any human intervention. We use the well known Selective Search approach to produce an initial still-image based segmentation of the video frames.   This initial set of proposals is pruned and temporally extended using optical flow and transductive learning.    Specifically, we propose to  use Dense Trajectories in order to robustly match and track candidate boxes over different frames. The obtained box tracks are used to collect samples for  unsupervised training of track-specific detectors. Finally, the detectors are run on the videos  to extract the final tubes. The combination of appearance-based static ''objectness'' (Selective Search), motion information (Dense Trajectories) and transductive learning (detectors are forced to \"overfit\" on the unsupervised data used for training) makes the proposed approach extremely robust. We  outperform state-of-the-art systems by a large margin on common benchmarks used for tube proposal evaluation.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Puscas_Unsupervised_Tube_Extraction_ICCV_2015_paper.pdf",
        "aff": "Department of Information Engineering and Computer Science, University of Trento, Italy; Department of Information Engineering and Computer Science, University of Trento, Italy; Department of Information Engineering and Computer Science, University of Trento, Italy + Department of Industrial Engineering and Management, University of Novi Sad, Serbia; Department of Information Engineering and Computer Science, University of Trento, Italy",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1614134,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5949829725114931517&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "unitn.it;unitn.it;unitn.it;disi.unitn.it",
        "email": "unitn.it;unitn.it;unitn.it;disi.unitn.it",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Puscas_Unsupervised_Tube_Extraction_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0+1;0",
        "aff_unique_norm": "University of Trento;University of Novi Sad",
        "aff_unique_dep": "Department of Information Engineering and Computer Science;Department of Industrial Engineering and Management",
        "aff_unique_url": "https://www.unitn.it;https://www.uns.ac.rs",
        "aff_unique_abbr": "UniTN;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+1;0",
        "aff_country_unique": "Italy;Serbia",
        "bibtex": "@InProceedings{Puscas_2015_ICCV,\n    \n    author = {\n    Puscas,\n    Mihai Marian and Sangineto,\n    Enver and Culibrk,\n    Dubravko and Sebe,\n    Nicu\n},\n    title = {\n    Unsupervised Tube Extraction Using Transductive Learning and Dense Trajectories\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "fc4ab321de",
        "title": "Unsupervised Visual Representation Learning by Context Prediction",
        "session": "learning representations and attributes",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "Carl Doersch, Abhinav Gupta, Alexei A. Efros",
        "author": "Carl Doersch; Abhinav Gupta; Alexei A. Efros",
        "abstract": "This work explores the use of spatial context as a source of free and plentiful supervisory signal for training a rich visual representation.  Given only a large, unlabeled image collection, we extract random pairs of patches from each image and train a convolutional neural net to predict the position of the second patch relative to the first. We argue that doing well on this task requires the model to learn to recognize objects and their parts. We demonstrate that the feature representation learned using this within-image context indeed captures visual similarity across images. For example, this representation allows us to perform unsupervised visual discovery of objects like cats, people, and even birds from the Pascal VOC 2011 detection dataset. Furthermore, we show that the learned ConvNet can be used in the R-CNN framework and provides a significant boost over a randomly-initialized ConvNet, resulting in state-of-the-art performance among algorithms which use only Pascal-provided training set annotations.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Doersch_Unsupervised_Visual_Representation_ICCV_2015_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3765017,
        "gs_citation": 3543,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2604183921270291244&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Doersch_Unsupervised_Visual_Representation_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Doersch_2015_ICCV,\n    \n    author = {\n    Doersch,\n    Carl and Gupta,\n    Abhinav and Efros,\n    Alexei A.\n},\n    title = {\n    Unsupervised Visual Representation Learning by Context Prediction\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "e4b119cefe",
        "title": "VQA: Visual Question Answering",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, Devi Parikh",
        "author": "Stanislaw Antol; Aishwarya Agrawal; Jiasen Lu; Margaret Mitchell; Dhruv Batra; C. Lawrence Zitnick; Devi Parikh",
        "abstract": "We propose the task of free-form and open-ended Visual Question Answering (VQA). Given an image and a natural language question about the image, the task is to provide an accurate natural language answer. Mirroring real-world scenarios, such as helping the visually impaired, both the questions and answers are open-ended. Visual questions selectively target different areas of an image, including background details and underlying context. As a result, a system that succeeds at VQA typically needs a more detailed understanding of the image and complex reasoning than a system producing generic image captions. Moreover, VQA is amenable to automatic evaluation, since many open-ended answers contain only a few words or a closed set of answers that can be provided in a multiple-choice format. We provide a dataset containing  0.25M images,  0.76M questions, and  10M answers (www.visualqa.org), and discuss the information it provides. Numerous baselines for VQA are provided and compared with human performance.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Antol_VQA_Visual_Question_ICCV_2015_paper.pdf",
        "aff": "Virginia Tech; Virginia Tech; Virginia Tech; Microsoft Research; Virginia Tech; Microsoft Research; Virginia Tech",
        "project": "www.visualqa.org",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1864011,
        "gs_citation": 7071,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11651121946809783243&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 31,
        "aff_domain": "vt.edu;vt.edu;vt.edu;microsoft.com;vt.edu;microsoft.com;vt.edu",
        "email": "vt.edu;vt.edu;vt.edu;microsoft.com;vt.edu;microsoft.com;vt.edu",
        "author_num": 7,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Antol_VQA_Visual_Question_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;1;0;1;0",
        "aff_unique_norm": "Virginia Tech;Microsoft Corporation",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.vt.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "VT;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Antol_2015_ICCV,\n    \n    author = {\n    Antol,\n    Stanislaw and Agrawal,\n    Aishwarya and Lu,\n    Jiasen and Mitchell,\n    Margaret and Batra,\n    Dhruv and Zitnick,\n    C. Lawrence and Parikh,\n    Devi\n},\n    title = {\n    VQA: Visual Question Answering\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "e385f4835c",
        "title": "Variational Depth Superresolution Using Example-Based Edge Representations",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "David Ferstl, Matthias Rüther, Horst Bischof",
        "author": "David Ferstl; Matthias Ruther; Horst Bischof",
        "abstract": "In this paper we propose a novel method for depth image superresolution which combines recent advances in example based upsampling with variational superresolution based on a known blur kernel. Most traditional depth superresolution approaches try to use additional high resolution intensity images as guidance for superresolution. In our method we learn a dictionary of edge priors from an external database of high and low resolution examples. In a novel variational sparse coding approach this dictionary is used to infer strong edge priors. Additionally to the traditional sparse coding constraints the difference in the overlap of neighboring edge patches is minimized in our optimization. These edge priors are used in a novel variational superresolution as anisotropic guidance of the higher order regularization. Both the sparse coding and the variational superresolution of the depth are solved based on a primal-dual formulation. In an exhaustive numerical and visual evaluation we show that our method clearly outperforms existing approaches on multiple real and synthetic datasets.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Ferstl_Variational_Depth_Superresolution_ICCV_2015_paper.pdf",
        "aff": "Graz University of Technology; Graz University of Technology; Graz University of Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1340220,
        "gs_citation": 107,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13699924165463632853&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "icg.tugraz.at;icg.tugraz.at;icg.tugraz.at",
        "email": "icg.tugraz.at;icg.tugraz.at;icg.tugraz.at",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Ferstl_Variational_Depth_Superresolution_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Graz University of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.tugraz.at",
        "aff_unique_abbr": "TUGraz",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Austria",
        "bibtex": "@InProceedings{Ferstl_2015_ICCV,\n    \n    author = {\n    Ferstl,\n    David and Ruther,\n    Matthias and Bischof,\n    Horst\n},\n    title = {\n    Variational Depth Superresolution Using Example-Based Edge Representations\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "ead04a502d",
        "title": "Variational PatchMatch MultiView Reconstruction and Refinement",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Philipp Heise, Brian Jensen, Sebastian Klose, Alois Knoll",
        "author": "Philipp Heise; Brian Jensen; Sebastian Klose; Alois Knoll",
        "abstract": "In this work we propose a novel approach to the problem of multi-view stereo reconstruction. Building upon the previously proposed PatchMatch stereo and PM-Huber algorithm we introduce an extension to the multi-view scenario that employs an iterative refinement scheme. Our proposed approach uses an extended and robustified volumetric truncated signed distance function representation, which is advantageous for the fusion of refined depth maps and also for raycasting the current reconstruction estimation together with estimated depth normals into arbitrary camera views. We formulate the combined multi-view stereo reconstruction and refinement as a variational optimization problem. The newly introduced plane based smoothing term in the energy formulation is guided by the current reconstruction confidence and the image contents. Further we propose an extension of the PatchMatch scheme with an additional KLT step to avoid unnecessary sampling iterations. Improper camera poses are corrected by a direct image aligment step that performs robust outlier compensation by means of a recently proposed kernel lifting framework. To speed up the optimization of the variational formulation an adapted scheme is used for faster convergence.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Heise_Variational_PatchMatch_MultiView_ICCV_2015_paper.pdf",
        "aff": "Department of Informatics, Technische Universit ¨at M ¨unchen, Germany; Department of Informatics, Technische Universit ¨at M ¨unchen, Germany; Department of Informatics, Technische Universit ¨at M ¨unchen, Germany; Department of Informatics, Technische Universit ¨at M ¨unchen, Germany",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1270640,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9065417163674858710&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff_domain": "in.tum.de;in.tum.de;in.tum.de;in.tum.de",
        "email": "in.tum.de;in.tum.de;in.tum.de;in.tum.de",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Heise_Variational_PatchMatch_MultiView_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Technische Universität München",
        "aff_unique_dep": "Department of Informatics",
        "aff_unique_url": "https://www.tum.de",
        "aff_unique_abbr": "TUM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Germany",
        "bibtex": "@InProceedings{Heise_2015_ICCV,\n    \n    author = {\n    Heise,\n    Philipp and Jensen,\n    Brian and Klose,\n    Sebastian and Knoll,\n    Alois\n},\n    title = {\n    Variational PatchMatch MultiView Reconstruction and Refinement\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "127bb8978d",
        "title": "Video Matting via Sparse and Low-Rank Representation",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Dongqing Zou, Xiaowu Chen, Guangying Cao, Xiaogang Wang",
        "author": "Dongqing Zou; Xiaowu Chen; Guangying Cao; Xiaogang Wang",
        "abstract": "We introduce a novel method of video matting via sparse and low-rank representation. Previous matting methods [10, 9] introduced a nonlocal prior to estimate the alpha matte and have achieved impressive results on some data. However, on one hand, searching inadequate or excessive samples may miss good samples or introduce noise; on the other hand, it is difficult to construct consistent nonlocal structures for pixels with similar features, yielding spatially and temporally inconsistent video mattes. In this paper, we proposed a novel video matting method to achieve spatially and temporally consistent matting result. Toward this end, a sparse and low-rank representation model is introduced to pursue consistent nonlocal structures for pixels with similar features. The sparse representation is used to adaptively select best samples and accurately construct the nonlocal structures for all pixels, while the low-rank representation is used to globally ensure consistent nonlocal structures for pixels with similar features. The two representations are combined to generate consistent video mattes. Experimental results show that our method has achieved high quality results in a variety of challenging examples featuring illumination changes, feature ambiguity, topology changes, transparency variation, dis-occlusion, fast motion and motion blur.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Zou_Video_Matting_via_ICCV_2015_paper.pdf",
        "aff": "State Key Laboratory of Virtual Reality Technology and Systems; State Key Laboratory of Virtual Reality Technology and Systems; State Key Laboratory of Virtual Reality Technology and Systems; State Key Laboratory of Virtual Reality Technology and Systems",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1650387,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14003739659372986905&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";buaa.edu.cn; ; ",
        "email": ";buaa.edu.cn; ; ",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Zou_Video_Matting_via_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "State Key Laboratory of Virtual Reality Technology and Systems",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Zou_2015_ICCV,\n    \n    author = {\n    Zou,\n    Dongqing and Chen,\n    Xiaowu and Cao,\n    Guangying and Wang,\n    Xiaogang\n},\n    title = {\n    Video Matting via Sparse and Low-Rank Representation\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "04cbaf83f8",
        "title": "Video Restoration Against Yin-Yang Phasing",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Xiaolin Wu, Zhenhao Li, Xiaowei Deng",
        "author": "Xiaolin Wu; Zhenhao Li; Xiaowei Deng",
        "abstract": "A common video degradation problem, which is largely untreated in literature, is what we call Yin-Yang Phasing (YYP).  YYP is characterized by involuntary, dramatic flip-flop in the intensity and possibly chromaticity of an object as the video plays.  Such temporal artifacts occur under ill illumination conditions and are triggered by object or/and camera motions, which mislead the settings of camera's auto-exposure and white point.  In this paper, we investigate the problem and propose a video restoration technique to suppress YYP artifacts and retain temporal consistency of objects appearance via inter-frame, spatially-adaptive, optimal tone mapping. The video quality can be further improved by a novel image enhancer designed in Weber's perception principle and by exploiting the second-order statistics of the scene.  Experimental results are encouraging, pointing to an effective, practical solution for a common but surprisingly understudied problem.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Wu_Video_Restoration_Against_ICCV_2015_paper.pdf",
        "aff": "McMaster University, Canada+Shanghai Jiao Tong University, China; McMaster University, Canada; McMaster University, Canada",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 979871,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:o6Zmu5aIUO8J:scholar.google.com/&scioq=Video+Restoration+Against+Yin-Yang+Phasing&hl=en&as_sdt=0,33",
        "gs_version_total": 6,
        "aff_domain": "mail.ece.mcmaster.ca;mcmaster.ca;mcmaster.ca",
        "email": "mail.ece.mcmaster.ca;mcmaster.ca;mcmaster.ca",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Wu_Video_Restoration_Against_ICCV_2015_paper.html",
        "aff_unique_index": "0+1;0;0",
        "aff_unique_norm": "McMaster University;Shanghai Jiao Tong University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.mcmaster.ca;https://www.sjtu.edu.cn",
        "aff_unique_abbr": "McMaster;SJTU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0;0",
        "aff_country_unique": "Canada;China",
        "bibtex": "@InProceedings{Wu_2015_ICCV,\n    \n    author = {\n    Wu,\n    Xiaolin and Li,\n    Zhenhao and Deng,\n    Xiaowei\n},\n    title = {\n    Video Restoration Against Yin-Yang Phasing\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "cbcd6ea644",
        "title": "Video Segmentation With Just a Few Strokes",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Naveen Shankar Nagaraja, Frank R. Schmidt, Thomas Brox",
        "author": "Naveen Shankar Nagaraja; Frank R. Schmidt; Thomas Brox",
        "abstract": "As the use of videos is becoming more popular in computer vision, the need for annotated video datasets increases.  Such datasets are required either as training data or simply as ground truth for benchmark datasets. A particular challenge in video segmentation is due to disocclusions, which hamper frame-to-frame propagation, in conjunction with non-moving objects. We show that a combination of motion from point trajectories, as known from motion segmentation, along with minimal supervision can largely help solve this problem. Moreover, we integrate a new constraint that enforces consistency of the color distribution in successive frames. We quantify user interaction effort with respect to segmentation quality on challenging ego motion videos.  We compare our approach to a diverse set of algorithms in terms of user effort and in terms of performance on common video segmentation benchmarks.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Nagaraja_Video_Segmentation_With_ICCV_2015_paper.pdf",
        "aff": "Computer Vision Group, University of Freiburg, Germany; Computer Vision Group, University of Freiburg, Germany; Computer Vision Group, University of Freiburg, Germany",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1580481,
        "gs_citation": 147,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9548103718584520585&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff_domain": "cs.uni-freiburg.de;cs.uni-freiburg.de;cs.uni-freiburg.de",
        "email": "cs.uni-freiburg.de;cs.uni-freiburg.de;cs.uni-freiburg.de",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Nagaraja_Video_Segmentation_With_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Freiburg",
        "aff_unique_dep": "Computer Vision Group",
        "aff_unique_url": "https://www.uni-freiburg.de",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Freiburg",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany",
        "bibtex": "@InProceedings{Nagaraja_2015_ICCV,\n    \n    author = {\n    Nagaraja,\n    Naveen Shankar and Schmidt,\n    Frank R. and Brox,\n    Thomas\n},\n    title = {\n    Video Segmentation With Just a Few Strokes\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "f6b28da01e",
        "title": "Video Super-Resolution via Deep Draft-Ensemble Learning",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Renjie Liao, Xin Tao, Ruiyu Li, Ziyang Ma, Jiaya Jia",
        "author": "Renjie Liao; Xin Tao; Ruiyu Li; Ziyang Ma; Jiaya Jia",
        "abstract": "We propose a new direction for fast video super-resolution (VideoSR) via a SR draft ensemble, which is defined as the set of high-resolution patch candidates before final image deconvolution. Our method contains two main components -- i.e., SR draft ensemble generation and its optimal reconstruction. The first component is to renovate traditional feedforward reconstruction pipeline and greatly enhance its ability to compute different super resolution results considering large motion variation and possible errors arising in this process. Then we combine SR drafts through the nonlinear process in a deep convolutional neural network (CNN). We analyze why this framework is proposed and explain its unique advantages compared to previous iterative methods to update different modules in passes. Promising experimental results are shown on natural video sequences.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Liao_Video_Super-Resolution_via_ICCV_2015_paper.pdf",
        "aff": "The Chinese University of Hong Kong; The Chinese University of Hong Kong; The Chinese University of Hong Kong; University of Chinese Academy of Sciences; The Chinese University of Hong Kong",
        "project": "http://www.cse.cuhk.edu.hk/leojia/projects/DeepSR/",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2867407,
        "gs_citation": 313,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6027865473581409539&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Liao_Video_Super-Resolution_via_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "The Chinese University of Hong Kong;University of Chinese Academy of Sciences",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cuhk.edu.hk;http://www.ucas.ac.cn",
        "aff_unique_abbr": "CUHK;UCAS",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Liao_2015_ICCV,\n    \n    author = {\n    Liao,\n    Renjie and Tao,\n    Xin and Li,\n    Ruiyu and Ma,\n    Ziyang and Jia,\n    Jiaya\n},\n    title = {\n    Video Super-Resolution via Deep Draft-Ensemble Learning\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "50e61016e5",
        "title": "Visual Madlibs: Fill in the Blank Description Generation and Question Answering",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Licheng Yu, Eunbyung Park, Alexander C. Berg, Tamara L. Berg",
        "author": "Licheng Yu; Eunbyung Park; Alexander C. Berg; Tamara L. Berg",
        "abstract": "In this paper, we introduce a new dataset consisting of 360,001 focused natural language descriptions for 10,738 images.  This dataset, the Visual Madlibs dataset, is collected using automatically produced fill-in-the-blank templates designed to gather targeted descriptions about: people and objects, their appearances, activities, and interactions, as well as inferences about the general scene or its broader context. We provide several analyses of the Visual Madlibs dataset and demonstrate its applicability to two new description generation tasks: focused description generation, and multiple-choice question-answering for images. Experiments using joint-embedding and deep learning methods show promising results on these tasks.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Yu_Visual_Madlibs_Fill_ICCV_2015_paper.pdf",
        "aff": "Department of Computer Science, University of North Carolina, Chapel Hill; Department of Computer Science, University of North Carolina, Chapel Hill; Department of Computer Science, University of North Carolina, Chapel Hill; Department of Computer Science, University of North Carolina, Chapel Hill",
        "project": "http://tamaraberg.com/visualmadlibs/",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1268426,
        "gs_citation": 181,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11624169766078608301&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff_domain": "cs.unc.edu;cs.unc.edu;cs.unc.edu;cs.unc.edu",
        "email": "cs.unc.edu;cs.unc.edu;cs.unc.edu;cs.unc.edu",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Yu_Visual_Madlibs_Fill_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of North Carolina",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.unc.edu",
        "aff_unique_abbr": "UNC",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Chapel Hill",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Yu_2015_ICCV,\n    \n    author = {\n    Yu,\n    Licheng and Park,\n    Eunbyung and Berg,\n    Alexander C. and Berg,\n    Tamara L.\n},\n    title = {\n    Visual Madlibs: Fill in the Blank Description Generation and Question Answering\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "72cfa2c14b",
        "title": "Visual Phrases for Exemplar Face Detection",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Vijay Kumar, Anoop Namboodiri, C. V. Jawahar",
        "author": "Vijay Kumar; Anoop Namboodiri; C. V. Jawahar",
        "abstract": "Recently, exemplar based approaches have been successfully applied for face detection in the wild. Contrary to traditional approaches that model face variations from a large and diverse set of training examples, exemplar-based approaches use a collection of discriminatively trained exemplars for detection. In this paradigm, each exemplar casts a vote using retrieval framework and generalized Hough voting, to locate the faces in the target image. The advantage of this approach is that by having a large database that covers all possible variations, faces in challenging conditions can be detected without having to learn explicit models for different variations.  Current schemes, however, make an assumption of independence between the visual words, ignoring their relations in the process. They also ignore the spatial consistency of the visual words. Consequently, every exemplar word contributes equally during voting regardless of its location. In this paper, we propose a novel approach that incorporates higher order information in the voting process. We discover visual phrases that contain semantically related visual words and exploit them for detection along with the visual words. For spatial consistency, we estimate the spatial distribution of visual words and phrases from the entire database and then weigh their occurrence in exemplars. This ensures that a visual word or a phrase in an exemplar makes a major contribution only if it occurs at its semantic location, thereby suppressing the noise significantly. We perform extensive experiments on standard FDDB, AFW and G-album datasets and show significant improvement over previous exemplar approaches.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Kumar_Visual_Phrases_for_ICCV_2015_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2671822,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5685427367066475070&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Kumar_Visual_Phrases_for_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Kumar_2015_ICCV,\n    \n    author = {\n    Kumar,\n    Vijay and Namboodiri,\n    Anoop and Jawahar,\n    C. V.\n},\n    title = {\n    Visual Phrases for Exemplar Face Detection\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "c8196b3985",
        "title": "Visual Tracking With Fully Convolutional Networks",
        "session": "statistical methods and learning, motion and tracking, and video analysis i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Lijun Wang, Wanli Ouyang, Xiaogang Wang, Huchuan Lu",
        "author": "Lijun Wang; Wanli Ouyang; Xiaogang Wang; Huchuan Lu",
        "abstract": "We propose a new approach for general object tracking with fully convolutional neural network. Instead of treating convolutional neural network (CNN) as a black-box feature extractor, we conduct in-depth study on the properties of CNN features offline pre-trained on massive image data and classification task on ImageNet. The discoveries motivate the design of our tracking system. It is found that convolutional layers in different levels characterize the target from different perspectives. A top layer encodes more semantic features and serves as a category detector, while a lower layer carries more discriminative information and can better separate the target from distracters with similar appearance. Both layers are jointly used with a switch mechanism during tracking. It is also found that for a tracking target, only a subset of neurons are relevant. A feature map selection method is developed to remove noisy and irrelevant feature maps, which can reduce computation redundancy and improve tracking accuracy. Extensive evaluation on the widely used tracking benchmark shows that the proposed tacker outperforms the state-of-the-art significantly.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Wang_Visual_Tracking_With_ICCV_2015_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1053481,
        "gs_citation": 1276,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17022027794987507287&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Wang_Visual_Tracking_With_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Wang_2015_ICCV,\n    \n    author = {\n    Wang,\n    Lijun and Ouyang,\n    Wanli and Wang,\n    Xiaogang and Lu,\n    Huchuan\n},\n    title = {\n    Visual Tracking With Fully Convolutional Networks\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "41c5937bd3",
        "title": "Volumetric Bias in Segmentation and Reconstruction: Secrets and Solutions",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Yuri Boykov, Hossam Isack, Carl Olsson, Ismail Ben Ayed",
        "author": "Yuri Boykov; Hossam Isack; Carl Olsson; Ismail Ben Ayed",
        "abstract": "Many standard optimization methods for segmentation and reconstruction compute ML model estimates for appearance or geometry of segments, e.g. Zhu-Yuille 1996, Torr 1998,  Chan-Vese 2001, GrabCut 2004, Delong et al. 2012.  We observe that the standard likelihood term in these formulations corresponds to a generalized probabilistic K-means energy. In learning it is well known that this energy has a strong bias to clusters of equal size, which we express as a penalty for KL divergence from a uniform distribution of cardinalities. However, this volumetric bias has been mostly ignored in computer vision. We demonstrate significant artifacts in standard segmentation and reconstruction methods due to this bias. Moreover, we propose binary and multi-label optimization techniques that either (a) remove this bias or (b) replace it by a KL divergence term for any given target volume distribution. Our general ideas apply to continuous or discrete energy formulations in segmentation, stereo, and other reconstruction problems.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Boykov_Volumetric_Bias_in_ICCV_2015_paper.pdf",
        "aff": "University of Western Ontario, Canada; University of Western Ontario, Canada; Lund University, Sweden; École de Technologie Supérieure, University of Quebec, Canada",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1258273,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10984831098424430235&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff_domain": "csd.uwo.ca;csd.uwo.ca;maths.lth.se;etsmtl.ca",
        "email": "csd.uwo.ca;csd.uwo.ca;maths.lth.se;etsmtl.ca",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Boykov_Volumetric_Bias_in_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;1;2",
        "aff_unique_norm": "University of Western Ontario;Lund University;University of Quebec",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.uwo.ca;https://www.lunduniversity.lu.se;https://www.etsmtl.ca",
        "aff_unique_abbr": "UWO;LU;ETS",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";École de Technologie Supérieure",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "Canada;Sweden",
        "bibtex": "@InProceedings{Boykov_2015_ICCV,\n    \n    author = {\n    Boykov,\n    Yuri and Isack,\n    Hossam and Olsson,\n    Carl and Ben Ayed,\n    Ismail\n},\n    title = {\n    Volumetric Bias in Segmentation and Reconstruction: Secrets and Solutions\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "f1bf0bf418",
        "title": "Weakly Supervised Graph Based Semantic Segmentation by Learning Communities of Image-Parts",
        "session": "segmentation, edges and saliency",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "Niloufar Pourian, S. Karthikeyan, B.S. Manjunath",
        "author": "Niloufar Pourian; S. Karthikeyan; B.S. Manjunath",
        "abstract": "We present a weakly-supervised approach to semantic segmentation. The goal is to assign pixel-level labels given only partial information, for example, image-level labels. This is an important problem in many application scenarios where it is difficult to get accurate segmentation or not feasible to obtain detailed annotations. The proposed approach starts with an initial coarse segmentation, followed by a spectral clustering approach that groups related image parts into communities. A community-driven graph is then constructed that captures spatial and feature relationships between communities while a label graph captures correlations between image labels. Finally, mapping the image level labels to appropriate communities is formulated as a convex optimization problem. The proposed approach does not require location information for image level labels and can be trained using partially labeled datasets. Compared to the state-of-the-art weakly supervised approaches, we achieve a significant performance improvement of 9% on MSRC-21 dataset and 11% on LabelMe dataset, while being more than 300 times faster.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Pourian_Weakly_Supervised_Graph_ICCV_2015_paper.pdf",
        "aff": "Department of Electrical and Computer Engineering, University of California, Santa Barbara, CA; Department of Electrical and Computer Engineering, University of California, Santa Barbara, CA; Department of Electrical and Computer Engineering, University of California, Santa Barbara, CA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1190705,
        "gs_citation": 60,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12550377857681989354&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "ece.ucsb.edu;ece.ucsb.edu;ece.ucsb.edu",
        "email": "ece.ucsb.edu;ece.ucsb.edu;ece.ucsb.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Pourian_Weakly_Supervised_Graph_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Santa Barbara",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.ucsb.edu",
        "aff_unique_abbr": "UCSB",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Santa Barbara",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Pourian_2015_ICCV,\n    \n    author = {\n    Pourian,\n    Niloufar and Karthikeyan,\n    S. and Manjunath,\n    B.S.\n},\n    title = {\n    Weakly Supervised Graph Based Semantic Segmentation by Learning Communities of Image-Parts\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "c2d988d5ae",
        "title": "Weakly- and Semi-Supervised Learning of a Deep Convolutional Network for Semantic Image Segmentation",
        "session": "optimization, segmentation, and recognition",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "George Papandreou, Liang-Chieh Chen, Kevin P. Murphy, Alan L. Yuille",
        "author": "George Papandreou; Liang-Chieh Chen; Kevin P. Murphy; Alan L. Yuille",
        "abstract": "Deep convolutional neural networks (DCNNs) trained on a large number of images with strong pixel-level annotations have recently significantly pushed the state-of-art in semantic image segmentation. We study the more challenging problem of learning DCNNs for semantic image segmentation from either (1) weakly annotated training data such as bounding boxes or image-level labels or (2) a combination of few strongly labeled and many weakly labeled images, sourced from one or multiple datasets. We develop Expectation-Maximization (EM) methods for semantic image segmentation model training under these weakly supervised and semi-supervised settings. Extensive experimental evaluation shows that the proposed techniques can learn models delivering competitive results on the challenging PASCAL VOC 2012 image segmentation benchmark, while requiring significantly less annotation effort. We share source code implementing the proposed system at https://bitbucket.org/deeplab/deeplab-public.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Papandreou_Weakly-_and_Semi-Supervised_ICCV_2015_paper.pdf",
        "aff": "Google, Inc.; UCLA; Google, Inc.; UCLA",
        "project": "https://bitbucket.org/deeplab/deeplab-public",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1156707,
        "gs_citation": 1612,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3675119962220609031&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "google.com;cs.ucla.edu;google.com;stat.ucla.edu",
        "email": "google.com;cs.ucla.edu;google.com;stat.ucla.edu",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Papandreou_Weakly-_and_Semi-Supervised_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "Google;University of California, Los Angeles",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.google.com;https://www.ucla.edu",
        "aff_unique_abbr": "Google;UCLA",
        "aff_campus_unique_index": "0;1;0;1",
        "aff_campus_unique": "Mountain View;Los Angeles",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Papandreou_2015_ICCV,\n    \n    author = {\n    Papandreou,\n    George and Chen,\n    Liang-Chieh and Murphy,\n    Kevin P. and Yuille,\n    Alan L.\n},\n    title = {\n    Weakly- and Semi-Supervised Learning of a Deep Convolutional Network for Semantic Image Segmentation\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "427f4e76cd",
        "title": "Weakly-Supervised Alignment of Video With Text",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Piotr Bojanowski, Rémi Lajugie, Edouard Grave, Francis Bach, Ivan Laptev, Jean Ponce, Cordelia Schmid",
        "author": "Piotr Bojanowski; Remi Lajugie; Edouard Grave; Francis Bach; Ivan Laptev; Jean Ponce; Cordelia Schmid",
        "abstract": "Suppose that we are given a set of videos, along with natural language descriptions in the form of multiple sentences (e.g., manual annotations, movie scripts, sport summaries etc.), and that these sentences appear in the same temporal order as their visual counterparts. We propose in this paper a method for aligning the two modalities, i.e., automatically providing a time (frame) stamp for every sentence. Given vectorial features for both video and text, this can be cast as a temporal assignment problem, with an implicit linear mapping between the two feature modalities. We formulate this problem as an integer quadratic program, and solve its continuous convex relaxation using an efficient conditional gradient algorithm. Several rounding procedures are proposed to construct the final integer solution. After demonstrating significant improvements over the state of the art on the related task of aligning video with symbolic labels, we  evaluate our method on a challenging dataset of videos with associated textual descriptions, and explore bag-of-words and continuous representations for text.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Bojanowski_Weakly-Supervised_Alignment_of_ICCV_2015_paper.pdf",
        "aff": ";;;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1443300,
        "gs_citation": 171,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13343505114683397671&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 41,
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "author_num": 7,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Bojanowski_Weakly-Supervised_Alignment_of_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Bojanowski_2015_ICCV,\n    \n    author = {\n    Bojanowski,\n    Piotr and Lajugie,\n    Remi and Grave,\n    Edouard and Bach,\n    Francis and Laptev,\n    Ivan and Ponce,\n    Jean and Schmid,\n    Cordelia\n},\n    title = {\n    Weakly-Supervised Alignment of Video With Text\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "5e60364166",
        "title": "Weakly-Supervised Structured Output Learning With Flexible and Latent Graphs Using High-Order Loss Functions",
        "session": "recognition, low-level vision, and biomedical image analysis",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Gustavo Carneiro, Tingying Peng, Christine Bayer, Nassir Navab",
        "author": "Gustavo Carneiro; Tingying Peng; Christine Bayer; Nassir Navab",
        "abstract": "We introduce two new structured output models that use a latent graph, which is flexible in terms of the number of nodes and structure, where the training process minimises a high-order loss function using a weakly annotated training set. These models are developed in the context of microscopy imaging of malignant tumours, where the estimation of the number and proportion of classes of microcirculatory supply units (MCSU) is important in the assessment of the efficacy of common cancer treatments (an MCSU is a region of the tumour tissue supplied by a microvessel).  The proposed methodologies take as input multimodal microscopy images of a tumour, and estimate the number and proportion of MCSU classes. This estimation is facilitated by the use of an underlying latent graph (not present in the manual annotations), where each MCSU is represented by a node in this graph, labelled with the MCSU class and image location. The training process uses the manual weak annotations available, consisting of the number of MCSU classes per training image, where the training objective is the minimisation of a high-order loss function based on the norm of the error between the manual and estimated  annotations.  One of the models proposed is based on a new flexible latent structure support vector machine (FLSSVM) and the other is based on a deep convolutional neural network (DCNN) model. Using a dataset of 89 weakly annotated pairs of multimodal images from eight tumours, we show that the quantitative results from DCNN are superior, but the qualitative results from FLSSVM are better and both display high correlation values regarding the number and proportion of MCSU classes compared to the manual annotations.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Carneiro_Weakly-Supervised_Structured_Output_ICCV_2015_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 5487712,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10372716539922950064&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Carneiro_Weakly-Supervised_Structured_Output_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Carneiro_2015_ICCV,\n    \n    author = {\n    Carneiro,\n    Gustavo and Peng,\n    Tingying and Bayer,\n    Christine and Navab,\n    Nassir\n},\n    title = {\n    Weakly-Supervised Structured Output Learning With Flexible and Latent Graphs Using High-Order Loss Functions\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "58e0786449",
        "title": "Web-Scale Image Clustering Revisited",
        "session": "statistical methods and learning",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "Yannis Avrithis, Yannis Kalantidis, Evangelos Anagnostopoulos, Ioannis Z. Emiris",
        "author": "Yannis Avrithis; Yannis Kalantidis; Evangelos Anagnostopoulos; Ioannis Z. Emiris",
        "abstract": "Large scale duplicate detection, clustering and mining of documents or images has been conventionally treated with seed detection via hashing, followed by seed growing heuristics using fast search. Principled clustering methods, especially kernelized and spectral ones, have higher complexity and are difficult to scale above millions. Under the assumption of documents or images embedded in Euclidean space, we revisit recent advances in approximate k-means variants, and borrow their best ingredients to introduce a new one, inverted-quantized k-means (IQ-means). Key underlying concepts are quantization of data points and multi-index based inverted search from centroids to cells. Its quantization is a form of hashing and analogous to seed detection, while its updates are analogous to seed growing, yet principled in the sense of distortion minimization. We further design a dynamic variant that is able to determine the number of clusters k in a single run at nearly zero additional cost. Combined with powerful deep learned representations, we achieve clustering of a 100 million image collection on a single machine in less than one hour.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Avrithis_Web-Scale_Image_Clustering_ICCV_2015_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1242289,
        "gs_citation": 47,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15368968510247268806&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Avrithis_Web-Scale_Image_Clustering_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Avrithis_2015_ICCV,\n    \n    author = {\n    Avrithis,\n    Yannis and Kalantidis,\n    Yannis and Anagnostopoulos,\n    Evangelos and Emiris,\n    Ioannis Z.\n},\n    title = {\n    Web-Scale Image Clustering Revisited\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "a5bf5afaa6",
        "title": "Webly Supervised Learning of Convolutional Networks",
        "session": "learning representations and attributes",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "Xinlei Chen, Abhinav Gupta",
        "author": "Xinlei Chen; Abhinav Gupta",
        "abstract": "We present an approach to utilize large amounts of web data for learning CNNs. Specifically inspired by curriculum learning, we present a two-step approach for CNN training. First, we use easy images to train an initial visual representation. We then use this initial CNN and adapt it to harder, more realistic images by leveraging the structure of data and categories. We demonstrate that our two-stage CNN outperforms a fine-tuned CNN trained on ImageNet on Pascal VOC 2012. We also demonstrate the strength of webly supervised learning by localizing objects in web images and training a R-CNN style detector. It achieves the best performance on VOC 2007 where no VOC training data is used. Finally, we show our approach is quite robust to noise and performs comparably even when we use image search results from March 2013 (pre-CNN image search era).",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Chen_Webly_Supervised_Learning_ICCV_2015_paper.pdf",
        "aff": ";",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 472,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3840358374055397013&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Chen_Webly_Supervised_Learning_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Chen_2015_ICCV,\n    \n    author = {\n    Chen,\n    Xinlei and Gupta,\n    Abhinav\n},\n    title = {\n    Webly Supervised Learning of Convolutional Networks\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "14e8b92c42",
        "title": "What Makes Tom Hanks Look Like Tom Hanks",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Supasorn Suwajanakorn, Steven M. Seitz, Ira Kemelmacher-Shlizerman",
        "author": "Supasorn Suwajanakorn; Steven M. Seitz; Ira Kemelmacher-Shlizerman",
        "abstract": "We reconstruct a controllable model of a person from a large photo collection that captures his or her persona, i.e., physical appearance and behavior.  The ability to operate on unstructured photo collections enables modeling a huge number of people, including celebrities and other well photographed people without requiring them to be scanned.  Moreover, we show the ability to drive or puppeteer the captured person B using any other video of a different person A.  In this scenario, B acts out the role of person A, but retains his/her own personality and character.  Our system is based on a novel combination of 3D face reconstruction, tracking, alignment, and multi-texture modeling, applied to the puppeteering problem.  We demonstrate convincing results on a large variety of celebrities derived from Internet imagery and video.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Suwajanakorn_What_Makes_Tom_ICCV_2015_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1709178,
        "gs_citation": 102,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4462780279055685332&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Suwajanakorn_What_Makes_Tom_ICCV_2015_paper.html",
        "bibtex": "@InProceedings{Suwajanakorn_2015_ICCV,\n    \n    author = {\n    Suwajanakorn,\n    Supasorn and Seitz,\n    Steven M. and Kemelmacher-Shlizerman,\n    Ira\n},\n    title = {\n    What Makes Tom Hanks Look Like Tom Hanks\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "482462f5b8",
        "title": "What Makes an Object Memorable?",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Rachit Dubey, Joshua Peterson, Aditya Khosla, Ming-Hsuan Yang, Bernard Ghanem",
        "author": "Rachit Dubey; Joshua Peterson; Aditya Khosla; Ming-Hsuan Yang; Bernard Ghanem",
        "abstract": "Recent studies on image memorability have shed light on what distinguishes the memorability of different images and the intrinsic and extrinsic properties that make those images memorable. However, a clear understanding of the memorability of specific objects inside an image remains elusive. In this paper, we provide the first attempt to answer the question: what exactly is remembered about an image? We augment both the images and object segmentations from the PASCAL-S dataset with ground truth memorability scores and shed light on the various factors and properties that make an object memorable (or forgettable) to humans. We analyze various visual factors that may influence object memorability (e.g. color, visual saliency, and object categories). We also study the correlation between object and image memorability and find that image memorability is greatly affected by the memorability of its most memorable object. Lastly, we explore the effectiveness of deep learning and other computational approaches in predicting object memorability in images. Our efforts offer a deeper understanding of memorability in general thereby opening up avenues for a wide variety of applications.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Dubey_What_Makes_an_ICCV_2015_paper.pdf",
        "aff": "King Abdullah University of Science and Technology; University of California, Berkeley; Massachusetts Institute of Technology; University of California, Merced; King Abdullah University of Science and Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3223979,
        "gs_citation": 150,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12966247741668000930&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Dubey_What_Makes_an_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;2;3;0",
        "aff_unique_norm": "King Abdullah University of Science and Technology;University of California, Berkeley;Massachusetts Institute of Technology;University of California, Merced",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.kast.kau.edu.sa;https://www.berkeley.edu;https://web.mit.edu;https://www.ucmerced.edu",
        "aff_unique_abbr": "KAUST;UC Berkeley;MIT;UC Merced",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Berkeley;Merced",
        "aff_country_unique_index": "0;1;1;1;0",
        "aff_country_unique": "Saudi Arabia;United States",
        "bibtex": "@InProceedings{Dubey_2015_ICCV,\n    \n    author = {\n    Dubey,\n    Rachit and Peterson,\n    Joshua and Khosla,\n    Aditya and Yang,\n    Ming-Hsuan and Ghanem,\n    Bernard\n},\n    title = {\n    What Makes an Object Memorable?\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "8d32c44d2e",
        "title": "Where to Buy It: Matching Street Clothing Photos in Online Shops",
        "session": "vision and people",
        "status": "Oral",
        "track": "main",
        "pid": "",
        "author_site": "M. Hadi Kiapour, Xufeng Han, Svetlana Lazebnik, Alexander C. Berg, Tamara L. Berg",
        "author": "M. Hadi Kiapour; Xufeng Han; Svetlana Lazebnik; Alexander C. Berg; Tamara L. Berg",
        "abstract": "In this paper, we define a new task, Exact Street to Shop, where our goal is to match a real-world example of a garment item to the same item in an online shop. This is an extremely challenging task due to visual differences between street photos (pictures of people wearing clothing in everyday uncontrolled settings) and online shop photos (pictures of clothing items on people, mannequins, or in isolation, captured by professionals in more controlled settings). We collect a new dataset for this application containing  404,683 shop photos collected from 25 different online retailers and 20,357 street photos,  providing a total of  39,479 clothing item matches between street and shop photos. We develop three different methods for Exact Street to Shop retrieval, including two deep learning baseline methods, and a method to learn a similarity measure between the street and shop domains. Experiments demonstrate that our learned similarity significantly outperforms our baselines that use existing deep learning based representations.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Kiapour_Where_to_Buy_ICCV_2015_paper.pdf",
        "aff": "University of North Carolina at Chapel Hill; University of North Carolina at Chapel Hill; University of Illinois at Urbana-Champaign; University of North Carolina at Chapel Hill; University of North Carolina at Chapel Hill",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3078553,
        "gs_citation": 578,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17723397459715777138&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 21,
        "aff_domain": "cs.unc.edu;cs.unc.edu;illinois.edu;cs.unc.edu;cs.unc.edu",
        "email": "cs.unc.edu;cs.unc.edu;illinois.edu;cs.unc.edu;cs.unc.edu",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Kiapour_Where_to_Buy_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "University of North Carolina;University of Illinois at Urbana-Champaign",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.unc.edu;https://illinois.edu",
        "aff_unique_abbr": "UNC;UIUC",
        "aff_campus_unique_index": "0;0;1;0;0",
        "aff_campus_unique": "Chapel Hill;Urbana-Champaign",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Kiapour_2015_ICCV,\n    \n    author = {\n    Kiapour,\n    M. Hadi and Han,\n    Xufeng and Lazebnik,\n    Svetlana and Berg,\n    Alexander C. and Berg,\n    Tamara L.\n},\n    title = {\n    Where to Buy It: Matching Street Clothing Photos in Online Shops\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "4f66cacc1c",
        "title": "Wide Baseline Stereo Matching With Convex Bounded Distortion Constraints",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Meirav Galun, Tal Amir, Tal Hassner, Ronen Basri, Yaron Lipman",
        "author": "Meirav Galun; Tal Amir; Tal Hassner; Ronen Basri; Yaron Lipman",
        "abstract": "Finding correspondences in wide baseline setups is a challenging problem. Existing approaches have focused largely on developing better feature descriptors for correspondence and on accurate recovery of epipolar line constraints. This paper focuses on the challenging problem of finding correspondences once approximate epipolar constraints are given. We introduce a novel method that integrates a deformation model. Specifically, we formulate the problem as finding the largest number of corresponding points related by a bounded distortion map that obeys the given epipolar constraints. We show that, while the set of bounded distortion maps is not convex, the subset of maps that obey the epipolar line constraints is convex, allowing us to introduce an efficient algorithm for matching. We further utilize a robust cost function for matching and employ majorization-minimization for its optimization. Our experiments indicate that our method finds significantly more accurate maps than existing approaches.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Galun_Wide_Baseline_Stereo_ICCV_2015_paper.pdf",
        "aff": "The Weizmann Institute of Science, Israel; The Weizmann Institute of Science, Israel; The Open University, Israel; The Weizmann Institute of Science, Israel; The Weizmann Institute of Science, Israel",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2351619,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9425701132506696213&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff_domain": "; ; ; ; ",
        "email": "; ; ; ; ",
        "author_num": 5,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Galun_Wide_Baseline_Stereo_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "Weizmann Institute of Science;The Open University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.weizmann.org.il;https://www.openu.ac.il",
        "aff_unique_abbr": "Weizmann;OpenU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Israel",
        "bibtex": "@InProceedings{Galun_2015_ICCV,\n    \n    author = {\n    Galun,\n    Meirav and Amir,\n    Tal and Hassner,\n    Tal and Basri,\n    Ronen and Lipman,\n    Yaron\n},\n    title = {\n    Wide Baseline Stereo Matching With Convex Bounded Distortion Constraints\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "7f87b33046",
        "title": "Wide-Area Image Geolocalization With Aerial Reference Imagery",
        "session": "computational photography, face and gesture, and vision for x",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Scott Workman, Richard Souvenir, Nathan Jacobs",
        "author": "Scott Workman; Richard Souvenir; Nathan Jacobs",
        "abstract": "We propose to use deep convolutional neural networks to address the problem of cross-view image geolocalization, in which the geolocation of a ground-level query image is estimated by matching to georeferenced aerial images. We use state-of-the-art feature representations for ground-level images and introduce a cross-view training approach for learning a joint semantic feature representation for aerial images. We also propose a network architecture that fuses features extracted from aerial images at multiple spatial scales.  To support training these networks, we introduce a massive database that contains pairs of aerial and ground-level images from across the United States.  Our methods significantly out-perform the state of the art on two benchmark datasets. We also show, qualitatively, that the proposed feature representations are discriminative at both local and continental spatial scales.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Workman_Wide-Area_Image_Geolocalization_ICCV_2015_paper.pdf",
        "aff": "University of Kentucky; University of North Carolina at Charlotte; University of Kentucky",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2086594,
        "gs_citation": 404,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13620675612906861413&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "cs.uky.edu;uncc.edu;cs.uky.edu",
        "email": "cs.uky.edu;uncc.edu;cs.uky.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Workman_Wide-Area_Image_Geolocalization_ICCV_2015_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Kentucky;University of North Carolina at Charlotte",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uky.edu;https://www.uncc.edu",
        "aff_unique_abbr": "UK;UNCC",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Charlotte",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Workman_2015_ICCV,\n    \n    author = {\n    Workman,\n    Scott and Souvenir,\n    Richard and Jacobs,\n    Nathan\n},\n    title = {\n    Wide-Area Image Geolocalization With Aerial Reference Imagery\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "4a14e7e7fd",
        "title": "You Are Here: Mimicking the Human Thinking Process in Reading Floor-Plans",
        "session": "recognition and 3d computer vision ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Hang Chu, Dong Ki Kim, Tsuhan Chen",
        "author": "Hang Chu; Dong Ki Kim; Tsuhan Chen",
        "abstract": "A human can easily find his or her way in an unfamiliar building, by walking around and reading the floor-plan. We try to mimic and automate this human thinking process. More precisely, we introduce a new and useful task of locating an user in the floor-plan, by using only a camera and a floor-plan without any other prior information. We address the problem with a novel matching-localization algorithm that is inspired by human logic. We demonstrate through experiments that our method outperforms state-of-the-art floor-plan-based localization methods by a large margin, while also being highly efficient for real-time applications.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Chu_You_Are_Here_ICCV_2015_paper.pdf",
        "aff": "Cornell University; Cornell University; Cornell University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1123158,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11198549021989485711&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "cornell.edu;cornell.edu;ece.cornell.edu",
        "email": "cornell.edu;cornell.edu;ece.cornell.edu",
        "author_num": 3,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Chu_You_Are_Here_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Cornell University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cornell.edu",
        "aff_unique_abbr": "Cornell",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Chu_2015_ICCV,\n    \n    author = {\n    Chu,\n    Hang and Kim,\n    Dong Ki and Chen,\n    Tsuhan\n},\n    title = {\n    You Are Here: Mimicking the Human Thinking Process in Reading Floor-Plans\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "f101111d81",
        "title": "Zero-Shot Learning via Semantic Similarity Embedding",
        "session": "statistical methods and learning, motion and tracking, and video analysis ii",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Ziming Zhang, Venkatesh Saligrama",
        "author": "Ziming Zhang; Venkatesh Saligrama",
        "abstract": "In this paper we consider a version of the zero-shot learning problem where seen class source and target domain data are provided. The goal during test-time is to accurately predict the class label of an unseen target domain instance based on revealed source domain side information (e.g. attributes) for unseen classes. Our method is based on viewing each source or target data as a mixture of seen class proportions and we postulate that the mixture patterns have to be similar if the two instances belong to the same unseen class. This perspective leads us to learning source/target embedding functions that map an arbitrary source/target domain data into a same semantic space where similarity can be readily measured. We develop a max-margin framework to learn these similarity functions and jointly optimize parameters by means of cross validation. Our test results are compelling, leading to significant improvement in terms of accuracy on most benchmark datasets for zero-shot recognition.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Zhang_Zero-Shot_Learning_via_ICCV_2015_paper.pdf",
        "aff": "Department of Electrical & Computer Engineering, Boston University; Department of Electrical & Computer Engineering, Boston University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1389315,
        "gs_citation": 774,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3847883737916740072&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff_domain": "bu.edu;bu.edu",
        "email": "bu.edu;bu.edu",
        "author_num": 2,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Zhang_Zero-Shot_Learning_via_ICCV_2015_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Boston University",
        "aff_unique_dep": "Department of Electrical & Computer Engineering",
        "aff_unique_url": "https://www.bu.edu",
        "aff_unique_abbr": "BU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States",
        "bibtex": "@InProceedings{Zhang_2015_ICCV,\n    \n    author = {\n    Zhang,\n    Ziming and Saligrama,\n    Venkatesh\n},\n    title = {\n    Zero-Shot Learning via Semantic Similarity Embedding\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    },
    {
        "id": "5ec3fdd6a1",
        "title": "kNN Hashing With Factorized Neighborhood Representation",
        "session": "recognition and 3d computer vision i",
        "status": "Poster",
        "track": "main",
        "pid": "",
        "author_site": "Kun Ding, Chunlei Huo, Bin Fan, Chunhong Pan",
        "author": "Kun Ding; Chunlei Huo; Bin Fan; Chunhong Pan",
        "abstract": "Hashing is very effective for many tasks in reducing the processing time and in compressing massive databases. Although lots of approaches have been developed to learn data-dependent hash functions in recent years, how to learn hash functions to yield good performance with acceptable computational and memory cost is still a challenging problem. Based on the observation that retrieval precision is highly related to the kNN classification accuracy, this paper proposes a novel kNN-based supervised hashing method, which learns hash functions by directly maximizing the kNN accuracy of the Hamming-embedded training data. To make it scalable well to large problem, we propose a factorized neighborhood representation to parsimoniously model the neighborhood relationships inherent in training data. Considering that real-world data are often linearly inseparable, we further kernelize this basic model to improve its performance. As a result, the proposed method is able to learn accurate hashing functions with tolerable computation and storage cost. Experiments on four benchmarks demonstrate that our method outperforms the state-of-the-arts.",
        "pdf": "http://openaccess.thecvf.com/content_iccv_2015/papers/Ding_kNN_Hashing_With_ICCV_2015_paper.pdf",
        "aff": "National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, P.R. China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, P.R. China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, P.R. China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, P.R. China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1186477,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11153372989198028671&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "email": "nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "author_num": 4,
        "oa": "http://openaccess.thecvf.com/content_iccv_2015/html/Ding_kNN_Hashing_With_ICCV_2015_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Automation",
        "aff_unique_url": "http://www.ia.cas.cn",
        "aff_unique_abbr": "CAS",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China",
        "bibtex": "@InProceedings{Ding_2015_ICCV,\n    \n    author = {\n    Ding,\n    Kun and Huo,\n    Chunlei and Fan,\n    Bin and Pan,\n    Chunhong\n},\n    title = {\n    kNN Hashing With Factorized Neighborhood Representation\n},\n    booktitle = {\n    Proceedings of the IEEE International Conference on Computer Vision (ICCV)\n},\n    month = {\n    December\n},\n    year = {\n    2015\n} \n}"
    }
]