[
    {
        "title": "A Bayesian Analysis of the Radioactive Releases of Fukushima",
        "site": "https://proceedings.mlr.press/v22/tomioka12.html",
        "author": "Ryota Tomioka; Morten Mrup",
        "abstract": "The Fukushima Daiichi disaster 11 March, 2011 is considered the largest nuclear accident since the 1986 Chernobyl disaster and has been rated at level 7 on the International Nuclear Event Scale. As different radioactive materials have different effects to human body, it is important to know the types of nuclides and their levels of concentration from the recorded mixture of radiations to well take necessary measures. We presently formulate a Bayesian generative model for the data available on radioactive releases from the Fukushima Daiichi disaster across Japan. The model can infer from the sparsely sampled measurements what nuclides are present as well as their concentration levels. An important property of the proposed model is that it admits unique recovery of the parameters. On synthetic data we demonstrate that our model is able to infer the underlying components and on data from the Fukushima Daiichi plant we establish that the model is able to well account for the data. We further demonstrate how the model extends to include all the available measurements recorded throughout Japan. The model can be considered a first attempt to apply Bayesian learning unsupervised in order to give a more detailed account also of the latent structure present in the data of the Fukushima Daiichi disaster.",
        "bibtex": "@InProceedings{pmlr-v22-tomioka12,\n  title = \t {A Bayesian Analysis of the Radioactive Releases of Fukushima},\n  author = \t {Tomioka, Ryota and Mrup, Morten},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1243--1251},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/tomioka12/tomioka12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/tomioka12.html},\n  abstract = \t {The Fukushima Daiichi disaster 11 March, 2011 is considered the largest nuclear accident since the 1986 Chernobyl disaster and has been rated at level 7 on the International Nuclear Event Scale. As different radioactive materials have different effects to human body, it is important to know the types of nuclides and their levels of concentration from the recorded mixture of radiations to well take necessary measures. We presently formulate a Bayesian generative model for the data available on radioactive releases from the Fukushima Daiichi disaster across Japan. The model can infer from the sparsely sampled measurements what nuclides are present as well as their concentration levels. An important property of the proposed model is that it admits unique recovery of the parameters. On synthetic data we demonstrate that our model is able to infer the underlying components and on data from the Fukushima Daiichi plant we establish that the model is able to well account for the data. We further demonstrate how the model extends to include all the available measurements recorded throughout Japan. The model can be considered a first attempt to apply Bayesian learning unsupervised in order to give a more detailed account also of the latent structure present in the data of the Fukushima Daiichi disaster.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/tomioka12/tomioka12.pdf",
        "supp": "",
        "pdf_size": 2347048,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11713391482354120541&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Mathematical Informatics, University of Tokyo; Section for Cognitive Systems, Technical University of Denmark",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Tokyo;Technical University of Denmark",
        "aff_unique_dep": "Department of Mathematical Informatics;Section for Cognitive Systems",
        "aff_unique_url": "https://www.u-tokyo.ac.jp;https://www.teknologisk.dk",
        "aff_unique_abbr": "UTokyo;DTU",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Tokyo;",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Japan;Denmark"
    },
    {
        "title": "A Composite Likelihood View for Multi-Label Classification",
        "site": "https://proceedings.mlr.press/v22/zhang12b.html",
        "author": "Yi Zhang; Jeff Schneider",
        "abstract": "Given limited training samples, learning to classify multiple labels is challenging. Problem decomposition is widely used in this case, where the original problem is decomposed into a set of easier-to-learn subproblems, and predictions from subproblems are combined to make the final decision. In this paper we show the connection between composite likelihoods and many multi-label decomposition methods, e.g., one-vs-all, one-vs-one, calibrated label ranking, probabilistic classifier chain. This connection holds promise for improving problem decomposition in both the choice of subproblems and the combination of subproblem decisions. As an attempt to exploit this connection, we design a composite marginal method that improves pairwise decomposition. Pairwise label comparisons, which seem to be a natural choice for subproblems, are replaced by bivariate label densities, which are more informative and natural components in a composite likelihood.  For combining subproblem decisions, we propose a new mean-field approximation that minimizes the notion of composite divergence and is potentially more robust to inaccurate estimations in subproblems. Empirical studies on five data sets show that, given limited training samples, the proposed method outperforms many alternatives.",
        "bibtex": "@InProceedings{pmlr-v22-zhang12b,\n  title = \t {A Composite Likelihood View for Multi-Label Classification},\n  author = \t {Zhang, Yi and Schneider, Jeff},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1407--1415},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/zhang12b/zhang12b.pdf},\n  url = \t {https://proceedings.mlr.press/v22/zhang12b.html},\n  abstract = \t {Given limited training samples, learning to classify multiple labels is challenging. Problem decomposition is widely used in this case, where the original problem is decomposed into a set of easier-to-learn subproblems, and predictions from subproblems are combined to make the final decision. In this paper we show the connection between composite likelihoods and many multi-label decomposition methods, e.g., one-vs-all, one-vs-one, calibrated label ranking, probabilistic classifier chain. This connection holds promise for improving problem decomposition in both the choice of subproblems and the combination of subproblem decisions. As an attempt to exploit this connection, we design a composite marginal method that improves pairwise decomposition. Pairwise label comparisons, which seem to be a natural choice for subproblems, are replaced by bivariate label densities, which are more informative and natural components in a composite likelihood.  For combining subproblem decisions, we propose a new mean-field approximation that minimizes the notion of composite divergence and is potentially more robust to inaccurate estimations in subproblems. Empirical studies on five data sets show that, given limited training samples, the proposed method outperforms many alternatives.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/zhang12b/zhang12b.pdf",
        "supp": "",
        "pdf_size": 289490,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4068753988977288676&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "School of Computer Science, Carnegie Mellon University; School of Computer Science, Carnegie Mellon University",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "A Differentially Private Stochastic Gradient Descent Algorithm for Multiparty Classification",
        "site": "https://proceedings.mlr.press/v22/rajkumar12.html",
        "author": "Arun Rajkumar; Shivani Agarwal",
        "abstract": "We consider the problem of developing privacy preserving machine learning algorithms in a distributed multiparty setting. Here different parties own different parts of a data set, and the goal is to learn a classifier from the entire data set without any party revealing any information about the individual data points it owns. Pathak et al (2010) recently proposed a solution to this problem in which each party learns a local classifier from its own data, and a third party then aggregates these classifiers in a privacy-preserving manner using a cryptographic scheme. The generalization performance of their algorithm is sensitive to the number of parties and the relative fractions of data owned by the different parties. In this paper, we describe a new differentially private algorithm for the multiparty setting that uses a stochastic gradient descent based procedure to directly optimize the overall multiparty objective rather than combining classifiers learned from optimizing local objectives. The algorithm achieves a slightly weaker form of differential privacy than that of Pathak et al (2010), but provides improved generalization guarantees that do not depend on the number of parties or the relative sizes of the individual data sets. Experimental results corroborate our theoretical findings.",
        "bibtex": "@InProceedings{pmlr-v22-rajkumar12,\n  title = \t {A Differentially Private Stochastic Gradient Descent Algorithm for Multiparty Classification},\n  author = \t {Rajkumar, Arun and Agarwal, Shivani},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {933--941},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/rajkumar12/rajkumar12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/rajkumar12.html},\n  abstract = \t {We consider the problem of developing privacy preserving machine learning algorithms in a distributed multiparty setting. Here different parties own different parts of a data set, and the goal is to learn a classifier from the entire data set without any party revealing any information about the individual data points it owns. Pathak et al (2010) recently proposed a solution to this problem in which each party learns a local classifier from its own data, and a third party then aggregates these classifiers in a privacy-preserving manner using a cryptographic scheme. The generalization performance of their algorithm is sensitive to the number of parties and the relative fractions of data owned by the different parties. In this paper, we describe a new differentially private algorithm for the multiparty setting that uses a stochastic gradient descent based procedure to directly optimize the overall multiparty objective rather than combining classifiers learned from optimizing local objectives. The algorithm achieves a slightly weaker form of differential privacy than that of Pathak et al (2010), but provides improved generalization guarantees that do not depend on the number of parties or the relative sizes of the individual data sets. Experimental results corroborate our theoretical findings.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/rajkumar12/rajkumar12.pdf",
        "supp": "",
        "pdf_size": 346311,
        "gs_citation": 178,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9938747755272308229&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Science and Automation, Indian Institute of Science, Bangalore 560012, India; Department of Computer Science and Automation, Indian Institute of Science, Bangalore 560012, India",
        "aff_domain": "csa.iisc.ernet.in;csa.iisc.ernet.in",
        "email": "csa.iisc.ernet.in;csa.iisc.ernet.in",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Indian Institute of Science",
        "aff_unique_dep": "Department of Computer Science and Automation",
        "aff_unique_url": "https://www.iisc.ac.in",
        "aff_unique_abbr": "IISc",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Bangalore",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "A Family of MCMC Methods on Implicitly Defined Manifolds",
        "site": "https://proceedings.mlr.press/v22/brubaker12.html",
        "author": "Marcus Brubaker; Mathieu Salzmann; Raquel Urtasun",
        "abstract": "Traditional MCMC methods are only applicable to distributions which can be defined on \\mathbbR^n. However, there exist many application domains where the distributions cannot easily be defined on a Euclidean space. To address this limitation, we propose a general constrained version of Hamiltonian Monte Carlo, and  give conditions under which the Markov chain  is convergent. Based on this general framework we define a family of MCMC methods which can be applied to sample from distributions on non-linear manifolds. We demonstrate the effectiveness of our approach  on a variety of problems including sampling from the Bingham-von Mises-Fisher distribution, collaborative filtering and  human pose estimation.",
        "bibtex": "@InProceedings{pmlr-v22-brubaker12,\n  title = \t {A Family of MCMC Methods on Implicitly Defined Manifolds},\n  author = \t {Brubaker, Marcus and Salzmann, Mathieu and Urtasun, Raquel},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {161--172},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/brubaker12/brubaker12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/brubaker12.html},\n  abstract = \t {Traditional MCMC methods are only applicable to distributions which can be defined on \\mathbbR^n. However, there exist many application domains where the distributions cannot easily be defined on a Euclidean space. To address this limitation, we propose a general constrained version of Hamiltonian Monte Carlo, and  give conditions under which the Markov chain  is convergent. Based on this general framework we define a family of MCMC methods which can be applied to sample from distributions on non-linear manifolds. We demonstrate the effectiveness of our approach  on a variety of problems including sampling from the Bingham-von Mises-Fisher distribution, collaborative filtering and  human pose estimation.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/brubaker12/brubaker12.pdf",
        "supp": "",
        "pdf_size": 596935,
        "gs_citation": 129,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5215735456655847136&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "A General Framework for Structured Sparsity via Proximal Optimization",
        "site": "https://proceedings.mlr.press/v22/baldassarre12.html",
        "author": "Luca Baldassarre; Jean Morales; Andreas Argyriou; Massimiliano Pontil",
        "abstract": "We study a generalized framework for structured sparsity. It extends the well-known methods of Lasso and Group Lasso by incorporating additional constraints on the variables as part of a convex optimization problem. This framework provides a straightforward way of favoring prescribed sparsity patterns, such as orderings, contiguous regions and overlapping groups, among others. Available optimization methods are limited to specific constraint sets and tend to not scale well with sample size and dimensionality.  We propose a first order proximal method, which builds upon results on fixed points and successive approximations. The algorithm can be applied to a general class of conic and norm constraints sets and relies on a proximity operator subproblem which can be computed numerically. Experiments on different regression problems demonstrate state-of-the art statistical performance, which improves over Lasso, Group Lasso and StructOMP. They also demonstrate the efficiency of the optimization algorithm and its scalability with the size of the problem.",
        "bibtex": "@InProceedings{pmlr-v22-baldassarre12,\n  title = \t {A General Framework for Structured Sparsity via Proximal Optimization},\n  author = \t {Baldassarre, Luca and Morales, Jean and Argyriou, Andreas and Pontil, Massimiliano},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {82--90},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/baldassarre12/baldassarre12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/baldassarre12.html},\n  abstract = \t {We study a generalized framework for structured sparsity. It extends the well-known methods of Lasso and Group Lasso by incorporating additional constraints on the variables as part of a convex optimization problem. This framework provides a straightforward way of favoring prescribed sparsity patterns, such as orderings, contiguous regions and overlapping groups, among others. Available optimization methods are limited to specific constraint sets and tend to not scale well with sample size and dimensionality.  We propose a first order proximal method, which builds upon results on fixed points and successive approximations. The algorithm can be applied to a general class of conic and norm constraints sets and relies on a proximity operator subproblem which can be computed numerically. Experiments on different regression problems demonstrate state-of-the art statistical performance, which improves over Lasso, Group Lasso and StructOMP. They also demonstrate the efficiency of the optimization algorithm and its scalability with the size of the problem.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/baldassarre12/baldassarre12.pdf",
        "supp": "",
        "pdf_size": 383922,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7246420842076529747&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "A Hybrid Neural Network-Latent Topic Model",
        "site": "https://proceedings.mlr.press/v22/wan12.html",
        "author": "Li Wan; Leo Zhu; Rob Fergus",
        "abstract": "This paper introduces a hybrid model that combines a neural network with a latent topic model. The neural network provides a low dimensional embedding for the input data, whose subsequent distribution is captured by the topic model. The neural network thus acts as a trainable feature extractor while the topic model captures the group structure of the data. Following an initial pretraining phase to separately initialize each part of the model, a unified training scheme is introduced that allows for discriminative training of the entire model. The approach is evaluated on visual data in scene classification task, where the hybrid model is shown to outperform models based solely on neural networks or topic models, as well as other baseline methods.",
        "bibtex": "@InProceedings{pmlr-v22-wan12,\n  title = \t {A Hybrid Neural Network-Latent Topic Model},\n  author = \t {Wan, Li and Zhu, Leo and Fergus, Rob},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1287--1294},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/wan12/wan12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/wan12.html},\n  abstract = \t {This paper introduces a hybrid model that combines a neural network with a latent topic model. The neural network provides a low dimensional embedding for the input data, whose subsequent distribution is captured by the topic model. The neural network thus acts as a trainable feature extractor while the topic model captures the group structure of the data. Following an initial pretraining phase to separately initialize each part of the model, a unified training scheme is introduced that allows for discriminative training of the entire model. The approach is evaluated on visual data in scene classification task, where the hybrid model is shown to outperform models based solely on neural networks or topic models, as well as other baseline methods.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/wan12/wan12.pdf",
        "supp": "",
        "pdf_size": 379440,
        "gs_citation": 59,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5219028622510770923&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Dept. of Computer Science, Courant institute, New York University; Dept. of Computer Science, Courant institute, New York University; Dept. of Computer Science, Courant institute, New York University",
        "aff_domain": "cs.nyu.edu;cs.nyu.edu;cs.nyu.edu",
        "email": "cs.nyu.edu;cs.nyu.edu;cs.nyu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "New York University",
        "aff_unique_dep": "Dept. of Computer Science",
        "aff_unique_url": "https://www.nyu.edu",
        "aff_unique_abbr": "NYU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "New York",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "A Nonparametric Bayesian Model for Multiple Clustering with Overlapping Feature Views",
        "site": "https://proceedings.mlr.press/v22/niu12.html",
        "author": "Donglin Niu; Jennifer Dy; Zoubin Ghahramani",
        "abstract": "Most clustering algorithms produce a single clustering solution. This is inadequate for many data sets that are multi-faceted and can be grouped and interpreted in many different ways.  Moreover, for high-dimensional data, different features may be relevant or irrelevant to each clustering solution, suggesting the need for feature selection in clustering. Features relevant to one clustering interpretation may be different from the ones relevant for an alternative interpretation or view of the data.  In this paper, we introduce a probabilistic nonparametric Bayesian model that can discover multiple clustering solutions from data and the feature subsets that are relevant for the clusters in each view. In our model, the features in different views may be shared and therefore the sets of relevant features are allowed to overlap.  We model feature relevance to each view using an Indian Buffet Process and the cluster membership in each view using a Chinese Restaurant Process.  We provide an inference approach to learn the latent parameters corresponding to this multiple partitioning problem.  Our model not only learns the features and clusters in each view but also automatically learns the number of clusters, number of views and number of features in each view.",
        "bibtex": "@InProceedings{pmlr-v22-niu12,\n  title = \t {A Nonparametric Bayesian Model for Multiple Clustering with Overlapping Feature Views},\n  author = \t {Niu, Donglin and Dy, Jennifer and Ghahramani, Zoubin},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {814--822},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/niu12/niu12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/niu12.html},\n  abstract = \t {Most clustering algorithms produce a single clustering solution. This is inadequate for many data sets that are multi-faceted and can be grouped and interpreted in many different ways.  Moreover, for high-dimensional data, different features may be relevant or irrelevant to each clustering solution, suggesting the need for feature selection in clustering. Features relevant to one clustering interpretation may be different from the ones relevant for an alternative interpretation or view of the data.  In this paper, we introduce a probabilistic nonparametric Bayesian model that can discover multiple clustering solutions from data and the feature subsets that are relevant for the clusters in each view. In our model, the features in different views may be shared and therefore the sets of relevant features are allowed to overlap.  We model feature relevance to each view using an Indian Buffet Process and the cluster membership in each view using a Chinese Restaurant Process.  We provide an inference approach to learn the latent parameters corresponding to this multiple partitioning problem.  Our model not only learns the features and clusters in each view but also automatically learns the number of clusters, number of views and number of features in each view.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/niu12/niu12.pdf",
        "supp": "",
        "pdf_size": 430589,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16229497634594110548&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 10,
        "aff": "Northeastern University; Northeastern University; University of Cambridge",
        "aff_domain": "ece.neu.edu;ece.neu.edu;eng.cam.ac.uk",
        "email": "ece.neu.edu;ece.neu.edu;eng.cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Northeastern University;University of Cambridge",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.northeastern.edu;https://www.cam.ac.uk",
        "aff_unique_abbr": "NEU;Cambridge",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "title": "A Simple Geometric Interpretation of SVM using Stochastic Adversaries",
        "site": "https://proceedings.mlr.press/v22/livni12.html",
        "author": "Roi Livni; Koby Crammer; Amir Globerson",
        "abstract": "We present a minimax framework for classification that considers stochastic adversarial perturbations to the training data. We show that for binary classification it is equivalent to SVM, but with a very natural interpretation of regularization parameter. In the multiclass case, we obtain that our formulation is equivalent to regularizing the hinge loss with the maximum norm of the weight vector (i.e., the two-infinity norm). We test this new regularization scheme and show that it is competitive with the Frobenius regularization commonly used for multiclass SVM. We proceed to analyze various forms of stochastic perturbations and obtain compact optimization problems for the optimal classifiers. Taken together, our results illustrate the advantage of using stochastic perturbations rather than deterministic ones,  as well as offer a simple geometric interpretation for SVM optimization in the non-separable case.",
        "bibtex": "@InProceedings{pmlr-v22-livni12,\n  title = \t {A Simple Geometric Interpretation of SVM using Stochastic Adversaries},\n  author = \t {Livni, Roi and Crammer, Koby and Globerson, Amir},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {722--730},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/livni12/livni12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/livni12.html},\n  abstract = \t {We present a minimax framework for classification that considers stochastic adversarial perturbations to the training data. We show that for binary classification it is equivalent to SVM, but with a very natural interpretation of regularization parameter. In the multiclass case, we obtain that our formulation is equivalent to regularizing the hinge loss with the maximum norm of the weight vector (i.e., the two-infinity norm). We test this new regularization scheme and show that it is competitive with the Frobenius regularization commonly used for multiclass SVM. We proceed to analyze various forms of stochastic perturbations and obtain compact optimization problems for the optimal classifiers. Taken together, our results illustrate the advantage of using stochastic perturbations rather than deterministic ones,  as well as offer a simple geometric interpretation for SVM optimization in the non-separable case.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/livni12/livni12.pdf",
        "supp": "",
        "pdf_size": 421062,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11468021718269469939&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "ELSC-ICNC Edmond & Lily Safra Center for Brain Sciences, The Hebrew University, Jerusalem, Israel; Department of Electrical Engineering, The Technion, Haifa, Israel; School of Computer Science and Engineering, The Hebrew University, Jerusalem, Israel",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "The Hebrew University;The Technion",
        "aff_unique_dep": "Edmond & Lily Safra Center for Brain Sciences;Department of Electrical Engineering",
        "aff_unique_url": "http://www.huji.ac.il;http://www.technion.ac.il",
        "aff_unique_abbr": "HUJI;Technion",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Jerusalem;Haifa",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "title": "A Stick-Breaking Likelihood for Categorical Data Analysis with Latent Gaussian Models",
        "site": "https://proceedings.mlr.press/v22/khan12.html",
        "author": "Mohammad Khan; Shakir Mohamed; Benjamin Marlin; Kevin Murphy",
        "abstract": "The development of accurate models and efficient algorithms for the analysis of multivariate categorical data are important and long-standing problems in machine learning and computational statistics. In this paper, we focus on modeling categorical data using Latent Gaussian Models (LGMs). We propose a novel stick-breaking likelihood function for categorical LGMs that exploits accurate linear and quadratic bounds on the logistic log-partition function, leading to an effective variational inference and learning framework. We thoroughly compare our approach to existing algorithms for multinomial logit/probit likelihoods on several problems, including inference in multinomial Gaussian process classification and learning in latent factor models. Our extensive comparisons demonstrate that our stick-breaking model effectively captures correlation in discrete data and is well suited for the analysis of categorical data.",
        "bibtex": "@InProceedings{pmlr-v22-khan12,\n  title = \t {A Stick-Breaking Likelihood for Categorical Data Analysis with Latent Gaussian Models},\n  author = \t {Khan, Mohammad and Mohamed, Shakir and Marlin, Benjamin and Murphy, Kevin},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {610--618},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/khan12/khan12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/khan12.html},\n  abstract = \t {The development of accurate models and efficient algorithms for the analysis of multivariate categorical data are important and long-standing problems in machine learning and computational statistics. In this paper, we focus on modeling categorical data using Latent Gaussian Models (LGMs). We propose a novel stick-breaking likelihood function for categorical LGMs that exploits accurate linear and quadratic bounds on the logistic log-partition function, leading to an effective variational inference and learning framework. We thoroughly compare our approach to existing algorithms for multinomial logit/probit likelihoods on several problems, including inference in multinomial Gaussian process classification and learning in latent factor models. Our extensive comparisons demonstrate that our stick-breaking model effectively captures correlation in discrete data and is well suited for the analysis of categorical data.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/khan12/khan12.pdf",
        "supp": "",
        "pdf_size": 397100,
        "gs_citation": 62,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7072730000302614567&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "A Two-Graph Guided Multi-task Lasso Approach for eQTL Mapping",
        "site": "https://proceedings.mlr.press/v22/chen12b.html",
        "author": "Xiaohui Chen; Xinghua Shi; Xing Xu; Zhiyong Wang; Ryan Mills; Charles Lee; Jinbo Xu",
        "abstract": "Learning a small number of genetic variants associated with multiple complex genetic traits is of practical importance and remains challenging due to the high dimensional nature of data. In this paper, we proposed a two-graph guided multi-task Lasso to address this issue with an emphasis on estimating subnetwork-to-subnetwork associations in expression quantitative trait loci (eQTL) mapping. The proposed model can learn such subnetwork-to-subnetwork associations and therefore can be seen as a generalization of several state-of-the-art multi-task feature selection methods. Additionally, this model has a nice property of allowing flexible structured sparsity on both feature and label domains. Simulation study shows the improved performance of our model and a human eQTL data set is analyzed to further demonstrate the applications of the model.",
        "bibtex": "@InProceedings{pmlr-v22-chen12b,\n  title = \t {A Two-Graph Guided Multi-task Lasso Approach for eQTL Mapping},\n  author = \t {Chen, Xiaohui and Shi, Xinghua and Xu, Xing and Wang, Zhiyong and Mills, Ryan and Lee, Charles and Xu, Jinbo},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {208--217},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/chen12b/chen12b.pdf},\n  url = \t {https://proceedings.mlr.press/v22/chen12b.html},\n  abstract = \t {Learning a small number of genetic variants associated with multiple complex genetic traits is of practical importance and remains challenging due to the high dimensional nature of data. In this paper, we proposed a two-graph guided multi-task Lasso to address this issue with an emphasis on estimating subnetwork-to-subnetwork associations in expression quantitative trait loci (eQTL) mapping. The proposed model can learn such subnetwork-to-subnetwork associations and therefore can be seen as a generalization of several state-of-the-art multi-task feature selection methods. Additionally, this model has a nice property of allowing flexible structured sparsity on both feature and label domains. Simulation study shows the improved performance of our model and a human eQTL data set is analyzed to further demonstrate the applications of the model.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/chen12b/chen12b.pdf",
        "supp": "",
        "pdf_size": 363142,
        "gs_citation": 59,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13422743704609527976&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Toyota Technological Institute at Chicago; Brigham Women\u2019s Hosptial and Harvard Medical School; Toyota Technological Institute at Chicago; Toyota Technological Institute at Chicago; Brigham Women\u2019s Hosptial and Harvard Medical School; Brigham Women\u2019s Hosptial and Harvard Medical School; Toyota Technological Institute at Chicago",
        "aff_domain": "; ; ; ; ; ; ",
        "email": "; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0;1;1;0",
        "aff_unique_norm": "Toyota Technological Institute at Chicago;Brigham Women's Hospital",
        "aff_unique_dep": ";Harvard Medical School",
        "aff_unique_url": "https://www.tti-chicago.org;https://www.brighamandwomens.org",
        "aff_unique_abbr": "TTI Chicago;BWH",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Chicago;",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "A Variance Minimization Criterion to Active Learning on Graphs",
        "site": "https://proceedings.mlr.press/v22/ji12.html",
        "author": "Ming Ji; Jiawei Han",
        "abstract": "We consider the problem of active learning over the vertices in a graph, without feature representation. Our study is based on the common graph smoothness assumption, which is formulated in a Gaussian random field model. We analyze the probability distribution over the unlabeled vertices conditioned on the label information, which is a multivariate normal with the mean being the harmonic solution over the field. Then we select the nodes to label such that the total variance of the distribution on the unlabeled data, as well as the expected prediction error, is minimized. In this way, the classifier we obtain is theoretically more robust. Compared with existing methods, our algorithm has the advantage of selecting data in a batch offline mode with solid theoretical support. We show improved performance over existing label selection criteria on several real world data sets.",
        "bibtex": "@InProceedings{pmlr-v22-ji12,\n  title = \t {A Variance Minimization Criterion to Active Learning on Graphs},\n  author = \t {Ji, Ming and Han, Jiawei},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {556--564},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/ji12/ji12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/ji12.html},\n  abstract = \t {We consider the problem of active learning over the vertices in a graph, without feature representation. Our study is based on the common graph smoothness assumption, which is formulated in a Gaussian random field model. We analyze the probability distribution over the unlabeled vertices conditioned on the label information, which is a multivariate normal with the mean being the harmonic solution over the field. Then we select the nodes to label such that the total variance of the distribution on the unlabeled data, as well as the expected prediction error, is minimized. In this way, the classifier we obtain is theoretically more robust. Compared with existing methods, our algorithm has the advantage of selecting data in a batch offline mode with solid theoretical support. We show improved performance over existing label selection criteria on several real world data sets.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/ji12/ji12.pdf",
        "supp": "",
        "pdf_size": 773454,
        "gs_citation": 115,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17016488106730388024&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science, University of Illinois at Urbana-Champaign; Department of Computer Science, University of Illinois at Urbana-Champaign",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Illinois at Urbana-Champaign",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://illinois.edu",
        "aff_unique_abbr": "UIUC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Urbana-Champaign",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "A metric learning perspective of SVM: on the relation of LMNN and SVM",
        "site": "https://proceedings.mlr.press/v22/do12.html",
        "author": "Huyen Do; Alexandros Kalousis; Jun Wang; Adam Woznica",
        "abstract": "Support Vector Machines, SVMs, and the Large Margin Nearest Neighbor algorithm, LMNN, are two very popular learning algorithms with quite different learning biases. In this paper we bring them into a unified view and show that they have a much stronger relation than what is commonly thought. We analyze SVMs from a metric learning perspective and cast them as a metric learning problem, a view which helps us uncover the relations  of the two algorithms. We show that LMNN can be seen as learning a set of local SVM-like models in a quadratic space. Along the way and  inspired by the metric-based interpretation of SVMs we derive a novel variant of SVMs, \u03b5-SVM, to which LMNN is even more similar.  We give a unified view of  LMNN and the different SVM variants. Finally we provide some preliminary experiments on a number of benchmark datasets in which show that \u03b5-SVM compares favorably both with respect to LMNN and SVM.",
        "bibtex": "@InProceedings{pmlr-v22-do12,\n  title = \t {A metric learning perspective of SVM: on the relation of LMNN and SVM},\n  author = \t {Do, Huyen and Kalousis, Alexandros and Wang, Jun and Woznica, Adam},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {308--317},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/do12/do12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/do12.html},\n  abstract = \t {Support Vector Machines, SVMs, and the Large Margin Nearest Neighbor algorithm, LMNN, are two very popular learning algorithms with quite different learning biases. In this paper we bring them into a unified view and show that they have a much stronger relation than what is commonly thought. We analyze SVMs from a metric learning perspective and cast them as a metric learning problem, a view which helps us uncover the relations  of the two algorithms. We show that LMNN can be seen as learning a set of local SVM-like models in a quadratic space. Along the way and  inspired by the metric-based interpretation of SVMs we derive a novel variant of SVMs, \u03b5-SVM, to which LMNN is even more similar.  We give a unified view of  LMNN and the different SVM variants. Finally we provide some preliminary experiments on a number of benchmark datasets in which show that \u03b5-SVM compares favorably both with respect to LMNN and SVM.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/do12/do12.pdf",
        "supp": "",
        "pdf_size": 627763,
        "gs_citation": 59,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17409541632526499723&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Computer Science Dept., University of Geneva, Switzerland + Business Informatics, University of Applied Science, West. Switzerland; Computer Science Dept., University of Geneva, Switzerland; Computer Science Dept., University of Geneva, Switzerland; Computer Science Dept., University of Geneva, Switzerland",
        "aff_domain": "unige.ch;hesge.ch;unige.ch;unige.ch",
        "email": "unige.ch;hesge.ch;unige.ch;unige.ch",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;0;0",
        "aff_unique_norm": "University of Geneva;University of Applied Sciences Western Switzerland",
        "aff_unique_dep": "Computer Science Dept.;Business Informatics",
        "aff_unique_url": "https://www.unige.ch;https://www.hes-so.ch/en",
        "aff_unique_abbr": "UNIGE;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "title": "Active Learning from Multiple Knowledge Sources",
        "site": "https://proceedings.mlr.press/v22/yan12.html",
        "author": "Yan Yan; Romer Rosales; Glenn Fung; Faisal Farooq; Bharat Rao; Jennifer Dy",
        "abstract": "Some supervised learning tasks do not fit the usual single annotator scenario. In these problems, ground-truth may not exist and multiple annotators are generally available. A few approaches have been proposed to address this learning problem. In this setting active learning (AL), the problem of optimally selecting unlabeled samples for labeling, offers new challenges and has received little attention. In multiple annotator AL, it is not sufficient to select a sample for labeling since, in addition, an optimal annotator must also be selected. This setting is of great interest as annotators\u2019 expertise generally varies and could depend on the given sample itself; additionally, some annotators may be adversarial. Thus, clearly the information provided by some annotators should be more valuable than that provided by others and it could vary across data points. We propose an AL approach for this new scenario motivated by information theoretic principles. Specifically, we focus on maximizing the information that an annotator label provides about the true (but unknown) label of the data point. We develop this concept, propose an algorithm for active learning, and experimentally validate the proposed approach.",
        "bibtex": "@InProceedings{pmlr-v22-yan12,\n  title = \t {Active Learning from Multiple Knowledge Sources},\n  author = \t {Yan, Yan and Rosales, Romer and Fung, Glenn and Farooq, Faisal and Rao, Bharat and Dy, Jennifer},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1350--1357},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/yan12/yan12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/yan12.html},\n  abstract = \t {Some supervised learning tasks do not fit the usual single annotator scenario. In these problems, ground-truth may not exist and multiple annotators are generally available. A few approaches have been proposed to address this learning problem. In this setting active learning (AL), the problem of optimally selecting unlabeled samples for labeling, offers new challenges and has received little attention. In multiple annotator AL, it is not sufficient to select a sample for labeling since, in addition, an optimal annotator must also be selected. This setting is of great interest as annotators\u2019 expertise generally varies and could depend on the given sample itself; additionally, some annotators may be adversarial. Thus, clearly the information provided by some annotators should be more valuable than that provided by others and it could vary across data points. We propose an AL approach for this new scenario motivated by information theoretic principles. Specifically, we focus on maximizing the information that an annotator label provides about the true (but unknown) label of the data point. We develop this concept, propose an algorithm for active learning, and experimentally validate the proposed approach.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/yan12/yan12.pdf",
        "supp": "",
        "pdf_size": 405088,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16595692077372680325&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Adaptive MCMC with Bayesian Optimization",
        "site": "https://proceedings.mlr.press/v22/mahendran12.html",
        "author": "Nimalan Mahendran; Ziyu Wang; Firas Hamze; Nando De Freitas",
        "abstract": "This paper proposes a new randomized strategy for adaptive MCMC using Bayesian optimization. This approach applies to non-differentiable objective functions and trades off exploration and exploitation to reduce the number of potentially costly objective function evaluations. We demonstrate the strategy in the complex setting of sampling from constrained, discrete and densely connected probabilistic graphical models where, for each variation of the problem, one needs to adjust the parameters of the proposal mechanism automatically to ensure efficient mixing of the Markov chains.",
        "bibtex": "@InProceedings{pmlr-v22-mahendran12,\n  title = \t {Adaptive MCMC with Bayesian Optimization},\n  author = \t {Mahendran, Nimalan and Wang, Ziyu and Hamze, Firas and Freitas, Nando De},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {751--760},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/mahendran12/mahendran12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/mahendran12.html},\n  abstract = \t {This paper proposes a new randomized strategy for adaptive MCMC using Bayesian optimization. This approach applies to non-differentiable objective functions and trades off exploration and exploitation to reduce the number of potentially costly objective function evaluations. We demonstrate the strategy in the complex setting of sampling from constrained, discrete and densely connected probabilistic graphical models where, for each variation of the problem, one needs to adjust the parameters of the proposal mechanism automatically to ensure efficient mixing of the Markov chains.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/mahendran12/mahendran12.pdf",
        "supp": "",
        "pdf_size": 1654079,
        "gs_citation": 104,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2745755929540686191&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Adaptive Metropolis with Online Relabeling",
        "site": "https://proceedings.mlr.press/v22/bardenet12.html",
        "author": "Remi Bardenet; Olivier Cappe; Gersende Fort; Balazs Kegl",
        "abstract": "We propose a novel adaptive MCMC algorithm named AMOR (Adaptive Metropolis with Online Relabeling) for efficiently simulating from permutation-invariant targets occurring in, for example, Bayesian analysis of mixture models. An important feature of the algorithm is to tie the adaptation of the proposal distribution to the choice of a particular restriction of the target to a domain where label switching cannot occur. The algorithm relies on a stochastic approximation procedure for which we exhibit a Lyapunov function that formally defines the criterion used for selecting the relabeling rule. This criterion reveals an interesting connection with the problem of optimal quantifier design in vector quantization which was only implicit in previous works on the label switching problem. In benchmark examples, the algorithm turns out to be fast converging and efficient at selecting meaningful non-trivial relabeling rules to allow accurate parameter inference.",
        "bibtex": "@InProceedings{pmlr-v22-bardenet12,\n  title = \t {Adaptive Metropolis with Online Relabeling},\n  author = \t {Bardenet, Remi and Cappe, Olivier and Fort, Gersende and Kegl, Balazs},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {91--99},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/bardenet12/bardenet12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/bardenet12.html},\n  abstract = \t {We propose a novel adaptive MCMC algorithm named AMOR (Adaptive Metropolis with Online Relabeling) for efficiently simulating from permutation-invariant targets occurring in, for example, Bayesian analysis of mixture models. An important feature of the algorithm is to tie the adaptation of the proposal distribution to the choice of a particular restriction of the target to a domain where label switching cannot occur. The algorithm relies on a stochastic approximation procedure for which we exhibit a Lyapunov function that formally defines the criterion used for selecting the relabeling rule. This criterion reveals an interesting connection with the problem of optimal quantifier design in vector quantization which was only implicit in previous works on the label switching problem. In benchmark examples, the algorithm turns out to be fast converging and efficient at selecting meaningful non-trivial relabeling rules to allow accurate parameter inference.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/bardenet12/bardenet12.pdf",
        "supp": "",
        "pdf_size": 2034349,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3332638509539326450&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "LAL & LRI, University Paris-Sud; LTCI, Telecom ParisTech & CNRS; LTCI, Telecom ParisTech & CNRS; LAL & LRI, University Paris-Sud & CNRS",
        "aff_domain": "lri.fr;telecom-paristech.fr;telecom-paristech.fr;gmail.com",
        "email": "lri.fr;telecom-paristech.fr;telecom-paristech.fr;gmail.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "University Paris-Sud;Telecom ParisTech",
        "aff_unique_dep": "LAL & LRI;LTCI",
        "aff_unique_url": "https://www.universite-paris-sud.fr;https://www.telecom-paristech.fr",
        "aff_unique_abbr": "UPS;Telecom ParisTech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "France"
    },
    {
        "title": "Age-Layered Expectation Maximization for Parameter Learning in Bayesian Networks",
        "site": "https://proceedings.mlr.press/v22/saluja12.html",
        "author": "Avneesh Saluja; Priya Krishnan Sundararajan; Ole J Mengshoel",
        "abstract": "The expectation maximization (EM) algorithm is a popular algorithm for parameter estimation in models with hidden variables. However, the algorithm has several non-trivial limitations, a significant one being variation in eventual solutions found, due to convergence to local optima. Several techniques have been proposed to allay this problem, for example initializing EM from multiple random starting points and selecting the highest likelihood out of all runs. In this work, we a) show that this method can be very expensive computationally for difficult Bayesian networks, and b) in response we propose an age-layered EM approach (ALEM) that efficiently discards less promising runs well before convergence. Our experiments show a significant reduction in the number of iterations, typically two- to four-fold, with minimal or no reduction in solution quality, indicating the potential for ALEM to streamline parameter estimation in Bayesian networks.",
        "bibtex": "@InProceedings{pmlr-v22-saluja12,\n  title = \t {Age-Layered Expectation Maximization for Parameter Learning in Bayesian Networks},\n  author = \t {Saluja, Avneesh and Sundararajan, Priya Krishnan and Mengshoel, Ole J},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {984--992},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/saluja12/saluja12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/saluja12.html},\n  abstract = \t {The expectation maximization (EM) algorithm is a popular algorithm for parameter estimation in models with hidden variables. However, the algorithm has several non-trivial limitations, a significant one being variation in eventual solutions found, due to convergence to local optima. Several techniques have been proposed to allay this problem, for example initializing EM from multiple random starting points and selecting the highest likelihood out of all runs. In this work, we a) show that this method can be very expensive computationally for difficult Bayesian networks, and b) in response we propose an age-layered EM approach (ALEM) that efficiently discards less promising runs well before convergence. Our experiments show a significant reduction in the number of iterations, typically two- to four-fold, with minimal or no reduction in solution quality, indicating the potential for ALEM to streamline parameter estimation in Bayesian networks.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/saluja12/saluja12.pdf",
        "supp": "",
        "pdf_size": 540505,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13873172424017978679&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University",
        "aff_domain": "cmu.edu;sv.cmu.edu;sv.cmu.edu",
        "email": "cmu.edu;sv.cmu.edu;sv.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "An Autoregressive Approach to Nonparametric Hierarchical Dependent Modeling",
        "site": "https://proceedings.mlr.press/v22/zhang12c.html",
        "author": "Zhihua Zhang; Dakan Wang; Edward Chang",
        "abstract": "We propose a conditional autoregression framework for a collection of random probability measures. Under this framework, we devise a conditional autoregressive Dirichlet process (DP) that we call one-parameter dependent DP (wDDP). The appealing properties of this specification are that it has two equivalent representations and its inference can be implemented in a conditional Polya urn scheme. Moreover, these two representations bear a resemblance to the Polya urn scheme and the stick-breaking representation in the conventional DP. We apply this wDDP to Bayesian multivariate-response regression problems. An efficient Markov chain Monte Carlo algorithm is developed for Bayesian computation and prediction.",
        "bibtex": "@InProceedings{pmlr-v22-zhang12c,\n  title = \t {An Autoregressive Approach to Nonparametric Hierarchical Dependent Modeling},\n  author = \t {Zhang, Zhihua and Wang, Dakan and Chang, Edward},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1416--1424},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/zhang12c/zhang12c.pdf},\n  url = \t {https://proceedings.mlr.press/v22/zhang12c.html},\n  abstract = \t {We propose a conditional autoregression framework for a collection of random probability measures. Under this framework, we devise a conditional autoregressive Dirichlet process (DP) that we call one-parameter dependent DP (wDDP). The appealing properties of this specification are that it has two equivalent representations and its inference can be implemented in a conditional Polya urn scheme. Moreover, these two representations bear a resemblance to the Polya urn scheme and the stick-breaking representation in the conventional DP. We apply this wDDP to Bayesian multivariate-response regression problems. An efficient Markov chain Monte Carlo algorithm is developed for Bayesian computation and prediction.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/zhang12c/zhang12c.pdf",
        "supp": "",
        "pdf_size": 965728,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2392827099970321659&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "College of Comp. Sci. and Tech., Zhejiang University, Zhejiang 310027, China; Department of Computer Science, Stanford University, Stanford, CA 94305; Google Research, Beijing 100084, China",
        "aff_domain": "cs.zju.edu.cn;stanford.edu;google.com",
        "email": "cs.zju.edu.cn;stanford.edu;google.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Zhejiang University;Stanford University;Google Research",
        "aff_unique_dep": "College of Computer Science and Technology;Department of Computer Science;Google Research",
        "aff_unique_url": "http://www.zju.edu.cn;https://www.stanford.edu;https://research.google",
        "aff_unique_abbr": "ZJU;Stanford;Google Research",
        "aff_campus_unique_index": "0;1;2",
        "aff_campus_unique": "Zhejiang;Stanford;Beijing",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "Approximate Inference by Intersecting Semidefinite Bound and Local Polytope",
        "site": "https://proceedings.mlr.press/v22/peng12.html",
        "author": "Jian Peng; Tamir Hazan; Nathan Srebro; Jinbo Xu",
        "abstract": "Inference in probabilistic graphical models can be represented as a constrained optimization problem of a free-energy functional. Substantial research has been focused on the approximation of the constraint set, also known as the marginal polytope. This paper presents a novel inference algorithm that tightens and solves the optimization problem by intersecting the popular local polytope and the semidefinite outer bound of the marginal polytope. Using dual decomposition, our method separates the optimization problem into two subproblems: a semidefinite program (SDP), which is solved by a low-rank SDP algorithm, and a free-energy based optimization problem, which is solved by convex belief propagation. Our method has a very reasonable computational complexity and its actual running time is typically within a small factor (=10) of convex belief propagation. Tested on both synthetic data and a real-world protein side-chain packing benchmark, our method significantly outperforms tree-reweighted belief propagation in both marginal probability inference and MAP inference.  Our method is competitive with the state-of-the-art in MRF inference, solving all protein tasks solved by the recently presented MPLP method, and beating MPLP on lattices with strong edge potentials.",
        "bibtex": "@InProceedings{pmlr-v22-peng12,\n  title = \t {Approximate Inference by Intersecting Semidefinite Bound and Local Polytope},\n  author = \t {Peng, Jian and Hazan, Tamir and Srebro, Nathan and Xu, Jinbo},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {868--876},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/peng12/peng12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/peng12.html},\n  abstract = \t {Inference in probabilistic graphical models can be represented as a constrained optimization problem of a free-energy functional. Substantial research has been focused on the approximation of the constraint set, also known as the marginal polytope. This paper presents a novel inference algorithm that tightens and solves the optimization problem by intersecting the popular local polytope and the semidefinite outer bound of the marginal polytope. Using dual decomposition, our method separates the optimization problem into two subproblems: a semidefinite program (SDP), which is solved by a low-rank SDP algorithm, and a free-energy based optimization problem, which is solved by convex belief propagation. Our method has a very reasonable computational complexity and its actual running time is typically within a small factor (=10) of convex belief propagation. Tested on both synthetic data and a real-world protein side-chain packing benchmark, our method significantly outperforms tree-reweighted belief propagation in both marginal probability inference and MAP inference.  Our method is competitive with the state-of-the-art in MRF inference, solving all protein tasks solved by the recently presented MPLP method, and beating MPLP on lattices with strong edge potentials.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/peng12/peng12.pdf",
        "supp": "",
        "pdf_size": 362045,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6147583734793628603&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Toyota Technological Institute at Chicago; Toyota Technological Institute at Chicago; Toyota Technological Institute at Chicago; Toyota Technological Institute at Chicago",
        "aff_domain": "ttic.edu;ttic.edu;ttic.edu;ttic.edu",
        "email": "ttic.edu;ttic.edu;ttic.edu;ttic.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Toyota Technological Institute at Chicago",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.tti-chicago.org",
        "aff_unique_abbr": "TTI Chicago",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Chicago",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Approximate Inference in Additive Factorial HMMs with Application to Energy Disaggregation",
        "site": "https://proceedings.mlr.press/v22/zico12.html",
        "author": "J. Zico Kolter; Tommi Jaakkola",
        "abstract": "This paper considers additive factorial hidden Markov models, an extension to HMMs where the state factors into multiple independent chains, and the output is an additive function of all the hidden states.  Although such models are very powerful, accurate inference is unfortunately difficult: exact inference is not computationally tractable, and existing approximate inference techniques are highly susceptible to local optima.  In this paper we propose an alternative inference method for such models, which exploits their additive structure by 1) looking at the observed difference signal of the observation, 2) incorporating a \u201crobust\u201d mixture component that can account for unmodeled observations, and 3) constraining the posterior to allow at most one hidden state to change at a time.  Combining these elements we develop a convex formulation of approximate inference that is computationally efficient, has no issues of local optima, and which performs much better than existing approaches in practice.  The method is motivated by the problem of energy disaggregation, the task of taking a whole home electricity signal and decomposing it into its component appliances; applied to this task, our algorithm achieves state-of-the-art performance, and is able to separate many appliances almost perfectly using just the total aggregate signal.",
        "bibtex": "@InProceedings{pmlr-v22-zico12,\n  title = \t {Approximate Inference in Additive Factorial HMMs with Application to Energy Disaggregation},\n  author = \t {Kolter, J. Zico and Jaakkola, Tommi},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1472--1482},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/zico12/zico12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/zico12.html},\n  abstract = \t {This paper considers additive factorial hidden Markov models, an extension to HMMs where the state factors into multiple independent chains, and the output is an additive function of all the hidden states.  Although such models are very powerful, accurate inference is unfortunately difficult: exact inference is not computationally tractable, and existing approximate inference techniques are highly susceptible to local optima.  In this paper we propose an alternative inference method for such models, which exploits their additive structure by 1) looking at the observed difference signal of the observation, 2) incorporating a \u201crobust\u201d mixture component that can account for unmodeled observations, and 3) constraining the posterior to allow at most one hidden state to change at a time.  Combining these elements we develop a convex formulation of approximate inference that is computationally efficient, has no issues of local optima, and which performs much better than existing approaches in practice.  The method is motivated by the problem of energy disaggregation, the task of taking a whole home electricity signal and decomposing it into its component appliances; applied to this task, our algorithm achieves state-of-the-art performance, and is able to separate many appliances almost perfectly using just the total aggregate signal.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/zico12/zico12.pdf",
        "supp": "",
        "pdf_size": 743902,
        "gs_citation": 893,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10091373205265289298&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "MIT CSAIL; MIT CSAIL",
        "aff_domain": "csail.mit.edu;csail.mit.edu",
        "email": "csail.mit.edu;csail.mit.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory",
        "aff_unique_url": "https://www.csail.mit.edu",
        "aff_unique_abbr": "MIT CSAIL",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Bandit Theory meets Compressed Sensing for high dimensional Stochastic Linear Bandit",
        "site": "https://proceedings.mlr.press/v22/carpentier12.html",
        "author": "Alexandra Carpentier; Remi Munos",
        "abstract": "We consider a linear stochastic bandit problem where the dimension K of the unknown parameter \theta is larger than the sampling budget n. Since usual linear bandit algorithms have a regret in O(K\\sqrtn), it is in general impossible to obtain a sub-linear regret without further assumption. In this paper we make the assumption that \theta is S-sparse, i.e. has at most S-non-zero components, and that the space of arms is the unit ball for the ||.||_2 norm. We combine ideas from Compressed Sensing and Bandit Theory to derive an algorithm with a regret bound in O(S\\sqrtn). We detail an application to the problem of optimizing a function that depends on many variables but among which only a small number of them (initially unknown) are relevant.",
        "bibtex": "@InProceedings{pmlr-v22-carpentier12,\n  title = \t {Bandit Theory meets Compressed Sensing for high dimensional Stochastic Linear Bandit},\n  author = \t {Carpentier, Alexandra and Munos, Remi},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {190--198},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/carpentier12/carpentier12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/carpentier12.html},\n  abstract = \t {We consider a linear stochastic bandit problem where the dimension K of the unknown parameter \theta is larger than the sampling budget n. Since usual linear bandit algorithms have a regret in O(K\\sqrtn), it is in general impossible to obtain a sub-linear regret without further assumption. In this paper we make the assumption that \theta is S-sparse, i.e. has at most S-non-zero components, and that the space of arms is the unit ball for the ||.||_2 norm. We combine ideas from Compressed Sensing and Bandit Theory to derive an algorithm with a regret bound in O(S\\sqrtn). We detail an application to the problem of optimizing a function that depends on many variables but among which only a small number of them (initially unknown) are relevant.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/carpentier12/carpentier12.pdf",
        "supp": "",
        "pdf_size": 2270639,
        "gs_citation": 134,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1012081107196892250&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "Sequel team, INRIA Lille - Nord Europe; Sequel team, INRIA Lille - Nord Europe",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "INRIA Lille - Nord Europe",
        "aff_unique_dep": "Sequel team",
        "aff_unique_url": "https://www.inria.fr/en",
        "aff_unique_abbr": "INRIA",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Lille",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "France"
    },
    {
        "title": "Bayesian Classifier Combination",
        "site": "https://proceedings.mlr.press/v22/kim12.html",
        "author": "Hyun-Chul Kim; Zoubin Ghahramani",
        "abstract": "Bayesian model averaging linearly mixes the probabilistic predictions of multiple models, each weighted by its posterior probability. This is the coherent Bayesian way of combining multiple models only under certain restrictive assumptions, which we outline. We explore a general framework for Bayesian model combination (which differs from model averaging) in the context of classification. This framework explicitly models the relationship between each model\u2019s output and the unknown true label. The framework does not require that the models be probabilistic (they can even be human assessors), that they share prior information or receive the same training data, or that they be independent in their errors. Finally, the Bayesian combiner does not need to believe any of the models is in fact correct.  We test several variants of this classifier combination procedure starting from a classic statistical model proposed by Dawid and Skene (1979) and using MCMC to add more complex but important features to the model. Comparisons on several data sets to simpler methods like majority voting show that the Bayesian methods not only perform well but result in interpretable diagnostics on the data points and the models.",
        "bibtex": "@InProceedings{pmlr-v22-kim12,\n  title = \t {Bayesian Classifier Combination},\n  author = \t {Kim, Hyun-Chul and Ghahramani, Zoubin},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {619--627},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/kim12/kim12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/kim12.html},\n  abstract = \t {Bayesian model averaging linearly mixes the probabilistic predictions of multiple models, each weighted by its posterior probability. This is the coherent Bayesian way of combining multiple models only under certain restrictive assumptions, which we outline. We explore a general framework for Bayesian model combination (which differs from model averaging) in the context of classification. This framework explicitly models the relationship between each model\u2019s output and the unknown true label. The framework does not require that the models be probabilistic (they can even be human assessors), that they share prior information or receive the same training data, or that they be independent in their errors. Finally, the Bayesian combiner does not need to believe any of the models is in fact correct.  We test several variants of this classifier combination procedure starting from a classic statistical model proposed by Dawid and Skene (1979) and using MCMC to add more complex but important features to the model. Comparisons on several data sets to simpler methods like majority voting show that the Bayesian methods not only perform well but result in interpretable diagnostics on the data points and the models.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/kim12/kim12.pdf",
        "supp": "",
        "pdf_size": 347995,
        "gs_citation": 316,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13455939598228221651&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Korea Institute of Science and Technology; University of Cambridge",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Korea Institute of Science and Technology;University of Cambridge",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.kist.re.kr;https://www.cam.ac.uk",
        "aff_unique_abbr": "KIST;Cambridge",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "South Korea;United Kingdom"
    },
    {
        "title": "Bayesian Comparison of Machine Learning Algorithms on Single and Multiple Datasets",
        "site": "https://proceedings.mlr.press/v22/lacoste12.html",
        "author": "Alexandre Lacoste; Francois Laviolette; Mario Marchand",
        "abstract": "We propose a new method for comparing learning algorithms on multiple tasks which is based on a novel non-parametric test that we call the Poisson binomial test. The key aspect of this work is that we provide a formal definition for what is meant to have an algorithm that is better than another. Also, we are able to take into account the dependencies induced when evaluating classifiers on the same test set. Finally we make optimal use (in the Bayesian sense) of all the testing data we have. We demonstrate empirically that our approach is more reliable than the sign test and the Wilcoxon signed rank test, the current state of the art for algorithm comparisons.",
        "bibtex": "@InProceedings{pmlr-v22-lacoste12,\n  title = \t {Bayesian Comparison of Machine Learning Algorithms on Single and Multiple Datasets},\n  author = \t {Lacoste, Alexandre and Laviolette, Francois and Marchand, Mario},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {665--675},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/lacoste12/lacoste12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/lacoste12.html},\n  abstract = \t {We propose a new method for comparing learning algorithms on multiple tasks which is based on a novel non-parametric test that we call the Poisson binomial test. The key aspect of this work is that we provide a formal definition for what is meant to have an algorithm that is better than another. Also, we are able to take into account the dependencies induced when evaluating classifiers on the same test set. Finally we make optimal use (in the Bayesian sense) of all the testing data we have. We demonstrate empirically that our approach is more reliable than the sign test and the Wilcoxon signed rank test, the current state of the art for algorithm comparisons.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/lacoste12/lacoste12.pdf",
        "supp": "",
        "pdf_size": 479397,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15893658484164485097&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "D\u00b4epartement d\u2019informatique et de g\u00b4enie logiciel, Universit\u00b4e Laval, Qu\u00b4ebec, Canada; D\u00b4epartement d\u2019informatique et de g\u00b4enie logiciel, Universit\u00b4e Laval, Qu\u00b4ebec, Canada; D\u00b4epartement d\u2019informatique et de g\u00b4enie logiciel, Universit\u00b4e Laval, Qu\u00b4ebec, Canada",
        "aff_domain": "ulaval.ca;ift.ulaval.ca;ift.ulaval.ca",
        "email": "ulaval.ca;ift.ulaval.ca;ift.ulaval.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Universit u00e9 Laval",
        "aff_unique_dep": "D u00e9partement d\u2019informatique et de g u00e9nie logiciel",
        "aff_unique_url": "https://www.ulaval.ca",
        "aff_unique_abbr": "UL",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Qu u00e9bec",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "Bayesian Group Factor Analysis",
        "site": "https://proceedings.mlr.press/v22/virtanen12.html",
        "author": "Seppo Virtanen; Arto Klami; Suleiman Khan; Samuel Kaski",
        "abstract": "We introduce a factor analysis model that summarizes the dependencies between observed variable groups, instead of dependencies between individual variables as standard factor analysis does. A group may correspond to one view of the same set of objects, one of many data sets tied by co-occurrence, or a set of alternative variables collected from statistics tables to measure one property of interest. We show that by assuming group-wise sparse factors, active in a subset of the sets, the variation can be decomposed into factors explaining relationships between the sets and factors explaining away set-specific variation. We formulate the assumptions in a Bayesian model providing the factors, and apply the model to two data analysis tasks, in neuroimaging and chemical systems biology.",
        "bibtex": "@InProceedings{pmlr-v22-virtanen12,\n  title = \t {Bayesian Group Factor Analysis},\n  author = \t {Virtanen, Seppo and Klami, Arto and Khan, Suleiman and Kaski, Samuel},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1269--1277},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/virtanen12/virtanen12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/virtanen12.html},\n  abstract = \t {We introduce a factor analysis model that summarizes the dependencies between observed variable groups, instead of dependencies between individual variables as standard factor analysis does. A group may correspond to one view of the same set of objects, one of many data sets tied by co-occurrence, or a set of alternative variables collected from statistics tables to measure one property of interest. We show that by assuming group-wise sparse factors, active in a subset of the sets, the variation can be decomposed into factors explaining relationships between the sets and factors explaining away set-specific variation. We formulate the assumptions in a Bayesian model providing the factors, and apply the model to two data analysis tasks, in neuroimaging and chemical systems biology.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/virtanen12/virtanen12.pdf",
        "supp": "",
        "pdf_size": 455467,
        "gs_citation": 106,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16037703290957370330&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Bayesian Inference for Change Points in Dynamical Systems with Reusable States - a Chinese Restaurant Process Approach",
        "site": "https://proceedings.mlr.press/v22/stimberg12.html",
        "author": "Florian Stimberg; Andreas Ruttor; Manfred Opper",
        "abstract": "We study a model of a stochastic process with unobserved parameters which suddenly change at random times. The possible parameter values are assumed to be from a finite but unknown set. Using a Chinese restaurant process prior over parameters we develop an efficient MCMC procedure for Bayesian inference. We demonstrate the significance of our approach with an application to systems biology data.",
        "bibtex": "@InProceedings{pmlr-v22-stimberg12,\n  title = \t {Bayesian Inference for Change Points in Dynamical Systems with Reusable States - a Chinese Restaurant Process Approach},\n  author = \t {Stimberg, Florian and Ruttor, Andreas and Opper, Manfred},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1117--1124},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/stimberg12/stimberg12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/stimberg12.html},\n  abstract = \t {We study a model of a stochastic process with unobserved parameters which suddenly change at random times. The possible parameter values are assumed to be from a finite but unknown set. Using a Chinese restaurant process prior over parameters we develop an efficient MCMC procedure for Bayesian inference. We demonstrate the significance of our approach with an application to systems biology data.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/stimberg12/stimberg12.pdf",
        "supp": "",
        "pdf_size": 552860,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10787437496875815613&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Bayesian Quadrature for Ratios",
        "site": "https://proceedings.mlr.press/v22/osborne12.html",
        "author": "Michael Osborne; Roman Garnett; Stephen Roberts; Christopher Hart; Suzanne Aigrain; Neale Gibson",
        "abstract": "We describe a novel approach to quadrature for ratios of probabilistic integrals, such as are used to compute posterior probabilities. It offers performance superior to Monte Carlo methods by exploiting a Bayesian quadrature framework. We improve upon previous Bayesian quadrature techniques by explicitly modelling the non-negativity of our integrands, and the correlations that exist between them. It offers most where the integrand is multi-modal and expensive to evaluate, as is commonplace in exoplanets research; we demonstrate the efficacy of our method on data from the Kepler spacecraft.",
        "bibtex": "@InProceedings{pmlr-v22-osborne12,\n  title = \t {Bayesian Quadrature for Ratios},\n  author = \t {Osborne, Michael and Garnett, Roman and Roberts, Stephen and Hart, Christopher and Aigrain, Suzanne and Gibson, Neale},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {832--840},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/osborne12/osborne12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/osborne12.html},\n  abstract = \t {We describe a novel approach to quadrature for ratios of probabilistic integrals, such as are used to compute posterior probabilities. It offers performance superior to Monte Carlo methods by exploiting a Bayesian quadrature framework. We improve upon previous Bayesian quadrature techniques by explicitly modelling the non-negativity of our integrands, and the correlations that exist between them. It offers most where the integrand is multi-modal and expensive to evaluate, as is commonplace in exoplanets research; we demonstrate the efficacy of our method on data from the Kepler spacecraft.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/osborne12/osborne12.pdf",
        "supp": "",
        "pdf_size": 692143,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17770837831479662040&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "Department of Engineering Science, University of Oxford, Oxford OX1 3PJ, UK; Robotics Institute, Carnegie Mellon University, Pittsburgh PA 15213, USA; Department of Engineering Science, University of Oxford, Oxford OX1 3PJ, UK; Department of Physics, University of Oxford, Oxford OX1 3RH, UK; Department of Physics, University of Oxford, Oxford OX1 3RH, UK; Department of Physics, University of Oxford, Oxford OX1 3RH, UK",
        "aff_domain": "robots.ox.ac.uk;andrew.cmu.edu;robots.ox.ac.uk;astro.ox.ac.uk;astro.ox.ac.uk;astro.ox.ac.uk",
        "email": "robots.ox.ac.uk;andrew.cmu.edu;robots.ox.ac.uk;astro.ox.ac.uk;astro.ox.ac.uk;astro.ox.ac.uk",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0;0;0",
        "aff_unique_norm": "University of Oxford;Carnegie Mellon University",
        "aff_unique_dep": "Department of Engineering Science;Robotics Institute",
        "aff_unique_url": "https://www.ox.ac.uk;https://www.cmu.edu",
        "aff_unique_abbr": "Oxford;CMU",
        "aff_campus_unique_index": "0;1;0;0;0;0",
        "aff_campus_unique": "Oxford;Pittsburgh",
        "aff_country_unique_index": "0;1;0;0;0;0",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "title": "Bayesian regularization of non-homogeneous dynamic Bayesian networks by globally coupling interaction parameters",
        "site": "https://proceedings.mlr.press/v22/grzegorzyk12.html",
        "author": "Marco Grzegorzyk; Dirk Husmeier",
        "abstract": "To relax the homogeneity assumption of classical dynamic Bayesian networks (DBNs), various recent studies have combined DBNs with multiple changepoint processes. The underlying assumption is that the parameters associated with time series segments delimited by multiple changepoints are a priori independent. Under weak regularity conditions, the parameters can be integrated out in the likelihood, leading to a closed-form expression of the marginal likelihood. However, the assumption of prior independence is unrealistic in many real-world applications, where the segment-specific regulatory relationships among the interdependent quantities tend to undergo gradual evolutionary adaptations. We therefore propose a Bayesian coupling scheme to introduce systematic information sharing among the segment-specific interaction parameters. We investigate the effect this model improvement has on the network reconstruction accuracy in a reverse engineering context, where the objective is to learn the structure of a gene regulatory network from temporal gene expression profiles.",
        "bibtex": "@InProceedings{pmlr-v22-grzegorzyk12,\n  title = \t {Bayesian regularization of non-homogeneous dynamic Bayesian networks by globally coupling interaction parameters},\n  author = \t {Grzegorzyk, Marco and Husmeier, Dirk},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {467--476},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/grzegorzyk12/grzegorzyk12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/grzegorzyk12.html},\n  abstract = \t {To relax the homogeneity assumption of classical dynamic Bayesian networks (DBNs), various recent studies have combined DBNs with multiple changepoint processes. The underlying assumption is that the parameters associated with time series segments delimited by multiple changepoints are a priori independent. Under weak regularity conditions, the parameters can be integrated out in the likelihood, leading to a closed-form expression of the marginal likelihood. However, the assumption of prior independence is unrealistic in many real-world applications, where the segment-specific regulatory relationships among the interdependent quantities tend to undergo gradual evolutionary adaptations. We therefore propose a Bayesian coupling scheme to introduce systematic information sharing among the segment-specific interaction parameters. We investigate the effect this model improvement has on the network reconstruction accuracy in a reverse engineering context, where the objective is to learn the structure of a gene regulatory network from temporal gene expression profiles.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/grzegorzyk12/grzegorzyk12.pdf",
        "supp": "",
        "pdf_size": 528569,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10431356223430476322&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Statistics, TU Dortmund University; School of Mathematics and Statistics, University of Glasgow",
        "aff_domain": "statistik.tu-dortmund.de;glasgow.ac.uk",
        "email": "statistik.tu-dortmund.de;glasgow.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "TU Dortmund University;University of Glasgow",
        "aff_unique_dep": "Department of Statistics;School of Mathematics and Statistics",
        "aff_unique_url": "https://www.tu-dortmund.de;https://www.gla.ac.uk",
        "aff_unique_abbr": "TUDO;UoG",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Germany;United Kingdom"
    },
    {
        "title": "Beta-Negative Binomial Process and Poisson Factor Analysis",
        "site": "https://proceedings.mlr.press/v22/zhou12c.html",
        "author": "Mingyuan Zhou; Lauren Hannah; David Dunson; Lawrence Carin",
        "abstract": "A beta-negative binomial (BNB) process is proposed, leading to a beta-gamma-Poisson process, which may be viewed as a \u201cmulti-scoop\u201d generalization of the beta-Bernoulli process. The BNB process is augmented into a beta-gamma-gamma-Poisson hierarchical structure, and applied as a nonparametric Bayesian prior for an infinite Poisson factor analysis model. A finite approximation for the beta process Levy random measure is constructed for convenient implementation. Efficient MCMC computations are performed with data augmentation and marginalization techniques. Encouraging results are shown on document count matrix factorization.",
        "bibtex": "@InProceedings{pmlr-v22-zhou12c,\n  title = \t {Beta-Negative Binomial Process and Poisson Factor Analysis},\n  author = \t {Zhou, Mingyuan and Hannah, Lauren and Dunson, David and Carin, Lawrence},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1462--1471},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/zhou12c/zhou12c.pdf},\n  url = \t {https://proceedings.mlr.press/v22/zhou12c.html},\n  abstract = \t {A beta-negative binomial (BNB) process is proposed, leading to a beta-gamma-Poisson process, which may be viewed as a \u201cmulti-scoop\u201d generalization of the beta-Bernoulli process. The BNB process is augmented into a beta-gamma-gamma-Poisson hierarchical structure, and applied as a nonparametric Bayesian prior for an infinite Poisson factor analysis model. A finite approximation for the beta process Levy random measure is constructed for convenient implementation. Efficient MCMC computations are performed with data augmentation and marginalization techniques. Encouraging results are shown on document count matrix factorization.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/zhou12c/zhou12c.pdf",
        "supp": "",
        "pdf_size": 423892,
        "gs_citation": 299,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2023855989014707753&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Beyond Logarithmic Bounds in Online Learning",
        "site": "https://proceedings.mlr.press/v22/orabona12.html",
        "author": "Francesco Orabona; Nicolo Cesa-Bianchi; Claudio Gentile",
        "abstract": "We prove logarithmic regret bounds that depend on the loss L_T^* of the competitor rather than on the number T of time steps. In the general online convex optimization setting,  our bounds hold for any smooth and exp-concave loss (such as the square loss or the logistic loss). This bridges the gap between the O(ln T) regret exhibited by exp-concave losses and the O(sqrt(L_T^*)) regret exhibited by smooth losses. We also show that these bounds are tight for specific losses, thus they cannot be improved in general. For online regression with square loss, our analysis can be used to derive a sparse randomized variant of the online Newton step, whose expected number of updates scales with the algorithm\u2019s loss. For online classification, we prove the first logarithmic mistake bounds that do not rely on prior knowledge of a bound on the competitor\u2019s norm.",
        "bibtex": "@InProceedings{pmlr-v22-orabona12,\n  title = \t {Beyond Logarithmic Bounds in Online Learning},\n  author = \t {Orabona, Francesco and Cesa-Bianchi, Nicolo and Gentile, Claudio},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {823--831},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/orabona12/orabona12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/orabona12.html},\n  abstract = \t {We prove logarithmic regret bounds that depend on the loss L_T^* of the competitor rather than on the number T of time steps. In the general online convex optimization setting,  our bounds hold for any smooth and exp-concave loss (such as the square loss or the logistic loss). This bridges the gap between the O(ln T) regret exhibited by exp-concave losses and the O(sqrt(L_T^*)) regret exhibited by smooth losses. We also show that these bounds are tight for specific losses, thus they cannot be improved in general. For online regression with square loss, our analysis can be used to derive a sparse randomized variant of the online Newton step, whose expected number of updates scales with the algorithm\u2019s loss. For online classification, we prove the first logarithmic mistake bounds that do not rely on prior knowledge of a bound on the competitor\u2019s norm.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/orabona12/orabona12.pdf",
        "supp": "",
        "pdf_size": 377561,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7907275032439975307&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "Toyota Technological Institute, Chicago, IL, USA + DSI, Universit\bf1 degli Studi di Milano, Milano, Italy; DSI, Universit\bf1 degli Studi di Milano, Milano, Italy; DICOM, Universit\bf1 dell\u2019Insubria, Varese, Italy",
        "aff_domain": "orabona.com;unimi.it;uninsubria.it",
        "email": "orabona.com;unimi.it;uninsubria.it",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1;2",
        "aff_unique_norm": "Toyota Technological Institute at Chicago;Universit\u00e0 degli Studi di Milano;Universit\u00e0 dell\u2019Insubria",
        "aff_unique_dep": ";DSI;DICOM",
        "aff_unique_url": "https://www.tti-chicago.org;https://www.unimi.it;https://www.uninsubria.it",
        "aff_unique_abbr": "TTI-Chicago;;",
        "aff_campus_unique_index": "0+1;1;2",
        "aff_campus_unique": "Chicago;Milano;Varese",
        "aff_country_unique_index": "0+1;1;1",
        "aff_country_unique": "United States;Italy"
    },
    {
        "title": "Causality with Gates",
        "site": "https://proceedings.mlr.press/v22/winn12.html",
        "author": "John Winn",
        "abstract": "An intervention on a variable removes the influences that usually have a causal effect on that variable. Gates are a general-purpose graphical modelling notation for representing such context-specific independencies in the structure of a graphical model. We extend d-separation to cover gated graphical models and show that it subsumes do calculus when gates are used to represent interventions. We also show how standard message passing inference algorithms, such as belief propagation, can be applied to the gated graph. This demonstrates that causal reasoning can be performed by probabilistic inference alone.",
        "bibtex": "@InProceedings{pmlr-v22-winn12,\n  title = \t {Causality with Gates},\n  author = \t {Winn, John},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1314--1322},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/winn12/winn12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/winn12.html},\n  abstract = \t {An intervention on a variable removes the influences that usually have a causal effect on that variable. Gates are a general-purpose graphical modelling notation for representing such context-specific independencies in the structure of a graphical model. We extend d-separation to cover gated graphical models and show that it subsumes do calculus when gates are used to represent interventions. We also show how standard message passing inference algorithms, such as belief propagation, can be applied to the gated graph. This demonstrates that causal reasoning can be performed by probabilistic inference alone.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/winn12/winn12.pdf",
        "supp": "",
        "pdf_size": 791704,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12984302931320295144&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Microsoft Research, Cambridge, UK",
        "aff_domain": "orabona.com;unimi.it;uninsubria.it",
        "email": "orabona.com;unimi.it;uninsubria.it",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Microsoft Research",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "MSR",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Classifier Cascade for Minimizing Feature Evaluation Cost",
        "site": "https://proceedings.mlr.press/v22/chen12c.html",
        "author": "Minmin Chen; Zhixiang Xu; Kilian Weinberger; Olivier Chapelle; Dor Kedem",
        "abstract": "Machine learning algorithms are increasingly used in large-scale industrial settings. Here, the operational cost during test-time has to be taken into account when an algorithm is designed.  This operational cost is affected by the average running time and the computation time required for feature extraction. When a diverse set of features is used, the  latter can vary drastically. In this paper we propose an algorithm that constructs a cascade of classifiers that explicitly trades-off operational cost and classifier accuracy while accounting for on-demand feature extraction costs. Different from previous work, our algorithm re-optimizes trained classifiers and allows expensive features to be scheduled at any stage within the cascade to minimize overall cost. Experiments on actual web-search ranking data sets demonstrate that our framework leads to drastic test-time improvements.",
        "bibtex": "@InProceedings{pmlr-v22-chen12c,\n  title = \t {Classifier Cascade for Minimizing Feature Evaluation Cost},\n  author = \t {Chen, Minmin and Xu, Zhixiang and Weinberger, Kilian and Chapelle, Olivier and Kedem, Dor},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {218--226},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/chen12c/chen12c.pdf},\n  url = \t {https://proceedings.mlr.press/v22/chen12c.html},\n  abstract = \t {Machine learning algorithms are increasingly used in large-scale industrial settings. Here, the operational cost during test-time has to be taken into account when an algorithm is designed.  This operational cost is affected by the average running time and the computation time required for feature extraction. When a diverse set of features is used, the  latter can vary drastically. In this paper we propose an algorithm that constructs a cascade of classifiers that explicitly trades-off operational cost and classifier accuracy while accounting for on-demand feature extraction costs. Different from previous work, our algorithm re-optimizes trained classifiers and allows expensive features to be scheduled at any stage within the cascade to minimize overall cost. Experiments on actual web-search ranking data sets demonstrate that our framework leads to drastic test-time improvements.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/chen12c/chen12c.pdf",
        "supp": "",
        "pdf_size": 1149272,
        "gs_citation": 87,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7497274639569876595&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Washington University in Saint Louis; Washington University in Saint Louis; Washington University in Saint Louis; Yahoo! Research; Washington University in Saint Louis",
        "aff_domain": "wustl.edu;wustl.edu;wustl.edu;yahoo-inc.com;wustl.edu",
        "email": "wustl.edu;wustl.edu;wustl.edu;yahoo-inc.com;wustl.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "Washington University in St. Louis;Yahoo!",
        "aff_unique_dep": ";Yahoo! Research",
        "aff_unique_url": "https://wustl.edu;https://research.yahoo.com",
        "aff_unique_abbr": "WUSTL;Yahoo!",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "St. Louis;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Closed-Form Entropy Limits -  A Tool to Monitor Likelihood Optimization of Probabilistic Generative Models",
        "site": "https://proceedings.mlr.press/v22/lucke12.html",
        "author": "Jorg Lucke; Marc Henniges",
        "abstract": "The maximization of the data likelihood under a given probabilistic generative model is the essential goal of many algorithms for unsupervised learning. If expectation maximization is used for optimization, a lower bound on the data likelihood, the free-energy, is optimized. The parameter-dependent part of the free-energy (the difference between free-energy and posterior entropy) is the essential entity in the derivation of learning algorithms. Here we show that for many common generative models the optimal values of the parameter-dependent part can be derived in closed-form. These closed-form expressions are hereby given as sums of the negative (differential) entropies of the individual model distributions. We apply our theoretical results to derive such closed-form expressions for a number of common and recent models, including probabilistic PCA, factor analysis, different versions of sparse coding, and Linear Dynamical Systems. The main contribution of this work is theoretical but we show how the derived results can be used to efficiently compute free-energies, and how they can be used for consistency checks of learning algorithms.",
        "bibtex": "@InProceedings{pmlr-v22-lucke12,\n  title = \t {Closed-Form Entropy Limits -  A Tool to Monitor Likelihood Optimization of Probabilistic Generative Models},\n  author = \t {Lucke, Jorg and Henniges, Marc},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {731--740},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/lucke12/lucke12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/lucke12.html},\n  abstract = \t {The maximization of the data likelihood under a given probabilistic generative model is the essential goal of many algorithms for unsupervised learning. If expectation maximization is used for optimization, a lower bound on the data likelihood, the free-energy, is optimized. The parameter-dependent part of the free-energy (the difference between free-energy and posterior entropy) is the essential entity in the derivation of learning algorithms. Here we show that for many common generative models the optimal values of the parameter-dependent part can be derived in closed-form. These closed-form expressions are hereby given as sums of the negative (differential) entropies of the individual model distributions. We apply our theoretical results to derive such closed-form expressions for a number of common and recent models, including probabilistic PCA, factor analysis, different versions of sparse coding, and Linear Dynamical Systems. The main contribution of this work is theoretical but we show how the derived results can be used to efficiently compute free-energies, and how they can be used for consistency checks of learning algorithms.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/lucke12/lucke12.pdf",
        "supp": "",
        "pdf_size": 956972,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4764760544883228833&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "FIAS, Goethe-University Frankfurt, 60438 Frankfurt, Germany; FIAS, Goethe-University Frankfurt, 60438 Frankfurt, Germany",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Goethe-University Frankfurt",
        "aff_unique_dep": "FIAS",
        "aff_unique_url": "https://www.uni-frankfurt.de",
        "aff_unique_abbr": "Goethe-University",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Frankfurt",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Complexity of Bethe Approximation",
        "site": "https://proceedings.mlr.press/v22/shin12.html",
        "author": "Jinwoo Shin",
        "abstract": "This paper resolves a common complexity issue in the Bethe approximation of statistical physics and the sum-product Belief Propagation (BP) algorithm of artificial intelligence. The Bethe approximation reduces the problem of computing the partition function in a graphical model to that of solving a set of non-linear equations, so-called the Bethe equation. On the other hand, the BP algorithm is a popular heuristic method for estimating marginal distribution in a graphical model. Although they are inspired and developed from different directions, Yedidia, Freeman and Weiss (2004) established a somewhat surprising connection: the BP algorithm solves the Bethe equation if it converges (however, it often does not). This naturally motivates the following important question to understand their limitations and empirical successes: the Bethe equation is computationally easy to solve? We present a message passing algorithm solving the Bethe equation in polynomial number of bitwise operations for arbitrary binary graphical models of n nodes where the maximum degree in the underlying graph is O(log n). Our algorithm, an alternative to BP fixing its convergence issue, is the first fully polynomial-time approximation scheme for the BP fixed point computation in such a large class of graphical models. Moreover, we believe that our technique is of broader interest to understand the computational complexity of the cavity method in statistical physics.",
        "bibtex": "@InProceedings{pmlr-v22-shin12,\n  title = \t {Complexity of Bethe Approximation},\n  author = \t {Shin, Jinwoo},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1037--1045},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/shin12/shin12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/shin12.html},\n  abstract = \t {This paper resolves a common complexity issue in the Bethe approximation of statistical physics and the sum-product Belief Propagation (BP) algorithm of artificial intelligence. The Bethe approximation reduces the problem of computing the partition function in a graphical model to that of solving a set of non-linear equations, so-called the Bethe equation. On the other hand, the BP algorithm is a popular heuristic method for estimating marginal distribution in a graphical model. Although they are inspired and developed from different directions, Yedidia, Freeman and Weiss (2004) established a somewhat surprising connection: the BP algorithm solves the Bethe equation if it converges (however, it often does not). This naturally motivates the following important question to understand their limitations and empirical successes: the Bethe equation is computationally easy to solve? We present a message passing algorithm solving the Bethe equation in polynomial number of bitwise operations for arbitrary binary graphical models of n nodes where the maximum degree in the underlying graph is O(log n). Our algorithm, an alternative to BP fixing its convergence issue, is the first fully polynomial-time approximation scheme for the BP fixed point computation in such a large class of graphical models. Moreover, we believe that our technique is of broader interest to understand the computational complexity of the cavity method in statistical physics.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/shin12/shin12.pdf",
        "supp": "",
        "pdf_size": 392185,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7428438277332696075&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Georgia Institute of Technology",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Georgia Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.gatech.edu",
        "aff_unique_abbr": "Georgia Tech",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Consistency and Rates for Clustering with DBSCAN",
        "site": "https://proceedings.mlr.press/v22/sriperumbudur12.html",
        "author": "Bharath Sriperumbudur; Ingo Steinwart",
        "abstract": "We propose a simple and efficient modification of the popular DBSCAN clustering algorithm. This modification is able to detect the most interesting vertical threshold level in an automated, data-driven way. We establish both consistency and optimal learning rates for this modification.",
        "bibtex": "@InProceedings{pmlr-v22-sriperumbudur12,\n  title = \t {Consistency and Rates for Clustering with DBSCAN},\n  author = \t {Sriperumbudur, Bharath and Steinwart, Ingo},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1090--1098},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/sriperumbudur12/sriperumbudur12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/sriperumbudur12.html},\n  abstract = \t {We propose a simple and efficient modification of the popular DBSCAN clustering algorithm. This modification is able to detect the most interesting vertical threshold level in an automated, data-driven way. We establish both consistency and optimal learning rates for this modification.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/sriperumbudur12/sriperumbudur12.pdf",
        "supp": "",
        "pdf_size": 384910,
        "gs_citation": 55,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12446716591756545622&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Gatsby Computational Neuroscience Unit, University College London; Institute for Stochastics and Applications, University of Stuttgart, Germany",
        "aff_domain": "gatsby.ucl.ac.uk;mathematik.uni-stuttgart.de",
        "email": "gatsby.ucl.ac.uk;mathematik.uni-stuttgart.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University College London;University of Stuttgart",
        "aff_unique_dep": "Gatsby Computational Neuroscience Unit;Institute for Stochastics and Applications",
        "aff_unique_url": "https://www.ucl.ac.uk;https://www.uni-stuttgart.de",
        "aff_unique_abbr": "UCL;",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "London;",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United Kingdom;Germany"
    },
    {
        "title": "Constrained 1-Spectral Clustering",
        "site": "https://proceedings.mlr.press/v22/sundar12.html",
        "author": "Syama Sundar Rangapuram; Matthias Hein",
        "abstract": "An important form of prior information in clustering comes in the form of cannot-link and must-link constraints of instances. We present a generalization of the popular spectral clustering technique which integrates such constraints. Motivated by the recently proposed 1-spectral clustering for the unconstrained normalized cut problem, our method  is based on a tight relaxation of the constrained normalized cut into a continuous optimization problem.   Opposite to all other methods which have been suggested for constrained spectral clustering, we can always guarantee to satisfy all constraints. Moreover, our soft formulation allows to optimize a trade-off between normalized cut and  the number of violated constraints. An efficient implementation is provided which scales to large datasets. We outperform consistently all other proposed methods in the experiments.",
        "bibtex": "@InProceedings{pmlr-v22-sundar12,\n  title = \t {Constrained 1-Spectral Clustering},\n  author = \t {Rangapuram, Syama Sundar and Hein, Matthias},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1143--1151},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/sundar12/sundar12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/sundar12.html},\n  abstract = \t {An important form of prior information in clustering comes in the form of cannot-link and must-link constraints of instances. We present a generalization of the popular spectral clustering technique which integrates such constraints. Motivated by the recently proposed 1-spectral clustering for the unconstrained normalized cut problem, our method  is based on a tight relaxation of the constrained normalized cut into a continuous optimization problem.   Opposite to all other methods which have been suggested for constrained spectral clustering, we can always guarantee to satisfy all constraints. Moreover, our soft formulation allows to optimize a trade-off between normalized cut and  the number of violated constraints. An efficient implementation is provided which scales to large datasets. We outperform consistently all other proposed methods in the experiments.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/sundar12/sundar12.pdf",
        "supp": "",
        "pdf_size": 760342,
        "gs_citation": 110,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2728798657805133553&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Max Planck Institute for Computer Science; Saarland University, Saarbr\u00fccken, Germany+Saarland University, Saarbr\u00fccken, Germany",
        "aff_domain": "mpi-inf.mpg.de;cs.uni-saarland.de",
        "email": "mpi-inf.mpg.de;cs.uni-saarland.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+1",
        "aff_unique_norm": "Max Planck Institute for Computer Science;Saarland University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://mpi-sws.org;https://www.uni-saarland.de",
        "aff_unique_abbr": "MPI-SWS;UdS",
        "aff_campus_unique_index": "1+1",
        "aff_campus_unique": ";Saarbr\u00fccken",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Contextual Bandit Learning with Predictable Rewards",
        "site": "https://proceedings.mlr.press/v22/agarwal12.html",
        "author": "Alekh Agarwal; Miroslav Dudik; Satyen Kale; John Langford; Robert Schapire",
        "abstract": "Contextual bandit learning is a reinforcement learning problem where   the learner repeatedly receives a set of features (context), takes   an action and receives a reward based on the action and context. We   consider this problem under a realizability assumption: there exists   a function in a (known) function class, always capable of predicting   the expected reward, given the action and context.  Under this   assumption, we show three things. We present a new algorithm\u2013Regressor Elimination \u2013 with a regret similar to the agnostic setting (i.e. in the absence of realizability assumption). We prove a new lower bound showing no algorithm can achieve superior performance in the worst case even with the realizability assumption. However, we do show that for \\emphany set of policies (mapping contexts to actions), there is a distribution over rewards (given context) such that our new algorithm has \\em constant regret unlike the previous approaches.",
        "bibtex": "@InProceedings{pmlr-v22-agarwal12,\n  title = \t {Contextual Bandit Learning with Predictable Rewards},\n  author = \t {Agarwal, Alekh and Dudik, Miroslav and Kale, Satyen and Langford, John and Schapire, Robert},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {19--26},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/agarwal12/agarwal12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/agarwal12.html},\n  abstract = \t {Contextual bandit learning is a reinforcement learning problem where   the learner repeatedly receives a set of features (context), takes   an action and receives a reward based on the action and context. We   consider this problem under a realizability assumption: there exists   a function in a (known) function class, always capable of predicting   the expected reward, given the action and context.  Under this   assumption, we show three things. We present a new algorithm\u2013Regressor Elimination \u2013 with a regret similar to the agnostic setting (i.e. in the absence of realizability assumption). We prove a new lower bound showing no algorithm can achieve superior performance in the worst case even with the realizability assumption. However, we do show that for \\emphany set of policies (mapping contexts to actions), there is a distribution over rewards (given context) such that our new algorithm has \\em constant regret unlike the previous approaches.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/agarwal12/agarwal12.pdf",
        "supp": "",
        "pdf_size": 288287,
        "gs_citation": 114,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5351565816567366849&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "University of California, Berkeley; Yahoo Inc.; IBM; Yahoo Inc.; Princeton University",
        "aff_domain": "cs.berkeley.edu;yahoo-inc.com;us.ibm.com;yahoo-inc.com;cs.princeton.edu",
        "email": "cs.berkeley.edu;yahoo-inc.com;us.ibm.com;yahoo-inc.com;cs.princeton.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;1;3",
        "aff_unique_norm": "University of California, Berkeley;Yahoo;International Business Machines Corporation;Princeton University",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.berkeley.edu;https://www.yahoo.com;https://www.ibm.com;https://www.princeton.edu",
        "aff_unique_abbr": "UC Berkeley;Yahoo;IBM;Princeton",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Controlling Selection Bias in Causal Inference",
        "site": "https://proceedings.mlr.press/v22/bareinboim12.html",
        "author": "Elias Bareinboim; Judea Pearl",
        "abstract": "Selection bias, caused by preferential exclusion of samples from the data, is a major obstacle to valid causal and statistical inferences; it cannot be removed by randomized experiments and can hardly be detected  in either experimental or observational studies. This paper highlights several graphical and algebraic methods capable of mitigating and sometimes eliminating this bias. These nonparametric methods generalize previously reported results, and identify the type of knowledge that is needed for reasoning in the presence of selection bias.  Specifically,  we derive a general condition together with a procedure for  deciding recoverability of the odds ratio (OR) from s-biased data. We show that recoverability  is feasible if and only if our condition holds.  We further offer a new method of controlling selection bias using instrumental variables that permits the  recovery of other effect measures besides OR.",
        "bibtex": "@InProceedings{pmlr-v22-bareinboim12,\n  title = \t {Controlling Selection Bias in Causal Inference},\n  author = \t {Bareinboim, Elias and Pearl, Judea},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {100--108},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/bareinboim12/bareinboim12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/bareinboim12.html},\n  abstract = \t {Selection bias, caused by preferential exclusion of samples from the data, is a major obstacle to valid causal and statistical inferences; it cannot be removed by randomized experiments and can hardly be detected  in either experimental or observational studies. This paper highlights several graphical and algebraic methods capable of mitigating and sometimes eliminating this bias. These nonparametric methods generalize previously reported results, and identify the type of knowledge that is needed for reasoning in the presence of selection bias.  Specifically,  we derive a general condition together with a procedure for  deciding recoverability of the odds ratio (OR) from s-biased data. We show that recoverability  is feasible if and only if our condition holds.  We further offer a new method of controlling selection bias using instrumental variables that permits the  recovery of other effect measures besides OR.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/bareinboim12/bareinboim12.pdf",
        "supp": "",
        "pdf_size": 602817,
        "gs_citation": 251,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=554816199016265449&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Cognitive Systems Laboratory, Department of Computer Science, University of California, Los Angeles, Los Angeles, CA. 90095; Cognitive Systems Laboratory, Department of Computer Science, University of California, Los Angeles, Los Angeles, CA. 90095",
        "aff_domain": "cs.ucla.edu;cs.ucla.edu",
        "email": "cs.ucla.edu;cs.ucla.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Los Angeles",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ucla.edu",
        "aff_unique_abbr": "UCLA",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Copula Network Classifiers (CNCs)",
        "site": "https://proceedings.mlr.press/v22/elidan12a.html",
        "author": "Gal Elidan",
        "abstract": "The task of classification is of paramount importance and extensive research has been aimed at developing general purpose classifiers that can be used effectively in a variety of  domains. Network-based classifiers, such as the tree augmented naive Bayes model, are  appealing since they are easily interpretable, can naturally handle missing data, and are often quite effective. Yet, for complex domains with continuous explanatory variables, practical performance is often sub-optimal. To overcome this limitation, we introduce Copula Network Classifiers (CNCs), a model that combines the flexibility of a graph based representation with the modeling power of copulas. As we  demonstrate on ten varied continuous real-life datasets, CNCs offer better overall performance than linear and non-linear standard generative models, as well as discriminative RBF and polynomial kernel SVMs. In addition, since no parameter tuning is required, CNCs can be trained dramatically faster than SVMs.",
        "bibtex": "@InProceedings{pmlr-v22-elidan12a,\n  title = \t {Copula Network Classifiers (CNCs)},\n  author = \t {Elidan, Gal},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {346--354},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/elidan12a/elidan12a.pdf},\n  url = \t {https://proceedings.mlr.press/v22/elidan12a.html},\n  abstract = \t {The task of classification is of paramount importance and extensive research has been aimed at developing general purpose classifiers that can be used effectively in a variety of  domains. Network-based classifiers, such as the tree augmented naive Bayes model, are  appealing since they are easily interpretable, can naturally handle missing data, and are often quite effective. Yet, for complex domains with continuous explanatory variables, practical performance is often sub-optimal. To overcome this limitation, we introduce Copula Network Classifiers (CNCs), a model that combines the flexibility of a graph based representation with the modeling power of copulas. As we  demonstrate on ten varied continuous real-life datasets, CNCs offer better overall performance than linear and non-linear standard generative models, as well as discriminative RBF and polynomial kernel SVMs. In addition, since no parameter tuning is required, CNCs can be trained dramatically faster than SVMs.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/elidan12a/elidan12a.pdf",
        "supp": "",
        "pdf_size": 529990,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14495167996851519768&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Statistics, The Hebrew University",
        "aff_domain": "cs.ucla.edu;cs.ucla.edu",
        "email": "cs.ucla.edu;cs.ucla.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "The Hebrew University",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "http://www.huji.ac.il",
        "aff_unique_abbr": "HUJI",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Israel"
    },
    {
        "title": "CorrLog: Correlated Logistic Models for Joint Prediction of Multiple Labels",
        "site": "https://proceedings.mlr.press/v22/bian12.html",
        "author": "Wei Bian; Bo Xie; Dacheng Tao",
        "abstract": "In this paper, we present a simple but effective method for multi-label classification (MLC), termed Correlated Logistic Models (Corrlog), which extends multiple Independent Logistic Regressions (ILRs) by modeling the pairwise correlation between labels. Algorithmically, we propose an efficient method for learning parameters of Corrlog, which is based on regularized maximum pseudo-likelihood estimation and has a linear computational complexity with respect to the number of labels. Theoretically, we show that Corrlog enjoys a satisfying generalization bound which is independent of the number of labels. The effectiveness of Corrlog on modeling label correlations is illustrated by a toy example, and further experiments on real data show that Corrlog achieves competitive performance compared with popular MLC algorithms.",
        "bibtex": "@InProceedings{pmlr-v22-bian12,\n  title = \t {CorrLog: Correlated Logistic Models for Joint Prediction of Multiple Labels},\n  author = \t {Bian, Wei and Xie, Bo and Tao, Dacheng},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {109--117},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/bian12/bian12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/bian12.html},\n  abstract = \t {In this paper, we present a simple but effective method for multi-label classification (MLC), termed Correlated Logistic Models (Corrlog), which extends multiple Independent Logistic Regressions (ILRs) by modeling the pairwise correlation between labels. Algorithmically, we propose an efficient method for learning parameters of Corrlog, which is based on regularized maximum pseudo-likelihood estimation and has a linear computational complexity with respect to the number of labels. Theoretically, we show that Corrlog enjoys a satisfying generalization bound which is independent of the number of labels. The effectiveness of Corrlog on modeling label correlations is illustrated by a toy example, and further experiments on real data show that Corrlog achieves competitive performance compared with popular MLC algorithms.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/bian12/bian12.pdf",
        "supp": "",
        "pdf_size": 360207,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12170076774351038880&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Centre for Quantum Computation and Intelligent Systems, University of Technology, Sydney; Georgia Tech Center for Music Technology, Georgia Institute of Technology; Centre for Quantum Computation and Intelligent Systems, University of Technology, Sydney",
        "aff_domain": "gmail.com;gatech.edu;gmail.com",
        "email": "gmail.com;gatech.edu;gmail.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Technology, Sydney;Georgia Institute of Technology",
        "aff_unique_dep": "Centre for Quantum Computation and Intelligent Systems;Center for Music Technology",
        "aff_unique_url": "https://www.uts.edu.au;https://www.gatech.edu",
        "aff_unique_abbr": "UTS;Georgia Tech",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Sydney;Georgia Tech",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Australia;United States"
    },
    {
        "title": "Data dependent kernels in nearly-linear time",
        "site": "https://proceedings.mlr.press/v22/lever12.html",
        "author": "Guy Lever; Tom Diethe; John Shawe-Taylor",
        "abstract": "We propose a method to efficiently construct data dependent kernels which can make use of large quantities of (unlabeled) data. Our construction makes an approximation in the standard construction of semi-supervised kernels in Sindhwani et al. (2005). In typical cases these kernels can be computed in nearly-linear time (in the amount of data), improving on the cubic time of the standard construction, enabling large scale semi-supervised learning in a variety of contexts. The methods are validated on semi-supervised and unsupervised problems on data sets containing upto 64,000 sample points.",
        "bibtex": "@InProceedings{pmlr-v22-lever12,\n  title = \t {Data dependent kernels in nearly-linear time},\n  author = \t {Lever, Guy and Diethe, Tom and Shawe-Taylor, John},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {685--693},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/lever12/lever12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/lever12.html},\n  abstract = \t {We propose a method to efficiently construct data dependent kernels which can make use of large quantities of (unlabeled) data. Our construction makes an approximation in the standard construction of semi-supervised kernels in Sindhwani et al. (2005). In typical cases these kernels can be computed in nearly-linear time (in the amount of data), improving on the cubic time of the standard construction, enabling large scale semi-supervised learning in a variety of contexts. The methods are validated on semi-supervised and unsupervised problems on data sets containing upto 64,000 sample points.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/lever12/lever12.pdf",
        "supp": "",
        "pdf_size": 587954,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12543236067448695688&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Deep Boltzmann Machines as Feed-Forward Hierarchies",
        "site": "https://proceedings.mlr.press/v22/montavon12.html",
        "author": "Gregoire Montavon; Mikio Braun; Klaus-Robert Muller",
        "abstract": "The deep Boltzmann machine is a powerful model that extracts the hierarchical structure of observed data. While inference is typically slow due to its undirected nature, we argue that the emerging feature hierarchy is still explicit enough to be traversed in a feed-forward fashion. The claim is corroborated by training a set of deep neural networks on real data and measuring the evolution of the representation layer after layer. The analysis reveals that the deep Boltzmann machine produces a feed-forward hierarchy of increasingly invariant representations that clearly surpasses the layer-wise approach.",
        "bibtex": "@InProceedings{pmlr-v22-montavon12,\n  title = \t {Deep Boltzmann Machines as Feed-Forward Hierarchies},\n  author = \t {Montavon, Gregoire and Braun, Mikio and Muller, Klaus-Robert},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {798--804},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/montavon12/montavon12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/montavon12.html},\n  abstract = \t {The deep Boltzmann machine is a powerful model that extracts the hierarchical structure of observed data. While inference is typically slow due to its undirected nature, we argue that the emerging feature hierarchy is still explicit enough to be traversed in a feed-forward fashion. The claim is corroborated by training a set of deep neural networks on real data and measuring the evolution of the representation layer after layer. The analysis reveals that the deep Boltzmann machine produces a feed-forward hierarchy of increasingly invariant representations that clearly surpasses the layer-wise approach.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/montavon12/montavon12.pdf",
        "supp": "",
        "pdf_size": 475035,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15272875982202282209&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Deep Learning Made Easier by Linear Transformations in Perceptrons",
        "site": "https://proceedings.mlr.press/v22/raiko12.html",
        "author": "Tapani Raiko; Harri Valpola; Yann Lecun",
        "abstract": "We transform the outputs of each hidden neuron in a multi-layer perceptron network to have zero activation and zero slope on average, and use separate shortcut connections to model the linear dependencies instead. This transformation aims at separating the problems of learning the linear and nonlinear parts of the whole input-output mapping, which has many benefits. We study the theoretical properties of the transformation by noting that they make the Fisher information matrix closer to a diagonal matrix, and thus standard gradient closer to the natural gradient. We experimentally confirm the usefulness of the transformations by noting that they make basic stochastic gradient learning competitive with state-of-the-art learning algorithms in speed, and that they seem also to help find solutions that generalize better. The experiments include both classification of small images and learning a low-dimensional representation for images by using a deep unsupervised auto-encoder network. The transformations were beneficial in all cases, with and without regularization and with networks from two to five hidden layers.",
        "bibtex": "@InProceedings{pmlr-v22-raiko12,\n  title = \t {Deep Learning Made Easier by Linear Transformations in Perceptrons},\n  author = \t {Raiko, Tapani and Valpola, Harri and Lecun, Yann},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {924--932},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/raiko12/raiko12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/raiko12.html},\n  abstract = \t {We transform the outputs of each hidden neuron in a multi-layer perceptron network to have zero activation and zero slope on average, and use separate shortcut connections to model the linear dependencies instead. This transformation aims at separating the problems of learning the linear and nonlinear parts of the whole input-output mapping, which has many benefits. We study the theoretical properties of the transformation by noting that they make the Fisher information matrix closer to a diagonal matrix, and thus standard gradient closer to the natural gradient. We experimentally confirm the usefulness of the transformations by noting that they make basic stochastic gradient learning competitive with state-of-the-art learning algorithms in speed, and that they seem also to help find solutions that generalize better. The experiments include both classification of small images and learning a low-dimensional representation for images by using a deep unsupervised auto-encoder network. The transformations were beneficial in all cases, with and without regularization and with networks from two to five hidden layers.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/raiko12/raiko12.pdf",
        "supp": "",
        "pdf_size": 881116,
        "gs_citation": 1505,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=623608584055994978&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Detecting Network Cliques with Radon Basis Pursuit",
        "site": "https://proceedings.mlr.press/v22/jiang12.html",
        "author": "Xiaoye Jiang; Yuan Yao; Han Liu; Leonidas Guibas",
        "abstract": "In this paper, we propose a novel formulation of the network clique detection problem by introducing a general network data representation framework. We show connections between our formulation with a new algebraic tool, namely Radon basis pursuit in homogeneous spaces. Such a connection allows us to identify rigorous recovery conditions for clique detection problems. Practical approximation algorithms are also developed for solving empirical problems and their usefulness is demonstrated on real-world datasets. Our work connects two seemingly different areas: network data analysis and compressed sensing, which helps to bridge the gap between the research of network data and the classical theory of statistical learning and signal processing.",
        "bibtex": "@InProceedings{pmlr-v22-jiang12,\n  title = \t {Detecting Network Cliques with Radon Basis Pursuit},\n  author = \t {Jiang, Xiaoye and Yao, Yuan and Liu, Han and Guibas, Leonidas},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {565--573},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/jiang12/jiang12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/jiang12.html},\n  abstract = \t {In this paper, we propose a novel formulation of the network clique detection problem by introducing a general network data representation framework. We show connections between our formulation with a new algebraic tool, namely Radon basis pursuit in homogeneous spaces. Such a connection allows us to identify rigorous recovery conditions for clique detection problems. Practical approximation algorithms are also developed for solving empirical problems and their usefulness is demonstrated on real-world datasets. Our work connects two seemingly different areas: network data analysis and compressed sensing, which helps to bridge the gap between the research of network data and the classical theory of statistical learning and signal processing.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/jiang12/jiang12.pdf",
        "supp": "",
        "pdf_size": 482252,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3449080643550391651&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Deterministic Annealing for Semi-Supervised Structured Output Learning",
        "site": "https://proceedings.mlr.press/v22/dhillon12.html",
        "author": "Paramveer Dhillon; Sathiya Keerthi; Kedar Bellare; Olivier Chapelle; Sundararajan Sellamanickam",
        "abstract": "In this paper we propose a new approach for semi-supervised structured output learning. Our approach uses relaxed labeling on unlabeled data to deal with the combinatorial nature of the label space and further uses domain constraints to guide the learning.  Since the overall objective is non-convex, we alternate between the optimization of the model parameters and the label distribution of unlabeled data. The alternating optimization coupled with deterministic annealing helps us achieve better local optima and as a result our approach leads to better constraint satisfaction during inference. Experimental results on sequence labeling benchmarks show superior performance of our approach compared to Constraint Driven Learning (CoDL) and Posterior Regularization (PR).",
        "bibtex": "@InProceedings{pmlr-v22-dhillon12,\n  title = \t {Deterministic Annealing for Semi-Supervised Structured Output Learning},\n  author = \t {Dhillon, Paramveer and Keerthi, Sathiya and Bellare, Kedar and Chapelle, Olivier and Sellamanickam, Sundararajan},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {299--307},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/dhillon12/dhillon12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/dhillon12.html},\n  abstract = \t {In this paper we propose a new approach for semi-supervised structured output learning. Our approach uses relaxed labeling on unlabeled data to deal with the combinatorial nature of the label space and further uses domain constraints to guide the learning.  Since the overall objective is non-convex, we alternate between the optimization of the model parameters and the label distribution of unlabeled data. The alternating optimization coupled with deterministic annealing helps us achieve better local optima and as a result our approach leads to better constraint satisfaction during inference. Experimental results on sequence labeling benchmarks show superior performance of our approach compared to Constraint Driven Learning (CoDL) and Posterior Regularization (PR).}\n}",
        "pdf": "http://proceedings.mlr.press/v22/dhillon12/dhillon12.pdf",
        "supp": "",
        "pdf_size": 341574,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6429884599416154284&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Computer & Information Science, University of Pennsylvania; Yahoo! Labs; Yahoo! Labs; Yahoo! Labs; Yahoo! Labs",
        "aff_domain": "cis.upenn.edu;yahoo-inc.com;yahoo-inc.com;yahoo-inc.com;yahoo-inc.com",
        "email": "cis.upenn.edu;yahoo-inc.com;yahoo-inc.com;yahoo-inc.com;yahoo-inc.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1;1",
        "aff_unique_norm": "University of Pennsylvania;Yahoo!",
        "aff_unique_dep": "Computer & Information Science;Yahoo! Labs",
        "aff_unique_url": "https://www.upenn.edu;https://yahoo.com",
        "aff_unique_abbr": "UPenn;Yahoo!",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Discriminative Mixtures of Sparse Latent Fields for Risk Management",
        "site": "https://proceedings.mlr.press/v22/agakov12.html",
        "author": "Felix Agakov; Peter Orchard; Amos Storkey",
        "abstract": "We describe a simple and efficient approach to learning structures of sparse high-dimensional latent variable models. Standard algorithms either learn structures of specific predefined forms, or estimate sparse graphs in the data space ignoring the possibility of the latent variables. In contrast, our method learns rich dependencies and allows for latent variables that may confound the relations between the observations. We extend the model to conditional mixtures with side information and non-Gaussian marginal distributions of the observations. We then show that our model may be used for learning sparse latent variable structures corresponding to multiple unknown states, and for uncovering features useful for explaining and predicting structural changes. We apply the model to real-world financial data with heavy-tailed marginals covering the low- and high- market volatility periods of 2005-2011. We show that our method tends to give rise to significantly higher likelihoods of test data than standard network learning methods exploiting the sparsity assumption. We also demonstrate that our approach may be practical for financial stress testing and visualization of dependencies between financial instruments.",
        "bibtex": "@InProceedings{pmlr-v22-agakov12,\n  title = \t {Discriminative Mixtures of Sparse Latent Fields for Risk Management},\n  author = \t {Agakov, Felix and Orchard, Peter and Storkey, Amos},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {10--18},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/agakov12/agakov12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/agakov12.html},\n  abstract = \t {We describe a simple and efficient approach to learning structures of sparse high-dimensional latent variable models. Standard algorithms either learn structures of specific predefined forms, or estimate sparse graphs in the data space ignoring the possibility of the latent variables. In contrast, our method learns rich dependencies and allows for latent variables that may confound the relations between the observations. We extend the model to conditional mixtures with side information and non-Gaussian marginal distributions of the observations. We then show that our model may be used for learning sparse latent variable structures corresponding to multiple unknown states, and for uncovering features useful for explaining and predicting structural changes. We apply the model to real-world financial data with heavy-tailed marginals covering the low- and high- market volatility periods of 2005-2011. We show that our method tends to give rise to significantly higher likelihoods of test data than standard network learning methods exploiting the sparsity assumption. We also demonstrate that our approach may be practical for financial stress testing and visualization of dependencies between financial instruments.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/agakov12/agakov12.pdf",
        "supp": "",
        "pdf_size": 408365,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17925025827356448220&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "Pharmatics Ltd, Edinburgh, UK + University of Edinburgh; University of Edinburgh; University of Edinburgh",
        "aff_domain": "pharmaticsltd.com;gmail.com;storkey.org",
        "email": "pharmaticsltd.com;gmail.com;storkey.org",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1;1",
        "aff_unique_norm": "Pharmatics Ltd;University of Edinburgh",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://www.ed.ac.uk",
        "aff_unique_abbr": ";Edinburgh",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Domain Adaptation: A Small Sample Statistical Approach",
        "site": "https://proceedings.mlr.press/v22/salakhutdinov12.html",
        "author": "Ruslan Salakhutdinov; Sham Kakade; Dean Foster",
        "abstract": "We study the prevalent problem when a test distribution differs from the training distribution. We consider a setting where our training set consists of a small number of sample domains, but where we have many samples in each domain. Our goal is to generalize to a new domain. For example, we may want to learn a similarity function using only certain classes of objects, but we desire that this similarity function be applicable to object classes not present in our training sample (e.g. we might seek to learn that \u201cdogs are similar to dogs\u201d even though images of dogs were absent from our training set). Our theoretical analysis shows that we can select many more features than domains while avoiding overfitting by utilizing data-dependent variance properties. We present a greedy feature selection algorithm based on using T-statistics. Our experiments validate this theory showing that our T-statistic based greedy feature selection is more robust at avoiding overfitting than the classical greedy procedure.",
        "bibtex": "@InProceedings{pmlr-v22-salakhutdinov12,\n  title = \t {Domain Adaptation: A Small Sample Statistical Approach},\n  author = \t {Salakhutdinov, Ruslan and Kakade, Sham and Foster, Dean},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {960--968},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/salakhutdinov12/salakhutdinov12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/salakhutdinov12.html},\n  abstract = \t {We study the prevalent problem when a test distribution differs from the training distribution. We consider a setting where our training set consists of a small number of sample domains, but where we have many samples in each domain. Our goal is to generalize to a new domain. For example, we may want to learn a similarity function using only certain classes of objects, but we desire that this similarity function be applicable to object classes not present in our training sample (e.g. we might seek to learn that \u201cdogs are similar to dogs\u201d even though images of dogs were absent from our training set). Our theoretical analysis shows that we can select many more features than domains while avoiding overfitting by utilizing data-dependent variance properties. We present a greedy feature selection algorithm based on using T-statistics. Our experiments validate this theory showing that our T-statistic based greedy feature selection is more robust at avoiding overfitting than the classical greedy procedure.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/salakhutdinov12/salakhutdinov12.pdf",
        "supp": "",
        "pdf_size": 520039,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:DuaPs6T0TqcJ:scholar.google.com/&scioq=Domain+Adaptation:+A+Small+Sample+Statistical+Approach&hl=en&as_sdt=0,33",
        "gs_version_total": 17,
        "aff": "Department of Statistics, University of Pennsylvania + Microsoft Research, New England; Department of Statistics, University of Pennsylvania; Department of Statistics, University of Toronto",
        "aff_domain": "wharton.upenn.edu;wharton.upenn.edu;utstat.toronto.edu",
        "email": "wharton.upenn.edu;wharton.upenn.edu;utstat.toronto.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;2",
        "aff_unique_norm": "University of Pennsylvania;Microsoft Research;University of Toronto",
        "aff_unique_dep": "Department of Statistics;;Department of Statistics",
        "aff_unique_url": "https://www.upenn.edu;https://www.microsoft.com/en-us/research/group/newengland;https://www.utoronto.ca",
        "aff_unique_abbr": "UPenn;MSR;U of T",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";New England;Toronto",
        "aff_country_unique_index": "0+0;0;1",
        "aff_country_unique": "United States;Canada"
    },
    {
        "title": "Efficient Distributed Linear Classification Algorithms via the Alternating Direction Method of Multipliers",
        "site": "https://proceedings.mlr.press/v22/zhang12a.html",
        "author": "Caoxie Zhang; Honglak Lee; Kang Shin",
        "abstract": "Linear classification has demonstrated success in many areas of applications. Modern algorithms for linear classification can train reasonably good models while going through the data in only tens of rounds. However, large data often does not fit in the memory of a single machine, which makes the bottleneck in large-scale learning the disk I/O, not the CPU. Following this observation, Yu et al. (2010) made significant progress in reducing disk usage, and their algorithms now outperform LIBLINEAR. In this paper, rather than optimizing algorithms on a single machine, we propose and implement distributed algorithms that achieve parallel disk loading and access the disk only once. Our large-scale learning algorithms are based on the framework of alternating direction methods of multipliers. The framework derives a subproblem that remains to be solved efficiently for which we propose using dual coordinate descent. Our experimental evaluations on large datasets demonstrate that the proposed algorithms achieve significant speedup over the classifier proposed by Yu et al. running on a single machine. Our algorithms are faster than existing distributed solvers, such as Zinkevich et al. (2010)\u2019s parallel stochastic gradient descent and Vowpal Wabbit.",
        "bibtex": "@InProceedings{pmlr-v22-zhang12a,\n  title = \t {Efficient Distributed Linear Classification Algorithms via the Alternating Direction Method of Multipliers},\n  author = \t {Zhang, Caoxie and Lee, Honglak and Shin, Kang},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1398--1406},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/zhang12a/zhang12a.pdf},\n  url = \t {https://proceedings.mlr.press/v22/zhang12a.html},\n  abstract = \t {Linear classification has demonstrated success in many areas of applications. Modern algorithms for linear classification can train reasonably good models while going through the data in only tens of rounds. However, large data often does not fit in the memory of a single machine, which makes the bottleneck in large-scale learning the disk I/O, not the CPU. Following this observation, Yu et al. (2010) made significant progress in reducing disk usage, and their algorithms now outperform LIBLINEAR. In this paper, rather than optimizing algorithms on a single machine, we propose and implement distributed algorithms that achieve parallel disk loading and access the disk only once. Our large-scale learning algorithms are based on the framework of alternating direction methods of multipliers. The framework derives a subproblem that remains to be solved efficiently for which we propose using dual coordinate descent. Our experimental evaluations on large datasets demonstrate that the proposed algorithms achieve significant speedup over the classifier proposed by Yu et al. running on a single machine. Our algorithms are faster than existing distributed solvers, such as Zinkevich et al. (2010)\u2019s parallel stochastic gradient descent and Vowpal Wabbit.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/zhang12a/zhang12a.pdf",
        "supp": "",
        "pdf_size": 1079171,
        "gs_citation": 87,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3059940840866216174&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of EECS, University of Michigan, Ann Arbor, MI 48109, USA; Department of EECS, University of Michigan, Ann Arbor, MI 48109, USA; Department of EECS, University of Michigan, Ann Arbor, MI 48109, USA",
        "aff_domain": "umich.edu;eecs.umich.edu;eecs.umich.edu",
        "email": "umich.edu;eecs.umich.edu;eecs.umich.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "Department of EECS",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Ann Arbor",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Efficient Gaussian Process Inference for Short-Scale Spatio-Temporal Modeling",
        "site": "https://proceedings.mlr.press/v22/luttinen12.html",
        "author": "Jaakko Luttinen; Alexander Ilin",
        "abstract": "This paper presents an efficient Gaussian process inference scheme for modeling shortscale phenomena in spatio-temporal datasets. Our model uses a sum of separable, compactly supported covariance functions, which yields a full covariance matrix represented in terms of small sparse matrices operating either on the spatial or temporal domain. The proposed inference procedure is based on Gibbs sampling, in which samples from the conditional distribution of the latent function values are obtained by applying a simple linear transformation to samples drawn from the joint distribution of the function values and the observations. We make use of the proposed model structure and the conjugate gradient method to compute the required transformation. In the experimental part, the proposed algorithm is compared to the standard approach using the sparse Cholesky decomposition and it is shown to be much faster and computationally feasible for 100-1000 times larger datasets. We demonstrate the advantages of the proposed method in the problem of reconstructing sea surface temperature, which requires processing of a real-world dataset with 10^6 observations.",
        "bibtex": "@InProceedings{pmlr-v22-luttinen12,\n  title = \t {Efficient Gaussian Process Inference for Short-Scale Spatio-Temporal Modeling},\n  author = \t {Luttinen, Jaakko and Ilin, Alexander},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {741--750},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/luttinen12/luttinen12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/luttinen12.html},\n  abstract = \t {This paper presents an efficient Gaussian process inference scheme for modeling shortscale phenomena in spatio-temporal datasets. Our model uses a sum of separable, compactly supported covariance functions, which yields a full covariance matrix represented in terms of small sparse matrices operating either on the spatial or temporal domain. The proposed inference procedure is based on Gibbs sampling, in which samples from the conditional distribution of the latent function values are obtained by applying a simple linear transformation to samples drawn from the joint distribution of the function values and the observations. We make use of the proposed model structure and the conjugate gradient method to compute the required transformation. In the experimental part, the proposed algorithm is compared to the standard approach using the sparse Cholesky decomposition and it is shown to be much faster and computationally feasible for 100-1000 times larger datasets. We demonstrate the advantages of the proposed method in the problem of reconstructing sea surface temperature, which requires processing of a real-world dataset with 10^6 observations.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/luttinen12/luttinen12.pdf",
        "supp": "",
        "pdf_size": 809272,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3838508761674507759&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Aalto University, Finland; Aalto University, Finland",
        "aff_domain": "aalto.fi;aalto.fi",
        "email": "aalto.fi;aalto.fi",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Aalto University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.aalto.fi",
        "aff_unique_abbr": "Aalto",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Finland"
    },
    {
        "title": "Efficient Hypergraph Clustering",
        "site": "https://proceedings.mlr.press/v22/leordeanu12.html",
        "author": "Marius Leordeanu; Cristian Sminchisescu",
        "abstract": "Data clustering is an essential problem in data mining, machine learning and computer vision. In this paper we present a novel method for the hypergraph clustering problem, in which second or higher order affinities between sets of data points are considered. Our algorithm has important theoretical properties, such as convergence and satisfaction of first order necessary optimality conditions. It is based on an efficient iterative procedure, which by updating the cluster membership of all points in parallel, is able to achieve state of the art results in very few steps. We outperform current hypergraph clustering methods especially in terms of computational speed, but also in terms of accuracy. Moreover, we show that our method could be successfully applied both to higher-order assignment problems and to image segmentation.",
        "bibtex": "@InProceedings{pmlr-v22-leordeanu12,\n  title = \t {Efficient Hypergraph Clustering},\n  author = \t {Leordeanu, Marius and Sminchisescu, Cristian},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {676--684},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/leordeanu12/leordeanu12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/leordeanu12.html},\n  abstract = \t {Data clustering is an essential problem in data mining, machine learning and computer vision. In this paper we present a novel method for the hypergraph clustering problem, in which second or higher order affinities between sets of data points are considered. Our algorithm has important theoretical properties, such as convergence and satisfaction of first order necessary optimality conditions. It is based on an efficient iterative procedure, which by updating the cluster membership of all points in parallel, is able to achieve state of the art results in very few steps. We outperform current hypergraph clustering methods especially in terms of computational speed, but also in terms of accuracy. Moreover, we show that our method could be successfully applied both to higher-order assignment problems and to image segmentation.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/leordeanu12/leordeanu12.pdf",
        "supp": "",
        "pdf_size": 2606759,
        "gs_citation": 65,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8699390624405881815&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Institute of Mathematics of the Romanian Academy; Faculty of Mathematics and Natural Science, University of Bonn + Institute of Mathematics of the Romanian Academy",
        "aff_domain": "imar.ro;ins.uni-bonn.de",
        "email": "imar.ro;ins.uni-bonn.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+0",
        "aff_unique_norm": "Romanian Academy;University of Bonn",
        "aff_unique_dep": "Institute of Mathematics;Faculty of Mathematics and Natural Science",
        "aff_unique_url": "https://www.math.ro/;https://www.uni-bonn.de",
        "aff_unique_abbr": "IMAR;Uni Bonn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1+0",
        "aff_country_unique": "Romania;Germany"
    },
    {
        "title": "Efficient Sampling from Combinatorial Space via Bridging",
        "site": "https://proceedings.mlr.press/v22/lin12.html",
        "author": "Dahua Lin; John Fisher",
        "abstract": "MCMC sampling has been extensively studied and used in probabilistic inference. Many algorithms rely on local updates to explore the space, often resulting in slow convergence or failure to mix when there is no path from one set of states to another via local changes. We propose an efficient method for sampling from combinatorial spaces that addresses these issues via \u201cbridging states\u201d that facilitate the communication between different parts of the space. Such states can be created dynamically, providing more flexibility than methods relying on specific space structures to design jump proposals. Theoretical analysis of the approach yields bounds on mixing times. Empirical analysis demonstrates the practical utility on two problems: constrained map labeling and inferring partial order of object layers in a video.",
        "bibtex": "@InProceedings{pmlr-v22-lin12,\n  title = \t {Efficient Sampling from Combinatorial Space via Bridging},\n  author = \t {Lin, Dahua and Fisher, John},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {694--702},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/lin12/lin12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/lin12.html},\n  abstract = \t {MCMC sampling has been extensively studied and used in probabilistic inference. Many algorithms rely on local updates to explore the space, often resulting in slow convergence or failure to mix when there is no path from one set of states to another via local changes. We propose an efficient method for sampling from combinatorial spaces that addresses these issues via \u201cbridging states\u201d that facilitate the communication between different parts of the space. Such states can be created dynamically, providing more flexibility than methods relying on specific space structures to design jump proposals. Theoretical analysis of the approach yields bounds on mixing times. Empirical analysis demonstrates the practical utility on two problems: constrained map labeling and inferring partial order of object layers in a video.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/lin12/lin12.pdf",
        "supp": "",
        "pdf_size": 1151862,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14969748222617862815&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "CSAIL, MIT; CSAIL, MIT",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory",
        "aff_unique_url": "https://www.csail.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Efficient and Exact MAP-MRF Inference using Branch and Bound",
        "site": "https://proceedings.mlr.press/v22/sun12.html",
        "author": "Min Sun; Murali Telaprolu; Honglak Lee; Silvio Savarese",
        "abstract": "We propose two novel Branch-and-Bound (BB) methods to efficiently solve exact MAP-MRF inference on problems with a large number of states (per variable) H. By organizing the data in a suitable structure, the time complexity of our best method for evaluating the bound at each branch is reduced from O(H^2) to O(H). This permits searching for the MAP solution in O(BH+Q) instead of O(BH^2) (without using the data structure), where B is the number of branches and Q is the one-time cost to build the data structure which is proportional to H^2. Our analysis on synthetic data shows that, given a limited time budget, our method solves problems that are characterized by a much larger number of states when compared to state-of-the-art exact inference algorithms (e.g., Marinescu and Dechter (2007); Sontag et al. (2008)) and other baseline BB methods. We further show that our method is well suited for computer vision and computational biology problems where the state space (per variable) is large. In particular, our approach is an order of magnitude faster on average than Sontag et al.\u2019s Cluster Pursuit\u00a0(CP) on human pose estimation problems. Moreover, given a time budget of up to 20 minutes, our method consistently solves more protein design problems than CP does. Finally, we successfully explore different branching strategies and ways to utilize domain knowledge of the problem to significantly reduce the empirical inference time.",
        "bibtex": "@InProceedings{pmlr-v22-sun12,\n  title = \t {Efficient and Exact MAP-MRF Inference using Branch and Bound},\n  author = \t {Sun, Min and Telaprolu, Murali and Lee, Honglak and Savarese, Silvio},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1134--1142},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/sun12/sun12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/sun12.html},\n  abstract = \t {We propose two novel Branch-and-Bound (BB) methods to efficiently solve exact MAP-MRF inference on problems with a large number of states (per variable) H. By organizing the data in a suitable structure, the time complexity of our best method for evaluating the bound at each branch is reduced from O(H^2) to O(H). This permits searching for the MAP solution in O(BH+Q) instead of O(BH^2) (without using the data structure), where B is the number of branches and Q is the one-time cost to build the data structure which is proportional to H^2. Our analysis on synthetic data shows that, given a limited time budget, our method solves problems that are characterized by a much larger number of states when compared to state-of-the-art exact inference algorithms (e.g., Marinescu and Dechter (2007); Sontag et al. (2008)) and other baseline BB methods. We further show that our method is well suited for computer vision and computational biology problems where the state space (per variable) is large. In particular, our approach is an order of magnitude faster on average than Sontag et al.\u2019s Cluster Pursuit\u00a0(CP) on human pose estimation problems. Moreover, given a time budget of up to 20 minutes, our method consistently solves more protein design problems than CP does. Finally, we successfully explore different branching strategies and ways to utilize domain knowledge of the problem to significantly reduce the empirical inference time.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/sun12/sun12.pdf",
        "supp": "",
        "pdf_size": 3693453,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12762117806983289289&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI 48109; Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI 48109; Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI 48109; Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI 48109",
        "aff_domain": "umich.edu;umich.edu;eecs.umich.edu;eecs.umich.edu",
        "email": "umich.edu;umich.edu;eecs.umich.edu;eecs.umich.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "Department of Electrical Engineering and Computer Science",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Ann Arbor",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Error bounds for Kernel Fisher Linear Discriminant in Gaussian Hilbert space",
        "site": "https://proceedings.mlr.press/v22/durrant12.html",
        "author": "Robert Durrant; Ata Kaban",
        "abstract": "We give a non-trivial, non-asymptotic upper bound on the classification error of the popular Kernel Fisher Linear Discriminant classifier under  the assumption that the kernel-induced space is a Gaussian Hilbert space.",
        "bibtex": "@InProceedings{pmlr-v22-durrant12,\n  title = \t {Error bounds for Kernel Fisher Linear Discriminant in Gaussian Hilbert space},\n  author = \t {Durrant, Robert and Kaban, Ata},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {337--345},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/durrant12/durrant12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/durrant12.html},\n  abstract = \t {We give a non-trivial, non-asymptotic upper bound on the classification error of the popular Kernel Fisher Linear Discriminant classifier under  the assumption that the kernel-induced space is a Gaussian Hilbert space.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/durrant12/durrant12.pdf",
        "supp": "",
        "pdf_size": 395380,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1384130285038410591&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "School of Computer Science, University of Birmingham, Edgbaston, UK, B15 2TT; School of Computer Science, University of Birmingham, Edgbaston, UK, B15 2TT",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Birmingham",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.birmingham.ac.uk",
        "aff_unique_abbr": "UoB",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Edgbaston",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Evaluation of marginal likelihoods via the density of states",
        "site": "https://proceedings.mlr.press/v22/habeck12.html",
        "author": "Michael Habeck",
        "abstract": "Bayesian model comparison involves the evaluation of the marginal likelihood, the expectation of the likelihood under the prior distribution. Typically, this high-dimensional integral over all model parameters is approximated using Markov chain Monte Carlo methods. Thermodynamic integration is a popular method to estimate the marginal likelihood by using samples from annealed posteriors. Here we show that there exists a robust and flexible alternative. The new method estimates the density of states, which counts the number of states associated with a particular value of the likelihood. If the density of states is known, computation of the marginal likelihood reduces to a one- dimensional integral. We outline a maximum likelihood procedure to estimate the density of states from annealed posterior samples. We apply our method to various likelihoods and show that it is superior to thermodynamic integration in that it is more flexible with regard to the annealing schedule and the family of bridging distributions. Finally, we discuss the relation of our method with Skilling\u2019s nested sampling.",
        "bibtex": "@InProceedings{pmlr-v22-habeck12,\n  title = \t {Evaluation of marginal likelihoods via the density of states},\n  author = \t {Habeck, Michael},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {486--494},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/habeck12/habeck12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/habeck12.html},\n  abstract = \t {Bayesian model comparison involves the evaluation of the marginal likelihood, the expectation of the likelihood under the prior distribution. Typically, this high-dimensional integral over all model parameters is approximated using Markov chain Monte Carlo methods. Thermodynamic integration is a popular method to estimate the marginal likelihood by using samples from annealed posteriors. Here we show that there exists a robust and flexible alternative. The new method estimates the density of states, which counts the number of states associated with a particular value of the likelihood. If the density of states is known, computation of the marginal likelihood reduces to a one- dimensional integral. We outline a maximum likelihood procedure to estimate the density of states from annealed posterior samples. We apply our method to various likelihoods and show that it is superior to thermodynamic integration in that it is more flexible with regard to the annealing schedule and the family of bridging distributions. Finally, we discuss the relation of our method with Skilling\u2019s nested sampling.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/habeck12/habeck12.pdf",
        "supp": "",
        "pdf_size": 2240759,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13327923482632149364&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Exact Subspace Segmentation and Outlier Detection by Low-Rank Representation",
        "site": "https://proceedings.mlr.press/v22/liu12a.html",
        "author": "Guangcan Liu; Huan Xu; Shuicheng Yan",
        "abstract": "In this work, we address the following matrix recovery problem: suppose we are given a set of data points containing two parts, one part consists of samples drawn from a union of multiple subspaces and the other part consists of outliers. We do not know which data points are outliers, or how many outliers there are. The rank and number of the subspaces are unknown either. Can we detect the outliers and segment the samples into their right subspaces, efficiently and exactly? We utilize a so-called Low-Rank Representation (LRR) method to solve this problem, and prove that under mild technical conditions, any solution to LRR exactly recover the row space of the samples and detect the outliers as well. Since the subspace membership is provably determined by the row space, this further implies that LRR can perform exact subspace segmentation and outlier detection, in an efficient way.",
        "bibtex": "@InProceedings{pmlr-v22-liu12a,\n  title = \t {Exact Subspace Segmentation and Outlier Detection by Low-Rank Representation},\n  author = \t {Liu, Guangcan and Xu, Huan and Yan, Shuicheng},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {703--711},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/liu12a/liu12a.pdf},\n  url = \t {https://proceedings.mlr.press/v22/liu12a.html},\n  abstract = \t {In this work, we address the following matrix recovery problem: suppose we are given a set of data points containing two parts, one part consists of samples drawn from a union of multiple subspaces and the other part consists of outliers. We do not know which data points are outliers, or how many outliers there are. The rank and number of the subspaces are unknown either. Can we detect the outliers and segment the samples into their right subspaces, efficiently and exactly? We utilize a so-called Low-Rank Representation (LRR) method to solve this problem, and prove that under mild technical conditions, any solution to LRR exactly recover the row space of the samples and detect the outliers as well. Since the subspace membership is provably determined by the row space, this further implies that LRR can perform exact subspace segmentation and outlier detection, in an efficient way.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/liu12a/liu12a.pdf",
        "supp": "",
        "pdf_size": 523434,
        "gs_citation": 81,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6667781767921740187&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Electrical and Computer Engineering, National University of Singapore; Mechanical Engineering, National University of Singapore; Electrical and Computer Engineering, National University of Singapore",
        "aff_domain": "gmail.com;nus.edu.sg;nus.edu.sg",
        "email": "gmail.com;nus.edu.sg;nus.edu.sg",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "National University of Singapore",
        "aff_unique_dep": "Electrical and Computer Engineering",
        "aff_unique_url": "https://www.nus.edu.sg",
        "aff_unique_abbr": "NUS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "title": "Exchangeability Characterizes Optimality of Sequential Normalized Maximum Likelihood and Bayesian Prediction with Jeffreys Prior",
        "site": "https://proceedings.mlr.press/v22/hedayati12.html",
        "author": "Fares Hedayati; Peter Bartlett",
        "abstract": "We study online prediction of individual sequences under logarithmic loss with parametric constant experts. The optimal strategy, normalized maximum likelihood (NML), is computationally demanding and requires the length of the game to be known. We consider two simpler strategies: sequential normalized maximum likelihood (SNML), which computes the NML forecasts at each round as if it were the last round, and Bayesian prediction. Under appropriate conditions, both are known to achieve near-optimal regret.  In this paper, we investigate when these strategies are optimal. We show that SNML is optimal iff the joint distribution on sequences defined by SNML is exchangeable. In the case of exponential families, this is equivalent to the optimality of any Bayesian prediction strategy, and the optimal prior is Jeffreys prior.",
        "bibtex": "@InProceedings{pmlr-v22-hedayati12,\n  title = \t {Exchangeability Characterizes Optimality of Sequential Normalized Maximum Likelihood and Bayesian Prediction with Jeffreys Prior},\n  author = \t {Hedayati, Fares and Bartlett, Peter},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {504--510},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/hedayati12/hedayati12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/hedayati12.html},\n  abstract = \t {We study online prediction of individual sequences under logarithmic loss with parametric constant experts. The optimal strategy, normalized maximum likelihood (NML), is computationally demanding and requires the length of the game to be known. We consider two simpler strategies: sequential normalized maximum likelihood (SNML), which computes the NML forecasts at each round as if it were the last round, and Bayesian prediction. Under appropriate conditions, both are known to achieve near-optimal regret.  In this paper, we investigate when these strategies are optimal. We show that SNML is optimal iff the joint distribution on sequences defined by SNML is exchangeable. In the case of exponential families, this is equivalent to the optimality of any Bayesian prediction strategy, and the optimal prior is Jeffreys prior.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/hedayati12/hedayati12.pdf",
        "supp": "",
        "pdf_size": 309827,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11831677767403287816&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Computer Science Division, University of California at Berkeley; EECS and Statistics, University of California at Berkeley + Queensland University of Technology",
        "aff_domain": "; ",
        "email": "; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "University of California, Berkeley;Queensland University of Technology",
        "aff_unique_dep": "Computer Science Division;",
        "aff_unique_url": "https://www.berkeley.edu;https://www.qut.edu.au",
        "aff_unique_abbr": "UC Berkeley;QUT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;0+1",
        "aff_country_unique": "United States;Australia"
    },
    {
        "title": "Exploiting Unrelated Tasks in Multi-Task Learning",
        "site": "https://proceedings.mlr.press/v22/romera12.html",
        "author": "Bernardino Romera Paredes; Andreas Argyriou; Nadia Berthouze; Massimiliano Pontil",
        "abstract": "We study the problem of learning a group of principal tasks using a group of auxiliary tasks, unrelated to the principal ones. In many applications, joint learning of unrelated tasks which use the same input data can be beneficial. The reason is that prior knowledge about which tasks are unrelated can lead to sparser and more informative representations for each task, essentially screening out idiosyncrasies of the data distribution. We propose a novel method which builds on a prior multitask methodology by favoring a shared low dimensional  representation within each group of tasks. In addition, we impose a penalty on tasks from different groups which encourages the two representations to be orthogonal. We further discuss a condition which ensures convexity of the optimization problem and argue that it can be solved by alternating minimization. We present experiments on synthetic and real data, which indicate that incorporating unrelated tasks can improve significantly over standard multi-task learning methods.",
        "bibtex": "@InProceedings{pmlr-v22-romera12,\n  title = \t {Exploiting Unrelated Tasks in Multi-Task Learning},\n  author = \t {Paredes, Bernardino Romera and Argyriou, Andreas and Berthouze, Nadia and Pontil, Massimiliano},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {951--959},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/romera12/romera12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/romera12.html},\n  abstract = \t {We study the problem of learning a group of principal tasks using a group of auxiliary tasks, unrelated to the principal ones. In many applications, joint learning of unrelated tasks which use the same input data can be beneficial. The reason is that prior knowledge about which tasks are unrelated can lead to sparser and more informative representations for each task, essentially screening out idiosyncrasies of the data distribution. We propose a novel method which builds on a prior multitask methodology by favoring a shared low dimensional  representation within each group of tasks. In addition, we impose a penalty on tasks from different groups which encourages the two representations to be orthogonal. We further discuss a condition which ensures convexity of the optimization problem and argue that it can be solved by alternating minimization. We present experiments on synthetic and real data, which indicate that incorporating unrelated tasks can improve significantly over standard multi-task learning methods.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/romera12/romera12.pdf",
        "supp": "",
        "pdf_size": 557195,
        "gs_citation": 153,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=378451327004704313&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Factorized Asymptotic Bayesian Inference for Mixture Modeling",
        "site": "https://proceedings.mlr.press/v22/fujimaki12.html",
        "author": "Ryohei Fujimaki; Satoshi Morinaga",
        "abstract": "This paper proposes a novel Bayesian approximation inference method for mixture modeling. Our key idea is to factorize marginal log-likelihood using a variational distribution over latent variables. An asymptotic approximation, a factorized information criterion (FIC), is obtained by applying the Laplace method to each of the factorized components. In order to evaluate FIC, we propose factorized asymptotic Bayesian inference (FAB), which maximizes an asymptotically-consistent lower bound of FIC. FIC and FAB have several desirable properties: 1) asymptotic consistency with the marginal log-likelihood, 2) automatic component selection on the basis of an intrinsic shrinkage mechanism, and 3) parameter identifiability in mixture modeling. Experimental results show that FAB outperforms state-of-the-art VB methods.",
        "bibtex": "@InProceedings{pmlr-v22-fujimaki12,\n  title = \t {Factorized Asymptotic Bayesian Inference for Mixture Modeling},\n  author = \t {Fujimaki, Ryohei and Morinaga, Satoshi},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {400--408},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/fujimaki12/fujimaki12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/fujimaki12.html},\n  abstract = \t {This paper proposes a novel Bayesian approximation inference method for mixture modeling. Our key idea is to factorize marginal log-likelihood using a variational distribution over latent variables. An asymptotic approximation, a factorized information criterion (FIC), is obtained by applying the Laplace method to each of the factorized components. In order to evaluate FIC, we propose factorized asymptotic Bayesian inference (FAB), which maximizes an asymptotically-consistent lower bound of FIC. FIC and FAB have several desirable properties: 1) asymptotic consistency with the marginal log-likelihood, 2) automatic component selection on the basis of an intrinsic shrinkage mechanism, and 3) parameter identifiability in mixture modeling. Experimental results show that FAB outperforms state-of-the-art VB methods.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/fujimaki12/fujimaki12.pdf",
        "supp": "",
        "pdf_size": 726059,
        "gs_citation": 76,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10289948598567363841&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "NEC Laboratories America; NEC Corporation",
        "aff_domain": "sv.nec-labs.com;cw.jp.nec.com",
        "email": "sv.nec-labs.com;cw.jp.nec.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "NEC Laboratories America;NEC Corporation",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.nec-labs.com;https://www.nec.com",
        "aff_unique_abbr": "NEC Labs America;NEC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United States;Japan"
    },
    {
        "title": "Factorized Diffusion Map Approximation",
        "site": "https://proceedings.mlr.press/v22/amizadeh12.html",
        "author": "Saeed Amizadeh; Hamed Valizadegan; Milos Hauskrecht",
        "abstract": "Diffusion maps are among the most powerful Machine Learning tools to analyze and work with complex high-dimensional datasets. Unfortunately, the estimation of these maps from a finite sample is known to suffer from the curse of dimensionality. Motivated by other machine learning models for which   the existence of structure in the underlying distribution of data can reduce the complexity of estimation, we study and show how the factorization of the underlying  distribution into independent subspaces can help us to estimate diffusion maps more accurately. Building upon this result, we propose and develop  an algorithm that can automatically factorize a high dimensional data space in order to  minimize the error of estimation of its diffusion map, even in the case when the underlying distribution is not decomposable. Experiments on both the synthetic and real-world datasets demonstrate improved estimation performance of our method over  the regular diffusion-map framework.",
        "bibtex": "@InProceedings{pmlr-v22-amizadeh12,\n  title = \t {Factorized Diffusion Map Approximation},\n  author = \t {Amizadeh, Saeed and Valizadegan, Hamed and Hauskrecht, Milos},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {37--46},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/amizadeh12/amizadeh12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/amizadeh12.html},\n  abstract = \t {Diffusion maps are among the most powerful Machine Learning tools to analyze and work with complex high-dimensional datasets. Unfortunately, the estimation of these maps from a finite sample is known to suffer from the curse of dimensionality. Motivated by other machine learning models for which   the existence of structure in the underlying distribution of data can reduce the complexity of estimation, we study and show how the factorization of the underlying  distribution into independent subspaces can help us to estimate diffusion maps more accurately. Building upon this result, we propose and develop  an algorithm that can automatically factorize a high dimensional data space in order to  minimize the error of estimation of its diffusion map, even in the case when the underlying distribution is not decomposable. Experiments on both the synthetic and real-world datasets demonstrate improved estimation performance of our method over  the regular diffusion-map framework.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/amizadeh12/amizadeh12.pdf",
        "supp": "",
        "pdf_size": 503359,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16863764632791083523&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Fast Learning Rate of Multiple Kernel Learning: Trade-Off between Sparsity and Smoothness",
        "site": "https://proceedings.mlr.press/v22/suzuki12.html",
        "author": "Taiji Suzuki; Masashi Sugiyama",
        "abstract": "We investigate the learning rate of multiple kernel leaning (MKL) with L1 and elastic-net regularizations. The elastic-net regularization is a composition of an L1-regularizer for inducing the sparsity and an L2-regularizer for controlling the smoothness. We focus on a sparse setting where the total number of kernels is large but the number of non-zero components of the ground truth is relatively small, and show sharper convergence rates than the learning rates ever shown for both L1 and elastic-net regularizations. Our analysis shows there appears a trade-off between the sparsity and the smoothness when it comes to selecting which of L1 and elastic-net regularizations to use; if the ground truth is smooth, the elastic-net regularization is preferred, otherwise the L1 regularization is preferred.",
        "bibtex": "@InProceedings{pmlr-v22-suzuki12,\n  title = \t {Fast Learning Rate of Multiple Kernel Learning: Trade-Off between Sparsity and Smoothness},\n  author = \t {Suzuki, Taiji and Sugiyama, Masashi},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1152--1183},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/suzuki12/suzuki12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/suzuki12.html},\n  abstract = \t {We investigate the learning rate of multiple kernel leaning (MKL) with L1 and elastic-net regularizations. The elastic-net regularization is a composition of an L1-regularizer for inducing the sparsity and an L2-regularizer for controlling the smoothness. We focus on a sparse setting where the total number of kernels is large but the number of non-zero components of the ground truth is relatively small, and show sharper convergence rates than the learning rates ever shown for both L1 and elastic-net regularizations. Our analysis shows there appears a trade-off between the sparsity and the smoothness when it comes to selecting which of L1 and elastic-net regularizations to use; if the ground truth is smooth, the elastic-net regularization is preferred, otherwise the L1 regularization is preferred.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/suzuki12/suzuki12.pdf",
        "supp": "",
        "pdf_size": 1025890,
        "gs_citation": 88,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3720129338730709591&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 16,
        "aff": "The University of Tokyo; Tokyo Institute of Technology",
        "aff_domain": "stat.t.u-tokyo.ac.jp;cs.titech.ac.jp",
        "email": "stat.t.u-tokyo.ac.jp;cs.titech.ac.jp",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Tokyo;Tokyo Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.u-tokyo.ac.jp;https://www.titech.ac.jp",
        "aff_unique_abbr": "UTokyo;Titech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Fast Variational Bayesian Inference for Non-Conjugate Matrix Factorization Models",
        "site": "https://proceedings.mlr.press/v22/seeger12.html",
        "author": "Matthias Seeger; Guillaume Bouchard",
        "abstract": "Probabilistic matrix factorization methods aim to extract meaningful correlation structure from an incomplete data matrix by postulating low rank constraints. Recently, variational Bayesian (VB) inference techniques have successfully been applied to such large scale bilinear models. However, current algorithms are of the alternate updating or stochastic gradient descent type, slow to converge and prone to getting stuck in shallow local minima. While for MAP or maximum margin estimation, singular value shrinkage algorithms have been proposed which can far outperform alternate updating, this methodological avenue remains unexplored for Bayesian techniques. In this paper, we show how to combine a recent singular value shrinkage characterization of fully observed spherical Gaussian VB matrix factorization with augmented Lagrangian techniques in order to obtain efficient VB inference for general MF models with arbitrary likelihood potentials. In particular, we show how to handle Poisson and Bernoulli potentials, far more suited for most MF applications than Gaussian likelihoods. Our algorithm can be run even for very large models and is easily implemented in \\em Matlab. It outperforms MAP estimation on a range of real-world datasets.",
        "bibtex": "@InProceedings{pmlr-v22-seeger12,\n  title = \t {Fast Variational Bayesian Inference for Non-Conjugate Matrix Factorization Models},\n  author = \t {Seeger, Matthias and Bouchard, Guillaume},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1012--1018},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/seeger12/seeger12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/seeger12.html},\n  abstract = \t {Probabilistic matrix factorization methods aim to extract meaningful correlation structure from an incomplete data matrix by postulating low rank constraints. Recently, variational Bayesian (VB) inference techniques have successfully been applied to such large scale bilinear models. However, current algorithms are of the alternate updating or stochastic gradient descent type, slow to converge and prone to getting stuck in shallow local minima. While for MAP or maximum margin estimation, singular value shrinkage algorithms have been proposed which can far outperform alternate updating, this methodological avenue remains unexplored for Bayesian techniques. In this paper, we show how to combine a recent singular value shrinkage characterization of fully observed spherical Gaussian VB matrix factorization with augmented Lagrangian techniques in order to obtain efficient VB inference for general MF models with arbitrary likelihood potentials. In particular, we show how to handle Poisson and Bernoulli potentials, far more suited for most MF applications than Gaussian likelihoods. Our algorithm can be run even for very large models and is easily implemented in \\em Matlab. It outperforms MAP estimation on a range of real-world datasets.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/seeger12/seeger12.pdf",
        "supp": "",
        "pdf_size": 304996,
        "gs_citation": 62,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10500216296286498563&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Probabilistic Machine Learning Laboratory, Ecole Polytechnique F\u00e9d\u00e9r\u00e1le de Lausanne; Xerox Research Centre Europe",
        "aff_domain": "epfl.ch;xerox.com",
        "email": "epfl.ch;xerox.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Ecole Polytechnique F\u00e9d\u00e9r\u00e1le de Lausanne;Xerox Research Centre Europe",
        "aff_unique_dep": "Probabilistic Machine Learning Laboratory;",
        "aff_unique_url": "https://epfl.ch;https://www.xerox.com/en-us/innovation/research-centers/europe",
        "aff_unique_abbr": "EPFL;XRCE",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Lausanne;",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Switzerland;Unknown"
    },
    {
        "title": "Fast Variational Mode-Seeking",
        "site": "https://proceedings.mlr.press/v22/thiesson12.html",
        "author": "Bo Thiesson; Jingu Kim",
        "abstract": "Mode-seeking algorithms (e.g., mean-shift) constitute a class of powerful non-parametric clustering methods, but they are slow. We present VMS, a dual-tree based variational EM framework for mode-seeking that greatly accelerates performance. VMS has a number of pleasing properties: it generalizes across different mode-seeking algorithms, it does not have typical homoscedasticity constraints on kernel bandwidths, and it is the first truly sub-quadratic acceleration method that maintains provable convergence for a well-defined objective function. Experimental results demonstrate acceleration benefits over competing methods and show that VMS is particularly desirable for data sets of massive size, where a coarser approximation is needed to improve the computational efficiency.",
        "bibtex": "@InProceedings{pmlr-v22-thiesson12,\n  title = \t {Fast Variational Mode-Seeking},\n  author = \t {Thiesson, Bo and Kim, Jingu},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1230--1242},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/thiesson12/thiesson12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/thiesson12.html},\n  abstract = \t {Mode-seeking algorithms (e.g., mean-shift) constitute a class of powerful non-parametric clustering methods, but they are slow. We present VMS, a dual-tree based variational EM framework for mode-seeking that greatly accelerates performance. VMS has a number of pleasing properties: it generalizes across different mode-seeking algorithms, it does not have typical homoscedasticity constraints on kernel bandwidths, and it is the first truly sub-quadratic acceleration method that maintains provable convergence for a well-defined objective function. Experimental results demonstrate acceleration benefits over competing methods and show that VMS is particularly desirable for data sets of massive size, where a coarser approximation is needed to improve the computational efficiency.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/thiesson12/thiesson12.pdf",
        "supp": "",
        "pdf_size": 526288,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12597165670080098399&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Microsoft Research; Georgia Institute of Technology",
        "aff_domain": "microsoft.com;cc.gatech.edu",
        "email": "microsoft.com;cc.gatech.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Microsoft Corporation;Georgia Institute of Technology",
        "aff_unique_dep": "Microsoft Research;",
        "aff_unique_url": "https://www.microsoft.com/en-us/research;https://www.gatech.edu",
        "aff_unique_abbr": "MSR;Georgia Tech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Fast interior-point inference in high-dimensional sparse, penalized state-space models",
        "site": "https://proceedings.mlr.press/v22/pnevmatikakis12.html",
        "author": "Eftychios Pnevmatikakis; Liam Paninski",
        "abstract": "We present an algorithm for fast posterior inference in penalized high-dimensional state-space models, suitable in the case where a few measurements are taken in each time step. We assume that the state prior and observation likelihoods are log-concave and have a special structure that allows fast matrix-vector operations. We derive a second-order algorithm for computing the maximum a posteriori state path estimate, where the cost per iteration scales linearly both in time and memory. This is done by computing an approximate Newton direction using an efficient forward-backward scheme based on a sequence of low rank updates. We formalize the conditions under which our algorithm is applicable and prove its stability and convergence. We show that the state vector can be drawn from a large class of prior distributions without affecting the linear complexity of our algorithm. This class includes both Gaussian and nonsmooth sparse and group sparse priors for which we employ an interior point modification of our algorithm. We discuss applications in text modeling and neuroscience.",
        "bibtex": "@InProceedings{pmlr-v22-pnevmatikakis12,\n  title = \t {Fast interior-point inference in high-dimensional sparse, penalized state-space models},\n  author = \t {Pnevmatikakis, Eftychios and Paninski, Liam},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {895--904},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/pnevmatikakis12/pnevmatikakis12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/pnevmatikakis12.html},\n  abstract = \t {We present an algorithm for fast posterior inference in penalized high-dimensional state-space models, suitable in the case where a few measurements are taken in each time step. We assume that the state prior and observation likelihoods are log-concave and have a special structure that allows fast matrix-vector operations. We derive a second-order algorithm for computing the maximum a posteriori state path estimate, where the cost per iteration scales linearly both in time and memory. This is done by computing an approximate Newton direction using an efficient forward-backward scheme based on a sequence of low rank updates. We formalize the conditions under which our algorithm is applicable and prove its stability and convergence. We show that the state vector can be drawn from a large class of prior distributions without affecting the linear complexity of our algorithm. This class includes both Gaussian and nonsmooth sparse and group sparse priors for which we employ an interior point modification of our algorithm. We discuss applications in text modeling and neuroscience.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/pnevmatikakis12/pnevmatikakis12.pdf",
        "supp": "",
        "pdf_size": 404456,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7290564783635139657&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Columbia University; Columbia University",
        "aff_domain": "stat.columbia.edu;stat.columbia.edu",
        "email": "stat.columbia.edu;stat.columbia.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Columbia University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.columbia.edu",
        "aff_unique_abbr": "Columbia",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Fast, Exact Model Selection and Permutation Testing for l2-Regularized Logistic Regression",
        "site": "https://proceedings.mlr.press/v22/conroy12.html",
        "author": "Bryan Conroy; Paul Sajda",
        "abstract": "Regularized logistic regression is a standard classification method used in statistics and machine learning. Unlike regularized least squares problems such as ridge regression, the parameter estimates cannot be computed in closed-form and instead must be estimated using an iterative technique. This paper addresses the computational problem of regularized logistic regression that is commonly encountered in model selection and classifier statistical significance testing, in which a large number of related logistic regression problems must be solved for. Our proposed approach solves the problems simultaneously through an iterative technique, which also garners computational efficiencies by leveraging the redundancies across the related problems. We demonstrate analytically that our method provides a substantial complexity reduction, which is further validated by our results on real-world datasets.",
        "bibtex": "@InProceedings{pmlr-v22-conroy12,\n  title = \t {Fast, Exact Model Selection and Permutation Testing for l2-Regularized Logistic Regression},\n  author = \t {Conroy, Bryan and Sajda, Paul},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {246--254},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/conroy12/conroy12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/conroy12.html},\n  abstract = \t {Regularized logistic regression is a standard classification method used in statistics and machine learning. Unlike regularized least squares problems such as ridge regression, the parameter estimates cannot be computed in closed-form and instead must be estimated using an iterative technique. This paper addresses the computational problem of regularized logistic regression that is commonly encountered in model selection and classifier statistical significance testing, in which a large number of related logistic regression problems must be solved for. Our proposed approach solves the problems simultaneously through an iterative technique, which also garners computational efficiencies by leveraging the redundancies across the related problems. We demonstrate analytically that our method provides a substantial complexity reduction, which is further validated by our results on real-world datasets.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/conroy12/conroy12.pdf",
        "supp": "",
        "pdf_size": 390050,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11866332957266542733&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 11,
        "aff": "Columbia University; Columbia University",
        "aff_domain": "columbia.edu;columbia.edu",
        "email": "columbia.edu;columbia.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Columbia University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.columbia.edu",
        "aff_unique_abbr": "Columbia",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Flexible Martingale Priors for Deep Hierarchies",
        "site": "https://proceedings.mlr.press/v22/steinhardt12.html",
        "author": "Jacob Steinhardt; Zoubin Ghahramani",
        "abstract": "When building priors over trees for Bayesian hierarchical models, there is a tension between maintaining desirable theoretical properties such as infinite exchangeability and important practical properties such as the ability to increase the depth of the tree to accommodate new data. We resolve this tension by presenting a family of infinitely exchangeable priors over discrete tree structures that allows the depth of the tree to grow with the data, and then showing that our family contains all hierarchical models with certain mild symmetry properties. We also show that deep hierarchical models are in general intimately tied to a process called a martingale, and use Doob\u2019s martingale convergence theorem to demonstrate some unexpected properties of deep hierarchies.",
        "bibtex": "@InProceedings{pmlr-v22-steinhardt12,\n  title = \t {Flexible Martingale Priors for Deep Hierarchies},\n  author = \t {Steinhardt, Jacob and Ghahramani, Zoubin},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1108--1116},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/steinhardt12/steinhardt12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/steinhardt12.html},\n  abstract = \t {When building priors over trees for Bayesian hierarchical models, there is a tension between maintaining desirable theoretical properties such as infinite exchangeability and important practical properties such as the ability to increase the depth of the tree to accommodate new data. We resolve this tension by presenting a family of infinitely exchangeable priors over discrete tree structures that allows the depth of the tree to grow with the data, and then showing that our family contains all hierarchical models with certain mild symmetry properties. We also show that deep hierarchical models are in general intimately tied to a process called a martingale, and use Doob\u2019s martingale convergence theorem to demonstrate some unexpected properties of deep hierarchies.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/steinhardt12/steinhardt12.pdf",
        "supp": "",
        "pdf_size": 990586,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4213036143426676998&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 13,
        "aff": "Massachusetts Institute of Technology; University of Cambridge",
        "aff_domain": "; ",
        "email": "; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Massachusetts Institute of Technology;University of Cambridge",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://web.mit.edu;https://www.cam.ac.uk",
        "aff_unique_abbr": "MIT;Cambridge",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "title": "Forward Basis Selection for Sparse Approximation over Dictionary",
        "site": "https://proceedings.mlr.press/v22/yuan12.html",
        "author": "Xiaotong Yuan; Shuicheng Yan",
        "abstract": "Recently, forward greedy selection method has been successfully applied to approximately solve sparse learning problems, characterized by a trade-off between sparsity and accuracy. In this paper, we generalize this method to the setup of sparse approximation over a pre-fixed dictionary. A fully corrective forward selection algorithm is proposed along with convergence analysis. The per-iteration computational overhead of the proposed algorithm is dominated by a subproblem of linear optimization over the dictionary and a subproblem to optimally adjust the aggregation weights. The former is cheaper in several applications than the Euclidean projection while the latter is typically an unconstrained optimization problem which is relatively easy to solve. Furthermore, we extend the proposed algorithm to the setting of non-negative/convex sparse approximation over a dictionary.Applications of our algorithms to several concrete learning problems are explored with efficiency validated on benchmark data sets.",
        "bibtex": "@InProceedings{pmlr-v22-yuan12,\n  title = \t {Forward Basis Selection for Sparse Approximation over Dictionary},\n  author = \t {Yuan, Xiaotong and Yan, Shuicheng},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1377--1388},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/yuan12/yuan12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/yuan12.html},\n  abstract = \t {Recently, forward greedy selection method has been successfully applied to approximately solve sparse learning problems, characterized by a trade-off between sparsity and accuracy. In this paper, we generalize this method to the setup of sparse approximation over a pre-fixed dictionary. A fully corrective forward selection algorithm is proposed along with convergence analysis. The per-iteration computational overhead of the proposed algorithm is dominated by a subproblem of linear optimization over the dictionary and a subproblem to optimally adjust the aggregation weights. The former is cheaper in several applications than the Euclidean projection while the latter is typically an unconstrained optimization problem which is relatively easy to solve. Furthermore, we extend the proposed algorithm to the setting of non-negative/convex sparse approximation over a dictionary.Applications of our algorithms to several concrete learning problems are explored with efficiency validated on benchmark data sets.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/yuan12/yuan12.pdf",
        "supp": "",
        "pdf_size": 833305,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16186776281707121638&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Statistics, Rutgers University; ECE Department, National University of Singapore",
        "aff_domain": "stat.rutgers.edu;nus.edu.sg",
        "email": "stat.rutgers.edu;nus.edu.sg",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Rutgers University;National University of Singapore",
        "aff_unique_dep": "Department of Statistics;ECE Department",
        "aff_unique_url": "https://www.rutgers.edu;https://www.nus.edu.sg",
        "aff_unique_abbr": "Rutgers;NUS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United States;Singapore"
    },
    {
        "title": "Gaussian Processes for time-marked time-series data",
        "site": "https://proceedings.mlr.press/v22/cunningham12.html",
        "author": "John Cunningham; Zoubin Ghahramani; Carl Rasmussen",
        "abstract": "In many settings, data is collected as multiple time series, where each recorded time series is an observation of some underlying dynamical process of interest.  These observations are often time-marked with known event times, and one desires to do a range of standard analyses.  When there is only one time marker, one simply aligns the observations temporally on that marker.  When multiple time-markers are present and are at different times on different time series observations, these analyses are more difficult.  We describe a Gaussian Process model for analyzing multiple time series with multiple time markings, and we test it on a variety of data.",
        "bibtex": "@InProceedings{pmlr-v22-cunningham12,\n  title = \t {Gaussian Processes for time-marked time-series data},\n  author = \t {Cunningham, John and Ghahramani, Zoubin and Rasmussen, Carl},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {255--263},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/cunningham12/cunningham12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/cunningham12.html},\n  abstract = \t {In many settings, data is collected as multiple time series, where each recorded time series is an observation of some underlying dynamical process of interest.  These observations are often time-marked with known event times, and one desires to do a range of standard analyses.  When there is only one time marker, one simply aligns the observations temporally on that marker.  When multiple time-markers are present and are at different times on different time series observations, these analyses are more difficult.  We describe a Gaussian Process model for analyzing multiple time series with multiple time markings, and we test it on a variety of data.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/cunningham12/cunningham12.pdf",
        "supp": "",
        "pdf_size": 385773,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2473987918024663982&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Generalized Optimal Reverse Prediction",
        "site": "https://proceedings.mlr.press/v22/white12.html",
        "author": "Martha White; Dale Schuurmans",
        "abstract": "Recently it has been shown that classical supervised and unsupervised training methods can be unified as special cases of so-called \u201coptimal reverse prediction\": predicting inputs from target labels while optimizing over both model parameters and missing labels. Although this perspective establishes links between classical training principles, the existing formulation only applies to linear predictors under squared loss, hence is extremely limited. We generalize the formulation of optimal reverse prediction to arbitrary Bregman divergences, and more importantly to nonlinear predictors. This extension is achieved by establishing a new, generalized form of forward-reverse minimization equivalence that holds for arbitrary matching losses. Several benefits follow. First, a new variant of Bregman divergence clustering can be recovered that incorporates a non-linear data reconstruction model. Second, normalized-cut and kernel-based extensions can be formulated coherently. Finally, a new semi-supervised training principle can be recovered for classification problems that demonstrates advantages over the state of the art.",
        "bibtex": "@InProceedings{pmlr-v22-white12,\n  title = \t {Generalized Optimal Reverse Prediction},\n  author = \t {White, Martha and Schuurmans, Dale},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1305--1313},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/white12/white12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/white12.html},\n  abstract = \t {Recently it has been shown that classical supervised and unsupervised training methods can be unified as special cases of so-called \u201coptimal reverse prediction\": predicting inputs from target labels while optimizing over both model parameters and missing labels. Although this perspective establishes links between classical training principles, the existing formulation only applies to linear predictors under squared loss, hence is extremely limited. We generalize the formulation of optimal reverse prediction to arbitrary Bregman divergences, and more importantly to nonlinear predictors. This extension is achieved by establishing a new, generalized form of forward-reverse minimization equivalence that holds for arbitrary matching losses. Several benefits follow. First, a new variant of Bregman divergence clustering can be recovered that incorporates a non-linear data reconstruction model. Second, normalized-cut and kernel-based extensions can be formulated coherently. Finally, a new semi-supervised training principle can be recovered for classification problems that demonstrates advantages over the state of the art.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/white12/white12.pdf",
        "supp": "",
        "pdf_size": 958995,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16218673356466025758&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computing Science, University of Alberta; Department of Computing Science, University of Alberta",
        "aff_domain": "cs.ualberta.ca;cs.ualberta.ca",
        "email": "cs.ualberta.ca;cs.ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Alberta",
        "aff_unique_dep": "Department of Computing Science",
        "aff_unique_url": "https://www.ualberta.ca",
        "aff_unique_abbr": "UAlberta",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "Generic Methods for Optimization-Based Modeling",
        "site": "https://proceedings.mlr.press/v22/domke12.html",
        "author": "Justin Domke",
        "abstract": "\"Energy\u201d models for continuous domains can be applied to many problems, but often suffer from high computational expense in training, due to the need to repeatedly minimize the energy function to high accuracy. This paper considers a modified setting, where the model is trained in terms of results after optimization is truncated to a fixed number of iterations. We derive \u201cbackpropagating\u201d versions of gradient descent, heavy-ball and LBFGS. These are simple to use, as they require as input only routines to compute the gradient of the energy with respect to the domain and parameters. Experimental results on denoising and image labeling problems show that learning with truncated optimization greatly reduces computational expense compared to \u201cfull\u201d fitting.",
        "bibtex": "@InProceedings{pmlr-v22-domke12,\n  title = \t {Generic Methods for Optimization-Based Modeling},\n  author = \t {Domke, Justin},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {318--326},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/domke12/domke12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/domke12.html},\n  abstract = \t {\"Energy\u201d models for continuous domains can be applied to many problems, but often suffer from high computational expense in training, due to the need to repeatedly minimize the energy function to high accuracy. This paper considers a modified setting, where the model is trained in terms of results after optimization is truncated to a fixed number of iterations. We derive \u201cbackpropagating\u201d versions of gradient descent, heavy-ball and LBFGS. These are simple to use, as they require as input only routines to compute the gradient of the energy with respect to the domain and parameters. Experimental results on denoising and image labeling problems show that learning with truncated optimization greatly reduces computational expense compared to \u201cfull\u201d fitting.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/domke12/domke12.pdf",
        "supp": "",
        "pdf_size": 904058,
        "gs_citation": 457,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15658051416211784407&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Rochester Institute of Technology",
        "aff_domain": "cs.ualberta.ca;cs.ualberta.ca",
        "email": "cs.ualberta.ca;cs.ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Rochester Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.rit.edu",
        "aff_unique_abbr": "RIT",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Globally Optimizing Graph Partitioning Problems Using Message Passing",
        "site": "https://proceedings.mlr.press/v22/mezuman12.html",
        "author": "Elad Mezuman; Yair Weiss",
        "abstract": "Graph partitioning algorithms play a central role in data analysis and machine learning. Most useful graph partitioning criteria correspond to optimizing a ratio between the cut and the size of the partitions, this ratio leads to an NP-hard problem that is only solved approximately. This makes it difficult to know whether failures of the algorithm are due to failures of the optimization or to the criterion being optimized. In this paper we present a framework that seeks and finds the optimal solution of several NP-hard graph partitioning problems. We use a classical approach to ratio problems where we repeatedly ask whether the optimal solution is greater than or less than some constant - lambda. Our main insight is the equivalence between this \u201clambda  question\u201d and performing inference in a graphical model with many local potentials and one high-order potential. We show that this specific form of the high-order potential is amenable to message-passing algorithms and how to obtain a bound on the optimal solution from the messages. Our experiments show that in many cases our approach yields the global optimum and improves the popular spectral solution.",
        "bibtex": "@InProceedings{pmlr-v22-mezuman12,\n  title = \t {Globally Optimizing Graph Partitioning Problems Using Message Passing},\n  author = \t {Mezuman, Elad and Weiss, Yair},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {770--778},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/mezuman12/mezuman12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/mezuman12.html},\n  abstract = \t {Graph partitioning algorithms play a central role in data analysis and machine learning. Most useful graph partitioning criteria correspond to optimizing a ratio between the cut and the size of the partitions, this ratio leads to an NP-hard problem that is only solved approximately. This makes it difficult to know whether failures of the algorithm are due to failures of the optimization or to the criterion being optimized. In this paper we present a framework that seeks and finds the optimal solution of several NP-hard graph partitioning problems. We use a classical approach to ratio problems where we repeatedly ask whether the optimal solution is greater than or less than some constant - lambda. Our main insight is the equivalence between this \u201clambda  question\u201d and performing inference in a graphical model with many local potentials and one high-order potential. We show that this specific form of the high-order potential is amenable to message-passing algorithms and how to obtain a bound on the optimal solution from the messages. Our experiments show that in many cases our approach yields the global optimum and improves the popular spectral solution.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/mezuman12/mezuman12.pdf",
        "supp": "",
        "pdf_size": 2448446,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14207341386242470760&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Interdisciplinary Center for Neural Computation + Edmond & Lily Safra Center for Brain Sciences + Hebrew University of Jerusalem; School of Computer Science and Engineering + Edmond & Lily Safra Center for Brain Sciences + Hebrew University of Jerusalem",
        "aff_domain": "cs.huji.ac.il;cs.huji.ac.il",
        "email": "cs.huji.ac.il;cs.huji.ac.il",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1+2;3+1+2",
        "aff_unique_norm": "Interdisciplinary Center for Neural Computation;Edmond & Lily Safra Center for Brain Sciences;Hebrew University of Jerusalem;University Affiliation Not Specified",
        "aff_unique_dep": "Neural Computation;Center for Brain Sciences;;School of Computer Science and Engineering",
        "aff_unique_url": ";;https://www.huji.ac.il;",
        "aff_unique_abbr": ";;HUJI;",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Jerusalem",
        "aff_country_unique_index": "1+2;1+2",
        "aff_country_unique": ";United States;Israel"
    },
    {
        "title": "Graphlet decomposition of a weighted network",
        "site": "https://proceedings.mlr.press/v22/azari12.html",
        "author": "Hossein Azari Soufiani; Edo Airoldi",
        "abstract": "We consider the problem of modeling networks with nonnegative edge weights. We develop a \\emphbit-string decomposition (BSD) for weighted networks, a new representation of social information based on social structure, with an underlying semi-parametric statistical model.  We develop a scalable inference algorithm, which combines Expectation-Maximization with Bron-Kerbosch in a novel fashion, for estimating the model\u2019s parameters from a network sample.  We present theoretical descriptions to the computational complexity of the method. Finally, we demonstrate the performance of the proposed methodology for synthetic data, academic networks from Facebook and finding communities in a historical data from 19th century.",
        "bibtex": "@InProceedings{pmlr-v22-azari12,\n  title = \t {Graphlet decomposition of a weighted network},\n  author = \t {Soufiani, Hossein Azari and Airoldi, Edo},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {54--63},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/azari12/azari12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/azari12.html},\n  abstract = \t {We consider the problem of modeling networks with nonnegative edge weights. We develop a \\emphbit-string decomposition (BSD) for weighted networks, a new representation of social information based on social structure, with an underlying semi-parametric statistical model.  We develop a scalable inference algorithm, which combines Expectation-Maximization with Bron-Kerbosch in a novel fashion, for estimating the model\u2019s parameters from a network sample.  We present theoretical descriptions to the computational complexity of the method. Finally, we demonstrate the performance of the proposed methodology for synthetic data, academic networks from Facebook and finding communities in a historical data from 19th century.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/azari12/azari12.pdf",
        "supp": "",
        "pdf_size": 781168,
        "gs_citation": 63,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8560557657970255757&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Harvard University; Harvard University",
        "aff_domain": "; ",
        "email": "; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Harvard University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.harvard.edu",
        "aff_unique_abbr": "Harvard",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Hierarchical Latent Dictionaries for Models of Brain Activation",
        "site": "https://proceedings.mlr.press/v22/fyshe12.html",
        "author": "Alona Fyshe; Emily Fox; David Dunson; Tom Mitchell",
        "abstract": "In this work, we propose a hierarchical latent dictionary approach to estimate the time-varying mean and covariance of a process for which we have only limited noisy samples.  We fully leverage the limited sample size and redundancy in sensor measurements by transferring knowledge through a hierarchy of lower dimensional latent processes.   As a case study, we utilize Magnetoencephalography (MEG) recordings of brain activity to identify the word being viewed by a human subject.  Specifically, we identify the word category for a single noisy MEG recording, when given only limited noisy samples on which to train.",
        "bibtex": "@InProceedings{pmlr-v22-fyshe12,\n  title = \t {Hierarchical Latent Dictionaries for Models of Brain Activation},\n  author = \t {Fyshe, Alona and Fox, Emily and Dunson, David and Mitchell, Tom},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {409--421},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/fyshe12/fyshe12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/fyshe12.html},\n  abstract = \t {In this work, we propose a hierarchical latent dictionary approach to estimate the time-varying mean and covariance of a process for which we have only limited noisy samples.  We fully leverage the limited sample size and redundancy in sensor measurements by transferring knowledge through a hierarchy of lower dimensional latent processes.   As a case study, we utilize Magnetoencephalography (MEG) recordings of brain activity to identify the word being viewed by a human subject.  Specifically, we identify the word category for a single noisy MEG recording, when given only limited noisy samples on which to train.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/fyshe12/fyshe12.pdf",
        "supp": "",
        "pdf_size": 790625,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3931747557405933173&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Hierarchical Relative Entropy Policy Search",
        "site": "https://proceedings.mlr.press/v22/daniel12.html",
        "author": "Christian Daniel; Gerhard Neumann; Jan Peters",
        "abstract": "Many real-world problems are inherently hi- erarchically structured. The use of this struc- ture in an agent\u2019s policy may well be the key to improved scalability and higher per- formance. However, such hierarchical struc- tures cannot be exploited by current policy search algorithms. We will concentrate on a basic, but highly relevant hierarchy - the \u2019mixed option\u2019 policy. Here, a gating network first decides which of the options to execute and, subsequently, the option-policy deter- mines the action. In this paper, we reformulate learning a hi- erarchical policy as a latent variable estima- tion problem and subsequently extend the Relative Entropy Policy Search (REPS) to the latent variable case. We show that our Hierarchical REPS can learn versatile solu- tions while also showing an increased perfor- mance in terms of learning speed and quality of the found policy in comparison to the non- hierarchical approach.",
        "bibtex": "@InProceedings{pmlr-v22-daniel12,\n  title = \t {Hierarchical Relative Entropy Policy Search},\n  author = \t {Daniel, Christian and Neumann, Gerhard and Peters, Jan},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {273--281},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/daniel12/daniel12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/daniel12.html},\n  abstract = \t {Many real-world problems are inherently hi- erarchically structured. The use of this struc- ture in an agent\u2019s policy may well be the key to improved scalability and higher per- formance. However, such hierarchical struc- tures cannot be exploited by current policy search algorithms. We will concentrate on a basic, but highly relevant hierarchy - the \u2019mixed option\u2019 policy. Here, a gating network first decides which of the options to execute and, subsequently, the option-policy deter- mines the action. In this paper, we reformulate learning a hi- erarchical policy as a latent variable estima- tion problem and subsequently extend the Relative Entropy Policy Search (REPS) to the latent variable case. We show that our Hierarchical REPS can learn versatile solu- tions while also showing an increased perfor- mance in terms of learning speed and quality of the found policy in comparison to the non- hierarchical approach.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/daniel12/daniel12.pdf",
        "supp": "",
        "pdf_size": 1842686,
        "gs_citation": 294,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17594861177872402653&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 22,
        "aff": "Technische Universit\u00a8at Darmstadt; Technische Universit\u00a8at Darmstadt; Technische Universit\u00a8at Darmstadt + Max Planck Institute for Intelligent Systems",
        "aff_domain": "ias.tu-darmstadt.de;ias.tu-darmstadt.de;ias.tu-darmstadt.de",
        "email": "ias.tu-darmstadt.de;ias.tu-darmstadt.de;ias.tu-darmstadt.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0+1",
        "aff_unique_norm": "Technische Universit\u00e4t Darmstadt;Max Planck Institute for Intelligent Systems",
        "aff_unique_dep": ";Intelligent Systems",
        "aff_unique_url": "https://www.tu-darmstadt.de;https://www.mpi-is.mpg.de",
        "aff_unique_abbr": "TUD;MPI-IS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "High-Dimensional Structured Feature Screening Using Binary Markov Random Fields",
        "site": "https://proceedings.mlr.press/v22/liu12b.html",
        "author": "Jie Liu; Chunming Zhang; Catherine Mccarty; Peggy Peissig; Elizabeth Burnside; David Page",
        "abstract": "Feature screening is a useful feature selection approach for high-dimensional data when the goal is to identify all the features relevant to the response variable. However, common feature screening methods do not take into account the correlation structure of the covariate space. We propose the concept of a feature relevance network, a binary Markov random field to represent the relevance of each individual feature by potentials on the nodes, and represent the correlation structure by potentials on the edges. By performing inference on the feature relevance network, we can accordingly select relevant features. Our algorithm does not yield sparsity, which is different from the particular popular family of feature selection approaches based on penalized least squares or penalized pseudo-likelihood.  We give one concrete algorithm under this framework and show its superior performance over common feature selection methods in terms of prediction error and recovery of the truly relevant features on real-world data and synthetic data.",
        "bibtex": "@InProceedings{pmlr-v22-liu12b,\n  title = \t {High-Dimensional Structured Feature Screening Using Binary Markov Random Fields},\n  author = \t {Liu, Jie and Zhang, Chunming and Mccarty, Catherine and Peissig, Peggy and Burnside, Elizabeth and Page, David},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {712--721},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/liu12b/liu12b.pdf},\n  url = \t {https://proceedings.mlr.press/v22/liu12b.html},\n  abstract = \t {Feature screening is a useful feature selection approach for high-dimensional data when the goal is to identify all the features relevant to the response variable. However, common feature screening methods do not take into account the correlation structure of the covariate space. We propose the concept of a feature relevance network, a binary Markov random field to represent the relevance of each individual feature by potentials on the nodes, and represent the correlation structure by potentials on the edges. By performing inference on the feature relevance network, we can accordingly select relevant features. Our algorithm does not yield sparsity, which is different from the particular popular family of feature selection approaches based on penalized least squares or penalized pseudo-likelihood.  We give one concrete algorithm under this framework and show its superior performance over common feature selection methods in terms of prediction error and recovery of the truly relevant features on real-world data and synthetic data.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/liu12b/liu12b.pdf",
        "supp": "",
        "pdf_size": 369558,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=932185473886345920&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Computer Sciences, Univ. of Wisconsin-Madison; Department of Statistics, Univ. of Wisconsin-Madison; Essentia Institute of Rural Health; Biomedical Informatics Research Center, Marshfield Clinic Research Foundation; Department of Radiology, Univ. of Wisconsin-Madison; Biostat. & Medical Informatics Dept., Univ. of Wisconsin-Madison",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2;0;0",
        "aff_unique_norm": "University of Wisconsin-Madison;Essentia Institute of Rural Health;Marshfield Clinic Research Foundation",
        "aff_unique_dep": "Department of Computer Sciences;Institute of Rural Health;Biomedical Informatics Research Center",
        "aff_unique_url": "https://www.wisc.edu;https://www.essentiahealth.org;https://www.marshfieldresearch.org",
        "aff_unique_abbr": "UW-Madison;;",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Madison;",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "High-Rank Matrix Completion",
        "site": "https://proceedings.mlr.press/v22/eriksson12.html",
        "author": "Brian Eriksson; Laura Balzano; Robert Nowak",
        "abstract": "This paper considers the problem of completing a matrix with many missing entries under the assumption that the columns of the matrix belong to a union of multiple low-rank subspaces. This generalizes the standard low-rank matrix completion problem to situations in which the matrix rank can be quite high or even full rank.  Since the columns belong to a union of subspaces, this problem may also be viewed as a missing-data version of the subspace clustering problem.  Let X be an nxN matrix whose (complete) columns lie in a union of at most k subspaces, each of rank = r  n, and assume Nkn. The main result of the paper shows that under mild assumptions each column of X can be perfectly recovered with high probability from an incomplete version so long as at least C r N \\log^2(n) entries of X are observed uniformly at random, with C1 a constant depending on the usual incoherence conditions, the geometrical arrangement of subspaces, and the distribution of columns over the subspaces. The result is illustrated with numerical experiments and an application to Internet distance matrix completion and topology identification.",
        "bibtex": "@InProceedings{pmlr-v22-eriksson12,\n  title = \t {High-Rank Matrix Completion},\n  author = \t {Eriksson, Brian and Balzano, Laura and Nowak, Robert},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {373--381},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/eriksson12/eriksson12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/eriksson12.html},\n  abstract = \t {This paper considers the problem of completing a matrix with many missing entries under the assumption that the columns of the matrix belong to a union of multiple low-rank subspaces. This generalizes the standard low-rank matrix completion problem to situations in which the matrix rank can be quite high or even full rank.  Since the columns belong to a union of subspaces, this problem may also be viewed as a missing-data version of the subspace clustering problem.  Let X be an nxN matrix whose (complete) columns lie in a union of at most k subspaces, each of rank = r  n, and assume Nkn. The main result of the paper shows that under mild assumptions each column of X can be perfectly recovered with high probability from an incomplete version so long as at least C r N \\log^2(n) entries of X are observed uniformly at random, with C1 a constant depending on the usual incoherence conditions, the geometrical arrangement of subspaces, and the distribution of columns over the subspaces. The result is illustrated with numerical experiments and an application to Internet distance matrix completion and topology identification.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/eriksson12/eriksson12.pdf",
        "supp": "",
        "pdf_size": 876493,
        "gs_citation": 152,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15852625633297791392&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "High-dimensional Sparse Inverse Covariance Estimation using Greedy Methods",
        "site": "https://proceedings.mlr.press/v22/johnson12.html",
        "author": "Christopher Johnson; Ali Jalali; Pradeep Ravikumar",
        "abstract": "In this paper we consider the task of estimating the non-zero pattern of the sparse inverse covariance matrix of a zero-mean Gaussian random vector from a set of iid samples. Note that this is also equivalent to recovering the underlying graph structure of a sparse Gaussian Markov Random Field (GMRF).  We present two novel greedy approaches to solving this problem.  The first estimates the non-zero covariates of the overall inverse covariance matrix using a series of global forward and backward greedy steps.  The second estimates the neighborhood of each node in the graph separately, again using greedy forward and backward steps, and combines the intermediate neighborhoods to form an overall estimate. The principal contribution of this paper is a rigorous analysis of the sparsistency, or consistency in recovering the sparsity pattern of the inverse covariance matrix. Surprisingly, we show that both the local and global greedy methods learn the full structure of the model with high probability given just O(d log(p)) samples, which is a significant improvement over state of the art L1-regularized Gaussian MLE (Graphical Lasso) that requires O(d^2 log(p)) samples. Moreover, the restricted eigenvalue and smoothness conditions imposed by our greedy methods are much weaker than the strong irrepresentable conditions required by the L1-regularization based methods. We corroborate our results with extensive simulations and examples, comparing our local and global greedy methods to the L1-regularized Gaussian MLE as well as the nodewise L1-regularized linear regression (Neighborhood Lasso).",
        "bibtex": "@InProceedings{pmlr-v22-johnson12,\n  title = \t {High-dimensional Sparse Inverse Covariance Estimation using Greedy Methods},\n  author = \t {Johnson, Christopher and Jalali, Ali and Ravikumar, Pradeep},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {574--582},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/johnson12/johnson12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/johnson12.html},\n  abstract = \t {In this paper we consider the task of estimating the non-zero pattern of the sparse inverse covariance matrix of a zero-mean Gaussian random vector from a set of iid samples. Note that this is also equivalent to recovering the underlying graph structure of a sparse Gaussian Markov Random Field (GMRF).  We present two novel greedy approaches to solving this problem.  The first estimates the non-zero covariates of the overall inverse covariance matrix using a series of global forward and backward greedy steps.  The second estimates the neighborhood of each node in the graph separately, again using greedy forward and backward steps, and combines the intermediate neighborhoods to form an overall estimate. The principal contribution of this paper is a rigorous analysis of the sparsistency, or consistency in recovering the sparsity pattern of the inverse covariance matrix. Surprisingly, we show that both the local and global greedy methods learn the full structure of the model with high probability given just O(d log(p)) samples, which is a significant improvement over state of the art L1-regularized Gaussian MLE (Graphical Lasso) that requires O(d^2 log(p)) samples. Moreover, the restricted eigenvalue and smoothness conditions imposed by our greedy methods are much weaker than the strong irrepresentable conditions required by the L1-regularization based methods. We corroborate our results with extensive simulations and examples, comparing our local and global greedy methods to the L1-regularized Gaussian MLE as well as the nodewise L1-regularized linear regression (Neighborhood Lasso).}\n}",
        "pdf": "http://proceedings.mlr.press/v22/johnson12/johnson12.pdf",
        "supp": "",
        "pdf_size": 885628,
        "gs_citation": 62,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1749223195499866885&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "CS, UT Austin; ECE, UT Austin; CS, UT Austin",
        "aff_domain": "cs.utexas.edu;mail.utexas.edu;cs.utexas.edu",
        "email": "cs.utexas.edu;mail.utexas.edu;cs.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "History-alignment models for bias-aware prediction of virological response to HIV combination therapy",
        "site": "https://proceedings.mlr.press/v22/bogojeska12.html",
        "author": "Jasmina Bogojeska; Daniel Stockel; Maurizio Zazzi; Rolf Kaiser; Francesca Incardona; Michal Rosen-Zvi; Thomas Lengauer",
        "abstract": "The relevant HIV data sets used for predicting outcomes of HIV combination therapies suffer from several problems: uneven therapy representation, different treatment backgrounds of the samples and uneven representation with respect to the level of therapy experience. Also, they comprise only viral strain(s) that can be detected in the patients\u2019 blood serum. The approach presented in this paper tackles these issues by considering not only the most recent therapies but also the different treatment backgrounds of the samples making up the clinical data sets when predicting the outcomes of HIV therapies. For this purpose, we introduce a similarity measure for sequences of therapies and use it for training separate linear models for predicting therapy outcome for each target sample. Compared to the most commonly used approach that encodes all available treatment information only by specific input features our approach has the advantage of delivering significantly more accurate predictions for therapy-experienced patients and for rare therapies. Additionally, the sample-specific models are more interpretable which is very important in medical applications.",
        "bibtex": "@InProceedings{pmlr-v22-bogojeska12,\n  title = \t {History-alignment models for bias-aware prediction of virological response to HIV combination therapy},\n  author = \t {Bogojeska, Jasmina and Stockel, Daniel and Zazzi, Maurizio and Kaiser, Rolf and Incardona, Francesca and Rosen-Zvi, Michal and Lengauer, Thomas},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {118--126},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/bogojeska12/bogojeska12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/bogojeska12.html},\n  abstract = \t {The relevant HIV data sets used for predicting outcomes of HIV combination therapies suffer from several problems: uneven therapy representation, different treatment backgrounds of the samples and uneven representation with respect to the level of therapy experience. Also, they comprise only viral strain(s) that can be detected in the patients\u2019 blood serum. The approach presented in this paper tackles these issues by considering not only the most recent therapies but also the different treatment backgrounds of the samples making up the clinical data sets when predicting the outcomes of HIV therapies. For this purpose, we introduce a similarity measure for sequences of therapies and use it for training separate linear models for predicting therapy outcome for each target sample. Compared to the most commonly used approach that encodes all available treatment information only by specific input features our approach has the advantage of delivering significantly more accurate predictions for therapy-experienced patients and for rare therapies. Additionally, the sample-specific models are more interpretable which is very important in medical applications.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/bogojeska12/bogojeska12.pdf",
        "supp": "",
        "pdf_size": 741926,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7527738835452328518&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 10,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Infinite-Dimensional Kalman Filtering Approach to Spatio-Temporal Gaussian Process Regression",
        "site": "https://proceedings.mlr.press/v22/sarkka12.html",
        "author": "Simo Sarkka; Jouni Hartikainen",
        "abstract": "We show how spatio-temporal Gaussian process (GP) regression problems (or the equivalent Kriging problems) can be formulated as infinite-dimensional Kalman filtering and Rauch-Tung-Striebel (RTS) smoothing problems, and present a procedure for converting spatio-temporal covariance functions into infinite-dimensional stochastic differential equations (SDEs). The resulting infinite-dimensional SDEs belong to the class of stochastic pseudo-differential equations and can be numerically treated using the methods developed for deterministic counterparts of the equations. The scaling of the computational cost in the proposed approach is linear in the number of time steps as opposed to the cubic scaling of the direct GP regression solution. We also show how separable covariance functions lead to a finite-dimensional Kalman filtering and RTS smoothing problem, present analytical and numerical examples, and discuss numerical methods for computing the solutions.",
        "bibtex": "@InProceedings{pmlr-v22-sarkka12,\n  title = \t {Infinite-Dimensional Kalman Filtering Approach to Spatio-Temporal Gaussian Process Regression},\n  author = \t {Sarkka, Simo and Hartikainen, Jouni},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {993--1001},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/sarkka12/sarkka12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/sarkka12.html},\n  abstract = \t {We show how spatio-temporal Gaussian process (GP) regression problems (or the equivalent Kriging problems) can be formulated as infinite-dimensional Kalman filtering and Rauch-Tung-Striebel (RTS) smoothing problems, and present a procedure for converting spatio-temporal covariance functions into infinite-dimensional stochastic differential equations (SDEs). The resulting infinite-dimensional SDEs belong to the class of stochastic pseudo-differential equations and can be numerically treated using the methods developed for deterministic counterparts of the equations. The scaling of the computational cost in the proposed approach is linear in the number of time steps as opposed to the cubic scaling of the direct GP regression solution. We also show how separable covariance functions lead to a finite-dimensional Kalman filtering and RTS smoothing problem, present analytical and numerical examples, and discuss numerical methods for computing the solutions.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/sarkka12/sarkka12.pdf",
        "supp": "",
        "pdf_size": 594666,
        "gs_citation": 97,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12777425980465238958&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Biomedical Engineering and Computational Science, Aalto University; Department of Biomedical Engineering and Computational Science, Aalto University",
        "aff_domain": "aalto.fi;aalto.fi",
        "email": "aalto.fi;aalto.fi",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Aalto University",
        "aff_unique_dep": "Department of Biomedical Engineering and Computational Science",
        "aff_unique_url": "https://www.aalto.fi",
        "aff_unique_abbr": "Aalto",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Finland"
    },
    {
        "title": "Information Theoretic Model Validation for Spectral Clustering",
        "site": "https://proceedings.mlr.press/v22/haghir12.html",
        "author": "Morteza Haghir Chehreghani; Alberto Giovanni Busetto; Joachim M. Buhmann",
        "abstract": "Model validation constitutes a fundamental step in data clustering. The central question is: Which cluster model and how many clusters are most appropriate for a certain application? In this study, we introduce a method for the validation of spectral clustering based upon approximation set coding. In particular, we compare correlation and pairwise clustering to analyze the correlations of temporal gene expression profiles. To evaluate and select clustering models, we calculate their reliable informativeness. Experimental results in the context of gene expression analysis show that pairwise clustering yields superior amounts of reliable information. The analysis results are consistent with the Bayesian Information Criterion (BIC), and exhibit higher generality than BIC.",
        "bibtex": "@InProceedings{pmlr-v22-haghir12,\n  title = \t {Information Theoretic Model Validation for Spectral Clustering},\n  author = \t {Chehreghani, Morteza Haghir and Busetto, Alberto Giovanni and Buhmann, Joachim M.},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {495--503},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/haghir12/haghir12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/haghir12.html},\n  abstract = \t {Model validation constitutes a fundamental step in data clustering. The central question is: Which cluster model and how many clusters are most appropriate for a certain application? In this study, we introduce a method for the validation of spectral clustering based upon approximation set coding. In particular, we compare correlation and pairwise clustering to analyze the correlations of temporal gene expression profiles. To evaluate and select clustering models, we calculate their reliable informativeness. Experimental results in the context of gene expression analysis show that pairwise clustering yields superior amounts of reliable information. The analysis results are consistent with the Bayesian Information Criterion (BIC), and exhibit higher generality than BIC.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/haghir12/haghir12.pdf",
        "supp": "",
        "pdf_size": 500862,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9591534038906066205&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "ETH Zurich; ETH Zurich and CC-SPMD, Zurich; ETH Zurich and CC-SPMD, Zurich",
        "aff_domain": "inf.ethz.ch;inf.ethz.ch;inf.ethz.ch",
        "email": "inf.ethz.ch;inf.ethz.ch;inf.ethz.ch",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "ETH Zurich",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ethz.ch",
        "aff_unique_abbr": "ETHZ",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Zurich",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "title": "Informative Priors for Markov Blanket Discovery",
        "site": "https://proceedings.mlr.press/v22/pocock12.html",
        "author": "Adam Pocock; Mikel Lujan; Gavin Brown",
        "abstract": "We present a novel interpretation of information theoretic feature selection as optimization of a discriminative model. We show that this formulation coincides with a group of mutual information based filter heuristics in the literature, and show how our probabilistic framework gives a well-founded extension for informative priors. We then derive a particular sparsity prior that recovers the well-known IAMB algorithm (Tsamardinos & Aliferis, 2003) and extend it to create a novel algorithm, IAMB-IP, that includes domain knowledge priors. In empirical evaluations, we find the new algorithm to improve Markov Blanket recovery even when a misspecified prior was used, in which half the prior knowledge was incorrect.",
        "bibtex": "@InProceedings{pmlr-v22-pocock12,\n  title = \t {Informative Priors for Markov Blanket Discovery},\n  author = \t {Pocock, Adam and Lujan, Mikel and Brown, Gavin},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {905--913},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/pocock12/pocock12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/pocock12.html},\n  abstract = \t {We present a novel interpretation of information theoretic feature selection as optimization of a discriminative model. We show that this formulation coincides with a group of mutual information based filter heuristics in the literature, and show how our probabilistic framework gives a well-founded extension for informative priors. We then derive a particular sparsity prior that recovers the well-known IAMB algorithm (Tsamardinos & Aliferis, 2003) and extend it to create a novel algorithm, IAMB-IP, that includes domain knowledge priors. In empirical evaluations, we find the new algorithm to improve Markov Blanket recovery even when a misspecified prior was used, in which half the prior knowledge was incorrect.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/pocock12/pocock12.pdf",
        "supp": "",
        "pdf_size": 355486,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6988031242576891702&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 13,
        "aff": "School of Computer Science, University of Manchester, Manchester, M13 9PL; School of Computer Science, University of Manchester, Manchester, M13 9PL; School of Computer Science, University of Manchester, Manchester, M13 9PL",
        "aff_domain": "cs.manchester.ac.uk;cs.manchester.ac.uk;cs.manchester.ac.uk",
        "email": "cs.manchester.ac.uk;cs.manchester.ac.uk;cs.manchester.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Manchester",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.manchester.ac.uk",
        "aff_unique_abbr": "UoM",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Manchester",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Joint Estimation of Structured Sparsity and Output Structure in Multiple-Output Regression via Inverse-Covariance Regularization",
        "site": "https://proceedings.mlr.press/v22/sohn12.html",
        "author": "Kyung-Ah Sohn; Seyoung Kim",
        "abstract": "We consider the problem of learning a sparse regression model for predicting multiple related outputs given high-dimensional inputs, where related outputs are likely to share common relevant inputs. Most of the previous methods for learning structured sparsity assumed that the structure over the outputs is known a priori, and focused on designing regularization functions that encourage structured sparsity reflecting the given output structure. In this paper, we propose a new approach for sparse multiple-output regression that can jointly learn both the output structure and regression coefficients with structured sparsity. Our approach reformulates the standard regression model into an alternative parameterization that leads to a conditional Gaussian graphical model, and employes an inverse-covariance regularization. We show that the orthant-wise quasi-Newton algorithm developed for L1-regularized log-linear model can be adopted for a fast optimization for our method. We demonstrate our method on simulated datasets and real datasets from genetics and finances applications.",
        "bibtex": "@InProceedings{pmlr-v22-sohn12,\n  title = \t {Joint Estimation of Structured Sparsity and Output Structure in Multiple-Output Regression via Inverse-Covariance Regularization},\n  author = \t {Sohn, Kyung-Ah and Kim, Seyoung},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1081--1089},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/sohn12/sohn12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/sohn12.html},\n  abstract = \t {We consider the problem of learning a sparse regression model for predicting multiple related outputs given high-dimensional inputs, where related outputs are likely to share common relevant inputs. Most of the previous methods for learning structured sparsity assumed that the structure over the outputs is known a priori, and focused on designing regularization functions that encourage structured sparsity reflecting the given output structure. In this paper, we propose a new approach for sparse multiple-output regression that can jointly learn both the output structure and regression coefficients with structured sparsity. Our approach reformulates the standard regression model into an alternative parameterization that leads to a conditional Gaussian graphical model, and employes an inverse-covariance regularization. We show that the orthant-wise quasi-Newton algorithm developed for L1-regularized log-linear model can be adopted for a fast optimization for our method. We demonstrate our method on simulated datasets and real datasets from genetics and finances applications.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/sohn12/sohn12.pdf",
        "supp": "",
        "pdf_size": 903324,
        "gs_citation": 111,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16739572664579111396&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "School of Computer Science, Carnegie Mellon University; School of Computer Science, Carnegie Mellon University",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Joint Learning of Words and Meaning Representations for Open-Text Semantic Parsing",
        "site": "https://proceedings.mlr.press/v22/bordes12.html",
        "author": "Antoine Bordes; Xavier Glorot; Jason Weston; Yoshua Bengio",
        "abstract": "Open-text semantic parsers are designed to interpret any statement in natural language by inferring a corresponding meaning representation (MR - a formal representation of its sense). Unfortunately, large scale systems cannot be easily machine-learned due to lack of directly supervised data. We propose a method that learns to assign MRs to a wide range of text (using a dictionary of more than 70,000 words mapped to more than 40,000 entities) thanks to a training scheme that combines learning from knowledge bases (e.g. WordNet) with learning from raw text. The model jointly learns representations of words, entities and MRs via a multi-task training process operating on these diverse sources of data. Hence, the system ends up providing methods for knowledge acquisition and word-sense disambiguation within the context of semantic parsing in a single elegant framework. Experiments on these various tasks indicate the promise of the approach.",
        "bibtex": "@InProceedings{pmlr-v22-bordes12,\n  title = \t {Joint Learning of Words and Meaning Representations for Open-Text Semantic Parsing},\n  author = \t {Bordes, Antoine and Glorot, Xavier and Weston, Jason and Bengio, Yoshua},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {127--135},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/bordes12/bordes12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/bordes12.html},\n  abstract = \t {Open-text semantic parsers are designed to interpret any statement in natural language by inferring a corresponding meaning representation (MR - a formal representation of its sense). Unfortunately, large scale systems cannot be easily machine-learned due to lack of directly supervised data. We propose a method that learns to assign MRs to a wide range of text (using a dictionary of more than 70,000 words mapped to more than 40,000 entities) thanks to a training scheme that combines learning from knowledge bases (e.g. WordNet) with learning from raw text. The model jointly learns representations of words, entities and MRs via a multi-task training process operating on these diverse sources of data. Hence, the system ends up providing methods for knowledge acquisition and word-sense disambiguation within the context of semantic parsing in a single elegant framework. Experiments on these various tasks indicate the promise of the approach.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/bordes12/bordes12.pdf",
        "supp": "",
        "pdf_size": 411766,
        "gs_citation": 495,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14428209779783045692&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Heudiasyc, UMR 7253 UTC, Compi`egne, France; DIRO, Universit\u00b4e de Montr\u00b4eal Montr\u00b4eal, QC, Canada; Google New York, NY, USA; DIRO, Universit\u00b4e de Montr\u00b4eal Montr\u00b4eal, QC, Canada",
        "aff_domain": "utc.fr;iro.umontreal.ca;google.com;iro.umontreal.ca",
        "email": "utc.fr;iro.umontreal.ca;google.com;iro.umontreal.ca",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;1",
        "aff_unique_norm": "Universit\u00e9 de Technologie de Compi\u00e8gne;Universit\u00e9 de Montr\u00e9al;Google",
        "aff_unique_dep": "Heudiasyc;DIRO;",
        "aff_unique_url": "https://www.utc.fr;https://www.umontreal.ca;https://www.google.com",
        "aff_unique_abbr": "UTC;UdeM;Google",
        "aff_campus_unique_index": "0;1;2;1",
        "aff_campus_unique": "Compi\u00e8gne;Montr\u00e9al;New York",
        "aff_country_unique_index": "0;1;2;1",
        "aff_country_unique": "France;Canada;United States"
    },
    {
        "title": "Kernel Topic Models",
        "site": "https://proceedings.mlr.press/v22/hennig12.html",
        "author": "Philipp Hennig; David Stern; Ralf Herbrich; Thore Graepel",
        "abstract": "Latent Dirichlet Allocation models discrete data as a mixture of discrete distributions, using Dirichlet beliefs over the mixture weights. We study a variation of this concept, in which the documents\u2019 mixture weight beliefs are replaced with squashed Gaussian distributions. This allows documents to be associated with elements of a Hilbert space, admitting kernel topic models (KTM), modelling temporal, spatial, hierarchical, social and other structure between documents. The main challenge is efficient approximate inference on the latent Gaussian. We present an approximate algorithm cast around a Laplace approximation in a transformed basis. The KTM can also be interpreted as a type of Gaussian process latent variable model, or as a topic model conditional on document features, uncovering links between earlier work in these areas.",
        "bibtex": "@InProceedings{pmlr-v22-hennig12,\n  title = \t {Kernel Topic Models},\n  author = \t {Hennig, Philipp and Stern, David and Herbrich, Ralf and Graepel, Thore},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {511--519},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/hennig12/hennig12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/hennig12.html},\n  abstract = \t {Latent Dirichlet Allocation models discrete data as a mixture of discrete distributions, using Dirichlet beliefs over the mixture weights. We study a variation of this concept, in which the documents\u2019 mixture weight beliefs are replaced with squashed Gaussian distributions. This allows documents to be associated with elements of a Hilbert space, admitting kernel topic models (KTM), modelling temporal, spatial, hierarchical, social and other structure between documents. The main challenge is efficient approximate inference on the latent Gaussian. We present an approximate algorithm cast around a Laplace approximation in a transformed basis. The KTM can also be interpreted as a type of Gaussian process latent variable model, or as a topic model conditional on document features, uncovering links between earlier work in these areas.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/hennig12/hennig12.pdf",
        "supp": "",
        "pdf_size": 409162,
        "gs_citation": 83,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10514560172061225319&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "Cavendish Laboratory, Cambridge, UK + Max Planck Institute for Intelligent Systems, T\u00fcbingen, Germany; Microsoft Research, Cambridge, UK; Microsoft Research, Cambridge, UK; Microsoft Research, Cambridge, UK",
        "aff_domain": "tuebingen.mpg.de;microsoft.com;microsoft.com;microsoft.com",
        "email": "tuebingen.mpg.de;microsoft.com;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2;2;2",
        "aff_unique_norm": "University of Cambridge;Max Planck Institute for Intelligent Systems;Microsoft Research",
        "aff_unique_dep": "Cavendish Laboratory;;",
        "aff_unique_url": "https://www.cam.ac.uk;https://www.mpi-is.mpg.de;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "Cambridge;MPI-IS;MSR",
        "aff_campus_unique_index": "0+1;0;0;0",
        "aff_campus_unique": "Cambridge;T\u00fcbingen",
        "aff_country_unique_index": "0+1;0;0;0",
        "aff_country_unique": "United Kingdom;Germany"
    },
    {
        "title": "Krylov Subspace Descent for Deep Learning",
        "site": "https://proceedings.mlr.press/v22/vinyals12.html",
        "author": "Oriol Vinyals; Daniel Povey",
        "abstract": "In this paper, we propose a second order optimization method to learn models where both the dimensionality of the parameter space and the number of training samples is high.  In our method, we construct on each iteration a Krylov subspace formed by the gradient and an approximation to the Hessian matrix, and then use a subset of the training data samples to optimize over this subspace.  As with the Hessian Free (HF) method of Martens (2010), the Hessian matrix is never explicitly constructed, and is computed using a subset of data.  In practice, as in HF, we typically use a positive definite substitute for the Hessian matrix such as the Gauss-Newton matrix.  We investigate the effectiveness of our proposed method on deep neural networks, and compare its performance to widely used methods such as stochastic gradient descent, conjugate gradient descent and L-BFGS, and also to HF. Our method leads to faster convergence than either L-BFGS or HF, and generally performs better than either of them in cross-validation accuracy.  It is also simpler and more general than HF, as it does not require a positive semidefinite approximation of the Hessian matrix to work well nor the setting of a damping parameter.  The chief drawback versus HF is the need for memory to store a basis for the Krylov subspace.",
        "bibtex": "@InProceedings{pmlr-v22-vinyals12,\n  title = \t {Krylov Subspace Descent for Deep Learning},\n  author = \t {Vinyals, Oriol and Povey, Daniel},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1261--1268},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/vinyals12/vinyals12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/vinyals12.html},\n  abstract = \t {In this paper, we propose a second order optimization method to learn models where both the dimensionality of the parameter space and the number of training samples is high.  In our method, we construct on each iteration a Krylov subspace formed by the gradient and an approximation to the Hessian matrix, and then use a subset of the training data samples to optimize over this subspace.  As with the Hessian Free (HF) method of Martens (2010), the Hessian matrix is never explicitly constructed, and is computed using a subset of data.  In practice, as in HF, we typically use a positive definite substitute for the Hessian matrix such as the Gauss-Newton matrix.  We investigate the effectiveness of our proposed method on deep neural networks, and compare its performance to widely used methods such as stochastic gradient descent, conjugate gradient descent and L-BFGS, and also to HF. Our method leads to faster convergence than either L-BFGS or HF, and generally performs better than either of them in cross-validation accuracy.  It is also simpler and more general than HF, as it does not require a positive semidefinite approximation of the Hessian matrix to work well nor the setting of a damping parameter.  The chief drawback versus HF is the need for memory to store a basis for the Krylov subspace.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/vinyals12/vinyals12.pdf",
        "supp": "",
        "pdf_size": 381698,
        "gs_citation": 173,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8291372343301186384&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "University of California, Berkeley; Microsoft Research",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of California, Berkeley;Microsoft Corporation",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.berkeley.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "UC Berkeley;MSR",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Learning Fourier Sparse Set Functions",
        "site": "https://proceedings.mlr.press/v22/stobbe12.html",
        "author": "Peter Stobbe; Andreas Krause",
        "abstract": "Can we learn a sparse graph from observing the value of a few random cuts? This and more general problems can be reduced to the challenge of learning set functions known to have sparse Fourier support contained in some collection. We prove that if we choose O(k log^4 n) sets uniformly at random, then with high probability, observing any k-sparse function on those sets is sufficient to recover that function exactly. We further show that other properties, such as symmetry or submodularity imply structure in the Fourier spectrum, which can be exploited to further reduce sample complexity. One interesting special case  is that it suffices to observe O(|E| log^4 |V|) values of a cut function to recover a graph. We demonstrate the effectiveness of our results on two real-world reconstruction problems:  graph sketching and obtaining fast approximate surrogates to expensive submodular objective functions.",
        "bibtex": "@InProceedings{pmlr-v22-stobbe12,\n  title = \t {Learning Fourier Sparse Set Functions},\n  author = \t {Stobbe, Peter and Krause, Andreas},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1125--1133},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/stobbe12/stobbe12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/stobbe12.html},\n  abstract = \t {Can we learn a sparse graph from observing the value of a few random cuts? This and more general problems can be reduced to the challenge of learning set functions known to have sparse Fourier support contained in some collection. We prove that if we choose O(k log^4 n) sets uniformly at random, then with high probability, observing any k-sparse function on those sets is sufficient to recover that function exactly. We further show that other properties, such as symmetry or submodularity imply structure in the Fourier spectrum, which can be exploited to further reduce sample complexity. One interesting special case  is that it suffices to observe O(|E| log^4 |V|) values of a cut function to recover a graph. We demonstrate the effectiveness of our results on two real-world reconstruction problems:  graph sketching and obtaining fast approximate surrogates to expensive submodular objective functions.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/stobbe12/stobbe12.pdf",
        "supp": "",
        "pdf_size": 368373,
        "gs_citation": 55,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5914254306149975258&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Caltech; ETH Z\u00fcrich",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "California Institute of Technology;ETH Z\u00fcrich",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.caltech.edu;https://www.ethz.ch",
        "aff_unique_abbr": "Caltech;ETHZ",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Pasadena;",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United States;Switzerland"
    },
    {
        "title": "Learning Low-order Models for Enforcing High-order Statistics",
        "site": "https://proceedings.mlr.press/v22/pletscher12b.html",
        "author": "Patrick Pletscher; Pushmeet Kohli",
        "abstract": "Models such as pairwise conditional random fields (CRFs) are extremely popular in computer vision and various other machine learning disciplines. However, they have limited expressive power and often cannot represent the posterior distribution correctly. While learning the parameters of such models which have insufficient expressivity, researchers use loss functions to penalize certain misrepresentations of the solution space. Till now, researchers have used only simplistic loss functions such as the hamming loss, to enable efficient inference. The paper shows how sophisticated and useful higher order loss functions can be incorporated in the learning process. These loss functions ensure that the MAP solution does not deviate much from the ground truth in terms of certain \\emphhigher order statistics. We propose a learning algorithm which uses the recently proposed lower-envelope representation of higher order functions to transform them to pairwise functions, which allow efficient inference. We test the efficacy of our method on the problem of foreground-background image segmentation. Experimental results show that the incorporation of higher order loss functions in the learning formulation using our method leads to much better results compared to those obtained by using the traditional hamming loss.",
        "bibtex": "@InProceedings{pmlr-v22-pletscher12b,\n  title = \t {Learning Low-order Models for Enforcing High-order Statistics},\n  author = \t {Pletscher, Patrick and Kohli, Pushmeet},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {886--894},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/pletscher12b/pletscher12b.pdf},\n  url = \t {https://proceedings.mlr.press/v22/pletscher12b.html},\n  abstract = \t {Models such as pairwise conditional random fields (CRFs) are extremely popular in computer vision and various other machine learning disciplines. However, they have limited expressive power and often cannot represent the posterior distribution correctly. While learning the parameters of such models which have insufficient expressivity, researchers use loss functions to penalize certain misrepresentations of the solution space. Till now, researchers have used only simplistic loss functions such as the hamming loss, to enable efficient inference. The paper shows how sophisticated and useful higher order loss functions can be incorporated in the learning process. These loss functions ensure that the MAP solution does not deviate much from the ground truth in terms of certain \\emphhigher order statistics. We propose a learning algorithm which uses the recently proposed lower-envelope representation of higher order functions to transform them to pairwise functions, which allow efficient inference. We test the efficacy of our method on the problem of foreground-background image segmentation. Experimental results show that the incorporation of higher order loss functions in the learning formulation using our method leads to much better results compared to those obtained by using the traditional hamming loss.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/pletscher12b/pletscher12b.pdf",
        "supp": "",
        "pdf_size": 3332525,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17622548734295811121&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "ETH Zurich; Microsoft Research",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "ETH Zurich;Microsoft Corporation",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.ethz.ch;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "ETHZ;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Switzerland;United States"
    },
    {
        "title": "Learning from Weak Teachers",
        "site": "https://proceedings.mlr.press/v22/urner12.html",
        "author": "Ruth Urner; Shai Ben David; Ohad Shamir",
        "abstract": "This paper addresses the problem of learning when high-quality labeled examples are an expensive resource, while samples with error-prone labeling (for example generated by crowdsourcing) are readily available. We introduce a formal framework for such learning scenarios with label sources of varying quality, and we propose a parametric model for such label sources (\u201cweak teachers\u201d), reflecting the intuition that their labeling is likely to be correct in label-homogeneous regions but may deteriorate near classification boundaries. We consider learning when the learner has access to weakly labeled random samples and, on top of that, can actively query the correct labels of sample points of its choice. We propose a learning algorithm for this scenario, analyze its sample complexity and prove that, under certain conditions on the underlying data distribution, our learner can utilize the weak labels to reduce the number of expert labels it requires. We view this paper as a first step towards the development of a theory of learning from labels generated by teachers of varying accuracy, a scenario that is relevant in various practical applications.",
        "bibtex": "@InProceedings{pmlr-v22-urner12,\n  title = \t {Learning from Weak Teachers},\n  author = \t {Urner, Ruth and David, Shai Ben and Shamir, Ohad},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1252--1260},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/urner12/urner12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/urner12.html},\n  abstract = \t {This paper addresses the problem of learning when high-quality labeled examples are an expensive resource, while samples with error-prone labeling (for example generated by crowdsourcing) are readily available. We introduce a formal framework for such learning scenarios with label sources of varying quality, and we propose a parametric model for such label sources (\u201cweak teachers\u201d), reflecting the intuition that their labeling is likely to be correct in label-homogeneous regions but may deteriorate near classification boundaries. We consider learning when the learner has access to weakly labeled random samples and, on top of that, can actively query the correct labels of sample points of its choice. We propose a learning algorithm for this scenario, analyze its sample complexity and prove that, under certain conditions on the underlying data distribution, our learner can utilize the weak labels to reduce the number of expert labels it requires. We view this paper as a first step towards the development of a theory of learning from labels generated by teachers of varying accuracy, a scenario that is relevant in various practical applications.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/urner12/urner12.pdf",
        "supp": "",
        "pdf_size": 336406,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9677796372294118721&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Lifted Linear Programming",
        "site": "https://proceedings.mlr.press/v22/mladenov12.html",
        "author": "Martin Mladenov; Babak Ahmadi; Kristian Kersting",
        "abstract": "Lifted inference approaches have rendered large, previously intractable probabilistic inference problems quickly solvable by handling whole sets of indistinguishable objects together. Triggered by this success, we show that another important AI technique is liftable, too, namely linear programming.  Intuitively, given a linear program (LP), we employ a lifted variant of Gaussian belief propagation (GaBP) to solve the systems of linear equations arising when running an interior-point method to solve the LP.   However, this naive solution cannot make use of standard solvers for linear equations and is doomed to construct lifted networks in each iteration of the interior-point method again,  an operation that can itself be quite costly. To address both issues, we show how to read off an equivalent LP from the lifted GaBP computations that can be solved using any off-the-shelf LP solver. We prove the correctness of this compilation approach, including a lifted duality theorem, and experimentally demonstrate that it can greatly reduce the cost of solving LPs.",
        "bibtex": "@InProceedings{pmlr-v22-mladenov12,\n  title = \t {Lifted Linear Programming},\n  author = \t {Mladenov, Martin and Ahmadi, Babak and Kersting, Kristian},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {788--797},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/mladenov12/mladenov12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/mladenov12.html},\n  abstract = \t {Lifted inference approaches have rendered large, previously intractable probabilistic inference problems quickly solvable by handling whole sets of indistinguishable objects together. Triggered by this success, we show that another important AI technique is liftable, too, namely linear programming.  Intuitively, given a linear program (LP), we employ a lifted variant of Gaussian belief propagation (GaBP) to solve the systems of linear equations arising when running an interior-point method to solve the LP.   However, this naive solution cannot make use of standard solvers for linear equations and is doomed to construct lifted networks in each iteration of the interior-point method again,  an operation that can itself be quite costly. To address both issues, we show how to read off an equivalent LP from the lifted GaBP computations that can be solved using any off-the-shelf LP solver. We prove the correctness of this compilation approach, including a lifted duality theorem, and experimentally demonstrate that it can greatly reduce the cost of solving LPs.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/mladenov12/mladenov12.pdf",
        "supp": "",
        "pdf_size": 549345,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16987122395487575166&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Lifted Variable Elimination with Arbitrary Constraints",
        "site": "https://proceedings.mlr.press/v22/taghipour12.html",
        "author": "Nima Taghipour; Daan Fierens; Jesse Davis; Hendrik Blockeel",
        "abstract": "Lifted probabilistic inference algorithms exploit regularities in the structure of graphical models to perform inference more efficiently. More specifically, they identify groups of interchangeable variables and perform inference once for each group, as opposed to once for each variable. The groups are defined by means of constraints, so the flexibility of the grouping is determined by the expressivity of the constraint language. Existing approaches for exact lifted inference rely on (in)equality constraints. We show how inference methods can be generalized to work with arbitrary constraints. This allows them to capture a broader range of symmetries, leading to more opportunities for lifting. We empirically demonstrate that this improves inference efficiency with orders of magnitude, allowing exact inference in cases where until now only approximate inference was feasible.",
        "bibtex": "@InProceedings{pmlr-v22-taghipour12,\n  title = \t {Lifted Variable Elimination with Arbitrary Constraints},\n  author = \t {Taghipour, Nima and Fierens, Daan and Davis, Jesse and Blockeel, Hendrik},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1194--1202},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/taghipour12/taghipour12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/taghipour12.html},\n  abstract = \t {Lifted probabilistic inference algorithms exploit regularities in the structure of graphical models to perform inference more efficiently. More specifically, they identify groups of interchangeable variables and perform inference once for each group, as opposed to once for each variable. The groups are defined by means of constraints, so the flexibility of the grouping is determined by the expressivity of the constraint language. Existing approaches for exact lifted inference rely on (in)equality constraints. We show how inference methods can be generalized to work with arbitrary constraints. This allows them to capture a broader range of symmetries, leading to more opportunities for lifting. We empirically demonstrate that this improves inference efficiency with orders of magnitude, allowing exact inference in cases where until now only approximate inference was feasible.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/taghipour12/taghipour12.pdf",
        "supp": "",
        "pdf_size": 489817,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6526741106342562499&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Lifted coordinate descent for learning with trace-norm regularization",
        "site": "https://proceedings.mlr.press/v22/dudik12.html",
        "author": "Miroslav Dudik; Zaid Harchaoui; Jerome Malick",
        "abstract": "We consider the minimization of a smooth loss with trace-norm regularization, which is a natural objective in multi-class and multi-task learning. Even though the problem is convex, existing approaches rely on optimizing a non-convex variational bound, which is not guaranteed to converge, or repeatedly perform singular-value decomposition, which prevents scaling beyond moderate matrix sizes. We lift the non-smooth convex problem into an infinitely dimensional smooth problem and apply coordinate descent to solve it. We prove that our approach converges to the optimum, and is competitive or outperforms state of the art.",
        "bibtex": "@InProceedings{pmlr-v22-dudik12,\n  title = \t {Lifted coordinate descent for learning with trace-norm regularization},\n  author = \t {Dudik, Miroslav and Harchaoui, Zaid and Malick, Jerome},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {327--336},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/dudik12/dudik12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/dudik12.html},\n  abstract = \t {We consider the minimization of a smooth loss with trace-norm regularization, which is a natural objective in multi-class and multi-task learning. Even though the problem is convex, existing approaches rely on optimizing a non-convex variational bound, which is not guaranteed to converge, or repeatedly perform singular-value decomposition, which prevents scaling beyond moderate matrix sizes. We lift the non-smooth convex problem into an infinitely dimensional smooth problem and apply coordinate descent to solve it. We prove that our approach converges to the optimum, and is competitive or outperforms state of the art.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/dudik12/dudik12.pdf",
        "supp": "",
        "pdf_size": 535496,
        "gs_citation": 127,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4772196962414602011&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Lightning-speed Structure Learning of Nonlinear Continuous Networks",
        "site": "https://proceedings.mlr.press/v22/elidan12b.html",
        "author": "Gal Elidan",
        "abstract": "Graphical models are widely used to reason about high-dimensional domains. Yet, learning the structure of the model from data remains a formidable challenge, particularly in complex continuous domains. We present a highly accelerated structure learning approach for continuous densities based on the recently introduced copula Bayesian network representation. For two common copula families, we prove that the expected likelihood of a building block edge in the model is monotonic in Spearman\u2019s rank correlation measure. We also show numerically that the same relationship holds for many other copula families. This allows us to perform structure learning while bypassing costly parameter estimation as well as explicit computation of the log-likelihood function.  We demonstrate the merit of our approach for structure learning in three varied real-life domains.  Importantly, the computational benefits are such that they open the door for practical scaling-up of structure learning in complex nonlinear continuous domains.",
        "bibtex": "@InProceedings{pmlr-v22-elidan12b,\n  title = \t {Lightning-speed Structure Learning of Nonlinear Continuous Networks},\n  author = \t {Elidan, Gal},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {355--363},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/elidan12b/elidan12b.pdf},\n  url = \t {https://proceedings.mlr.press/v22/elidan12b.html},\n  abstract = \t {Graphical models are widely used to reason about high-dimensional domains. Yet, learning the structure of the model from data remains a formidable challenge, particularly in complex continuous domains. We present a highly accelerated structure learning approach for continuous densities based on the recently introduced copula Bayesian network representation. For two common copula families, we prove that the expected likelihood of a building block edge in the model is monotonic in Spearman\u2019s rank correlation measure. We also show numerically that the same relationship holds for many other copula families. This allows us to perform structure learning while bypassing costly parameter estimation as well as explicit computation of the log-likelihood function.  We demonstrate the merit of our approach for structure learning in three varied real-life domains.  Importantly, the computational benefits are such that they open the door for practical scaling-up of structure learning in complex nonlinear continuous domains.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/elidan12b/elidan12b.pdf",
        "supp": "",
        "pdf_size": 685031,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14832933525290580621&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Statistics, The Hebrew University",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "The Hebrew University",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "http://www.huji.ac.il",
        "aff_unique_abbr": "HUJI",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Israel"
    },
    {
        "title": "Local Anomaly Detection",
        "site": "https://proceedings.mlr.press/v22/saligrama12.html",
        "author": "Venkatesh Saligrama; Manqi Zhao",
        "abstract": "Anomalies with spatial and temporal stamps arise in a number of applications including communication networks, traffic monitoring and video analysis. In these applications anomalies are temporally or spatially localized but otherwise unknown. We propose a novel graph-based statistical notion that unifies the idea of temporal and spatial locality. This notion lends itself to an elegant characterization of optimal decision rules and in turn suggests corresponding empirical rules based on local nearest neighbor distances. We compute a single composite score for the entire spatio-temporal data sample based on the local neighborhood distances. We declare data samples as containing local anomalies based on the composite score. We show that such rules not only asymptotically guarantee desired false alarm control but are also asymptotically optimal. We also show that our composite scoring scheme overcomes the inherent resolution issues of alternative multi-comparison approaches that are based on fusing the outcomes of location-by-location comparisons. We then verify our algorithms on synthetic and real data sets.",
        "bibtex": "@InProceedings{pmlr-v22-saligrama12,\n  title = \t {Local Anomaly Detection},\n  author = \t {Saligrama, Venkatesh and Zhao, Manqi},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {969--983},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/saligrama12/saligrama12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/saligrama12.html},\n  abstract = \t {Anomalies with spatial and temporal stamps arise in a number of applications including communication networks, traffic monitoring and video analysis. In these applications anomalies are temporally or spatially localized but otherwise unknown. We propose a novel graph-based statistical notion that unifies the idea of temporal and spatial locality. This notion lends itself to an elegant characterization of optimal decision rules and in turn suggests corresponding empirical rules based on local nearest neighbor distances. We compute a single composite score for the entire spatio-temporal data sample based on the local neighborhood distances. We declare data samples as containing local anomalies based on the composite score. We show that such rules not only asymptotically guarantee desired false alarm control but are also asymptotically optimal. We also show that our composite scoring scheme overcomes the inherent resolution issues of alternative multi-comparison approaches that are based on fusing the outcomes of location-by-location comparisons. We then verify our algorithms on synthetic and real data sets.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/saligrama12/saligrama12.pdf",
        "supp": "",
        "pdf_size": 1191150,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7461124492542889096&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Boston University; Google Inc.",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Boston University;Google",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.bu.edu;https://www.google.com",
        "aff_unique_abbr": "BU;Google",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Locality Preserving Feature Learning",
        "site": "https://proceedings.mlr.press/v22/gu12.html",
        "author": "Quanquan Gu; Marina Danilevsky; Zhenhui Li; Jiawei Han",
        "abstract": "Locality Preserving Indexing (LPI) has been quite successful in tackling document analysis problems, such as clustering or classification. The approach relies on the Locality Preserving Criterion, which preserves the locality of the data points. However, LPI takes every word in a data corpus into account, even though many words may not be useful for document clustering. To overcome this problem, we propose an approach called Locality Preserving Feature Learning (LPFL), which incorporates feature selection into LPI. Specifically, we aim to find a subset of features, and learn a linear transformation to optimize the Locality Preserving Criterion based on these features. The resulting optimization problem is a mixed integer programming problem, which we relax into a constrained Frobenius norm minimization problem, and solve using a variation of Alternating Direction Method (ADM). ADM, which iteratively updates the linear transformation matrix, the residue matrix and the Lagrangian multiplier, is theoretically guaranteed to converge at the rate O(1/t). Experiments on benchmark document datasets show that our proposed method outperforms LPI, as well as other state-of-the-art document analysis approaches.",
        "bibtex": "@InProceedings{pmlr-v22-gu12,\n  title = \t {Locality Preserving Feature Learning},\n  author = \t {Gu, Quanquan and Danilevsky, Marina and Li, Zhenhui and Han, Jiawei},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {477--485},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/gu12/gu12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/gu12.html},\n  abstract = \t {Locality Preserving Indexing (LPI) has been quite successful in tackling document analysis problems, such as clustering or classification. The approach relies on the Locality Preserving Criterion, which preserves the locality of the data points. However, LPI takes every word in a data corpus into account, even though many words may not be useful for document clustering. To overcome this problem, we propose an approach called Locality Preserving Feature Learning (LPFL), which incorporates feature selection into LPI. Specifically, we aim to find a subset of features, and learn a linear transformation to optimize the Locality Preserving Criterion based on these features. The resulting optimization problem is a mixed integer programming problem, which we relax into a constrained Frobenius norm minimization problem, and solve using a variation of Alternating Direction Method (ADM). ADM, which iteratively updates the linear transformation matrix, the residue matrix and the Lagrangian multiplier, is theoretically guaranteed to converge at the rate O(1/t). Experiments on benchmark document datasets show that our proposed method outperforms LPI, as well as other state-of-the-art document analysis approaches.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/gu12/gu12.pdf",
        "supp": "",
        "pdf_size": 709478,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15941353902803970157&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Department of Computer Science, University of Illinois at Urbana-Champaign, IL 61801, USA; Department of Computer Science, University of Illinois at Urbana-Champaign, IL 61801, USA; Department of Computer Science, University of Illinois at Urbana-Champaign, IL 61801, USA; Department of Computer Science, University of Illinois at Urbana-Champaign, IL 61801, USA",
        "aff_domain": "illinois.edu;illinois.edu;illinois.edu;illinois.edu",
        "email": "illinois.edu;illinois.edu;illinois.edu;illinois.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Illinois at Urbana-Champaign",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://illinois.edu",
        "aff_unique_abbr": "UIUC",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Urbana-Champaign",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Low rank continuous-space graphical models",
        "site": "https://proceedings.mlr.press/v22/smith12.html",
        "author": "Carl Smith; Frank Wood; Liam Paninski",
        "abstract": "Constructing tractable dependent probability distributions over structured continuous random vectors is a central problem in statistics and machine learning. It has proven difficult to find general constructions for models in which efficient exact inference is possible, outside of the classical cases of models with restricted graph structure (chain, tree, etc.) and linear-Gaussian or discrete potentials. In this work we identify a tree graphical model class in which exact inference can be performed efficiently, owing to a certain \u201clow-rank\u201d structure in the potentials. We explore this new class of models by applying the resulting inference methods to neural spike rate estimation and motion-capture joint-angle smoothing tasks.",
        "bibtex": "@InProceedings{pmlr-v22-smith12,\n  title = \t {Low rank continuous-space graphical models},\n  author = \t {Smith, Carl and Wood, Frank and Paninski, Liam},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1064--1072},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/smith12/smith12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/smith12.html},\n  abstract = \t {Constructing tractable dependent probability distributions over structured continuous random vectors is a central problem in statistics and machine learning. It has proven difficult to find general constructions for models in which efficient exact inference is possible, outside of the classical cases of models with restricted graph structure (chain, tree, etc.) and linear-Gaussian or discrete potentials. In this work we identify a tree graphical model class in which exact inference can be performed efficiently, owing to a certain \u201clow-rank\u201d structure in the potentials. We explore this new class of models by applying the resulting inference methods to neural spike rate estimation and motion-capture joint-angle smoothing tasks.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/smith12/smith12.pdf",
        "supp": "",
        "pdf_size": 1083921,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13071024721767752377&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Chemistry, Columbia University, New York, NY 10027; Department of Statistics, Columbia University, New York, NY 10027; Department of Statistics, Columbia University, New York, NY 10027",
        "aff_domain": "columbia.edu;stat.columbia.edu;stat.columbia.edu",
        "email": "columbia.edu;stat.columbia.edu;stat.columbia.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Columbia University",
        "aff_unique_dep": "Department of Chemistry",
        "aff_unique_url": "https://www.columbia.edu",
        "aff_unique_abbr": "Columbia",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "New York",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Marginal Regression For Multitask Learning",
        "site": "https://proceedings.mlr.press/v22/kolar12.html",
        "author": "Mladen Kolar; Han Liu",
        "abstract": "Variable selection is an important practical problem that arises in   analysis of many high-dimensional datasets. Convex optimization   procedures, that arise from relaxing the NP-hard subset selection   procedure, e.g., the Lasso or Dantzig selector, have become the   focus of intense theoretical investigations. Although many efficient   algorithms exist that solve these problems, finding a solution when   the number of variables is large, e.g., several hundreds of   thousands in problems arising in genome-wide association analysis,   is still computationally challenging. A practical solution for these   high-dimensional problems is the marginal regression, where the   output is regressed on each variable separately. We investigate   theoretical properties of the marginal regression in a multitask   framework. Our contribution include: i) sharp analysis for the   marginal regression in a single task setting with random design, ii)   sufficient conditions for the multitask screening to select the   relevant variables, iii) a lower bound on the Hamming distance   convergence for multitask variable selection problems. A simulation   study further demonstrates the performance of the marginal   regression.",
        "bibtex": "@InProceedings{pmlr-v22-kolar12,\n  title = \t {Marginal Regression For Multitask Learning},\n  author = \t {Kolar, Mladen and Liu, Han},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {647--655},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/kolar12/kolar12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/kolar12.html},\n  abstract = \t {Variable selection is an important practical problem that arises in   analysis of many high-dimensional datasets. Convex optimization   procedures, that arise from relaxing the NP-hard subset selection   procedure, e.g., the Lasso or Dantzig selector, have become the   focus of intense theoretical investigations. Although many efficient   algorithms exist that solve these problems, finding a solution when   the number of variables is large, e.g., several hundreds of   thousands in problems arising in genome-wide association analysis,   is still computationally challenging. A practical solution for these   high-dimensional problems is the marginal regression, where the   output is regressed on each variable separately. We investigate   theoretical properties of the marginal regression in a multitask   framework. Our contribution include: i) sharp analysis for the   marginal regression in a single task setting with random design, ii)   sufficient conditions for the multitask screening to select the   relevant variables, iii) a lower bound on the Hamming distance   convergence for multitask variable selection problems. A simulation   study further demonstrates the performance of the marginal   regression.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/kolar12/kolar12.pdf",
        "supp": "",
        "pdf_size": 370157,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5509180072093494012&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Machine Learning Department, Carnegie Mellon University; Biostatistics, Johns Hopkins University",
        "aff_domain": "cs.cmu.edu;jhsph.edu",
        "email": "cs.cmu.edu;jhsph.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Carnegie Mellon University;Johns Hopkins University",
        "aff_unique_dep": "Machine Learning Department;Biostatistics",
        "aff_unique_url": "https://www.cmu.edu;https://www.jhu.edu",
        "aff_unique_abbr": "CMU;JHU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Markov Logic Mixtures of Gaussian Processes: Towards Machines Reading Regression Data",
        "site": "https://proceedings.mlr.press/v22/schiegg12.html",
        "author": "Martin Schiegg; Marion Neumann; Kristian Kersting",
        "abstract": "We propose a novel mixtures of Gaussian processes model in which the gating function is interconnected with a probabilistic logical model, in our case Markov logic networks. In this way, the resulting mixed graphical model, called Markov logic mixtures of Gaussian processes (MLxGP), solves joint Bayesian non-parametric regression and probabilistic relational inference tasks. In turn, MLxGP facilitates novel, interesting tasks such as regression based on logical constraints or drawing probabilistic logical conclusions about regression data, thus putting \u201cmachines reading regression data\u201d in reach.",
        "bibtex": "@InProceedings{pmlr-v22-schiegg12,\n  title = \t {Markov Logic Mixtures of Gaussian Processes: Towards Machines Reading Regression Data},\n  author = \t {Schiegg, Martin and Neumann, Marion and Kersting, Kristian},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1002--1011},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/schiegg12/schiegg12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/schiegg12.html},\n  abstract = \t {We propose a novel mixtures of Gaussian processes model in which the gating function is interconnected with a probabilistic logical model, in our case Markov logic networks. In this way, the resulting mixed graphical model, called Markov logic mixtures of Gaussian processes (MLxGP), solves joint Bayesian non-parametric regression and probabilistic relational inference tasks. In turn, MLxGP facilitates novel, interesting tasks such as regression based on logical constraints or drawing probabilistic logical conclusions about regression data, thus putting \u201cmachines reading regression data\u201d in reach.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/schiegg12/schiegg12.pdf",
        "supp": "",
        "pdf_size": 675832,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9383179594630611713&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Institute of Applied Information Processing, University of Ulm; Knowledge Discovery Department, Fraunhofer IAIS; Knowledge Discovery Department, Fraunhofer IAIS",
        "aff_domain": "alumni.uni-ulm.de;iais.fraunhofer.de;iais.fraunhofer.de",
        "email": "alumni.uni-ulm.de;iais.fraunhofer.de;iais.fraunhofer.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of Ulm;Fraunhofer IAIS",
        "aff_unique_dep": "Institute of Applied Information Processing;Knowledge Discovery Department",
        "aff_unique_url": "https://www.uni-ulm.de;https://www.iais.fraunhofer.de/",
        "aff_unique_abbr": ";Fraunhofer IAIS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Max-Margin Min-Entropy Models",
        "site": "https://proceedings.mlr.press/v22/miller12.html",
        "author": "Kevin Miller; M. Pawan Kumar; Ben Packer; Danny Goodman; Daphne Koller",
        "abstract": "We propose a new family of latent variable models called max-margin min-entropy (M3E) models, which define a distribution over the output and the hidden variables conditioned on the input. Given an input, an M3E model predicts the output with the smallest corresponding Renyi entropy of generalized distribution. This is equivalent to minimizing a score that consists of two terms: (i) the negative log-likelihood of the output, ensuring that the output has a high probability; and (ii) a measure of uncertainty over the distribution of the hidden variables conditioned on the input and the output, ensuring that there is little confusion in the values of the hidden variables. Given a training dataset, the parameters of an M3E model are learned by maximizing the margin between the Renyi entropies of the ground-truth output and all other incorrect outputs. Training an M3E can be viewed as minimizing an upper bound on a user-defined loss, and includes, as a special case, the latent support vector machine framework. We demonstrate the efficacy of M3E models on two standard machine learning applications, discriminative motif finding and image classification, using publicly available datasets.",
        "bibtex": "@InProceedings{pmlr-v22-miller12,\n  title = \t {Max-Margin Min-Entropy Models},\n  author = \t {Miller, Kevin and Kumar, M. Pawan and Packer, Ben and Goodman, Danny and Koller, Daphne},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {779--787},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/miller12/miller12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/miller12.html},\n  abstract = \t {We propose a new family of latent variable models called max-margin min-entropy (M3E) models, which define a distribution over the output and the hidden variables conditioned on the input. Given an input, an M3E model predicts the output with the smallest corresponding Renyi entropy of generalized distribution. This is equivalent to minimizing a score that consists of two terms: (i) the negative log-likelihood of the output, ensuring that the output has a high probability; and (ii) a measure of uncertainty over the distribution of the hidden variables conditioned on the input and the output, ensuring that there is little confusion in the values of the hidden variables. Given a training dataset, the parameters of an M3E model are learned by maximizing the margin between the Renyi entropies of the ground-truth output and all other incorrect outputs. Training an M3E can be viewed as minimizing an upper bound on a user-defined loss, and includes, as a special case, the latent support vector machine framework. We demonstrate the efficacy of M3E models on two standard machine learning applications, discriminative motif finding and image classification, using publicly available datasets.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/miller12/miller12.pdf",
        "supp": "",
        "pdf_size": 466304,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17189450080174302700&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Stanford University; Ecole Centrale Paris & INRIA Saclay; Stanford University; Stanford University; Stanford University",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0;0",
        "aff_unique_norm": "Stanford University;Ecole Centrale Paris",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.stanford.edu;https://www.ecp.fr",
        "aff_unique_abbr": "Stanford;ECP",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;1;0;0;0",
        "aff_country_unique": "United States;France"
    },
    {
        "title": "Maximum Margin Temporal Clustering",
        "site": "https://proceedings.mlr.press/v22/hoai12.html",
        "author": "Minh Hoai; Fernando De La Torre",
        "abstract": "Temporal Clustering (TC) refers to the factorization of multiple time series into a set of non-overlapping segments that belong to k temporal clusters. Existing methods based on extensions of generative models such as k-means or Switching Linear Dynamical Systems often lead to intractable inference and lack a mechanism for feature selection, critical when dealing with high dimensional data. To overcome these limitations, this paper proposes Maximum Margin Temporal Clustering (MMTC). MMTC simultaneously determines the start and the end of each segment, while learning a multi-class Support Vector Machine to discriminate among temporal clusters. MMTC extends Maximum Margin Clustering in two ways: first, it incorporates the notion of TC, and second, it introduces additional constraints to achieve better balance between clusters. Experiments on clustering human actions and bee dancing motions illustrate the benefits of our approach compared to state-of-the-art methods.",
        "bibtex": "@InProceedings{pmlr-v22-hoai12,\n  title = \t {Maximum Margin Temporal Clustering},\n  author = \t {Hoai, Minh and Torre, Fernando De La},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {520--528},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/hoai12/hoai12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/hoai12.html},\n  abstract = \t {Temporal Clustering (TC) refers to the factorization of multiple time series into a set of non-overlapping segments that belong to k temporal clusters. Existing methods based on extensions of generative models such as k-means or Switching Linear Dynamical Systems often lead to intractable inference and lack a mechanism for feature selection, critical when dealing with high dimensional data. To overcome these limitations, this paper proposes Maximum Margin Temporal Clustering (MMTC). MMTC simultaneously determines the start and the end of each segment, while learning a multi-class Support Vector Machine to discriminate among temporal clusters. MMTC extends Maximum Margin Clustering in two ways: first, it incorporates the notion of TC, and second, it introduces additional constraints to achieve better balance between clusters. Experiments on clustering human actions and bee dancing motions illustrate the benefits of our approach compared to state-of-the-art methods.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/hoai12/hoai12.pdf",
        "supp": "",
        "pdf_size": 708639,
        "gs_citation": 69,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1026627037607129293&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Robotics Institute, Carnegie Mellon University, Pittsburgh, PA 15213, USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA 15213, USA",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Robotics Institute",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Memory-efficient inference in dynamic graphical models using multiple cores",
        "site": "https://proceedings.mlr.press/v22/andrew12.html",
        "author": "Galen Andrew; Jeff Bilmes",
        "abstract": "We introduce the archipelagos algorithm for memory-efficient multi-core inference in dynamic graphical models. By making use of several processors running in parallel, the archipelagos algorithm uses exponentially less memory compared to basic forward-backward message passing algorithms (O(log T) compared to O(T) on sequences of length T) and, under often-satisfied assumptions on the relative speed of passing forward and backward messages, runs no slower. We also describe a simple variant of the algorithm that achieves a factor of two speedup over forward-backward on a single core. Experiments with our implementation of archipelagos for the computation of posterior marginal probabilities in an HMM validate the space/time complexity analysis: using four cores, the required memory on our test problem was reduced from 8 GB to 319 KB (a factor of 25000) relative to forward-backward, but completed in essentially the same time. The archipelagos algorithm applies to any dynamic graphical model, including dynamic Bayesian networks, conditional random fields, and hidden conditional random fields.",
        "bibtex": "@InProceedings{pmlr-v22-andrew12,\n  title = \t {Memory-efficient inference in dynamic graphical models using multiple cores},\n  author = \t {Andrew, Galen and Bilmes, Jeff},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {47--53},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/andrew12/andrew12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/andrew12.html},\n  abstract = \t {We introduce the archipelagos algorithm for memory-efficient multi-core inference in dynamic graphical models. By making use of several processors running in parallel, the archipelagos algorithm uses exponentially less memory compared to basic forward-backward message passing algorithms (O(log T) compared to O(T) on sequences of length T) and, under often-satisfied assumptions on the relative speed of passing forward and backward messages, runs no slower. We also describe a simple variant of the algorithm that achieves a factor of two speedup over forward-backward on a single core. Experiments with our implementation of archipelagos for the computation of posterior marginal probabilities in an HMM validate the space/time complexity analysis: using four cores, the required memory on our test problem was reduced from 8 GB to 319 KB (a factor of 25000) relative to forward-backward, but completed in essentially the same time. The archipelagos algorithm applies to any dynamic graphical model, including dynamic Bayesian networks, conditional random fields, and hidden conditional random fields.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/andrew12/andrew12.pdf",
        "supp": "",
        "pdf_size": 1705837,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6959593887388251676&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "University of Washington, Department of CSE; University of Washington, Departments of EE & CSE",
        "aff_domain": "; ",
        "email": "; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Washington",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.washington.edu",
        "aff_unique_abbr": "UW",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Message-Passing Algorithms for MAP Estimation Using  DC Programming",
        "site": "https://proceedings.mlr.press/v22/kumar12.html",
        "author": "Akshat Kumar; Shlomo Zilberstein; Marc Toussaint",
        "abstract": "We address the problem of finding the most likely assignment or MAP estimation in a Markov random field. We analyze the linear programming formulation of MAP through the lens of difference of convex functions (DC) programming, and use the concave-convex procedure (CCCP) to develop efficient message-passing solvers. The resulting algorithms are guaranteed to converge to a global optimum of the well-studied local polytope, an outer bound on the MAP marginal polytope. To tighten the outer bound, we show how to combine it with the mean-field based inner bound and, again, solve it using CCCP. We also identify a useful relationship between the DC formulations and some recently proposed algorithms based on Bregman divergence.  Experimentally, this hybrid approach produces optimal solutions for a range of hard OR problems and near-optimal solutions for standard benchmarks.",
        "bibtex": "@InProceedings{pmlr-v22-kumar12,\n  title = \t {Message-Passing Algorithms for MAP Estimation Using  DC Programming},\n  author = \t {Kumar, Akshat and Zilberstein, Shlomo and Toussaint, Marc},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {656--664},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/kumar12/kumar12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/kumar12.html},\n  abstract = \t {We address the problem of finding the most likely assignment or MAP estimation in a Markov random field. We analyze the linear programming formulation of MAP through the lens of difference of convex functions (DC) programming, and use the concave-convex procedure (CCCP) to develop efficient message-passing solvers. The resulting algorithms are guaranteed to converge to a global optimum of the well-studied local polytope, an outer bound on the MAP marginal polytope. To tighten the outer bound, we show how to combine it with the mean-field based inner bound and, again, solve it using CCCP. We also identify a useful relationship between the DC formulations and some recently proposed algorithms based on Bregman divergence.  Experimentally, this hybrid approach produces optimal solutions for a range of hard OR problems and near-optimal solutions for standard benchmarks.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/kumar12/kumar12.pdf",
        "supp": "",
        "pdf_size": 576387,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10980499113756380690&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Minimax Rates of Estimation for Sparse PCA in High Dimensions",
        "site": "https://proceedings.mlr.press/v22/vu12.html",
        "author": "Vincent Vu; Jing Lei",
        "abstract": "We study sparse principal components analysis in the high-dimensional setting, where p (the number of variables) can be much larger  than n (the number of observations). We prove optimal minimax lower and upper bounds on the estimation error for the first leading eigenvector when it belongs to an \\ell_q ball for q \u2208[0,1]. Our bounds are tight in p and n for all q \u2208[0, 1] over a wide class of distributions. The upper bound is obtained by analyzing  the performance of \\ell_q-constrained PCA. In particular, our results provide convergence rates for \\ell_1-constrained PCA. Our methods and arguments are also extendable to multi-dimensional subspace estimation.",
        "bibtex": "@InProceedings{pmlr-v22-vu12,\n  title = \t {Minimax Rates of Estimation for Sparse PCA in High Dimensions},\n  author = \t {Vu, Vincent and Lei, Jing},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1278--1286},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/vu12/vu12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/vu12.html},\n  abstract = \t {We study sparse principal components analysis in the high-dimensional setting, where p (the number of variables) can be much larger  than n (the number of observations). We prove optimal minimax lower and upper bounds on the estimation error for the first leading eigenvector when it belongs to an \\ell_q ball for q \u2208[0,1]. Our bounds are tight in p and n for all q \u2208[0, 1] over a wide class of distributions. The upper bound is obtained by analyzing  the performance of \\ell_q-constrained PCA. In particular, our results provide convergence rates for \\ell_1-constrained PCA. Our methods and arguments are also extendable to multi-dimensional subspace estimation.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/vu12/vu12.pdf",
        "supp": "",
        "pdf_size": 1025816,
        "gs_citation": 159,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12455269667402568468&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Statistics, Carnegie Mellon University; Department of Statistics, Carnegie Mellon University",
        "aff_domain": "stat.cmu.edu;andrew.cmu.edu",
        "email": "stat.cmu.edu;andrew.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Minimax hypothesis testing for curve registration",
        "site": "https://proceedings.mlr.press/v22/collier12.html",
        "author": "Olivier Collier",
        "abstract": "This paper is concerned with the problem of goodness-of-fit for curve registration, and more precisely for the shifted curve model, whose application field reaches from computer vision and road trafic prediction to medicine. We give bounds for the asymptotic minimax separation rate, when the functions in the alternative lie in Sobolev balls and the separation from the null hypothesis is measured by the l2-norm. We use the generalized likelihood ratio to build a nonadaptive procedure depending on a tuning parameter, which we choose in an optimal way according to the smoothness of the ambient space. Then, a Bonferroni procedure is applied to give an adaptive test over a range of Sobolev balls. Both achieve the asymptotic minimax separation rates, up to possible logarithmic factors.",
        "bibtex": "@InProceedings{pmlr-v22-collier12,\n  title = \t {Minimax hypothesis testing for curve registration},\n  author = \t {Collier, Olivier},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {236--245},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/collier12/collier12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/collier12.html},\n  abstract = \t {This paper is concerned with the problem of goodness-of-fit for curve registration, and more precisely for the shifted curve model, whose application field reaches from computer vision and road trafic prediction to medicine. We give bounds for the asymptotic minimax separation rate, when the functions in the alternative lie in Sobolev balls and the separation from the null hypothesis is measured by the l2-norm. We use the generalized likelihood ratio to build a nonadaptive procedure depending on a tuning parameter, which we choose in an optimal way according to the smoothness of the ambient space. Then, a Bonferroni procedure is applied to give an adaptive test over a range of Sobolev balls. Both achieve the asymptotic minimax separation rates, up to possible logarithmic factors.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/collier12/collier12.pdf",
        "supp": "",
        "pdf_size": 377808,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4227864638730279556&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 20,
        "aff": "",
        "aff_domain": "stat.cmu.edu;andrew.cmu.edu",
        "email": "stat.cmu.edu;andrew.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Minimax rates for homology inference",
        "site": "https://proceedings.mlr.press/v22/balakrishnan12a.html",
        "author": "Sivaraman Balakrishnan; Alesandro Rinaldo; Don Sheehy; Aarti Singh; Larry Wasserman",
        "abstract": "Often, high dimensional data lie close to a low-dimensional submanifold and it is of interest to understand the geometry of these submanifolds. The homology groups of a (sub)manifold are important topological invariants that provide an algebraic summary of the manifold. These groups contain rich topological information, for instance, about the connected components, holes, tunnels and (sometimes) the dimension of the manifold. In this paper, we consider the statistical problem of estimating the homology of a manifold from noisy samples under several different noise models. We derive upper and lower bounds on the minimax risk for this problem. Our upper bounds are based on estimators which are constructed from a union of balls of appropriate radius around carefully selected sample points. In each case we establish complementary lower bounds using Le Cam\u2019s lemma.",
        "bibtex": "@InProceedings{pmlr-v22-balakrishnan12a,\n  title = \t {Minimax rates for homology inference},\n  author = \t {Balakrishnan, Sivaraman and Rinaldo, Alesandro and Sheehy, Don and Singh, Aarti and Wasserman, Larry},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {64--72},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/balakrishnan12a/balakrishnan12a.pdf},\n  url = \t {https://proceedings.mlr.press/v22/balakrishnan12a.html},\n  abstract = \t {Often, high dimensional data lie close to a low-dimensional submanifold and it is of interest to understand the geometry of these submanifolds. The homology groups of a (sub)manifold are important topological invariants that provide an algebraic summary of the manifold. These groups contain rich topological information, for instance, about the connected components, holes, tunnels and (sometimes) the dimension of the manifold. In this paper, we consider the statistical problem of estimating the homology of a manifold from noisy samples under several different noise models. We derive upper and lower bounds on the minimax risk for this problem. Our upper bounds are based on estimators which are constructed from a union of balls of appropriate radius around carefully selected sample points. In each case we establish complementary lower bounds using Le Cam\u2019s lemma.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/balakrishnan12a/balakrishnan12a.pdf",
        "supp": "",
        "pdf_size": 1486509,
        "gs_citation": 51,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=203703007357987828&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 21,
        "aff": "Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University + INRIA; Carnegie Mellon University; Carnegie Mellon University",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0+1;0;0",
        "aff_unique_norm": "Carnegie Mellon University;INRIA",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cmu.edu;https://www.inria.fr",
        "aff_unique_abbr": "CMU;INRIA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+1;0;0",
        "aff_country_unique": "United States;France"
    },
    {
        "title": "Movement Segmentation and Recognition for Imitation Learning",
        "site": "https://proceedings.mlr.press/v22/meier12.html",
        "author": "Franziska Meier; Evangelos Theodorou; Stefan Schaal",
        "abstract": "In human movement learning, it is most common to teach constituent elements of complex movements in isolation, before chaining them into complex movements. Segmentation and recognition of observed movement could thus proceed out of this existing knowledge, which is directly compatible with movement generation. In this paper, we address exactly this scenario. We assume that a library of movement primitives has already been taught, and we wish to identify elements of the library in a complex motor act, where the individual elements have been smoothed together, and, occasionally, there might be a movement segment that is not in our library yet. We employ a flexible machine learning representation of movement primitives based on learnable nonlinear attractor system. For the purpose of movement segmentation and recognition, it is possible to reformulate this representation as a controlled linear dynamical system.  An Expectation-Maximization algorithm can be developed to estimate the open parameters of a movement primitive from the library, using as input an observed trajectory piece. If no matching primitive from the library can be found, a new primitive is created. This process allows a straightforward sequential segmentation of observed movement into known and new primitives, which are suitable for robot imitation learning. We illustrate our approach with synthetic examples and data collected from human movement.",
        "bibtex": "@InProceedings{pmlr-v22-meier12,\n  title = \t {Movement Segmentation and Recognition for Imitation Learning},\n  author = \t {Meier, Franziska and Theodorou, Evangelos and Schaal, Stefan},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {761--769},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/meier12/meier12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/meier12.html},\n  abstract = \t {In human movement learning, it is most common to teach constituent elements of complex movements in isolation, before chaining them into complex movements. Segmentation and recognition of observed movement could thus proceed out of this existing knowledge, which is directly compatible with movement generation. In this paper, we address exactly this scenario. We assume that a library of movement primitives has already been taught, and we wish to identify elements of the library in a complex motor act, where the individual elements have been smoothed together, and, occasionally, there might be a movement segment that is not in our library yet. We employ a flexible machine learning representation of movement primitives based on learnable nonlinear attractor system. For the purpose of movement segmentation and recognition, it is possible to reformulate this representation as a controlled linear dynamical system.  An Expectation-Maximization algorithm can be developed to estimate the open parameters of a movement primitive from the library, using as input an observed trajectory piece. If no matching primitive from the library can be found, a new primitive is created. This process allows a straightforward sequential segmentation of observed movement into known and new primitives, which are suitable for robot imitation learning. We illustrate our approach with synthetic examples and data collected from human movement.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/meier12/meier12.pdf",
        "supp": "",
        "pdf_size": 523158,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=476906804055646748&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Computational Learning and Motor Control Lab, University of Southern California, Los Angeles; Department of Computer Science and Engineering, University of Washington, Seattle; Computational Learning and Motor Control Lab, University of Southern California, Los Angeles + Max-Planck-Institute for Intelligent Systems, T\u00fcbingen, Germany",
        "aff_domain": "usc.edu;cs.washington.edu;usc.edu",
        "email": "usc.edu;cs.washington.edu;usc.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0+2",
        "aff_unique_norm": "University of Southern California;University of Washington;Max-Planck-Institute for Intelligent Systems",
        "aff_unique_dep": "Computational Learning and Motor Control Lab;Department of Computer Science and Engineering;",
        "aff_unique_url": "https://www.usc.edu;https://www.washington.edu;https://www.mpi-is.mpg.de",
        "aff_unique_abbr": "USC;UW;MPI-IS",
        "aff_campus_unique_index": "0;1;0+2",
        "aff_campus_unique": "Los Angeles;Seattle;T\u00fcbingen",
        "aff_country_unique_index": "0;0;0+1",
        "aff_country_unique": "United States;Germany"
    },
    {
        "title": "Multi-armed Bandit Problems with History",
        "site": "https://proceedings.mlr.press/v22/shivaswamy12.html",
        "author": "Pannagadatta Shivaswamy; Thorsten Joachims",
        "abstract": "In this paper we consider the stochastic multi-armed bandit problem. However, unlike in the conventional version of this problem, we do not assume that the algorithm starts from scratch. Many applications offer observations of (some of) the arms even before the algorithm starts.  We propose three novel multi-armed bandit algorithms that can exploit this data. An upper bound on the regret is derived in each case. The results show that a logarithmic amount of historic data  can reduce  regret from logarithmic to constant. The effectiveness of the proposed algorithms  are demonstrated on a large-scale malicious URL detection problem.",
        "bibtex": "@InProceedings{pmlr-v22-shivaswamy12,\n  title = \t {Multi-armed Bandit Problems with History},\n  author = \t {Shivaswamy, Pannagadatta and Joachims, Thorsten},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1046--1054},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/shivaswamy12/shivaswamy12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/shivaswamy12.html},\n  abstract = \t {In this paper we consider the stochastic multi-armed bandit problem. However, unlike in the conventional version of this problem, we do not assume that the algorithm starts from scratch. Many applications offer observations of (some of) the arms even before the algorithm starts.  We propose three novel multi-armed bandit algorithms that can exploit this data. An upper bound on the regret is derived in each case. The results show that a logarithmic amount of historic data  can reduce  regret from logarithmic to constant. The effectiveness of the proposed algorithms  are demonstrated on a large-scale malicious URL detection problem.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/shivaswamy12/shivaswamy12.pdf",
        "supp": "",
        "pdf_size": 590354,
        "gs_citation": 67,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4872272775545546160&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science, Cornell University, Ithaca NY; Department of Computer Science, Cornell University, Ithaca NY",
        "aff_domain": "cs.cornell.edu;cs.cornell.edu",
        "email": "cs.cornell.edu;cs.cornell.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Cornell University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.cornell.edu",
        "aff_unique_abbr": "Cornell",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Ithaca",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Multi-label Subspace Ensemble",
        "site": "https://proceedings.mlr.press/v22/zhou12a.html",
        "author": "Tianyi Zhou; Dacheng Tao",
        "abstract": "A challenging problem of multi-label learning is that both the label space and the model complexity will grow rapidly with the increase in the number of labels, and thus makes the available training samples insufficient for training a proper model. In this paper, we eliminate this problem by learning a mapping of each label in the feature space as a robust subspace, and formulating the prediction as finding the group sparse representation of a given instance on the subspace ensemble. We term this approach as \u201cmulti-label subspace ensemble (MSE)\u201d. In the training stage, the data matrix is decomposed as the sum of several low-rank matrices and a sparse residual via a randomized optimization, where each low-rank part defines a subspace mapped by a label. In the prediction stage, the group sparse representation on the subspace ensemble is estimated by group lasso. Experiments on several benchmark datasets demonstrate the appealing performance of MSE.",
        "bibtex": "@InProceedings{pmlr-v22-zhou12a,\n  title = \t {Multi-label Subspace Ensemble},\n  author = \t {Zhou, Tianyi and Tao, Dacheng},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1444--1452},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/zhou12a/zhou12a.pdf},\n  url = \t {https://proceedings.mlr.press/v22/zhou12a.html},\n  abstract = \t {A challenging problem of multi-label learning is that both the label space and the model complexity will grow rapidly with the increase in the number of labels, and thus makes the available training samples insufficient for training a proper model. In this paper, we eliminate this problem by learning a mapping of each label in the feature space as a robust subspace, and formulating the prediction as finding the group sparse representation of a given instance on the subspace ensemble. We term this approach as \u201cmulti-label subspace ensemble (MSE)\u201d. In the training stage, the data matrix is decomposed as the sum of several low-rank matrices and a sparse residual via a randomized optimization, where each low-rank part defines a subspace mapped by a label. In the prediction stage, the group sparse representation on the subspace ensemble is estimated by group lasso. Experiments on several benchmark datasets demonstrate the appealing performance of MSE.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/zhou12a/zhou12a.pdf",
        "supp": "",
        "pdf_size": 240808,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4596448608903876398&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Centre for Quantum Computation & Intelligent Systems, University of Technology Sydney, Australia; Centre for Quantum Computation & Intelligent Systems, University of Technology Sydney, Australia",
        "aff_domain": "student.uts.edu.au;uts.edu.au",
        "email": "student.uts.edu.au;uts.edu.au",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Technology Sydney",
        "aff_unique_dep": "Centre for Quantum Computation & Intelligent Systems",
        "aff_unique_url": "https://www.uts.edu.au",
        "aff_unique_abbr": "UTS",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Sydney",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "Multiple Texture Boltzmann Machines",
        "site": "https://proceedings.mlr.press/v22/kivinen12.html",
        "author": "Jyri Kivinen; Christopher Williams",
        "abstract": "We assess the generative power of the mPoT-model of [10] with tiled-convolutional weight sharing as a model for visual textures by specifically training on this task, evaluating model performance on texture synthesis and inpainting tasks using quantitative metrics. We also analyze the relative importance of the mean and covariance parts of the mPoT model by comparing its performance to those of its subcomponents, tiled-convolutional versions of the PoT/FoE and Gaussian-Bernoulli restricted Boltzmann machine (GB-RBM). Our results suggest that while state-of-the-art or better performance can be achieved using the mPoT, similar performance can be achieved with the mean-only model. We then develop a model for multiple textures based on the GB-RBM, using a shared set of weights but texture-specific hidden unit biases. We show comparable performance of the multiple texture model to  individually trained texture models.",
        "bibtex": "@InProceedings{pmlr-v22-kivinen12,\n  title = \t {Multiple Texture Boltzmann Machines},\n  author = \t {Kivinen, Jyri and Williams, Christopher},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {638--646},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/kivinen12/kivinen12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/kivinen12.html},\n  abstract = \t {We assess the generative power of the mPoT-model of [10] with tiled-convolutional weight sharing as a model for visual textures by specifically training on this task, evaluating model performance on texture synthesis and inpainting tasks using quantitative metrics. We also analyze the relative importance of the mean and covariance parts of the mPoT model by comparing its performance to those of its subcomponents, tiled-convolutional versions of the PoT/FoE and Gaussian-Bernoulli restricted Boltzmann machine (GB-RBM). Our results suggest that while state-of-the-art or better performance can be achieved using the mPoT, similar performance can be achieved with the mean-only model. We then develop a model for multiple textures based on the GB-RBM, using a shared set of weights but texture-specific hidden unit biases. We show comparable performance of the multiple texture model to  individually trained texture models.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/kivinen12/kivinen12.pdf",
        "supp": "",
        "pdf_size": 1049684,
        "gs_citation": 55,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7264390085835523082&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Institute for Adaptive and Neural Computation, School of Informatics, University of Edinburgh, UK; Institute for Adaptive and Neural Computation, School of Informatics, University of Edinburgh, UK",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Edinburgh",
        "aff_unique_dep": "School of Informatics",
        "aff_unique_url": "https://www.ed.ac.uk",
        "aff_unique_abbr": "Edinburgh",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Multiresolution Deep Belief Networks",
        "site": "https://proceedings.mlr.press/v22/tang12.html",
        "author": "Yichuan Tang; Abdel-Rahman Mohamed",
        "abstract": "Motivated by the observation that coarse and fine resolutions of an image reveal different structures in the underlying visual phenomenon, we present a model based on the Deep Belief Network (DBN) which learns features from the multiscale representation of images. A Laplacian Pyramid is first constructed for each image. DBNs are then trained separately at each level of the pyramid. Finally, a top level RBM combines these DBNs into a single network we call the Multiresolution Deep Belief Network (MrDBN). Experiments show that MrDBNs generalize better than standard DBNs on NORB classification and TIMIT phone recognition. In the domain of generative learning, we demonstrate the superiority of MrDBNs at modeling face images.",
        "bibtex": "@InProceedings{pmlr-v22-tang12,\n  title = \t {Multiresolution Deep Belief Networks},\n  author = \t {Tang, Yichuan and Mohamed, Abdel-Rahman},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1203--1211},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/tang12/tang12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/tang12.html},\n  abstract = \t {Motivated by the observation that coarse and fine resolutions of an image reveal different structures in the underlying visual phenomenon, we present a model based on the Deep Belief Network (DBN) which learns features from the multiscale representation of images. A Laplacian Pyramid is first constructed for each image. DBNs are then trained separately at each level of the pyramid. Finally, a top level RBM combines these DBNs into a single network we call the Multiresolution Deep Belief Network (MrDBN). Experiments show that MrDBNs generalize better than standard DBNs on NORB classification and TIMIT phone recognition. In the domain of generative learning, we demonstrate the superiority of MrDBNs at modeling face images.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/tang12/tang12.pdf",
        "supp": "",
        "pdf_size": 1786358,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14083983438126425477&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Science, University of Toronto; Department of Computer Science, University of Toronto",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Toronto",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utoronto.ca",
        "aff_unique_abbr": "U of T",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Toronto",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "No Internal Regret via Neighborhood Watch",
        "site": "https://proceedings.mlr.press/v22/foster12.html",
        "author": "Dean Foster; Alexander Rakhlin",
        "abstract": "We present an algorithm which attains O(\\sqrtT) internal (and thus external) regret for finite games with partial monitoring under the local observability condition. Recently, this condition has been shown by Bartok, Pal, and Szepesvari (2011) to imply the O(\\sqrtT) rate for partial monitoring games against an i.i.d. opponent, and the authors conjectured that the same holds for non-stochastic adversaries. Our result is in the affirmative, and it completes the characterization of possible rates for finite partial-monitoring games, an open question stated by Cesa-Bianchi, Lugosi, and Stoltz (2006). Our regret guarantees also hold for the more general model of partial monitoring with random signals.",
        "bibtex": "@InProceedings{pmlr-v22-foster12,\n  title = \t {No Internal Regret via Neighborhood Watch},\n  author = \t {Foster, Dean and Rakhlin, Alexander},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {382--390},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/foster12/foster12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/foster12.html},\n  abstract = \t {We present an algorithm which attains O(\\sqrtT) internal (and thus external) regret for finite games with partial monitoring under the local observability condition. Recently, this condition has been shown by Bartok, Pal, and Szepesvari (2011) to imply the O(\\sqrtT) rate for partial monitoring games against an i.i.d. opponent, and the authors conjectured that the same holds for non-stochastic adversaries. Our result is in the affirmative, and it completes the characterization of possible rates for finite partial-monitoring games, an open question stated by Cesa-Bianchi, Lugosi, and Stoltz (2006). Our regret guarantees also hold for the more general model of partial monitoring with random signals.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/foster12/foster12.pdf",
        "supp": "",
        "pdf_size": 1008478,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11315285319756266654&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Statistics, University of Pennsylvania; Department of Statistics, University of Pennsylvania",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Pennsylvania",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "https://www.upenn.edu",
        "aff_unique_abbr": "UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Nonlinear low-dimensional regression using auxiliary coordinates",
        "site": "https://proceedings.mlr.press/v22/wang12.html",
        "author": "Weiran Wang; Miguel Carreira-Perpinan",
        "abstract": "When doing regression with inputs and outputs that are high-dimensional, it often makes sense to reduce the dimensionality of the inputs before mapping to the outputs. Much work in statistics and machine learning, such as reduced-rank regression, slice inverse regression and their variants, has focused on linear dimensionality reduction, or on estimating the dimensionality reduction first and then the mapping. We propose a method where both the dimensionality reduction and the mapping can be nonlinear and are estimated jointly. Our key idea is to define an objective function where the low-dimensional coordinates are free parameters, in addition to the dimensionality reduction and the mapping. This has the effect of decoupling many groups of parameters from each other, affording a far more effective optimization than if using a deep network with nested mappings, and to use a good initialization from slice inverse regression or spectral methods. Our experiments with image and robot applications show our approach to improve over direct regression and various existing approaches.",
        "bibtex": "@InProceedings{pmlr-v22-wang12,\n  title = \t {Nonlinear low-dimensional regression using auxiliary coordinates},\n  author = \t {Wang, Weiran and Carreira-Perpinan, Miguel},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1295--1304},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/wang12/wang12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/wang12.html},\n  abstract = \t {When doing regression with inputs and outputs that are high-dimensional, it often makes sense to reduce the dimensionality of the inputs before mapping to the outputs. Much work in statistics and machine learning, such as reduced-rank regression, slice inverse regression and their variants, has focused on linear dimensionality reduction, or on estimating the dimensionality reduction first and then the mapping. We propose a method where both the dimensionality reduction and the mapping can be nonlinear and are estimated jointly. Our key idea is to define an objective function where the low-dimensional coordinates are free parameters, in addition to the dimensionality reduction and the mapping. This has the effect of decoupling many groups of parameters from each other, affording a far more effective optimization than if using a deep network with nested mappings, and to use a good initialization from slice inverse regression or spectral methods. Our experiments with image and robot applications show our approach to improve over direct regression and various existing approaches.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/wang12/wang12.pdf",
        "supp": "",
        "pdf_size": 1804336,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9986070884161917408&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "EECS, School of Engineering, University of California, Merced; EECS, School of Engineering, University of California, Merced",
        "aff_domain": "; ",
        "email": "; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Merced",
        "aff_unique_dep": "School of Engineering",
        "aff_unique_url": "https://www.ucmerced.edu",
        "aff_unique_abbr": "UC Merced",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Merced",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Nonparametric Estimation of Conditional Information and Divergences",
        "site": "https://proceedings.mlr.press/v22/poczos12.html",
        "author": "Barnabas Poczos; Jeff Schneider",
        "abstract": "In this paper we propose new nonparametric estimators for a family of conditional mutual information and divergences. Our estimators are easy to compute; they only use simple k nearest neighbor based statistics. We prove that the proposed conditional information and divergence estimators are consistent under certain conditions, and demonstrate their consistency and applicability by numerical experiments on simulated and on real data as well.",
        "bibtex": "@InProceedings{pmlr-v22-poczos12,\n  title = \t {Nonparametric Estimation of Conditional Information and Divergences},\n  author = \t {Poczos, Barnabas and Schneider, Jeff},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {914--923},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/poczos12/poczos12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/poczos12.html},\n  abstract = \t {In this paper we propose new nonparametric estimators for a family of conditional mutual information and divergences. Our estimators are easy to compute; they only use simple k nearest neighbor based statistics. We prove that the proposed conditional information and divergence estimators are consistent under certain conditions, and demonstrate their consistency and applicability by numerical experiments on simulated and on real data as well.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/poczos12/poczos12.pdf",
        "supp": "",
        "pdf_size": 403367,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11307344121681911540&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA, 15213; School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA, 15213",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "On Average Reward Policy Evaluation in Infinite-State Partially Observable Systems",
        "site": "https://proceedings.mlr.press/v22/grinberg12.html",
        "author": "Yuri Grinberg; Doina Precup",
        "abstract": "We investigate the problem of estimating the average reward of given decision policies in discrete-time controllable dynamical systems with  finite action and observation sets, but possibly infinite state space.   Unlike in systems with finite state spaces, in infinite\u2013state systems the expected reward for some policies might not exist, so policy evaluation, which is a key step in optimal control methods, might fail.  Our main analysis tool is Ergodic theory, which allows learning potentially useful quantities from the system without building a model. Our main contribution is three-fold. First, we present several dynamical systems that demonstrate the difficulty of learning in the general case, without making additional assumptions. We state the necessary condition that the underlying system must satisfy to be amenable for learning.  Second, we discuss the relationship between this condition and state-of-the-art predictive representations, and we show that there are systems that satisfy the above condition but cannot be modeled by such representations. Third, we establish sufficient conditions for average-reward policy evaluation in this setting.",
        "bibtex": "@InProceedings{pmlr-v22-grinberg12,\n  title = \t {On Average Reward Policy Evaluation in Infinite-State Partially Observable Systems},\n  author = \t {Grinberg, Yuri and Precup, Doina},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {449--457},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/grinberg12/grinberg12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/grinberg12.html},\n  abstract = \t {We investigate the problem of estimating the average reward of given decision policies in discrete-time controllable dynamical systems with  finite action and observation sets, but possibly infinite state space.   Unlike in systems with finite state spaces, in infinite\u2013state systems the expected reward for some policies might not exist, so policy evaluation, which is a key step in optimal control methods, might fail.  Our main analysis tool is Ergodic theory, which allows learning potentially useful quantities from the system without building a model. Our main contribution is three-fold. First, we present several dynamical systems that demonstrate the difficulty of learning in the general case, without making additional assumptions. We state the necessary condition that the underlying system must satisfy to be amenable for learning.  Second, we discuss the relationship between this condition and state-of-the-art predictive representations, and we show that there are systems that satisfy the above condition but cannot be modeled by such representations. Third, we establish sufficient conditions for average-reward policy evaluation in this setting.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/grinberg12/grinberg12.pdf",
        "supp": "",
        "pdf_size": 568020,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18092761770489252548&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "School of Computer Science, McGill University; School of Computer Science, McGill University",
        "aff_domain": "cs.mcgill.ca;cs.mcgill.ca",
        "email": "cs.mcgill.ca;cs.mcgill.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "McGill University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.mcgill.ca",
        "aff_unique_abbr": "McGill",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Montreal",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "On Bayesian Upper Confidence Bounds for Bandit Problems",
        "site": "https://proceedings.mlr.press/v22/kaufmann12.html",
        "author": "Emilie Kaufmann; Olivier Cappe; Aurelien Garivier",
        "abstract": "Stochastic bandit problems have been analyzed from two different perspectives: a frequentist view, where the parameter is a deterministic unknown quantity, and a Bayesian approach, where the parameter is drawn from a prior distribution.  We show in this paper that methods derived from this second perspective prove optimal when evaluated using the frequentist cumulated regret as a measure of performance. We give a general formulation for a class of Bayesian index policies that rely on quantiles of the posterior distribution. For binary bandits, we prove that the corresponding algorithm, termed Bayes-UCB, satisfies finite-time regret bounds that imply its asymptotic optimality.  More generally, Bayes-UCB appears as an unifying framework for several variants of the UCB algorithm addressing different bandit problems (parametric multi-armed bandits, Gaussian bandits with unknown mean and variance, linear bandits). But the generality of the Bayesian approach makes it possible to address more challenging models. In particular, we show how to handle linear bandits with sparsity constraints by resorting to Gibbs sampling.",
        "bibtex": "@InProceedings{pmlr-v22-kaufmann12,\n  title = \t {On Bayesian Upper Confidence Bounds for Bandit Problems},\n  author = \t {Kaufmann, Emilie and Cappe, Olivier and Garivier, Aurelien},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {592--600},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/kaufmann12/kaufmann12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/kaufmann12.html},\n  abstract = \t {Stochastic bandit problems have been analyzed from two different perspectives: a frequentist view, where the parameter is a deterministic unknown quantity, and a Bayesian approach, where the parameter is drawn from a prior distribution.  We show in this paper that methods derived from this second perspective prove optimal when evaluated using the frequentist cumulated regret as a measure of performance. We give a general formulation for a class of Bayesian index policies that rely on quantiles of the posterior distribution. For binary bandits, we prove that the corresponding algorithm, termed Bayes-UCB, satisfies finite-time regret bounds that imply its asymptotic optimality.  More generally, Bayes-UCB appears as an unifying framework for several variants of the UCB algorithm addressing different bandit problems (parametric multi-armed bandits, Gaussian bandits with unknown mean and variance, linear bandits). But the generality of the Bayesian approach makes it possible to address more challenging models. In particular, we show how to handle linear bandits with sparsity constraints by resorting to Gibbs sampling.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/kaufmann12/kaufmann12.pdf",
        "supp": "",
        "pdf_size": 821571,
        "gs_citation": 493,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13106303343070323644&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "On Bisubmodular Maximization",
        "site": "https://proceedings.mlr.press/v22/singh12.html",
        "author": "Ajit Singh; Andrew Guillory; Jeff Bilmes",
        "abstract": "Bisubmodularity extends the concept of submodularity to set functions with two arguments. We show how bisubmodular maximization leads to richer value-of-information problems, using examples in sensor placement and feature selection. We present the first constant-factor approximation algorithm for a wide class of bisubmodular maximizations.",
        "bibtex": "@InProceedings{pmlr-v22-singh12,\n  title = \t {On Bisubmodular Maximization},\n  author = \t {Singh, Ajit and Guillory, Andrew and Bilmes, Jeff},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1055--1063},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/singh12/singh12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/singh12.html},\n  abstract = \t {Bisubmodularity extends the concept of submodularity to set functions with two arguments. We show how bisubmodular maximization leads to richer value-of-information problems, using examples in sensor placement and feature selection. We present the first constant-factor approximation algorithm for a wide class of bisubmodular maximizations.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/singh12/singh12.pdf",
        "supp": "",
        "pdf_size": 544928,
        "gs_citation": 78,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17437957878017477195&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "University of Washington, Seattle; University of Washington, Seattle; University of Washington, Seattle",
        "aff_domain": "ee.washington.edu;cs.washington.edu;ee.washington.edu",
        "email": "ee.washington.edu;cs.washington.edu;ee.washington.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Washington",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.washington.edu",
        "aff_unique_abbr": "UW",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Seattle",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "On Estimation and Selection for Topic Models",
        "site": "https://proceedings.mlr.press/v22/taddy12.html",
        "author": "Matt Taddy",
        "abstract": "This article describes posterior maximization for topic models, identifying computational and   conceptual gains from inference under a non-standard    parametrization.  We then show that fitted parameters can be used  as the basis for a novel approach to marginal likelihood estimation,   via block-diagonal approximation to the information matrix, that facilitates choosing the number of latent topics.  This   likelihood-based model selection is complemented with a goodness-of-fit analysis built around estimated residual dispersion.  Examples are provided to illustrate model selection as well as to compare our estimation against standard alternative techniques.",
        "bibtex": "@InProceedings{pmlr-v22-taddy12,\n  title = \t {On Estimation and Selection for Topic Models},\n  author = \t {Taddy, Matt},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1184--1193},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/taddy12/taddy12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/taddy12.html},\n  abstract = \t {This article describes posterior maximization for topic models, identifying computational and   conceptual gains from inference under a non-standard    parametrization.  We then show that fitted parameters can be used  as the basis for a novel approach to marginal likelihood estimation,   via block-diagonal approximation to the information matrix, that facilitates choosing the number of latent topics.  This   likelihood-based model selection is complemented with a goodness-of-fit analysis built around estimated residual dispersion.  Examples are provided to illustrate model selection as well as to compare our estimation against standard alternative techniques.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/taddy12/taddy12.pdf",
        "supp": "",
        "pdf_size": 1197036,
        "gs_citation": 386,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5717370054086326765&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Booth School of Business, University of Chicago",
        "aff_domain": "chicagobooth.edu",
        "email": "chicagobooth.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Chicago",
        "aff_unique_dep": "Booth School of Business",
        "aff_unique_url": "https://www.chicagobooth.edu",
        "aff_unique_abbr": "Chicago Booth",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Chicago",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "title": "On Nonparametric Guidance for Learning Autoencoder Representations",
        "site": "https://proceedings.mlr.press/v22/snoek12.html",
        "author": "Jasper Snoek; Ryan Adams; Hugo Larochelle",
        "abstract": "Unsupervised discovery of latent representations, in addition to being useful for density modeling, visualisation and exploratory data analysis, is also increasingly important for learning features relevant to discriminative tasks. Autoencoders, in particular, have proven to be an effective way to learn latent codes that reflect meaningful variations in data. A continuing challenge, however, is guiding an autoencoder toward representations that are useful for particular tasks. A complementary challenge is to find codes that are invariant to irrelevant transformations of the data. The most common way of introducing such problem-specific guidance in autoencoders has been through the incorporation of a parametric component that ties the latent representation to the label information. In this work, we argue that a preferable approach relies instead on a nonparametric guidance mechanism. Conceptually, it ensures that there exists a function that can predict the label information, without explicitly instantiating that function. The superiority of this guidance mechanism is confirmed on two datasets. In particular, this approach is able to incorporate invariance information (lighting, elevation, etc.) from the small NORB object recognition dataset and yields state-of-the-art performance for a single layer, non-convolutional network.",
        "bibtex": "@InProceedings{pmlr-v22-snoek12,\n  title = \t {On Nonparametric Guidance for Learning Autoencoder Representations},\n  author = \t {Snoek, Jasper and Adams, Ryan and Larochelle, Hugo},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1073--1080},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/snoek12/snoek12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/snoek12.html},\n  abstract = \t {Unsupervised discovery of latent representations, in addition to being useful for density modeling, visualisation and exploratory data analysis, is also increasingly important for learning features relevant to discriminative tasks. Autoencoders, in particular, have proven to be an effective way to learn latent codes that reflect meaningful variations in data. A continuing challenge, however, is guiding an autoencoder toward representations that are useful for particular tasks. A complementary challenge is to find codes that are invariant to irrelevant transformations of the data. The most common way of introducing such problem-specific guidance in autoencoders has been through the incorporation of a parametric component that ties the latent representation to the label information. In this work, we argue that a preferable approach relies instead on a nonparametric guidance mechanism. Conceptually, it ensures that there exists a function that can predict the label information, without explicitly instantiating that function. The superiority of this guidance mechanism is confirmed on two datasets. In particular, this approach is able to incorporate invariance information (lighting, elevation, etc.) from the small NORB object recognition dataset and yields state-of-the-art performance for a single layer, non-convolutional network.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/snoek12/snoek12.pdf",
        "supp": "",
        "pdf_size": 802514,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13631554480570808951&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "On Sparse, Spectral and Other Parameterizations of Binary Probabilistic Models",
        "site": "https://proceedings.mlr.press/v22/buchman12.html",
        "author": "David Buchman; Mark Schmidt; Shakir Mohamed; David Poole; Nando De Freitas",
        "abstract": "This paper studies issues relating to the parameterization of probability distributions over binary data sets.  Several such parameterizations of models for binary data are known, including the Ising, generalized Ising, canonical and full parameterizations.  We also discuss a parameterization that we call the \u201cspectral parameterization\u201d, which has received significantly less coverage in existing literature.  We provide this parameterization with a spectral interpretation by casting log-linear models in terms of orthogonal Walsh-Hadamard harmonic expansions.  Using various standard and group sparse regularizers for structural learning, we provide a comprehensive theoretical and empirical comparison of these parameterizations.  We show that the spectral parameterization, along with the canonical, has the best performance and sparsity levels, while the spectral does not depend on any particular reference state.  The spectral interpretation also provides a new starting point for analyzing the statistics of binary data sets; we measure the magnitude of higher order interactions in the underlying distributions for several data sets.",
        "bibtex": "@InProceedings{pmlr-v22-buchman12,\n  title = \t {On Sparse, Spectral and Other Parameterizations of Binary Probabilistic Models},\n  author = \t {Buchman, David and Schmidt, Mark and Mohamed, Shakir and Poole, David and Freitas, Nando De},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {173--181},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/buchman12/buchman12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/buchman12.html},\n  abstract = \t {This paper studies issues relating to the parameterization of probability distributions over binary data sets.  Several such parameterizations of models for binary data are known, including the Ising, generalized Ising, canonical and full parameterizations.  We also discuss a parameterization that we call the \u201cspectral parameterization\u201d, which has received significantly less coverage in existing literature.  We provide this parameterization with a spectral interpretation by casting log-linear models in terms of orthogonal Walsh-Hadamard harmonic expansions.  Using various standard and group sparse regularizers for structural learning, we provide a comprehensive theoretical and empirical comparison of these parameterizations.  We show that the spectral parameterization, along with the canonical, has the best performance and sparsity levels, while the spectral does not depend on any particular reference state.  The spectral interpretation also provides a new starting point for analyzing the statistics of binary data sets; we measure the magnitude of higher order interactions in the underlying distributions for several data sets.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/buchman12/buchman12.pdf",
        "supp": "",
        "pdf_size": 378406,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2283911448472849539&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "Department of Computer Science, University of British Columbia, Vancouver, B.C., Canada+INRIA \u2013 SIERRA Team, Laboratoire d\u2019Informatique de l\u2019 \u00b4Ecole Normale Sup\u00b4 erieure, Paris, France; INRIA \u2013 SIERRA Team, Laboratoire d\u2019Informatique de l\u2019 \u00b4Ecole Normale Sup\u00b4 erieure, Paris, France; Department of Computer Science, University of British Columbia, Vancouver, B.C., Canada; Department of Computer Science, University of British Columbia, Vancouver, B.C., Canada; Department of Computer Science, University of British Columbia, Vancouver, B.C., Canada",
        "aff_domain": "cs.ubc.ca;inria.fr;cs.ubc.ca;cs.ubc.ca;cs.ubc.ca",
        "email": "cs.ubc.ca;inria.fr;cs.ubc.ca;cs.ubc.ca;cs.ubc.ca",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1;0;0;0",
        "aff_unique_norm": "University of British Columbia;INRIA",
        "aff_unique_dep": "Department of Computer Science;SIERRA Team",
        "aff_unique_url": "https://www.ubc.ca;https://www.inria.fr",
        "aff_unique_abbr": "UBC;INRIA",
        "aff_campus_unique_index": "0+1;1;0;0;0",
        "aff_campus_unique": "Vancouver;Paris",
        "aff_country_unique_index": "0+1;1;0;0;0",
        "aff_country_unique": "Canada;France"
    },
    {
        "title": "On a Connection between Maximum Variance Unfolding, Shortest Path Problems and IsoMap",
        "site": "https://proceedings.mlr.press/v22/paprotny12.html",
        "author": "Alexander Paprotny; Jochen Garcke",
        "abstract": "We present an equivalent formulation of the Maximum Variance Unfolding (MVU) problem in terms of distance matrices. This yields a novel interpretation of the MVU problem as a regularized version of the shortest path problem on a graph. This interpretation enables us to establish an asymptotic convergence result for the case that the underlying data are drawn from a Riemannian manifold which is isometric to a convex subset of Euclidean space.",
        "bibtex": "@InProceedings{pmlr-v22-paprotny12,\n  title = \t {On a Connection between Maximum Variance Unfolding, Shortest Path Problems and IsoMap},\n  author = \t {Paprotny, Alexander and Garcke, Jochen},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {859--867},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/paprotny12/paprotny12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/paprotny12.html},\n  abstract = \t {We present an equivalent formulation of the Maximum Variance Unfolding (MVU) problem in terms of distance matrices. This yields a novel interpretation of the MVU problem as a regularized version of the shortest path problem on a graph. This interpretation enables us to establish an asymptotic convergence result for the case that the underlying data are drawn from a Riemannian manifold which is isometric to a convex subset of Euclidean space.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/paprotny12/paprotny12.pdf",
        "supp": "",
        "pdf_size": 355318,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9934241868288970184&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Inst. f\u00a8 ur Mathematik, TU Berlin; Inst. f. Numerische Simulation, Univ. Bonn + Fraunhofer SCAI",
        "aff_domain": "math.tu-berlin.de;ins.uni-bonn.de",
        "email": "math.tu-berlin.de;ins.uni-bonn.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+2",
        "aff_unique_norm": "Technische Universit\u00e4t Berlin;University of Bonn;Fraunhofer Society",
        "aff_unique_dep": "Institut f\u00fcr Mathematik;Institute for Numerical Simulation;SCAI (Supercomputing and Data Analysis)",
        "aff_unique_url": "https://www.tu-berlin.de;https://www.uni-bonn.de;https://www.scai.fraunhofer.de",
        "aff_unique_abbr": "TU Berlin;Uni Bonn;Fraunhofer SCAI",
        "aff_campus_unique_index": "0;",
        "aff_campus_unique": "Berlin;",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Online Clustering of Processes",
        "site": "https://proceedings.mlr.press/v22/khaleghi12.html",
        "author": "Azadeh Khaleghi; Daniil Ryabko; Jeremie Mary; Philippe Preux",
        "abstract": "The problem of online clustering is considered in the case where each data point is a sequence generated by a stationary ergodic process. Data arrive in an online fashion so that the sample received at every time-step is either a continuation of some previously received sequence or a new sequence. The dependence between the sequences can be arbitrary. No parametric or independence assumptions are made; the only assumption is that the marginal distribution of each sequence is stationary and ergodic. A novel, computationally efficient algorithm is proposed and is shown to be asymptotically consistent (under a natural notion of consistency). The performance of the proposed algorithm is evaluated on simulated data,  as well as on real datasets (motion classification).",
        "bibtex": "@InProceedings{pmlr-v22-khaleghi12,\n  title = \t {Online Clustering of Processes},\n  author = \t {Khaleghi, Azadeh and Ryabko, Daniil and Mary, Jeremie and Preux, Philippe},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {601--609},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/khaleghi12/khaleghi12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/khaleghi12.html},\n  abstract = \t {The problem of online clustering is considered in the case where each data point is a sequence generated by a stationary ergodic process. Data arrive in an online fashion so that the sample received at every time-step is either a continuation of some previously received sequence or a new sequence. The dependence between the sequences can be arbitrary. No parametric or independence assumptions are made; the only assumption is that the marginal distribution of each sequence is stationary and ergodic. A novel, computationally efficient algorithm is proposed and is shown to be asymptotically consistent (under a natural notion of consistency). The performance of the proposed algorithm is evaluated on simulated data,  as well as on real datasets (motion classification).}\n}",
        "pdf": "http://proceedings.mlr.press/v22/khaleghi12/khaleghi12.pdf",
        "supp": "",
        "pdf_size": 452342,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13616475565064146387&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "SequeL-INRIA/LIFL-CNRS, Universit\u00b4e de Lille, France; SequeL-INRIA/LIFL-CNRS, Universit\u00b4e de Lille, France; SequeL-INRIA/LIFL-CNRS, Universit\u00b4e de Lille, France; SequeL-INRIA/LIFL-CNRS, Universit\u00b4e de Lille, France",
        "aff_domain": "inria.fr;inria.fr;inria.fr;inria.fr",
        "email": "inria.fr;inria.fr;inria.fr;inria.fr",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Universit\u00e9 de Lille",
        "aff_unique_dep": "SequeL-INRIA/LIFL-CNRS",
        "aff_unique_url": "https://www.univ-lille.fr",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "France"
    },
    {
        "title": "Online Clustering with Experts",
        "site": "https://proceedings.mlr.press/v22/choromanska12.html",
        "author": "Anna Choromanska; Claire Monteleoni",
        "abstract": "Approximating the k-means clustering objective with an online learning algorithm is an open problem. We introduce a family of online clustering algorithms by extending algorithms for online supervised learning, with access to expert predictors, to the unsupervised learning setting. Instead of computing prediction errors in order to re-weight the experts, the algorithms compute an approximation to the current value of the k-means objective obtained by each expert.  When the experts are batch clustering algorithms with b-approximation guarantees with respect to the k-means objective (for example, the k-means++ or k-means# algorithms), applied to a sliding window of the data stream, our algorithms obtain approximation guarantees with respect to the k-means objective. The form of these online clustering approximation guarantees is novel, and extends an evaluation framework proposed by Dasgupta as an analog to regret. Notably, our approximation bounds are with respect to the optimal k-means cost on the entire data stream seen so far, even though the algorithm is online. Our algorithm\u2019s empirical performance tracks that of the best clustering algorithm in its expert set.",
        "bibtex": "@InProceedings{pmlr-v22-choromanska12,\n  title = \t {Online Clustering with Experts},\n  author = \t {Choromanska, Anna and Monteleoni, Claire},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {227--235},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/choromanska12/choromanska12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/choromanska12.html},\n  abstract = \t {Approximating the k-means clustering objective with an online learning algorithm is an open problem. We introduce a family of online clustering algorithms by extending algorithms for online supervised learning, with access to expert predictors, to the unsupervised learning setting. Instead of computing prediction errors in order to re-weight the experts, the algorithms compute an approximation to the current value of the k-means objective obtained by each expert.  When the experts are batch clustering algorithms with b-approximation guarantees with respect to the k-means objective (for example, the k-means++ or k-means# algorithms), applied to a sliding window of the data stream, our algorithms obtain approximation guarantees with respect to the k-means objective. The form of these online clustering approximation guarantees is novel, and extends an evaluation framework proposed by Dasgupta as an analog to regret. Notably, our approximation bounds are with respect to the optimal k-means cost on the entire data stream seen so far, even though the algorithm is online. Our algorithm\u2019s empirical performance tracks that of the best clustering algorithm in its expert set.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/choromanska12/choromanska12.pdf",
        "supp": "",
        "pdf_size": 1044872,
        "gs_citation": 64,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2257311651133442331&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Columbia University; George Washington University",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Columbia University;George Washington University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.columbia.edu;https://www.gwu.edu",
        "aff_unique_abbr": "Columbia;GWU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Online Incremental Feature Learning with Denoising Autoencoders",
        "site": "https://proceedings.mlr.press/v22/zhou12b.html",
        "author": "Guanyu Zhou; Kihyuk Sohn; Honglak Lee",
        "abstract": "While determining model complexity is an important problem in machine learning, many feature learning algorithms rely on cross-validation to choose an optimal number of features, which is usually infeasible for online learning from a massive stream of data. In this paper, we propose an incremental feature learning algorithm to determine the optimal model complexity for large-scale, online datasets based on the denoising autoencoder. This algorithm is composed of two processes: adding features and merging features. Specifically, it adds new features to minimize the objective function\u2019s residual and merges similar features to obtain a compact feature representation and prevent over-fitting. Our experiments show that the model quickly converges to the optimal number of features in a large-scale online setting, and outperforms the (non-incremental) denoising autoencoder, as well as deep belief networks and stacked denoising autoencoders for classification tasks. Further, the algorithm is particularly effective in recognizing new patterns when the data distribution changes over time in the massive online data stream.",
        "bibtex": "@InProceedings{pmlr-v22-zhou12b,\n  title = \t {Online Incremental Feature Learning with Denoising Autoencoders},\n  author = \t {Zhou, Guanyu and Sohn, Kihyuk and Lee, Honglak},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1453--1461},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/zhou12b/zhou12b.pdf},\n  url = \t {https://proceedings.mlr.press/v22/zhou12b.html},\n  abstract = \t {While determining model complexity is an important problem in machine learning, many feature learning algorithms rely on cross-validation to choose an optimal number of features, which is usually infeasible for online learning from a massive stream of data. In this paper, we propose an incremental feature learning algorithm to determine the optimal model complexity for large-scale, online datasets based on the denoising autoencoder. This algorithm is composed of two processes: adding features and merging features. Specifically, it adds new features to minimize the objective function\u2019s residual and merges similar features to obtain a compact feature representation and prevent over-fitting. Our experiments show that the model quickly converges to the optimal number of features in a large-scale online setting, and outperforms the (non-incremental) denoising autoencoder, as well as deep belief networks and stacked denoising autoencoders for classification tasks. Further, the algorithm is particularly effective in recognizing new patterns when the data distribution changes over time in the massive online data stream.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/zhou12b/zhou12b.pdf",
        "supp": "",
        "pdf_size": 559942,
        "gs_citation": 288,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17459170691275193546&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Department of EECS, University of Michigan, Ann Arbor, MI 48109; Department of EECS, University of Michigan, Ann Arbor, MI 48109; Department of EECS, University of Michigan, Ann Arbor, MI 48109",
        "aff_domain": "umich.edu;umich.edu;eecs.umich.edu",
        "email": "umich.edu;umich.edu;eecs.umich.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "Department of EECS",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Ann Arbor",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Online-to-Confidence-Set Conversions and Application to Sparse Stochastic Bandits",
        "site": "https://proceedings.mlr.press/v22/abbasi-yadkori12.html",
        "author": "Yasin Abbasi-Yadkori; David Pal; Csaba Szepesvari",
        "abstract": "We introduce a novel technique, which we call  online-to-confidence-set conversion. The technique allows us to construct high-probability confidence sets for linear prediction  with correlated inputs given the predictions of any algorithm   (e.g., online LASSO, exponentiated gradient algorithm,   online least-squares, p-norm algorithm)   targeting online learning  with linear predictors and the quadratic loss. By construction, the size of the confidence set is   directly governed by the regret of the online learning algorithm. Constructing tight confidence sets is interesting on its own, but  the new technique is given extra weight by the fact   having access tight confidence sets underlies a number of    important problems. The advantage of our construction here is that  progress in constructing better algorithms for online prediction problems   directly translates into tighter confidence sets. In this paper, this is demonstrated in the case of linear stochastic bandits. In particular, we introduce the sparse variant of linear stochastic  bandits and show that a recent online algorithm together with our  online-to-confidence-set conversion allows one to derive  algorithms that can exploit if the reward is a function of a sparse   linear combination of the components of the chosen action.",
        "bibtex": "@InProceedings{pmlr-v22-abbasi-yadkori12,\n  title = \t {Online-to-Confidence-Set Conversions and Application to Sparse Stochastic Bandits},\n  author = \t {Abbasi-Yadkori, Yasin and Pal, David and Szepesvari, Csaba},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1--9},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/abbasi-yadkori12/abbasi-yadkori12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/abbasi-yadkori12.html},\n  abstract = \t {We introduce a novel technique, which we call  online-to-confidence-set conversion. The technique allows us to construct high-probability confidence sets for linear prediction  with correlated inputs given the predictions of any algorithm   (e.g., online LASSO, exponentiated gradient algorithm,   online least-squares, p-norm algorithm)   targeting online learning  with linear predictors and the quadratic loss. By construction, the size of the confidence set is   directly governed by the regret of the online learning algorithm. Constructing tight confidence sets is interesting on its own, but  the new technique is given extra weight by the fact   having access tight confidence sets underlies a number of    important problems. The advantage of our construction here is that  progress in constructing better algorithms for online prediction problems   directly translates into tighter confidence sets. In this paper, this is demonstrated in the case of linear stochastic bandits. In particular, we introduce the sparse variant of linear stochastic  bandits and show that a recent online algorithm together with our  online-to-confidence-set conversion allows one to derive  algorithms that can exploit if the reward is a function of a sparse   linear combination of the components of the chosen action.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/abbasi-yadkori12/abbasi-yadkori12.pdf",
        "supp": "",
        "pdf_size": 364501,
        "gs_citation": 203,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17333892392421166652&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff": "Dept. of Computing Science, University of Alberta; Google, Inc., New York, NY; Dept. of Computing Science, University of Alberta",
        "aff_domain": "cs.ualberta.ca;google.com;cs.ualberta.ca",
        "email": "cs.ualberta.ca;google.com;cs.ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Alberta;Google, Inc.",
        "aff_unique_dep": "Dept. of Computing Science;",
        "aff_unique_url": "https://www.ualberta.ca;https://www.google.com",
        "aff_unique_abbr": "UAlberta;Google",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";New York",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Canada;United States"
    },
    {
        "title": "Optimistic planning for Markov decision processes",
        "site": "https://proceedings.mlr.press/v22/busoniu12.html",
        "author": "Lucian Busoniu; Remi Munos",
        "abstract": "The reinforcement learning community has recently intensified its interest in online planning methods, due to their relative independence on the state space size. However, tight near-optimality guarantees are not yet available for the general case of stochastic Markov decision processes and closed-loop, state-dependent planning policies. We therefore consider an algorithm related to AO* that optimistically explores a tree representation of the space of closed-loop policies, and we analyze the near-optimality of the action it returns after n tree node expansions. While this optimistic planning requires a finite number of actions and possible next states for each transition, its asymptotic performance does not depend directly on these numbers, but only on the subset of nodes that significantly impact near-optimal policies. We characterize this set by introducing a novel measure of problem complexity, called the near-optimality exponent. Specializing the exponent and performance bound for some interesting classes of MDPs illustrates the algorithm works better when there are fewer near-optimal policies and less uniform transition probabilities.",
        "bibtex": "@InProceedings{pmlr-v22-busoniu12,\n  title = \t {Optimistic planning for Markov decision processes},\n  author = \t {Busoniu, Lucian and Munos, Remi},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {182--189},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/busoniu12/busoniu12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/busoniu12.html},\n  abstract = \t {The reinforcement learning community has recently intensified its interest in online planning methods, due to their relative independence on the state space size. However, tight near-optimality guarantees are not yet available for the general case of stochastic Markov decision processes and closed-loop, state-dependent planning policies. We therefore consider an algorithm related to AO* that optimistically explores a tree representation of the space of closed-loop policies, and we analyze the near-optimality of the action it returns after n tree node expansions. While this optimistic planning requires a finite number of actions and possible next states for each transition, its asymptotic performance does not depend directly on these numbers, but only on the subset of nodes that significantly impact near-optimal policies. We characterize this set by introducing a novel measure of problem complexity, called the near-optimality exponent. Specializing the exponent and performance bound for some interesting classes of MDPs illustrates the algorithm works better when there are fewer near-optimal policies and less uniform transition probabilities.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/busoniu12/busoniu12.pdf",
        "supp": "",
        "pdf_size": 411048,
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7156620799682030475&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff": "INRIA Lille \u2013 Nord Europe; INRIA Lille \u2013 Nord Europe",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "INRIA",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.inria.fr",
        "aff_unique_abbr": "INRIA",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Lille",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "France"
    },
    {
        "title": "Part & Clamp: Efficient Structured Output Learning",
        "site": "https://proceedings.mlr.press/v22/pletscher12a.html",
        "author": "Patrick Pletscher; Cheng Soon Ong",
        "abstract": "Discriminative training for general graphical models is a challenging task, due to the intractability of the partition function. We propose a computationally efficient approach to estimate the partition sum in a structured learning problem. The key idea is a lower bound of the partition sum that can be evaluated in a fixed number of message passing iterations. The bound makes use of a subset of the variables, a feedback vertex set, which allows us to decompose the graph into tractable parts. Furthermore, a tightening strategy for the bound is presented, which finds the states of the feedback vertex set that maximally increase the bound, and clamps them. Based on this lower bound we derive batch and online learning algorithms and demonstrate their effectiveness on a computer vision problem.",
        "bibtex": "@InProceedings{pmlr-v22-pletscher12a,\n  title = \t {Part & Clamp: Efficient Structured Output Learning},\n  author = \t {Pletscher, Patrick and Ong, Cheng Soon},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {877--885},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/pletscher12a/pletscher12a.pdf},\n  url = \t {https://proceedings.mlr.press/v22/pletscher12a.html},\n  abstract = \t {Discriminative training for general graphical models is a challenging task, due to the intractability of the partition function. We propose a computationally efficient approach to estimate the partition sum in a structured learning problem. The key idea is a lower bound of the partition sum that can be evaluated in a fixed number of message passing iterations. The bound makes use of a subset of the variables, a feedback vertex set, which allows us to decompose the graph into tractable parts. Furthermore, a tightening strategy for the bound is presented, which finds the states of the feedback vertex set that maximally increase the bound, and clamps them. Based on this lower bound we derive batch and online learning algorithms and demonstrate their effectiveness on a computer vision problem.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/pletscher12a/pletscher12a.pdf",
        "supp": "",
        "pdf_size": 397929,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14602405609135691704&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "ETH Zurich; NICTA",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "ETH Zurich;National Information and Communications Technology Australia",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ethz.ch;https://www.nicta.com.au",
        "aff_unique_abbr": "ETHZ;NICTA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Switzerland;Australia"
    },
    {
        "title": "Perturbation based Large Margin Approach for Ranking",
        "site": "https://proceedings.mlr.press/v22/yang12.html",
        "author": "Eunho Yang; Ambuj Tewari; Pradeep Ravikumar",
        "abstract": "The use of the standard hinge loss for structured outputs, for the learning to rank problem, faces two main caveats: (a) the label space, the set of all possible permutations of items to be ranked, is too large, and also less amenable to the usual dynamic-programming based techniques used for structured outputs, and (b) the supervision or training data consists of instances with multiple labels per input, instead of just a single label. The most natural way to deal with such multiple labels leads, unfortunately, to a non-convex surrogate. In this paper, we propose a general class of perturbation-based surrogates that leverage the large margin approach, and are convex. We show that the standard hinge surrogate for classification actually falls within this class. We also find a surrogate within this class, for the ranking problem, that does not suffer from the caveats mentioned above. Indeed, our experiments demonstrate that it performs better than other candidate large margin proposals on both synthetic and real world ranking datasets.",
        "bibtex": "@InProceedings{pmlr-v22-yang12,\n  title = \t {Perturbation based Large Margin Approach for Ranking},\n  author = \t {Yang, Eunho and Tewari, Ambuj and Ravikumar, Pradeep},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1358--1366},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/yang12/yang12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/yang12.html},\n  abstract = \t {The use of the standard hinge loss for structured outputs, for the learning to rank problem, faces two main caveats: (a) the label space, the set of all possible permutations of items to be ranked, is too large, and also less amenable to the usual dynamic-programming based techniques used for structured outputs, and (b) the supervision or training data consists of instances with multiple labels per input, instead of just a single label. The most natural way to deal with such multiple labels leads, unfortunately, to a non-convex surrogate. In this paper, we propose a general class of perturbation-based surrogates that leverage the large margin approach, and are convex. We show that the standard hinge surrogate for classification actually falls within this class. We also find a surrogate within this class, for the ranking problem, that does not suffer from the caveats mentioned above. Indeed, our experiments demonstrate that it performs better than other candidate large margin proposals on both synthetic and real world ranking datasets.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/yang12/yang12.pdf",
        "supp": "",
        "pdf_size": 393185,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=59314698640675824&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 13,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Preface",
        "site": "https://proceedings.mlr.press/v22/lawrence12.html",
        "author": "Neil D. Lawrence; Mark Girolami",
        "abstract": "Preface to the Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics April 21-23, 2012 La Palma, Canary Islands.",
        "bibtex": "@InProceedings{pmlr-v22-lawrence12,\n  title = \t {Preface},\n  author = \t {Lawrence, Neil D. and Girolami, Mark},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {i--v},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/lawrence12/lawrence12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/lawrence12.html},\n  abstract = \t {Preface to the Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics April 21-23, 2012 La Palma, Canary Islands.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/lawrence12/lawrence12.pdf",
        "supp": "",
        "pdf_size": 65129,
        "gs_citation": 96,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14141043608347785016&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Primal-Dual methods for sparse constrained matrix completion",
        "site": "https://proceedings.mlr.press/v22/xin12.html",
        "author": "Yu Xin; Tommi Jaakkola",
        "abstract": "We develop scalable algorithms for regular and non-negative matrix completion. In particular, we base the methods on trace-norm regularization that induces a low rank predicted matrix. The regularization problem is solved via a constraint generation method that explicitly maintains a sparse dual and the corresponding low rank primal solution. We provide a new dual block coordinate descent algorithm for solving the dual problem with a few spectral constraints. Empirical results illustrate the effectiveness of our method in comparison to recently proposed alternatives.",
        "bibtex": "@InProceedings{pmlr-v22-xin12,\n  title = \t {Primal-Dual methods for sparse constrained matrix completion},\n  author = \t {Xin, Yu and Jaakkola, Tommi},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1323--1331},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/xin12/xin12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/xin12.html},\n  abstract = \t {We develop scalable algorithms for regular and non-negative matrix completion. In particular, we base the methods on trace-norm regularization that induces a low rank predicted matrix. The regularization problem is solved via a constraint generation method that explicitly maintains a sparse dual and the corresponding low rank primal solution. We provide a new dual block coordinate descent algorithm for solving the dual problem with a few spectral constraints. Empirical results illustrate the effectiveness of our method in comparison to recently proposed alternatives.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/xin12/xin12.pdf",
        "supp": "",
        "pdf_size": 295313,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5654800030904026026&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "MIT CSAIL; MIT CSAIL",
        "aff_domain": "; ",
        "email": "; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory",
        "aff_unique_url": "https://www.csail.mit.edu",
        "aff_unique_abbr": "MIT CSAIL",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Probabilistic acoustic tube: a probabilistic generative model of speech for speech analysis/synthesis",
        "site": "https://proceedings.mlr.press/v22/ou12.html",
        "author": "Zhijian Ou; Yang Zhang",
        "abstract": "Most speech analysis/synthesis systems are based on the basic physical model of speech production - the acoustic tube model. There are two main drawbacks with current speech analysis methods. First, a common design paradigm seems to build a special-purpose signal-processing front-end followed by (when appropriate) a back-end based on probabilistic models. A difficulty is that most features are nonlinear operators of the speech waveform, whose statistical behavior is hard to be modeled. Second, different tasks of speech analysis are carried out separately. These practices are admittedly useful but not optimal due to the incomplete use of available information. These examinations motivate us to directly model the spectrogram and to integrate together the three fundamental speech parameters - the pitch, energy and spectral envelope. We successfully devise such a model called probabilistic acoustic tube (PAT) model. The integration is performed in a principled manner with explicit physical meaning. We demonstrate the capability of PAT for a number of speech analysis/synthesis tasks, such as pitch tracking under both clean and additive noise conditions, speech synthesis, and phoneme clustering.",
        "bibtex": "@InProceedings{pmlr-v22-ou12,\n  title = \t {Probabilistic acoustic tube: a probabilistic generative model of speech for speech analysis/synthesis},\n  author = \t {Ou, Zhijian and Zhang, Yang},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {841--849},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/ou12/ou12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/ou12.html},\n  abstract = \t {Most speech analysis/synthesis systems are based on the basic physical model of speech production - the acoustic tube model. There are two main drawbacks with current speech analysis methods. First, a common design paradigm seems to build a special-purpose signal-processing front-end followed by (when appropriate) a back-end based on probabilistic models. A difficulty is that most features are nonlinear operators of the speech waveform, whose statistical behavior is hard to be modeled. Second, different tasks of speech analysis are carried out separately. These practices are admittedly useful but not optimal due to the incomplete use of available information. These examinations motivate us to directly model the spectrogram and to integrate together the three fundamental speech parameters - the pitch, energy and spectral envelope. We successfully devise such a model called probabilistic acoustic tube (PAT) model. The integration is performed in a principled manner with explicit physical meaning. We demonstrate the capability of PAT for a number of speech analysis/synthesis tasks, such as pitch tracking under both clean and additive noise conditions, speech synthesis, and phoneme clustering.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/ou12/ou12.pdf",
        "supp": "",
        "pdf_size": 293034,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12672554611963776255&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Electronic Engineering, Tsinghua University, Beijing 100084, China; Department of Electronic Engineering, Tsinghua University, Beijing 100084, China",
        "aff_domain": "tsinghua.edu.cn;gmail.com",
        "email": "tsinghua.edu.cn;gmail.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Tsinghua University",
        "aff_unique_dep": "Department of Electronic Engineering",
        "aff_unique_url": "https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "THU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Protocols for Learning Classifiers on Distributed Data",
        "site": "https://proceedings.mlr.press/v22/daume12.html",
        "author": "Hal Daume III; Jeff Phillips; Avishek Saha; Suresh Venkatasubramanian",
        "abstract": "We consider the problem of learning classifiers for labeled data that has been distributed across several nodes. Our goal is to find a single classifier, with small approximation error, across all datasets while minimizing the communication between nodes. This setting models real-world communication bottlenecks in the processing of massive distributed datasets. We present several very general sampling-based solutions as well as some two-way protocols which have a provable exponential speed-up over any one-way protocol. We focus on core problems for noiseless data distributed across two or more nodes. The techniques we introduce are reminiscent of active learning, but rather than actively probing labels, nodes actively communicate with each other, each node simultaneously learning the important data from another node.",
        "bibtex": "@InProceedings{pmlr-v22-daume12,\n  title = \t {Protocols for Learning Classifiers on Distributed Data},\n  author = \t {III, Hal Daume and Phillips, Jeff and Saha, Avishek and Venkatasubramanian, Suresh},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {282--290},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/daume12/daume12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/daume12.html},\n  abstract = \t {We consider the problem of learning classifiers for labeled data that has been distributed across several nodes. Our goal is to find a single classifier, with small approximation error, across all datasets while minimizing the communication between nodes. This setting models real-world communication bottlenecks in the processing of massive distributed datasets. We present several very general sampling-based solutions as well as some two-way protocols which have a provable exponential speed-up over any one-way protocol. We focus on core problems for noiseless data distributed across two or more nodes. The techniques we introduce are reminiscent of active learning, but rather than actively probing labels, nodes actively communicate with each other, each node simultaneously learning the important data from another node.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/daume12/daume12.pdf",
        "supp": "",
        "pdf_size": 452841,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17308177019979750784&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Dept. of Computer Sc., University of Maryland; School of Computing, University Of Utah; School of Computing, University Of Utah; School of Computing, University Of Utah",
        "aff_domain": "umiacs.umd.edu;cs.utah.edu;cs.utah.edu;cs.utah.edu",
        "email": "umiacs.umd.edu;cs.utah.edu;cs.utah.edu;cs.utah.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "University of Maryland;University of Utah",
        "aff_unique_dep": "Dept. of Computer Science;School of Computing",
        "aff_unique_url": "https://www/umd.edu;https://www.utah.edu",
        "aff_unique_abbr": "UMD;U of U",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Salt Lake City",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Quilting Stochastic Kronecker Product Graphs to Generate Multiplicative Attribute Graphs",
        "site": "https://proceedings.mlr.press/v22/yun12.html",
        "author": "Hyokun Yun; S V N Vishwanathan",
        "abstract": "We describe the first sub-quadratic sampling algorithm for Multiplicative Attribute Graph Model (MAGM) of Kim and Leskovec (2010). We exploit the close connection between MAGM and the Kronecker Product Graph Model (KPGM) of Leskovec et al. (2010), and show that to sample a graph from a MAGM it suffices to sample small number of KPGM graphs and quilt them together. Under a restricted set of technical conditions, our algorithm runs in $O((\\log_2(n))^3 |E|)$ time, where n is the number of nodes and |E| is the number of edges in the sampled graph. We demonstrate the scalability of our algorithm via extensive empirical evaluation; we can sample a MAGM graph with 8  million nodes and 20 billion edges in under 6 hours.",
        "bibtex": "@InProceedings{pmlr-v22-yun12,\n  title = \t {Quilting Stochastic Kronecker Product Graphs to Generate Multiplicative Attribute Graphs},\n  author = \t {Yun, Hyokun and Vishwanathan, S V N},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1389--1397},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/yun12/yun12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/yun12.html},\n  abstract = \t {We describe the first sub-quadratic sampling algorithm for Multiplicative Attribute Graph Model (MAGM) of Kim and Leskovec (2010). We exploit the close connection between MAGM and the Kronecker Product Graph Model (KPGM) of Leskovec et al. (2010), and show that to sample a graph from a MAGM it suffices to sample small number of KPGM graphs and quilt them together. Under a restricted set of technical conditions, our algorithm runs in $O((\\log_2(n))^3 |E|)$ time, where n is the number of nodes and |E| is the number of edges in the sampled graph. We demonstrate the scalability of our algorithm via extensive empirical evaluation; we can sample a MAGM graph with 8  million nodes and 20 billion edges in under 6 hours.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/yun12/yun12.pdf",
        "supp": "",
        "pdf_size": 1123108,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13848835775606889002&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Statistics, Purdue University; Department of Statistics and Computer Science, Purdue University",
        "aff_domain": "; ",
        "email": "; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Purdue University",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "https://www.purdue.edu",
        "aff_unique_abbr": "Purdue",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Random Feature Maps for Dot Product Kernels",
        "site": "https://proceedings.mlr.press/v22/kar12.html",
        "author": "Purushottam Kar; Harish Karnick",
        "abstract": "Approximating non-linear kernels using feature maps has gained a lot of interest in recent years due to applications in reducing training and testing times of SVM classifiers and other kernel based learning algorithms. We extend this line of work and present low distortion embeddings for dot product kernels into linear Euclidean spaces. We base our results on a classical result in harmonic analysis characterizing all dot product kernels and use it to define randomized feature maps into explicit low dimensional Euclidean spaces in which the native dot product provides an approximation to the dot product kernel with high confidence.",
        "bibtex": "@InProceedings{pmlr-v22-kar12,\n  title = \t {Random Feature Maps for Dot Product Kernels},\n  author = \t {Kar, Purushottam and Karnick, Harish},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {583--591},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/kar12/kar12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/kar12.html},\n  abstract = \t {Approximating non-linear kernels using feature maps has gained a lot of interest in recent years due to applications in reducing training and testing times of SVM classifiers and other kernel based learning algorithms. We extend this line of work and present low distortion embeddings for dot product kernels into linear Euclidean spaces. We base our results on a classical result in harmonic analysis characterizing all dot product kernels and use it to define randomized feature maps into explicit low dimensional Euclidean spaces in which the native dot product provides an approximation to the dot product kernel with high confidence.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/kar12/kar12.pdf",
        "supp": "",
        "pdf_size": 442990,
        "gs_citation": 317,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10526614000698710797&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Indian Institute of Technology Kanpur, INDIA; Indian Institute of Technology Kanpur, INDIA",
        "aff_domain": "cse.iitk.ac.in;cse.iitk.ac.in",
        "email": "cse.iitk.ac.in;cse.iitk.ac.in",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Indian Institute of Technology Kanpur",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iitk.ac.in",
        "aff_unique_abbr": "IIT Kanpur",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Kanpur",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "Randomized Optimum Models for Structured Prediction",
        "site": "https://proceedings.mlr.press/v22/tarlow12b.html",
        "author": "Daniel Tarlow; Ryan Adams; Richard Zemel",
        "abstract": "One approach to modeling structured discrete data is to describe the   probability of states via an energy function and Gibbs distribution.   A recurring difficulty in these models is the computation of the   partition function, which may require an intractable sum.  However,   in many such models, the mode can be found efficiently even when the partition   function is unavailable.  Recent work on Perturb-and-MAP (PM)   models (Papandreou and Yuille, 2011) has exploited this discrepancy to   approximate the Gibbs distribution for Markov random fields (MRFs).   Here, we explore a broader class of models, called  Randomized Optimum Models (RandOMs), which include PM as a special   case.  This new class of models encompasses not only MRFs, but also other models that have intractable partition   functions yet permit efficient mode-finding, such as those based on bipartite   matchings, shortest paths, or connected components in a graph.  We develop   likelihood-based learning algorithms for RandOMs, which,   empirical results indicate, can produce better models   than PM.",
        "bibtex": "@InProceedings{pmlr-v22-tarlow12b,\n  title = \t {Randomized Optimum Models for Structured Prediction},\n  author = \t {Tarlow, Daniel and Adams, Ryan and Zemel, Richard},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1221--1229},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/tarlow12b/tarlow12b.pdf},\n  url = \t {https://proceedings.mlr.press/v22/tarlow12b.html},\n  abstract = \t {One approach to modeling structured discrete data is to describe the   probability of states via an energy function and Gibbs distribution.   A recurring difficulty in these models is the computation of the   partition function, which may require an intractable sum.  However,   in many such models, the mode can be found efficiently even when the partition   function is unavailable.  Recent work on Perturb-and-MAP (PM)   models (Papandreou and Yuille, 2011) has exploited this discrepancy to   approximate the Gibbs distribution for Markov random fields (MRFs).   Here, we explore a broader class of models, called  Randomized Optimum Models (RandOMs), which include PM as a special   case.  This new class of models encompasses not only MRFs, but also other models that have intractable partition   functions yet permit efficient mode-finding, such as those based on bipartite   matchings, shortest paths, or connected components in a graph.  We develop   likelihood-based learning algorithms for RandOMs, which,   empirical results indicate, can produce better models   than PM.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/tarlow12b/tarlow12b.pdf",
        "supp": "",
        "pdf_size": 1076319,
        "gs_citation": 59,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16086275393354925378&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "University of Toronto, Dept. of Computer Science; Harvard University, School of Engineering & Applied Sciences; University of Toronto, Dept. of Computer Science",
        "aff_domain": "; ; ",
        "email": "; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Toronto;Harvard University",
        "aff_unique_dep": "Dept. of Computer Science;School of Engineering & Applied Sciences",
        "aff_unique_url": "https://www.utoronto.ca;https://www.harvard.edu",
        "aff_unique_abbr": "U of T;Harvard",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Canada;United States"
    },
    {
        "title": "Regression for sets of polynomial equations",
        "site": "https://proceedings.mlr.press/v22/kiraly12.html",
        "author": "Franz Kiraly; Paul Von Buenau; Jan Muller; Duncan Blythe; Frank Meinecke; Klaus-Robert Muller",
        "abstract": "We propose a method called ideal regression for approximating an arbitrary system of polynomial equations by a system of a particular type. Using techniques from approximate computational algebraic geometry, we show how we can solve ideal regression directly without resorting to numerical optimization. Ideal regression is useful whenever the solution to a learning problem can be described by a system of polynomial equations. As an example, we demonstrate how to formulate Stationary Subspace Analysis (SSA), a source separation problem, in terms of ideal regression, which also yields a consistent estimator for SSA. We then compare this estimator in simulations with previous optimization-based approaches for SSA.",
        "bibtex": "@InProceedings{pmlr-v22-kiraly12,\n  title = \t {Regression for sets of polynomial equations},\n  author = \t {Kiraly, Franz and Buenau, Paul Von and Muller, Jan and Blythe, Duncan and Meinecke, Frank and Muller, Klaus-Robert},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {628--637},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/kiraly12/kiraly12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/kiraly12.html},\n  abstract = \t {We propose a method called ideal regression for approximating an arbitrary system of polynomial equations by a system of a particular type. Using techniques from approximate computational algebraic geometry, we show how we can solve ideal regression directly without resorting to numerical optimization. Ideal regression is useful whenever the solution to a learning problem can be described by a system of polynomial equations. As an example, we demonstrate how to formulate Stationary Subspace Analysis (SSA), a source separation problem, in terms of ideal regression, which also yields a consistent estimator for SSA. We then compare this estimator in simulations with previous optimization-based approaches for SSA.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/kiraly12/kiraly12.pdf",
        "supp": "",
        "pdf_size": 616328,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6059156622278022652&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Berlin Institute of Technology (TU Berlin), Machine Learning group; Berlin Institute of Technology (TU Berlin), Machine Learning group; Berlin Institute of Technology (TU Berlin), Machine Learning group; Berlin Institute of Technology (TU Berlin), Machine Learning group; Berlin Institute of Technology (TU Berlin), Machine Learning group; Berlin Institute of Technology (TU Berlin), Machine Learning group + Discrete Geometry Group (FU Berlin)",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0+1",
        "aff_unique_norm": "Berlin Institute of Technology;Freie Universit\u00e4t Berlin",
        "aff_unique_dep": "Machine Learning group;Discrete Geometry Group",
        "aff_unique_url": "https://www.tu-berlin.de;https://www.fu-berlin.de",
        "aff_unique_abbr": "TU Berlin;FU Berlin",
        "aff_campus_unique_index": "0;0;0;0;0;0+0",
        "aff_campus_unique": "Berlin",
        "aff_country_unique_index": "0;0;0;0;0;0+0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Regularization Paths with Guarantees for Convex Semidefinite Optimization",
        "site": "https://proceedings.mlr.press/v22/giesen12.html",
        "author": "Joachim Giesen; Martin Jaggi; Soeren Laue",
        "abstract": "We devise a simple algorithm for computing an approximate solution path for parameterized semidefinite convex optimization problems that is guaranteed to be epsilon-close to the exact solution path.  As a consequence, we can compute the entire regularization path for many regularized matrix completion and factorization approaches, as well as nuclear norm or weighted nuclear norm regularized convex optimization problems.  This also includes robust PCA and variants of sparse PCA.  On the theoretical side, we show that the approximate solution path has low complexity. This implies that the whole solution path can be computed efficiently. Our experiments demonstrate the practicality of the approach for large matrix completion problems.",
        "bibtex": "@InProceedings{pmlr-v22-giesen12,\n  title = \t {Regularization Paths with Guarantees for Convex Semidefinite Optimization},\n  author = \t {Giesen, Joachim and Jaggi, Martin and Laue, Soeren},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {432--439},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/giesen12/giesen12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/giesen12.html},\n  abstract = \t {We devise a simple algorithm for computing an approximate solution path for parameterized semidefinite convex optimization problems that is guaranteed to be epsilon-close to the exact solution path.  As a consequence, we can compute the entire regularization path for many regularized matrix completion and factorization approaches, as well as nuclear norm or weighted nuclear norm regularized convex optimization problems.  This also includes robust PCA and variants of sparse PCA.  On the theoretical side, we show that the approximate solution path has low complexity. This implies that the whole solution path can be computed efficiently. Our experiments demonstrate the practicality of the approach for large matrix completion problems.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/giesen12/giesen12.pdf",
        "supp": "",
        "pdf_size": 461010,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9493904529132883585&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Robust Multi-task Regression with Grossly Corrupted Observations",
        "site": "https://proceedings.mlr.press/v22/xu12b.html",
        "author": "Huan Xu; Chenlei Leng",
        "abstract": "We consider the multiple-response regression problem, where the response is subject to *sparse gross errors*, in the high-dimensional setup. We propose a tractable regularized M-estimator that is robust to such error, where the sum of two individual regularization terms are used: the first one encourages row-sparse regression parameters, and the second one encourages a sparse error term. We obtain non-asymptotical  estimation error bounds of the proposed method. To the best of our knowledge, this is the first analysis of the robust multi-task regression problem with gross errors.",
        "bibtex": "@InProceedings{pmlr-v22-xu12b,\n  title = \t {Robust Multi-task Regression with Grossly Corrupted Observations},\n  author = \t {Xu, Huan and Leng, Chenlei},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1341--1349},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/xu12b/xu12b.pdf},\n  url = \t {https://proceedings.mlr.press/v22/xu12b.html},\n  abstract = \t {We consider the multiple-response regression problem, where the response is subject to *sparse gross errors*, in the high-dimensional setup. We propose a tractable regularized M-estimator that is robust to such error, where the sum of two individual regularization terms are used: the first one encourages row-sparse regression parameters, and the second one encourages a sparse error term. We obtain non-asymptotical  estimation error bounds of the proposed method. To the best of our knowledge, this is the first analysis of the robust multi-task regression problem with gross errors.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/xu12b/xu12b.pdf",
        "supp": "",
        "pdf_size": 357237,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1114108829533911104&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Mechanical Engineering, National University of Singapore; Department of Statistics and Applied Probability, National University of Singapore",
        "aff_domain": "nus.edu.sg;nus.edu.sg",
        "email": "nus.edu.sg;nus.edu.sg",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "National University of Singapore",
        "aff_unique_dep": "Department of Mechanical Engineering",
        "aff_unique_url": "https://www.nus.edu.sg",
        "aff_unique_abbr": "NUS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "title": "Sample Complexity of Composite Likelihood",
        "site": "https://proceedings.mlr.press/v22/bradley12.html",
        "author": "Joseph Bradley; Carlos Guestrin",
        "abstract": "We present the first PAC bounds for learning parameters of Conditional Random Fields (CRFs) with general structures over discrete and real-valued variables. Our bounds apply to composite likelihood, which generalizes maximum likelihood and pseudolikelihood. Moreover, we show that the only existing algorithm with a PAC bound for learning high-treewidth discrete models can be viewed as a computationally inefficient method for computing pseudolikelihood. We present an extensive empirical study of the statistical efficiency of these estimators, as predicted by our bounds. Finally, we use our bounds to show how to construct computationally and statistically efficient composite likelihood estimators.",
        "bibtex": "@InProceedings{pmlr-v22-bradley12,\n  title = \t {Sample Complexity of Composite Likelihood},\n  author = \t {Bradley, Joseph and Guestrin, Carlos},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {136--160},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/bradley12/bradley12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/bradley12.html},\n  abstract = \t {We present the first PAC bounds for learning parameters of Conditional Random Fields (CRFs) with general structures over discrete and real-valued variables. Our bounds apply to composite likelihood, which generalizes maximum likelihood and pseudolikelihood. Moreover, we show that the only existing algorithm with a PAC bound for learning high-treewidth discrete models can be viewed as a computationally inefficient method for computing pseudolikelihood. We present an extensive empirical study of the statistical efficiency of these estimators, as predicted by our bounds. Finally, we use our bounds to show how to construct computationally and statistically efficient composite likelihood estimators.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/bradley12/bradley12.pdf",
        "supp": "",
        "pdf_size": 1548580,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6597916406883816870&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Carnegie Mellon University; Carnegie Mellon University",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Scalable Inference on Kingman\u2019s Coalescent using Pair Similarity",
        "site": "https://proceedings.mlr.press/v22/gorur12.html",
        "author": "Dilan Gorur; Levi Boyles; Max Welling",
        "abstract": "We present a scalable sequential Monte Carlo algorithm and its greedy counterpart for models based on Kingman\u2019s coalescent. We utilize fast nearest neighbor algorithms to limit expensive computations to only a subset of data point pairs. For a dataset size of n, the resulting algorithm has O(n log n) computational complexity.  We empirically verify that we achieve a large speedup in computation. When the gain in speed is used for increasing the number of particles, we  can often  obtain significantly better samples than those of previous algorithms.  We apply our algorithm for learning visual taxonomies of birds on 6033 examples, a dataset size for which previous algorithms fail to be feasible.",
        "bibtex": "@InProceedings{pmlr-v22-gorur12,\n  title = \t {Scalable Inference on Kingman's Coalescent using Pair Similarity},\n  author = \t {Gorur, Dilan and Boyles, Levi and Welling, Max},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {440--448},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/gorur12/gorur12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/gorur12.html},\n  abstract = \t {We present a scalable sequential Monte Carlo algorithm and its greedy counterpart for models based on Kingman\u2019s coalescent. We utilize fast nearest neighbor algorithms to limit expensive computations to only a subset of data point pairs. For a dataset size of n, the resulting algorithm has O(n log n) computational complexity.  We empirically verify that we achieve a large speedup in computation. When the gain in speed is used for increasing the number of particles, we  can often  obtain significantly better samples than those of previous algorithms.  We apply our algorithm for learning visual taxonomies of birds on 6033 examples, a dataset size for which previous algorithms fail to be feasible.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/gorur12/gorur12.pdf",
        "supp": "",
        "pdf_size": 4147468,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=736625023005670302&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Bren School of Information and Computer Science, University of California, Irvine, Irvine, CA, USA; Bren School of Information and Computer Science, University of California, Irvine, Irvine, CA, USA; Bren School of Information and Computer Science, University of California, Irvine, Irvine, CA, USA",
        "aff_domain": "uci.edu;uci.edu;uci.edu",
        "email": "uci.edu;uci.edu;uci.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Irvine",
        "aff_unique_dep": "Bren School of Information and Computer Science",
        "aff_unique_url": "https://www.uci.edu",
        "aff_unique_abbr": "UCI",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Irvine",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Scalable Personalization of Long-Term Physiological Monitoring: Active Learning Methodologies for Epileptic Seizure Onset Detection",
        "site": "https://proceedings.mlr.press/v22/balakrishnan12b.html",
        "author": "Guha Balakrishnan; Zeeshan Syed",
        "abstract": "Patient-specific algorithms to detect adverse clinical events during long-term physiological monitoring substantially improve performance relative to patient-nonspecific ones. However, these algorithms often rely on the availability of expert hand-labeled data for training, which severely restricts the scalability of personalized monitoring within a real-world setting. While active learning offers a natural framework to address this issue, the relative merits of different active learning methodologies have not been extensively studied in the setting of developing clinically useful detectors for infrequent time-series events. In this paper, we identify a core set of principles that are relative to the specific goal of personalized long-term physiological monitoring. We describe and compare different approaches for initialization, batch selection and termination within the active learning process. We position this work in the context of epileptic seizure onset detection. When evaluated on a database of scalp EEG recordings from 23 epileptic patients, we show that a  combined distance- and diversity-based measure to determine the data to be queried, max-min clustering for identification of the initialization set, and a comparison of consecutive support vector sets to guide termination results in an active learning-based detector that can achieve similar performance to a patient-specific detector while requiring two orders of magnitude fewer labeled examples for training.",
        "bibtex": "@InProceedings{pmlr-v22-balakrishnan12b,\n  title = \t {Scalable Personalization of Long-Term Physiological Monitoring: Active Learning Methodologies for Epileptic Seizure Onset Detection},\n  author = \t {Balakrishnan, Guha and Syed, Zeeshan},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {73--81},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/balakrishnan12b/balakrishnan12b.pdf},\n  url = \t {https://proceedings.mlr.press/v22/balakrishnan12b.html},\n  abstract = \t {Patient-specific algorithms to detect adverse clinical events during long-term physiological monitoring substantially improve performance relative to patient-nonspecific ones. However, these algorithms often rely on the availability of expert hand-labeled data for training, which severely restricts the scalability of personalized monitoring within a real-world setting. While active learning offers a natural framework to address this issue, the relative merits of different active learning methodologies have not been extensively studied in the setting of developing clinically useful detectors for infrequent time-series events. In this paper, we identify a core set of principles that are relative to the specific goal of personalized long-term physiological monitoring. We describe and compare different approaches for initialization, batch selection and termination within the active learning process. We position this work in the context of epileptic seizure onset detection. When evaluated on a database of scalp EEG recordings from 23 epileptic patients, we show that a  combined distance- and diversity-based measure to determine the data to be queried, max-min clustering for identification of the initialization set, and a comparison of consecutive support vector sets to guide termination results in an active learning-based detector that can achieve similar performance to a patient-specific detector while requiring two orders of magnitude fewer labeled examples for training.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/balakrishnan12b/balakrishnan12b.pdf",
        "supp": "",
        "pdf_size": 319047,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=186857262906158866&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Massachusetts Institute of Technology; University of Michigan",
        "aff_domain": "; ",
        "email": "; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Massachusetts Institute of Technology;University of Michigan",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://web.mit.edu;https://www.umich.edu",
        "aff_unique_abbr": "MIT;UM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Scaling up Kernel SVM on Limited Resources: A Low-rank Linearization Approach",
        "site": "https://proceedings.mlr.press/v22/zhang12d.html",
        "author": "Kai Zhang; Liang Lan; Zhuang Wang; Fabian Moerchen",
        "abstract": "Kernel Support Vector Machine delivers state-of-the-art results in non-linear classification, but the need to maintain a large number of support vectors poses a challenge in large scale training and testing. In contrast, linear SVM is much more scalable even on limited computing recourses (e.g. daily life PCs), but the learned model cannot capture non-linear concepts. To scale up kernel SVM on limited resources, we propose a low-rank linearization approach that transforms a non-linear SVM to a linear one via a novel, approximate empirical kernel map computed from efficient low-rank approximation of kernel matrices. We call it LLSVM (Low-rank Linearized SVM). We theoretically study the gap between the solutions of the optimal and approximate kernel map, which is used in turn to provide important guidance on the sampling based kernel approximations. Our algorithm inherits high efficiency of linear SVMs and rich repesentability of kernel classifiers. Evaluation against large-scale linear and kernel SVMs on several truly large data sets shows that the proposed method achieves a better tradeoff between scalability and model representability.",
        "bibtex": "@InProceedings{pmlr-v22-zhang12d,\n  title = \t {Scaling up Kernel SVM on Limited Resources: A Low-rank Linearization Approach},\n  author = \t {Zhang, Kai and Lan, Liang and Wang, Zhuang and Moerchen, Fabian},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1425--1434},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/zhang12d/zhang12d.pdf},\n  url = \t {https://proceedings.mlr.press/v22/zhang12d.html},\n  abstract = \t {Kernel Support Vector Machine delivers state-of-the-art results in non-linear classification, but the need to maintain a large number of support vectors poses a challenge in large scale training and testing. In contrast, linear SVM is much more scalable even on limited computing recourses (e.g. daily life PCs), but the learned model cannot capture non-linear concepts. To scale up kernel SVM on limited resources, we propose a low-rank linearization approach that transforms a non-linear SVM to a linear one via a novel, approximate empirical kernel map computed from efficient low-rank approximation of kernel matrices. We call it LLSVM (Low-rank Linearized SVM). We theoretically study the gap between the solutions of the optimal and approximate kernel map, which is used in turn to provide important guidance on the sampling based kernel approximations. Our algorithm inherits high efficiency of linear SVMs and rich repesentability of kernel classifiers. Evaluation against large-scale linear and kernel SVMs on several truly large data sets shows that the proposed method achieves a better tradeoff between scalability and model representability.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/zhang12d/zhang12d.pdf",
        "supp": "",
        "pdf_size": 788378,
        "gs_citation": 209,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1049760275608510416&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Siemens Corporate Research & Technoplogy; Dept. Computer & Information Sciences, Temple University; Siemens Corporate Research & Technoplogy; Siemens Corporate Research & Technoplogy",
        "aff_domain": "siemens.com;temple.edu;siemens.com;siemens.com",
        "email": "siemens.com;temple.edu;siemens.com;siemens.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Siemens AG;Temple University",
        "aff_unique_dep": "Corporate Research & Technology;Dept. Computer & Information Sciences",
        "aff_unique_url": "https://www.siemens.com;https://www.temple.edu",
        "aff_unique_abbr": "Siemens;Temple",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "Germany;United States"
    },
    {
        "title": "Semiparametric Pseudo-Likelihood Estimation in Markov Random Fields",
        "site": "https://proceedings.mlr.press/v22/freno12.html",
        "author": "Antonino Freno",
        "abstract": "Probabilistic graphical models for continuous variables can be built out of either parametric or nonparametric conditional density estimators. While several research efforts have been focusing on parametric approaches (such as Gaussian models), kernel-based estimators are still the only viable and well-understood option for nonparametric density estimation. This paper develops a semiparametric estimator of probability density functions based on the nonparanormal transformation, which has been recently proposed for mapping arbitrarily distributed data samples onto normally distributed datasets. Pointwise and uniform consistency properties are established for the developed method. The resulting density model is then applied to pseudo-likelihood estimation in Markov random fields. An experimental evaluation on data distributed according to a variety of density functions indicates that such semiparametric Markov random field models significantly outperform both their Gaussian and kernel-based alternatives in terms of prediction accuracy.",
        "bibtex": "@InProceedings{pmlr-v22-freno12,\n  title = \t {Semiparametric Pseudo-Likelihood Estimation in Markov Random Fields},\n  author = \t {Freno, Antonino},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {391--399},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/freno12/freno12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/freno12.html},\n  abstract = \t {Probabilistic graphical models for continuous variables can be built out of either parametric or nonparametric conditional density estimators. While several research efforts have been focusing on parametric approaches (such as Gaussian models), kernel-based estimators are still the only viable and well-understood option for nonparametric density estimation. This paper develops a semiparametric estimator of probability density functions based on the nonparanormal transformation, which has been recently proposed for mapping arbitrarily distributed data samples onto normally distributed datasets. Pointwise and uniform consistency properties are established for the developed method. The resulting density model is then applied to pseudo-likelihood estimation in Markov random fields. An experimental evaluation on data distributed according to a variety of density functions indicates that such semiparametric Markov random field models significantly outperform both their Gaussian and kernel-based alternatives in terms of prediction accuracy.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/freno12/freno12.pdf",
        "supp": "",
        "pdf_size": 529340,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:E8QQdvgiqgUJ:scholar.google.com/&scioq=Semiparametric+Pseudo-Likelihood+Estimation+in+Markov+Random+Fields&hl=en&as_sdt=0,33",
        "gs_version_total": 11,
        "aff": "INRIA Lille \u2013 Nord Europe",
        "aff_domain": "inria.fr",
        "email": "inria.fr",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "INRIA",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.inria.fr",
        "aff_unique_abbr": "INRIA",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Lille",
        "aff_country_unique_index": "0",
        "aff_country_unique": "France"
    },
    {
        "title": "Sparse Additive Machine",
        "site": "https://proceedings.mlr.press/v22/zhao12.html",
        "author": "Tuo Zhao; Han Liu",
        "abstract": "We develop a high dimensional nonparametric classification method named sparse additive machine (SAM), which can be viewed as a functional version of support vector machines (SVM) combined with sparse additive modeling.  SAM is related  to multiple kernel learning (MKL), but is computationally more efficient and amenable to theoretical analysis. In terms of computation, we develop an efficient accelerated proximal gradient descent algorithm which is also scalable to large data sets with a provable O(1/k^2) convergence rate and k is the number of iterations.  In terms of theory, we provide the oracle properties of SAM under asymptotic frameworks. Empirical results on3 both synthetic and real data are reported to back up our theory.",
        "bibtex": "@InProceedings{pmlr-v22-zhao12,\n  title = \t {Sparse Additive Machine},\n  author = \t {Zhao, Tuo and Liu, Han},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1435--1443},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/zhao12/zhao12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/zhao12.html},\n  abstract = \t {We develop a high dimensional nonparametric classification method named sparse additive machine (SAM), which can be viewed as a functional version of support vector machines (SVM) combined with sparse additive modeling.  SAM is related  to multiple kernel learning (MKL), but is computationally more efficient and amenable to theoretical analysis. In terms of computation, we develop an efficient accelerated proximal gradient descent algorithm which is also scalable to large data sets with a provable O(1/k^2) convergence rate and k is the number of iterations.  In terms of theory, we provide the oracle properties of SAM under asymptotic frameworks. Empirical results on3 both synthetic and real data are reported to back up our theory.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/zhao12/zhao12.pdf",
        "supp": "",
        "pdf_size": 1232061,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11019477919123151279&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Biostatistics and Computer Science, Johns Hopkins University; Department of Biostatistics and Computer Science, Johns Hopkins University",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Johns Hopkins University",
        "aff_unique_dep": "Department of Biostatistics and Computer Science",
        "aff_unique_url": "https://www.jhu.edu",
        "aff_unique_abbr": "JHU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Sparse Higher-Order Principal Components Analysis",
        "site": "https://proceedings.mlr.press/v22/allen12.html",
        "author": "Genevera Allen",
        "abstract": "Traditional tensor decompositions such as the CANDECOMP / PARAFAC (CP) and Tucker decompositions yield higher-order principal components that have been used to understand tensor data in areas such as neuroimaging, microscopy, chemometrics, and remote sensing. Sparsity in high-dimensional matrix factorizations and principal components has been well-studied exhibiting many benefits; less attention has been given to sparsity in tensor decompositions. We propose two novel tensor decompositions that incorporate sparsity: the Sparse Higher-Order SVD and the Sparse CP Decomposition. The latter solves a 1-norm penalized relaxation of the single-factor CP optimization problem, thereby automatically selecting relevant features for each tensor factor. Through experiments and a scientific data analysis example, we demonstrate the utility of our methods for dimension reduction, feature selection, signal recovery, and exploratory data analysis of high-dimensional tensors.",
        "bibtex": "@InProceedings{pmlr-v22-allen12,\n  title = \t {Sparse Higher-Order Principal Components Analysis},\n  author = \t {Allen, Genevera},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {27--36},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/allen12/allen12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/allen12.html},\n  abstract = \t {Traditional tensor decompositions such as the CANDECOMP / PARAFAC (CP) and Tucker decompositions yield higher-order principal components that have been used to understand tensor data in areas such as neuroimaging, microscopy, chemometrics, and remote sensing. Sparsity in high-dimensional matrix factorizations and principal components has been well-studied exhibiting many benefits; less attention has been given to sparsity in tensor decompositions. We propose two novel tensor decompositions that incorporate sparsity: the Sparse Higher-Order SVD and the Sparse CP Decomposition. The latter solves a 1-norm penalized relaxation of the single-factor CP optimization problem, thereby automatically selecting relevant features for each tensor factor. Through experiments and a scientific data analysis example, we demonstrate the utility of our methods for dimension reduction, feature selection, signal recovery, and exploratory data analysis of high-dimensional tensors.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/allen12/allen12.pdf",
        "supp": "",
        "pdf_size": 424545,
        "gs_citation": 150,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17359434337123249791&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Baylor College of Medicine & Rice University",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Baylor College of Medicine",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.bcm.edu",
        "aff_unique_abbr": "BCM",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Sparsistency of the Edge Lasso over Graphs",
        "site": "https://proceedings.mlr.press/v22/sharpnack12.html",
        "author": "James Sharpnack; Aarti Singh; Alessandro Rinaldo",
        "abstract": "The fused lasso was proposed recently to enable recovery of high-dimensional patterns which are piece-wise constant on a graph, by penalizing the \\ell_1-norm of differences of measurements at vertices that share an edge.  While there have been some attempts at coming up with efficient algorithms for solving the fused lasso optimization, a theoretical analysis of its performance is mostly lacking except for the simple linear graph topology. In this paper, we investigate \\em sparsistency of  fused lasso for general graph structures, i.e. its ability to correctly recover the exact support of piece-wise constant graph-structured patterns asymptotically (for large-scale graphs). To emphasize this distinction over previous work,  we will refer to it as Edge Lasso. We focus on the (structured) normal means setting, and our results provide necessary and sufficient conditions on the graph properties as well as the signal-to-noise ratio needed to ensure sparsistency. We examplify our results using simple graph-structured patterns, and demonstrate that in some cases fused lasso is sparsistent at very weak signal-to-noise ratios (scaling as \\sqrt(\\log n)/|A|, where n is the number of vertices in the graph and A is the smallest set of vertices with constant activation).  In other cases, it performs no better than thresholding the difference of measurements at vertices which share an edge (which requires signal-to-noise ratio that scales as \\sqrt\\log n).",
        "bibtex": "@InProceedings{pmlr-v22-sharpnack12,\n  title = \t {Sparsistency of the Edge Lasso over Graphs},\n  author = \t {Sharpnack, James and Singh, Aarti and Rinaldo, Alessandro},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1028--1036},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/sharpnack12/sharpnack12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/sharpnack12.html},\n  abstract = \t {The fused lasso was proposed recently to enable recovery of high-dimensional patterns which are piece-wise constant on a graph, by penalizing the \\ell_1-norm of differences of measurements at vertices that share an edge.  While there have been some attempts at coming up with efficient algorithms for solving the fused lasso optimization, a theoretical analysis of its performance is mostly lacking except for the simple linear graph topology. In this paper, we investigate \\em sparsistency of  fused lasso for general graph structures, i.e. its ability to correctly recover the exact support of piece-wise constant graph-structured patterns asymptotically (for large-scale graphs). To emphasize this distinction over previous work,  we will refer to it as Edge Lasso. We focus on the (structured) normal means setting, and our results provide necessary and sufficient conditions on the graph properties as well as the signal-to-noise ratio needed to ensure sparsistency. We examplify our results using simple graph-structured patterns, and demonstrate that in some cases fused lasso is sparsistent at very weak signal-to-noise ratios (scaling as \\sqrt(\\log n)/|A|, where n is the number of vertices in the graph and A is the smallest set of vertices with constant activation).  In other cases, it performs no better than thresholding the difference of measurements at vertices which share an edge (which requires signal-to-noise ratio that scales as \\sqrt\\log n).}\n}",
        "pdf": "http://proceedings.mlr.press/v22/sharpnack12/sharpnack12.pdf",
        "supp": "",
        "pdf_size": 674186,
        "gs_citation": 90,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8847053331458351475&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Machine Learning Department; Statistics Department; Machine Learning Department",
        "aff_domain": "andrew.cmu.edu;cmu.edu;cmu.edu",
        "email": "andrew.cmu.edu;cmu.edu;cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Carnegie Mellon University;Statistics Department",
        "aff_unique_dep": "Machine Learning Department;Statistics Department",
        "aff_unique_url": "https://www.cs.cmu.edu/ml;",
        "aff_unique_abbr": "CMU ML;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States;"
    },
    {
        "title": "SpeedBoost: Anytime Prediction with Uniform Near-Optimality",
        "site": "https://proceedings.mlr.press/v22/grubb12.html",
        "author": "Alex Grubb; Drew Bagnell",
        "abstract": "We present SpeedBoost, a natural extension of functional gradient descent, for learning anytime predictors, which automatically trade computation time for predictive accuracy by selecting from a set of simpler candidate predictors. These anytime predictors not only generate approximate predictions rapidly, but are capable of using extra resources at prediction time, when available, to improve performance. We also demonstrate how our framework can be used to select weak predictors which target certain subsets of the data, allowing for efficient use of computational resources on difficult examples.  We also show that variants of the SpeedBoost algorithm produce predictors which are provably competitive with any possible sequence of weak predictors with the same total complexity.",
        "bibtex": "@InProceedings{pmlr-v22-grubb12,\n  title = \t {SpeedBoost: Anytime Prediction with Uniform Near-Optimality},\n  author = \t {Grubb, Alex and Bagnell, Drew},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {458--466},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/grubb12/grubb12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/grubb12.html},\n  abstract = \t {We present SpeedBoost, a natural extension of functional gradient descent, for learning anytime predictors, which automatically trade computation time for predictive accuracy by selecting from a set of simpler candidate predictors. These anytime predictors not only generate approximate predictions rapidly, but are capable of using extra resources at prediction time, when available, to improve performance. We also demonstrate how our framework can be used to select weak predictors which target certain subsets of the data, allowing for efficient use of computational resources on difficult examples.  We also show that variants of the SpeedBoost algorithm produce predictors which are provably competitive with any possible sequence of weak predictors with the same total complexity.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/grubb12/grubb12.pdf",
        "supp": "",
        "pdf_size": 1657975,
        "gs_citation": 108,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8064701757626800228&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Carnegie Mellon University; Carnegie Mellon University",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Statistical Optimization in High Dimensions",
        "site": "https://proceedings.mlr.press/v22/xu12a.html",
        "author": "Huan Xu; Constantine Caramanis; Shie Mannor",
        "abstract": "We consider optimization problems whose parameters are known only approximately, based on a noisy sample. Of particular interest is the high-dimensional regime, where the number of samples is roughly equal to the dimensionality of the problem, and the noise magnitude may greatly exceed the magnitude of the signal itself. This setup falls far outside the traditional scope of Robust  and Stochastic optimization. We propose three algorithms to address this setting, combining ideas from statistics, machine learning, and robust optimization. In the important case where noise artificially increases the dimensionality of the parameters, we show that combining robust optimization and dimensionality reduction can result in high-quality solutions at greatly reduced computational cost.",
        "bibtex": "@InProceedings{pmlr-v22-xu12a,\n  title = \t {Statistical Optimization in High Dimensions},\n  author = \t {Xu, Huan and Caramanis, Constantine and Mannor, Shie},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1332--1340},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/xu12a/xu12a.pdf},\n  url = \t {https://proceedings.mlr.press/v22/xu12a.html},\n  abstract = \t {We consider optimization problems whose parameters are known only approximately, based on a noisy sample. Of particular interest is the high-dimensional regime, where the number of samples is roughly equal to the dimensionality of the problem, and the noise magnitude may greatly exceed the magnitude of the signal itself. This setup falls far outside the traditional scope of Robust  and Stochastic optimization. We propose three algorithms to address this setting, combining ideas from statistics, machine learning, and robust optimization. In the important case where noise artificially increases the dimensionality of the parameters, we show that combining robust optimization and dimensionality reduction can result in high-quality solutions at greatly reduced computational cost.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/xu12a/xu12a.pdf",
        "supp": "",
        "pdf_size": 393077,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4045514666700217292&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 20,
        "aff": "Mechanical Engineering, National University of Singapore; Electrical and Computer Engineering, The University of Texas at Austin; Electrical Engineering, Technion, Israel",
        "aff_domain": "nus.edu.sg;mail.utexas.edu;ee.technion.ac.il",
        "email": "nus.edu.sg;mail.utexas.edu;ee.technion.ac.il",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "National University of Singapore;The University of Texas at Austin;Technion",
        "aff_unique_dep": "Mechanical Engineering;Electrical and Computer Engineering;Electrical Engineering",
        "aff_unique_url": "https://www.nus.edu.sg;https://www.utexas.edu;https://www.technion.ac.il",
        "aff_unique_abbr": "NUS;UT Austin;Technion",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Austin",
        "aff_country_unique_index": "0;1;2",
        "aff_country_unique": "Singapore;United States;Israel"
    },
    {
        "title": "Statistical test for consistent estimation of causal effects in linear non-Gaussian models",
        "site": "https://proceedings.mlr.press/v22/entner12.html",
        "author": "Doris Entner; Patrik Hoyer; Peter Spirtes",
        "abstract": "In many fields of science researchers are faced with the problem of estimating causal effects from non-experimental data. A key issue is to avoid inconsistent estimators due to confounding by measured or unmeasured covariates, a problem commonly solved by \u2019adjusting for\u2019 a subset of the observed variables. When the data generating process can be represented by a directed acyclic graph, and this graph structure is known, there exist simple graphical procedures for determining which subset of covariates should be adjusted for to obtain consistent estimators of the causal effects. However, when the graph is not known no general and complete procedures for this task are available. In this paper we introduce such a method for linear non-Gaussian models, requiring only partial knowledge about the temporal ordering of the variables: We provide a simple statistical test for inferring whether an estimator of a causal effect is consistent when controlling for a subset of measured covariates, and we present heuristics to search for such a set. We show empirically that this statistical test identifies consistent vs inconsistent estimates, and that the search heuristics outperform the naive approach of adjusting for all observed covariates.",
        "bibtex": "@InProceedings{pmlr-v22-entner12,\n  title = \t {Statistical test for consistent estimation of causal effects in linear non-Gaussian models},\n  author = \t {Entner, Doris and Hoyer, Patrik and Spirtes, Peter},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {364--372},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/entner12/entner12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/entner12.html},\n  abstract = \t {In many fields of science researchers are faced with the problem of estimating causal effects from non-experimental data. A key issue is to avoid inconsistent estimators due to confounding by measured or unmeasured covariates, a problem commonly solved by \u2019adjusting for\u2019 a subset of the observed variables. When the data generating process can be represented by a directed acyclic graph, and this graph structure is known, there exist simple graphical procedures for determining which subset of covariates should be adjusted for to obtain consistent estimators of the causal effects. However, when the graph is not known no general and complete procedures for this task are available. In this paper we introduce such a method for linear non-Gaussian models, requiring only partial knowledge about the temporal ordering of the variables: We provide a simple statistical test for inferring whether an estimator of a causal effect is consistent when controlling for a subset of measured covariates, and we present heuristics to search for such a set. We show empirically that this statistical test identifies consistent vs inconsistent estimates, and that the search heuristics outperform the naive approach of adjusting for all observed covariates.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/entner12/entner12.pdf",
        "supp": "",
        "pdf_size": 453389,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8619287685243724343&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Stick-Breaking Beta Processes and the Poisson Process",
        "site": "https://proceedings.mlr.press/v22/paisley12.html",
        "author": "John Paisley; David Blei; Michael Jordan",
        "abstract": "We show that the stick-breaking construction of the beta process due to \\citePaisley:2010 can be obtained from the characterization of the beta process as a Poisson process.  Specifically, we show that the mean measure of the underlying Poisson process is equal to that of the beta process. We use this underlying representation to derive error bounds on truncated beta processes that are tighter than those in the literature. We also develop a new MCMC inference algorithm for beta processes, based in part on our new Poisson process construction.",
        "bibtex": "@InProceedings{pmlr-v22-paisley12,\n  title = \t {Stick-Breaking Beta Processes and the Poisson Process},\n  author = \t {Paisley, John and Blei, David and Jordan, Michael},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {850--858},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/paisley12/paisley12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/paisley12.html},\n  abstract = \t {We show that the stick-breaking construction of the beta process due to \\citePaisley:2010 can be obtained from the characterization of the beta process as a Poisson process.  Specifically, we show that the mean measure of the underlying Poisson process is equal to that of the beta process. We use this underlying representation to derive error bounds on truncated beta processes that are tighter than those in the literature. We also develop a new MCMC inference algorithm for beta processes, based in part on our new Poisson process construction.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/paisley12/paisley12.pdf",
        "supp": "",
        "pdf_size": 603207,
        "gs_citation": 58,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12373677331256023984&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Stochastic Bandit Based on Empirical Moments",
        "site": "https://proceedings.mlr.press/v22/honda12.html",
        "author": "Junya Honda; Akimichi Takemura",
        "abstract": "In the multiarmed bandit problem a gambler chooses an arm of a slot machine to pull considering a tradeoff between exploration and exploitation. We study the stochastic bandit problem where each arm has a reward distribution supported in [0,1]. For this model, there exists a policy which achieves the theoretical bound asymptotically. However the optimal policy requires a computation of a convex optimization which involves the empirical distribution of each arm. In this paper, we propose a policy which exploits the first d empirical moments for arbitrary d fixed in advance. We show that the performance of the policy approaches the theoretical bound as d increases. This policy can be implemented by solving polynomial equations, which we derive the explicit solution for d smaller than 5. By choosing appropriate d, the proposed policy realizes a tradeoff between the computational complexity and the expected regret.",
        "bibtex": "@InProceedings{pmlr-v22-honda12,\n  title = \t {Stochastic Bandit Based on Empirical Moments},\n  author = \t {Honda, Junya and Takemura, Akimichi},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {529--537},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/honda12/honda12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/honda12.html},\n  abstract = \t {In the multiarmed bandit problem a gambler chooses an arm of a slot machine to pull considering a tradeoff between exploration and exploitation. We study the stochastic bandit problem where each arm has a reward distribution supported in [0,1]. For this model, there exists a policy which achieves the theoretical bound asymptotically. However the optimal policy requires a computation of a convex optimization which involves the empirical distribution of each arm. In this paper, we propose a policy which exploits the first d empirical moments for arbitrary d fixed in advance. We show that the performance of the policy approaches the theoretical bound as d increases. This policy can be implemented by solving polynomial equations, which we derive the explicit solution for d smaller than 5. By choosing appropriate d, the proposed policy realizes a tradeoff between the computational complexity and the expected regret.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/honda12/honda12.pdf",
        "supp": "",
        "pdf_size": 397135,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11593835941065447325&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "The University of Tokyo; The University of Tokyo",
        "aff_domain": "stat.t.u-tokyo.ac.jp;stat.t.u-tokyo.ac.jp",
        "email": "stat.t.u-tokyo.ac.jp;stat.t.u-tokyo.ac.jp",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Tokyo",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.u-tokyo.ac.jp",
        "aff_unique_abbr": "UTokyo",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Structured Output Learning with High Order Loss Functions",
        "site": "https://proceedings.mlr.press/v22/tarlow12a.html",
        "author": "Daniel Tarlow; Richard Zemel",
        "abstract": "Often when modeling structured domains, it is desirable to leverage information that is not naturally expressed  as simply a label. Examples include   knowledge about the evaluation measure that will be used at test time, and partial  (weak) label information. When the additional information has structure that factorizes according to small subsets of variables (i.e., is \\emphlow order, or \\emphdecomposable),  several approaches can be used to incorporate it into a learning procedure.   Our focus in this work is the more challenging case, where the additional information does not factorize according to low order graphical model structure; we call this the \\emphhigh order case. We propose to formalize various forms of this additional information as high order loss functions, which may have complex interactions over large subsets of variables. We then address the computational challenges inherent in learning according to such loss functions, particularly focusing on the loss-augmented inference problem that arises in large margin learning; we show that learning with high order loss functions is often practical, giving strong empirical results, with one popular and several novel high-order loss functions, in several settings.",
        "bibtex": "@InProceedings{pmlr-v22-tarlow12a,\n  title = \t {Structured Output Learning with High Order Loss Functions},\n  author = \t {Tarlow, Daniel and Zemel, Richard},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1212--1220},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/tarlow12a/tarlow12a.pdf},\n  url = \t {https://proceedings.mlr.press/v22/tarlow12a.html},\n  abstract = \t {Often when modeling structured domains, it is desirable to leverage information that is not naturally expressed  as simply a label. Examples include   knowledge about the evaluation measure that will be used at test time, and partial  (weak) label information. When the additional information has structure that factorizes according to small subsets of variables (i.e., is \\emphlow order, or \\emphdecomposable),  several approaches can be used to incorporate it into a learning procedure.   Our focus in this work is the more challenging case, where the additional information does not factorize according to low order graphical model structure; we call this the \\emphhigh order case. We propose to formalize various forms of this additional information as high order loss functions, which may have complex interactions over large subsets of variables. We then address the computational challenges inherent in learning according to such loss functions, particularly focusing on the loss-augmented inference problem that arises in large margin learning; we show that learning with high order loss functions is often practical, giving strong empirical results, with one popular and several novel high-order loss functions, in several settings.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/tarlow12a/tarlow12a.pdf",
        "supp": "",
        "pdf_size": 741829,
        "gs_citation": 67,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16063502358971279091&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science, University of Toronto; Department of Computer Science, University of Toronto",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Toronto",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utoronto.ca",
        "aff_unique_abbr": "U of T",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Toronto",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "Structured Sparse Canonical Correlation Analysis",
        "site": "https://proceedings.mlr.press/v22/chen12a.html",
        "author": "Xi Chen; Liu Han; Jaime Carbonell",
        "abstract": "In this paper, we propose to apply sparse canonical correlation analysis (sparse CCA) to an important genome-wide association study problem, eQTL mapping. Existing sparse CCA models do not incorporate structural information among variables such as pathways of genes. This work extends the sparse CCA so that it could exploit either the pre-given or unknown group structure via the structured-sparsity-inducing penalty. Such structured penalty poses new challenge on optimization techniques. To address this challenge, by specializing the excessive gap framework, we develop a scalable primal-dual optimization algorithm with a fast rate of convergence. Empirical results show that the proposed optimization algorithm is more efficient than existing state-of-the-art methods. We also demonstrate the effectiveness of the structured sparse CCA on both simulated and genetic datasets.",
        "bibtex": "@InProceedings{pmlr-v22-chen12a,\n  title = \t {Structured Sparse Canonical Correlation Analysis},\n  author = \t {Chen, Xi and Han, Liu and Carbonell, Jaime},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {199--207},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/chen12a/chen12a.pdf},\n  url = \t {https://proceedings.mlr.press/v22/chen12a.html},\n  abstract = \t {In this paper, we propose to apply sparse canonical correlation analysis (sparse CCA) to an important genome-wide association study problem, eQTL mapping. Existing sparse CCA models do not incorporate structural information among variables such as pathways of genes. This work extends the sparse CCA so that it could exploit either the pre-given or unknown group structure via the structured-sparsity-inducing penalty. Such structured penalty poses new challenge on optimization techniques. To address this challenge, by specializing the excessive gap framework, we develop a scalable primal-dual optimization algorithm with a fast rate of convergence. Empirical results show that the proposed optimization algorithm is more efficient than existing state-of-the-art methods. We also demonstrate the effectiveness of the structured sparse CCA on both simulated and genetic datasets.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/chen12a/chen12a.pdf",
        "supp": "",
        "pdf_size": 1470366,
        "gs_citation": 131,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17809286563510541153&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "Carnegie Mellon University; Johns Hopkins University; Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.jhu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.jhu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Carnegie Mellon University;Johns Hopkins University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cmu.edu;https://www.jhu.edu",
        "aff_unique_abbr": "CMU;JHU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Subset Infinite Relational Models",
        "site": "https://proceedings.mlr.press/v22/ishiguro12.html",
        "author": "Katsuhiko Ishiguro; Naonori Ueda; Hiroshi Sawada",
        "abstract": "We propose a new probabilistic generative model for analyzing sparse and noisy pairwise relational data, such as friend-links on SNSs and customer records in online shops. Real-world relational data often include a large portion of non-informative pairwise data entries.  Many existing stochastic blockmodels suffer from these irrelevant data entries because of their rather simpler forms of priors. The proposed model newly incorporates a latent variable that explicitly indicates whether each data entry is relevant or not to diminish the bad effects associated with such irrelevant data.  Through experimental results using synthetic and real data sets, we show that the proposed model can extract clusters with stronger relations among data within the cluster than clusters obtained by the conventional model.",
        "bibtex": "@InProceedings{pmlr-v22-ishiguro12,\n  title = \t {Subset Infinite Relational Models},\n  author = \t {Ishiguro, Katsuhiko and Ueda, Naonori and Sawada, Hiroshi},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {547--555},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/ishiguro12/ishiguro12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/ishiguro12.html},\n  abstract = \t {We propose a new probabilistic generative model for analyzing sparse and noisy pairwise relational data, such as friend-links on SNSs and customer records in online shops. Real-world relational data often include a large portion of non-informative pairwise data entries.  Many existing stochastic blockmodels suffer from these irrelevant data entries because of their rather simpler forms of priors. The proposed model newly incorporates a latent variable that explicitly indicates whether each data entry is relevant or not to diminish the bad effects associated with such irrelevant data.  Through experimental results using synthetic and real data sets, we show that the proposed model can extract clusters with stronger relations among data within the cluster than clusters obtained by the conventional model.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/ishiguro12/ishiguro12.pdf",
        "supp": "",
        "pdf_size": 164176,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11641860820424142645&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Testing for Membership to the IFRA and the NBU Classes of Distributions",
        "site": "https://proceedings.mlr.press/v22/srivastava12.html",
        "author": "Radhendushka Srivastava; Ping Li; Debasis Sengupta",
        "abstract": "This paper provides test procedures to determine whether the probability distribution underlying a set of non-negative valued samples belongs to the Increasing Failure Rate Average (IFRA) class or the New Better than Used (NBU) class. Membership of a distribution to one of these classes is known to have implications which are important in reliability, queuing theory, game theory and other disciplines. Our proposed test is based on the Kolmogorov-Smirnov distance between an empirical cumulative hazard function and its best approximation from the class of distributions constituting the null hypothesis. It turns out that the least favorable distribution, which produces the largest probability of Type I error of each of the tests, is the exponential distribution. This fact is used to produce an appropriate cut-off or p-value. Monte Carlo simulations are conducted to check small sample size (i.e., significance) and power of the test. Usefulness of the test is illustrated through the analysis of a set of monthly family expenditure data collected by the National Sample Survey Organization of the Government of India.",
        "bibtex": "@InProceedings{pmlr-v22-srivastava12,\n  title = \t {Testing for Membership to the IFRA and the NBU Classes of Distributions},\n  author = \t {Srivastava, Radhendushka and Li, Ping and Sengupta, Debasis},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1099--1107},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/srivastava12/srivastava12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/srivastava12.html},\n  abstract = \t {This paper provides test procedures to determine whether the probability distribution underlying a set of non-negative valued samples belongs to the Increasing Failure Rate Average (IFRA) class or the New Better than Used (NBU) class. Membership of a distribution to one of these classes is known to have implications which are important in reliability, queuing theory, game theory and other disciplines. Our proposed test is based on the Kolmogorov-Smirnov distance between an empirical cumulative hazard function and its best approximation from the class of distributions constituting the null hypothesis. It turns out that the least favorable distribution, which produces the largest probability of Type I error of each of the tests, is the exponential distribution. This fact is used to produce an appropriate cut-off or p-value. Monte Carlo simulations are conducted to check small sample size (i.e., significance) and power of the test. Usefulness of the test is illustrated through the analysis of a set of monthly family expenditure data collected by the National Sample Survey Organization of the Government of India.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/srivastava12/srivastava12.pdf",
        "supp": "",
        "pdf_size": 667773,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5018131896106481920&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Statistical Science, Cornell University; Department of Statistical Science, Cornell University; Applied Statistics Unit, Indian Statistical Institute",
        "aff_domain": "cornell.edu;cornell.edu;isical.ac.in",
        "email": "cornell.edu;cornell.edu;isical.ac.in",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Cornell University;Indian Statistical Institute",
        "aff_unique_dep": "Department of Statistical Science;Applied Statistics Unit",
        "aff_unique_url": "https://www.cornell.edu;https://www.isical.ac.in",
        "aff_unique_abbr": "Cornell;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "United States;India"
    },
    {
        "title": "The adversarial stochastic shortest path problem with unknown transition probabilities",
        "site": "https://proceedings.mlr.press/v22/neu12.html",
        "author": "Gergely Neu; Andras Gyorgy; Csaba Szepesvari",
        "abstract": "We consider online learning in a special class of episodic Markovian decision processes, namely, loop-free stochastic shortest path problems. In this problem, an agent has to traverse through a finite directed acyclic graph with random transitions while maximizing the obtained rewards along the way. We assume that the reward function can change arbitrarily between consecutive episodes, and is entirely revealed to the agent at the end of each episode. Previous work was concerned with the case when the stochastic dynamics is known ahead of time,  whereas the main novelty of this paper is that this assumption is lifted. We propose an algorithm called \u201cfollow the perturbed optimistic policy\u201d that combines ideas from the \u201cfollow the perturbed leader\u201d  method for online learning of arbitrary sequences and \u201cupper confidence reinforcement learning\u201d, an algorithm for regret minimization in Markovian decision processes (with a fixed reward function). We prove that the expected cumulative regret of our algorithm is of order L X A\\sqrtT up to logarithmic factors, where L is the length of the longest path in the graph, \\X is the state space, \\A is the action space and T is the number of episodes. To our knowledge this is the first algorithm that learns and controls stochastic and adversarial components in an online fashion at the same time.",
        "bibtex": "@InProceedings{pmlr-v22-neu12,\n  title = \t {The adversarial stochastic shortest path problem with unknown transition probabilities},\n  author = \t {Neu, Gergely and Gyorgy, Andras and Szepesvari, Csaba},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {805--813},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/neu12/neu12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/neu12.html},\n  abstract = \t {We consider online learning in a special class of episodic Markovian decision processes, namely, loop-free stochastic shortest path problems. In this problem, an agent has to traverse through a finite directed acyclic graph with random transitions while maximizing the obtained rewards along the way. We assume that the reward function can change arbitrarily between consecutive episodes, and is entirely revealed to the agent at the end of each episode. Previous work was concerned with the case when the stochastic dynamics is known ahead of time,  whereas the main novelty of this paper is that this assumption is lifted. We propose an algorithm called \u201cfollow the perturbed optimistic policy\u201d that combines ideas from the \u201cfollow the perturbed leader\u201d  method for online learning of arbitrary sequences and \u201cupper confidence reinforcement learning\u201d, an algorithm for regret minimization in Markovian decision processes (with a fixed reward function). We prove that the expected cumulative regret of our algorithm is of order L X A\\sqrtT up to logarithmic factors, where L is the length of the longest path in the graph, \\X is the state space, \\A is the action space and T is the number of episodes. To our knowledge this is the first algorithm that learns and controls stochastic and adversarial components in an online fashion at the same time.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/neu12/neu12.pdf",
        "supp": "",
        "pdf_size": 399032,
        "gs_citation": 104,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4555840657171798543&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Budapest Univ. of Technology and Economics and MTA SZTAKI, Budapest, Hungary; University of Alberta, Edmonton, Canada; University of Alberta, Edmonton, Canada",
        "aff_domain": "; ; ",
        "email": "; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Budapest University of Technology and Economics;University of Alberta",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.bme.hu;https://www.ualberta.ca",
        "aff_unique_abbr": "BUTE;UAlberta",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Budapest;Edmonton",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "Hungary;Canada"
    },
    {
        "title": "There\u2019s a Hole in My Data Space: Piecewise Predictors for Heterogeneous Learning Problems",
        "site": "https://proceedings.mlr.press/v22/dekel12.html",
        "author": "Ofer Dekel; Ohad Shamir",
        "abstract": "We study statistical learning problems where the data space is multimodal and heterogeneous, and constructing a single global predictor is difficult. We address such problems by iteratively identifying high-error regions in the data space and learning specialized predictors for these regions. While the idea of composing localized predictors is not new, our approach is unique in that we actively seek out predictors that clump errors together, making it easier to isolate the problematic regions. When errors are clumped together they are also easier to interpret and resolve through appropriate feature engineering and data preprocessing. We present an error-clumping classification algorithm based on a convex optimization problem, and an efficient stochastic optimization algorithm for this problem. We theoretically motivate our approach with a novel sample complexity analysis for piecewise predictors, and empirically demonstrate its behavior on an illustrative classification problem.",
        "bibtex": "@InProceedings{pmlr-v22-dekel12,\n  title = \t {There's a Hole in My Data Space: Piecewise Predictors for Heterogeneous Learning Problems},\n  author = \t {Dekel, Ofer and Shamir, Ohad},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {291--298},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/dekel12/dekel12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/dekel12.html},\n  abstract = \t {We study statistical learning problems where the data space is multimodal and heterogeneous, and constructing a single global predictor is difficult. We address such problems by iteratively identifying high-error regions in the data space and learning specialized predictors for these regions. While the idea of composing localized predictors is not new, our approach is unique in that we actively seek out predictors that clump errors together, making it easier to isolate the problematic regions. When errors are clumped together they are also easier to interpret and resolve through appropriate feature engineering and data preprocessing. We present an error-clumping classification algorithm based on a convex optimization problem, and an efficient stochastic optimization algorithm for this problem. We theoretically motivate our approach with a novel sample complexity analysis for piecewise predictors, and empirically demonstrate its behavior on an illustrative classification problem.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/dekel12/dekel12.pdf",
        "supp": "",
        "pdf_size": 350846,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=546802075622136827&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Transductive Learning of Structural SVMs via Prior Knowledge Constraints",
        "site": "https://proceedings.mlr.press/v22/yu12.html",
        "author": "Chun-Nam Yu",
        "abstract": "Reducing the number of labeled examples required to learn accurate prediction models is an important problem in structured output prediction. In this paper we propose a new transductive structural SVM algorithm that learns by incorporating prior knowledge constraints on unlabeled data.  Our formulation supports different types of prior knowledge constraints, and can be trained efficiently. Experiments on two citation and advertisement segmentation tasks show that our transductive structural SVM can learn effectively from unlabeled data, achieving similar prediction accuracies when compared against other state-of-art algorithms.",
        "bibtex": "@InProceedings{pmlr-v22-yu12,\n  title = \t {Transductive Learning of Structural SVMs via Prior Knowledge Constraints},\n  author = \t {Yu, Chun-Nam},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1367--1376},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/yu12/yu12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/yu12.html},\n  abstract = \t {Reducing the number of labeled examples required to learn accurate prediction models is an important problem in structured output prediction. In this paper we propose a new transductive structural SVM algorithm that learns by incorporating prior knowledge constraints on unlabeled data.  Our formulation supports different types of prior knowledge constraints, and can be trained efficiently. Experiments on two citation and advertisement segmentation tasks show that our transductive structural SVM can learn effectively from unlabeled data, achieving similar prediction accuracies when compared against other state-of-art algorithms.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/yu12/yu12.pdf",
        "supp": "",
        "pdf_size": 290854,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=207653257228767200&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Dept. of Computing Science & AICML, University of Alberta, AB, Canada",
        "aff_domain": "cs.ualberta.ca",
        "email": "cs.ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Alberta",
        "aff_unique_dep": "Department of Computing Science & AICML",
        "aff_unique_url": "https://www.ualberta.ca",
        "aff_unique_abbr": "UAlberta",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Edmonton",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "UPAL: Unbiased Pool Based Active Learning",
        "site": "https://proceedings.mlr.press/v22/ganti12.html",
        "author": "Ravi Ganti; Alexander Gray",
        "abstract": "In this paper we address the problem of pool based active learning, and provide an algorithm, called UPAL, that works by minimizing the unbiased estimator of the risk of a hypothesis in a given hypothesis space. For the space of linear classifiers and the squared loss we show that UPAL is equivalent to an exponentially weighted average forecaster. Exploiting some recent results regarding the spectra of random matrices allows us to analyze UPAL with squared losses for the noiseless setting. Empirical comparison with an active learner implementation in Vowpal Wabbit, and a previously proposed pool based active learner implementation show good empirical performance and better scalability.",
        "bibtex": "@InProceedings{pmlr-v22-ganti12,\n  title = \t {UPAL: Unbiased Pool Based Active Learning},\n  author = \t {Ganti, Ravi and Gray, Alexander},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {422--431},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/ganti12/ganti12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/ganti12.html},\n  abstract = \t {In this paper we address the problem of pool based active learning, and provide an algorithm, called UPAL, that works by minimizing the unbiased estimator of the risk of a hypothesis in a given hypothesis space. For the space of linear classifiers and the squared loss we show that UPAL is equivalent to an exponentially weighted average forecaster. Exploiting some recent results regarding the spectra of random matrices allows us to analyze UPAL with squared losses for the noiseless setting. Empirical comparison with an active learner implementation in Vowpal Wabbit, and a previously proposed pool based active learner implementation show good empirical performance and better scalability.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/ganti12/ganti12.pdf",
        "supp": "",
        "pdf_size": 384606,
        "gs_citation": 57,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2285203582143391125&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "College of Computing, Georgia Institute of Technology, Atlanta, GA, USA; College of Computing, Georgia Institute of Technology, Atlanta, GA, USA",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Georgia Institute of Technology",
        "aff_unique_dep": "College of Computing",
        "aff_unique_url": "https://www.gatech.edu",
        "aff_unique_abbr": "Georgia Tech",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Atlanta",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Universal Measurement Bounds for Structured Sparse Signal Recovery",
        "site": "https://proceedings.mlr.press/v22/rao12.html",
        "author": "Nikhil Rao; Ben Recht; Robert Nowak",
        "abstract": "Standard compressive sensing results state that to exactly recover an s sparse signal in Rp, one requires O(s log p) measurements. While this bound is extremely useful in practice, often real world signals are not only sparse, but also exhibit structure in the sparsity pattern. We focus on group-structured patterns in this paper. Under this model, groups of signal coefficients are active (or inactive) together. The groups are prede- fined, but the particular set of groups that are active (i.e., in the signal support) must be learned from measurements. We show that exploiting knowledge of groups can further reduce the number of measurements required for exact signal recovery, and derive universal bounds for the number of measurements needed. The bound is universal in the sense that it only depends on the number of groups under consideration, and not the particulars of the groups (e.g., compositions, sizes, ex- tents, overlaps, etc.). Experiments show that our result holds for a variety of overlapping group configurations.",
        "bibtex": "@InProceedings{pmlr-v22-rao12,\n  title = \t {Universal Measurement Bounds for Structured Sparse Signal Recovery},\n  author = \t {Rao, Nikhil and Recht, Ben and Nowak, Robert},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {942--950},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/rao12/rao12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/rao12.html},\n  abstract = \t {Standard compressive sensing results state that to exactly recover an s sparse signal in Rp, one requires O(s log p) measurements. While this bound is extremely useful in practice, often real world signals are not only sparse, but also exhibit structure in the sparsity pattern. We focus on group-structured patterns in this paper. Under this model, groups of signal coefficients are active (or inactive) together. The groups are prede- fined, but the particular set of groups that are active (i.e., in the signal support) must be learned from measurements. We show that exploiting knowledge of groups can further reduce the number of measurements required for exact signal recovery, and derive universal bounds for the number of measurements needed. The bound is universal in the sense that it only depends on the number of groups under consideration, and not the particulars of the groups (e.g., compositions, sizes, ex- tents, overlaps, etc.). Experiments show that our result holds for a variety of overlapping group configurations.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/rao12/rao12.pdf",
        "supp": "",
        "pdf_size": 324829,
        "gs_citation": 68,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11590905286253927757&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Using More Data to Speed-up Training Time",
        "site": "https://proceedings.mlr.press/v22/shalev-shwartz12.html",
        "author": "Shai Shalev-Shwartz; Ohad Shamir; Eran Tromer",
        "abstract": "In many recent applications, data is plentiful. By now, we have a rather clear understanding of how more data can be used to improve the accuracy of learning algorithms. Recently, there has been a growing interest in understanding how more data can be leveraged to reduce the required training runtime. In this paper, we study the runtime of learning as a function of the number of available training examples, and underscore the main high-level techniques. We provide the first formal positive result showing that even in the unrealizable case, the runtime can decrease exponentially while only  requiring a polynomial growth of the number of examples. Our construction corresponds to a synthetic learning problem and an interesting open question is whether the tradeoff can be shown for more natural learning problems. We spell out several interesting candidates of natural learning problems for which we conjecture that there is a tradeoff between computational and sample complexity.",
        "bibtex": "@InProceedings{pmlr-v22-shalev-shwartz12,\n  title = \t {Using More Data to Speed-up Training Time},\n  author = \t {Shalev-Shwartz, Shai and Shamir, Ohad and Tromer, Eran},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1019--1027},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/shalev-shwartz12/shalev-shwartz12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/shalev-shwartz12.html},\n  abstract = \t {In many recent applications, data is plentiful. By now, we have a rather clear understanding of how more data can be used to improve the accuracy of learning algorithms. Recently, there has been a growing interest in understanding how more data can be leveraged to reduce the required training runtime. In this paper, we study the runtime of learning as a function of the number of available training examples, and underscore the main high-level techniques. We provide the first formal positive result showing that even in the unrealizable case, the runtime can decrease exponentially while only  requiring a polynomial growth of the number of examples. Our construction corresponds to a synthetic learning problem and an interesting open question is whether the tradeoff can be shown for more natural learning problems. We spell out several interesting candidates of natural learning problems for which we conjecture that there is a tradeoff between computational and sample complexity.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/shalev-shwartz12/shalev-shwartz12.pdf",
        "supp": "",
        "pdf_size": 374921,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12535458573212184349&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "title": "Variable Selection for Gaussian Graphical Models",
        "site": "https://proceedings.mlr.press/v22/honorio12.html",
        "author": "Jean Honorio; Dimitris Samaras; Irina Rish; Guillermo Cecchi",
        "abstract": "We present a variable-selection structure learning approach for Gaussian graphical models. Unlike standard sparseness promoting techniques, our method aims at selecting the most-important variables besides simply sparsifying the set of edges. Through simulations, we show that our method outperforms the state-of-the-art in recovering the ground truth model. Our method also exhibits better generalization performance in a wide range of complex real-world datasets: brain fMRI, gene expression, NASDAQ stock prices and world weather. We also show that our resulting networks are more interpretable in the context of brain fMRI analysis, while retaining discriminability. From an optimization perspective, we show that a block coordinate descent method generates a sequence of positive definite solutions. Thus, we reduce the original problem into a sequence of strictly convex (\\ell_1,\\ell_p) regularized quadratic minimization subproblems for p\u2208{2,\u221e}. Our algorithm is well founded since the optimal solution of the maximization problem is unique and bounded.",
        "bibtex": "@InProceedings{pmlr-v22-honorio12,\n  title = \t {Variable Selection for Gaussian Graphical Models},\n  author = \t {Honorio, Jean and Samaras, Dimitris and Rish, Irina and Cecchi, Guillermo},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {538--546},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/honorio12/honorio12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/honorio12.html},\n  abstract = \t {We present a variable-selection structure learning approach for Gaussian graphical models. Unlike standard sparseness promoting techniques, our method aims at selecting the most-important variables besides simply sparsifying the set of edges. Through simulations, we show that our method outperforms the state-of-the-art in recovering the ground truth model. Our method also exhibits better generalization performance in a wide range of complex real-world datasets: brain fMRI, gene expression, NASDAQ stock prices and world weather. We also show that our resulting networks are more interpretable in the context of brain fMRI analysis, while retaining discriminability. From an optimization perspective, we show that a block coordinate descent method generates a sequence of positive definite solutions. Thus, we reduce the original problem into a sequence of strictly convex (\\ell_1,\\ell_p) regularized quadratic minimization subproblems for p\u2208{2,\u221e}. Our algorithm is well founded since the optimal solution of the maximization problem is unique and bounded.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/honorio12/honorio12.pdf",
        "supp": "",
        "pdf_size": 1073415,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9414860430599029241&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Stony Brook University; Stony Brook University; IBM Thomas J. Watson Research Center; IBM Thomas J. Watson Research Center",
        "aff_domain": "cs.sunysb.edu;cs.sunysb.edu;us.ibm.com;us.ibm.com",
        "email": "cs.sunysb.edu;cs.sunysb.edu;us.ibm.com;us.ibm.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;1",
        "aff_unique_norm": "Stony Brook University;IBM",
        "aff_unique_dep": ";Research",
        "aff_unique_url": "https://www.stonybrook.edu;https://www.ibm.com/research",
        "aff_unique_abbr": "SBU;IBM",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Yorktown Heights",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Wilks\u2019 phenomenon and penalized likelihood-ratio test for nonparametric curve registration",
        "site": "https://proceedings.mlr.press/v22/dalalyan12.html",
        "author": "Arnak Dalalyan; Olivier Collier",
        "abstract": "The problem of curve registration appears in many different areas of applications ranging from neuroscience to road traffic modeling. In the present work, we propose a nonparametric testing framework in which we develop a generalized likelihood ratio test to perform curve registration. We first prove that,  under the null hypothesis,  the resulting test statistic is asymptotically distributed as a chi-squared random variable (Wilks\u2019 phenomenon). We also prove that the proposed test is consistent, \textiti.e., its power is asymptotically equal to 1. Finite sample properties of the proposed methodology are demonstrated by numerical simulations.",
        "bibtex": "@InProceedings{pmlr-v22-dalalyan12,\n  title = \t {Wilks' phenomenon and penalized likelihood-ratio test for nonparametric curve registration},\n  author = \t {Dalalyan, Arnak and Collier, Olivier},\n  booktitle = \t {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {264--272},\n  year = \t {2012},\n  editor = \t {Lawrence, Neil D. and Girolami, Mark},\n  volume = \t {22},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {La Palma, Canary Islands},\n  month = \t {21--23 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v22/dalalyan12/dalalyan12.pdf},\n  url = \t {https://proceedings.mlr.press/v22/dalalyan12.html},\n  abstract = \t {The problem of curve registration appears in many different areas of applications ranging from neuroscience to road traffic modeling. In the present work, we propose a nonparametric testing framework in which we develop a generalized likelihood ratio test to perform curve registration. We first prove that,  under the null hypothesis,  the resulting test statistic is asymptotically distributed as a chi-squared random variable (Wilks\u2019 phenomenon). We also prove that the proposed test is consistent, \textiti.e., its power is asymptotically equal to 1. Finite sample properties of the proposed methodology are demonstrated by numerical simulations.}\n}",
        "pdf": "http://proceedings.mlr.press/v22/dalalyan12/dalalyan12.pdf",
        "supp": "",
        "pdf_size": 563938,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15223805416745105049&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "ENSAE / CREST; LIGM / Universit\u00b4e Paris-Est",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "\u00c9cole Nationale de la Statistique et de l'Administration \u00c9conomique;Universit\u00e9 Paris-Est",
        "aff_unique_dep": "CREST - Centre de Recherche en \u00c9conomie et Statistique;LIGM",
        "aff_unique_url": "https://www.ensae.fr;https://www.univ-mlv.fr",
        "aff_unique_abbr": "ENSAE;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "France"
    }
]