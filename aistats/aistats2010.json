[
    {
        "id": "9860cb899a",
        "title": "A Markov-Chain Monte Carlo Approach to Simultaneous Localization and Mapping",
        "site": "https://proceedings.mlr.press/v9/torma10a.html",
        "author": "Peter Torma; Andr\u00e1s Gy\u00f6rgy; Csaba Szepesv\u00e1ri",
        "abstract": "A Markov-Chain Monte Carlo based algorithm is provided to solve the simultaneous localization and mapping (SLAM) problem with general dynamical and observation models under open-loop control and provided that the map-representation is finite dimensional. To our knowledge this is the first provably consistent yet (close-to) practical solution to this problem. The superiority of our algorithm over alternative SLAM algorithms is demonstrated in a difficult loop closing situation.",
        "bibtex": "@InProceedings{pmlr-v9-torma10a,\n  title = \t {A Markov-Chain Monte Carlo Approach to Simultaneous Localization and Mapping},\n  author = \t {Torma, Peter and Gy\u00f6rgy, Andr\u00e1s and Szepesv\u00e1ri, Csaba},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {852--859},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/torma10a/torma10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/torma10a.html},\n  abstract = \t {A Markov-Chain Monte Carlo based algorithm is provided to solve the simultaneous localization and mapping (SLAM) problem with general dynamical and observation models under open-loop control and provided that the map-representation is finite dimensional. To our knowledge this is the first provably consistent yet (close-to) practical solution to this problem. The superiority of our algorithm over alternative SLAM algorithms is demonstrated in a difficult loop closing situation.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/torma10a/torma10a.pdf",
        "supp": "",
        "pdf_size": 2020362,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12298376487591424142&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": "GusGus AB, Hungary; Machine Learning Res. Group, MTA SZTAKI, Hungary; Dept. of Computing Sciences, University of Alberta, Canada",
        "aff_domain": "; ; ",
        "email": "; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "GusGus AB;MTA SZTAKI;University of Alberta",
        "aff_unique_dep": ";Machine Learning Res. Group;Dept. of Computing Sciences",
        "aff_unique_url": ";https://www.sztaki.hu;https://www.ualberta.ca",
        "aff_unique_abbr": ";MTA SZTAKI;UAlberta",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "Hungary;Canada"
    },
    {
        "id": "d19d1c7cba",
        "title": "A Potential-based Framework for Online Multi-class Learning with Partial Feedback",
        "site": "https://proceedings.mlr.press/v9/wang10a.html",
        "author": "Shijun Wang; Rong Jin; Hamed Valizadegan",
        "abstract": "We study the problem of online multi-class learning with partial feedback: in each trial of online learning, instead of providing the true class label for a given instance, the oracle will only reveal to the learner if the predicted class label is correct. We present a general framework for online multi-class learning with partial feedback that adapts the potential-based gradient descent approaches (Cesa-Bianchi & Lugosi, 2006). The generality of the proposed framework is verified by the fact that Banditron (Kakade et al., 2008) is indeed a special case of our work if the potential function is set to be the squared $L_2$ norm of the weight vector. We propose an exponential gradient algorithm for online multi-class learning with partial feedback. Compared to the Banditron algorithm, the exponential gradient algorithm is advantageous in that its mistake bound is independent from the dimension of data, making it suitable for classifying high dimensional data. Our empirical study with four data sets show that the proposed algorithm for online learning with partial feedback is more effective than the Banditron algorithm.",
        "bibtex": "@InProceedings{pmlr-v9-wang10a,\n  title = \t {A Potential-based Framework for Online Multi-class Learning with Partial Feedback},\n  author = \t {Wang, Shijun and Jin, Rong and Valizadegan, Hamed},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {900--907},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/wang10a/wang10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/wang10a.html},\n  abstract = \t {We study the problem of online multi-class learning with partial feedback: in each trial of online learning, instead of providing the true class label for a given instance, the oracle will only reveal to the learner if the predicted class label is correct. We present a general framework for online multi-class learning with partial feedback that adapts the potential-based gradient descent approaches (Cesa-Bianchi & Lugosi, 2006). The generality of the proposed framework is verified by the fact that Banditron (Kakade et al., 2008) is indeed a special case of our work if the potential function is set to be the squared $L_2$ norm of the weight vector. We propose an exponential gradient algorithm for online multi-class learning with partial feedback. Compared to the Banditron algorithm, the exponential gradient algorithm is advantageous in that its mistake bound is independent from the dimension of data, making it suitable for classifying high dimensional data. Our empirical study with four data sets show that the proposed algorithm for online learning with partial feedback is more effective than the Banditron algorithm.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/wang10a/wang10a.pdf",
        "supp": "",
        "pdf_size": 1132941,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1013092872450586434&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Radiology and Imaging Sciences, National Institutes of Health; Computer Science and Engineering, Michigan State University; Computer Science and Engineering, Michigan State University",
        "aff_domain": "cc.nih.gov;cse.msu.edu;cse.msu.edu",
        "email": "cc.nih.gov;cse.msu.edu;cse.msu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "National Institutes of Health;Michigan State University",
        "aff_unique_dep": "Radiology and Imaging Sciences;Computer Science and Engineering",
        "aff_unique_url": "https://www.nih.gov;https://www.msu.edu",
        "aff_unique_abbr": "NIH;MSU",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";East Lansing",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d000aacbe9",
        "title": "A Regularization Approach to Nonlinear Variable Selection",
        "site": "https://proceedings.mlr.press/v9/rosasco10a.html",
        "author": "Lorenzo Rosasco; Matteo Santoro; Sofia Mosci; Alessandro Verri; Silvia Villa",
        "abstract": "In this paper we consider a regularization approach to variable selection when the regression function depends nonlinearly on a few input variables. The proposed method is based on a regularized least square estimator penalizing large values of the partial derivatives. An efficient iterative procedure is proposed to solve the underlying variational problem, and its convergence is proved. The empirical properties of the obtained estimator are tested both for prediction and variable selection. The algorithm compares favorably to more standard ridge regression and L1 regularization schemes.",
        "bibtex": "@InProceedings{pmlr-v9-rosasco10a,\n  title = \t {A Regularization Approach to Nonlinear Variable Selection},\n  author = \t {Rosasco, Lorenzo and Santoro, Matteo and Mosci, Sofia and Verri, Alessandro and Villa, Silvia},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {653--660},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/rosasco10a/rosasco10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/rosasco10a.html},\n  abstract = \t {In this paper we consider a regularization approach to variable selection when the regression function depends nonlinearly on a few input variables. The proposed method is based on a regularized least square estimator penalizing large values of the partial derivatives. An efficient iterative procedure is proposed to solve the underlying variational problem, and its convergence is proved. The empirical properties of the obtained estimator are tested both for prediction and variable selection. The algorithm compares favorably to more standard ridge regression and L1 regularization schemes.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/rosasco10a/rosasco10a.pdf",
        "supp": "",
        "pdf_size": 925418,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10713579180220943980&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "CBCL, McGovern Institute, MIT; DISI, Universit`a di Genova; DISI, Universit`a di Genova; DIMA, Universit`a di Genova; DIMA, Universit`a di Genova",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1;1",
        "aff_unique_norm": "Massachusetts Institute of Technology;Universit\u00e0 di Genova",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory (CSAIL);DISI",
        "aff_unique_url": "https://web.mit.edu/;https://www.unige.it",
        "aff_unique_abbr": "MIT;",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;1;1;1;1",
        "aff_country_unique": "United States;Italy"
    },
    {
        "id": "3a2250679e",
        "title": "A Weighted Multi-Sequence Markov Model For Brain Lesion Segmentation",
        "site": "https://proceedings.mlr.press/v9/forbes10a.html",
        "author": "Florence Forbes; Senan Doyle; Daniel Garcia\u2013Lorenzo; Christian Barillot; Michel Dojat",
        "abstract": "We propose a technique for fusing the output of multiple Magnetic Resonance (MR) sequences to robustly and accurately segment brain lesions. It is based on an augmented multi-sequence Hidden Markov model that includes additional weight variables to account for the relative importance and control the impact of each sequence. The augmented framework has the advantage of allowing 1) the incorporation of expert knowledge on the  a priori relevant information content of each sequence and 2) a weighting scheme which is modified adaptively according to the data and the segmentation task under consideration. The model, applied to the detection of multiple sclerosis and stroke lesions shows promising results.",
        "bibtex": "@InProceedings{pmlr-v9-forbes10a,\n  title = \t {A Weighted Multi-Sequence Markov Model For Brain Lesion Segmentation},\n  author = \t {Forbes, Florence and Doyle, Senan and Garcia\u2013Lorenzo, Daniel and Barillot, Christian and Dojat, Michel},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {225--232},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/forbes10a/forbes10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/forbes10a.html},\n  abstract = \t {We propose a technique for fusing the output of multiple Magnetic Resonance (MR) sequences to robustly and accurately segment brain lesions. It is based on an augmented multi-sequence Hidden Markov model that includes additional weight variables to account for the relative importance and control the impact of each sequence. The augmented framework has the advantage of allowing 1) the incorporation of expert knowledge on the  a priori relevant information content of each sequence and 2) a weighting scheme which is modified adaptively according to the data and the segmentation task under consideration. The model, applied to the detection of multiple sclerosis and stroke lesions shows promising results.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/forbes10a/forbes10a.pdf",
        "supp": "",
        "pdf_size": 934664,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15476352137601827370&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": "INRIA Grenoble Rh\u00f4ne-Alpes, LJK, Mistis team, Montbonnot, France; INRIA Rennes Bretagne Atlantique, Visages team, Rennes, France; INSERM, GIN, Grenoble, France; INRIA Grenoble Rh\u00f4ne-Alpes, LJK, Mistis team, Montbonnot, France; INRIA Rennes Bretagne Atlantique, Visages team, Rennes, France",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0;1",
        "aff_unique_norm": "INRIA Grenoble Rh\u00f4ne-Alpes;INRIA Rennes Bretagne Atlantique;Institut National de la Sant\u00e9 et de la Recherche M\u00e9dicale",
        "aff_unique_dep": "LJK, Mistis team;Visages team;GIN",
        "aff_unique_url": "https://www.inria.fr;https://www.inria.fr/centre-rennes;https://www.inserm.fr",
        "aff_unique_abbr": "INRIA;INRIA;INSERM",
        "aff_campus_unique_index": "0;1;0;0;1",
        "aff_campus_unique": "Grenoble;Rennes",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "9d77f8b566",
        "title": "A generalization of the Multiple-try Metropolis algorithm for Bayesian estimation and model selection",
        "site": "https://proceedings.mlr.press/v9/pandolfi10a.html",
        "author": "Silvia Pandolfi; Francesco Bartolucci; Nial Friel",
        "abstract": "We propose a generalization of the Multiple-try Metropolis (MTM) algorithm of Liu et al. (2000), which is based on drawing several proposals at each step and randomly choosing one of them on the basis of weights that may be arbitrary chosen. In particular, for Bayesian estimation we also introduce a method based on weights depending on a quadratic approximation of the posterior distribution. The resulting algorithm cannot be reformulated as an MTM algorithm and leads to a comparable gain of efficiency with a lower computational effort. We also outline the extension of the proposed strategy, and then of the MTM strategy, to Bayesian model selection, casting it in a Reversible Jump framework. The approach is illustrated by real examples.",
        "bibtex": "@InProceedings{pmlr-v9-pandolfi10a,\n  title = \t {A generalization of the Multiple-try Metropolis algorithm for Bayesian estimation and model selection},\n  author = \t {Pandolfi, Silvia and Bartolucci, Francesco and Friel, Nial},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {581--588},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/pandolfi10a/pandolfi10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/pandolfi10a.html},\n  abstract = \t {We propose a generalization of the Multiple-try Metropolis (MTM) algorithm of Liu et al. (2000), which is based on drawing several proposals at each step and randomly choosing one of them on the basis of weights that may be arbitrary chosen. In particular, for Bayesian estimation we also introduce a method based on weights depending on a quadratic approximation of the posterior distribution. The resulting algorithm cannot be reformulated as an MTM algorithm and leads to a comparable gain of efficiency with a lower computational effort. We also outline the extension of the proposed strategy, and then of the MTM strategy, to Bayesian model selection, casting it in a Reversible Jump framework. The approach is illustrated by real examples.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/pandolfi10a/pandolfi10a.pdf",
        "supp": "",
        "pdf_size": 1151480,
        "gs_citation": 51,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16468657729340815914&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "dc9307725a",
        "title": "A highly efficient blocked Gibbs sampler reconstruction of multidimensional NMR spectra",
        "site": "https://proceedings.mlr.press/v9/yoon10a.html",
        "author": "Ji Won Yoon; Simon Wilson; K. Hun Mok",
        "abstract": "Projection Reconstruction Nuclear Magnetic Resonance (PR-NMR) is a new technique to generate multi-dimensional NMR spectra, which have discrete features that are relatively sparsely distributed in space. A small number of projections from lower dimensional NMR spectra are used to reconstruct the multi-dimensional NMR spectra. We propose an efficient algorithm which employs a blocked Gibbs sampler to accurately reconstruct NMR spectra. This statistical method generates samples in Bayesian scheme. Our proposed algorithm is tested on a set of six projections derived from the three-dimensional 700 MHz HNCO spectrum of HasA, a 187-residue heme binding protein.",
        "bibtex": "@InProceedings{pmlr-v9-yoon10a,\n  title = \t {A highly efficient blocked Gibbs sampler reconstruction of multidimensional NMR spectra},\n  author = \t {Yoon, Ji Won and Wilson, Simon and Mok, K. Hun},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {940--947},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/yoon10a/yoon10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/yoon10a.html},\n  abstract = \t {Projection Reconstruction Nuclear Magnetic Resonance (PR-NMR) is a new technique to generate multi-dimensional NMR spectra, which have discrete features that are relatively sparsely distributed in space. A small number of projections from lower dimensional NMR spectra are used to reconstruct the multi-dimensional NMR spectra. We propose an efficient algorithm which employs a blocked Gibbs sampler to accurately reconstruct NMR spectra. This statistical method generates samples in Bayesian scheme. Our proposed algorithm is tested on a set of six projections derived from the three-dimensional 700 MHz HNCO spectrum of HasA, a 187-residue heme binding protein.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/yoon10a/yoon10a.pdf",
        "supp": "",
        "pdf_size": 1237561,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14325901564073107447&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "3e9f729c69",
        "title": "Active Sequential Learning with Tactile Feedback",
        "site": "https://proceedings.mlr.press/v9/saal10a.html",
        "author": "Hannes Saal; Jo\u2013Anne Ting; Sethu Vijayakumar",
        "abstract": "We consider the problem of tactile discrimination, with the goal of estimating an underlying state parameter in a sequential setting. If the data is continuous and high-dimensional, collecting enough representative data samples becomes difficult. We present a framework that uses active learning to help with the sequential gathering of data samples, using information-theoretic criteria to find optimal actions at each time step. We consider two approaches to recursively update the state parameter belief: an analytical Gaussian approximation and a Monte Carlo sampling method. We show how both active frameworks improve convergence, demonstrating results on a real robotic hand-arm system that estimates the viscosity of liquids from tactile feedback data.",
        "bibtex": "@InProceedings{pmlr-v9-saal10a,\n  title = \t {Active Sequential Learning with Tactile Feedback},\n  author = \t {Saal, Hannes and Ting, Jo\u2013Anne and Vijayakumar, Sethu},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {677--684},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/saal10a/saal10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/saal10a.html},\n  abstract = \t {We consider the problem of tactile discrimination, with the goal of estimating an underlying state parameter in a sequential setting. If the data is continuous and high-dimensional, collecting enough representative data samples becomes difficult. We present a framework that uses active learning to help with the sequential gathering of data samples, using information-theoretic criteria to find optimal actions at each time step. We consider two approaches to recursively update the state parameter belief: an analytical Gaussian approximation and a Monte Carlo sampling method. We show how both active frameworks improve convergence, demonstrating results on a real robotic hand-arm system that estimates the viscosity of liquids from tactile feedback data.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/saal10a/saal10a.pdf",
        "supp": "",
        "pdf_size": 595945,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15942102549157792439&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "University of Edinburgh; University of British Columbia + University of Edinburgh; University of Edinburgh",
        "aff_domain": "ed.ac.uk;acm.org;ed.ac.uk",
        "email": "ed.ac.uk;acm.org;ed.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+0;0",
        "aff_unique_norm": "University of Edinburgh;University of British Columbia",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ed.ac.uk;https://www.ubc.ca",
        "aff_unique_abbr": "Edinburgh;UBC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1+0;0",
        "aff_country_unique": "United Kingdom;Canada"
    },
    {
        "id": "834cfaba19",
        "title": "An Alternative Prior Process for Nonparametric Bayesian Clustering",
        "site": "https://proceedings.mlr.press/v9/wallach10a.html",
        "author": "Hanna Wallach; Shane Jensen; Lee Dicker; Katherine Heller",
        "abstract": "Prior distributions play a crucial role in Bayesian approaches to clustering. Two commonly-used prior distributions are the Dirichlet and Pitman-Yor processes. In this paper, we investigate the predictive probabilities that underlie these processes, and the implicit \u201crich-get-richer\u201d characteristic of the resulting partitions. We explore an alternative prior for nonparametric Bayesian clustering, the uniform process, for applications where the \u201crich-get-richer\u201d property is undesirable. We also explore the cost of this new process: partitions are no longer exchangeable with respect to the ordering of variables. We present new asymptotic and simulation-based results for the clustering characteristics of the uniform process and compare these with known results for the Dirichlet and Pitman-Yor processes. Finally, we compare performance on a real document clustering task, demonstrating the practical advantage of the uniform process despite its lack of exchangeability over orderings.",
        "bibtex": "@InProceedings{pmlr-v9-wallach10a,\n  title = \t {An Alternative Prior Process for Nonparametric Bayesian Clustering},\n  author = \t {Wallach, Hanna and Jensen, Shane and Dicker, Lee and Heller, Katherine},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {892--899},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/wallach10a/wallach10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/wallach10a.html},\n  abstract = \t {Prior distributions play a crucial role in Bayesian approaches to clustering. Two commonly-used prior distributions are the Dirichlet and Pitman-Yor processes. In this paper, we investigate the predictive probabilities that underlie these processes, and the implicit \u201crich-get-richer\u201d characteristic of the resulting partitions. We explore an alternative prior for nonparametric Bayesian clustering, the uniform process, for applications where the \u201crich-get-richer\u201d property is undesirable. We also explore the cost of this new process: partitions are no longer exchangeable with respect to the ordering of variables. We present new asymptotic and simulation-based results for the clustering characteristics of the uniform process and compare these with known results for the Dirichlet and Pitman-Yor processes. Finally, we compare performance on a real document clustering task, demonstrating the practical advantage of the uniform process despite its lack of exchangeability over orderings.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/wallach10a/wallach10a.pdf",
        "supp": "",
        "pdf_size": 1544809,
        "gs_citation": 84,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9101959983818337905&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "814962940a",
        "title": "Approximate parameter inference in a stochastic reaction-diffusion model",
        "site": "https://proceedings.mlr.press/v9/ruttor10a.html",
        "author": "Andreas Ruttor; Manfred Opper",
        "abstract": "We present an approximate inference approach to parameter estimation in a spatio-temporal stochastic process of the reaction-diffusion type. The continuous space limit of an inference method for Markov jump processes leads to an approximation which is related to a spatial Gaussian process. An efficient solution in feature space using a Fourier basis is applied to inference on simulational data.",
        "bibtex": "@InProceedings{pmlr-v9-ruttor10a,\n  title = \t {Approximate parameter inference in a stochastic reaction-diffusion model},\n  author = \t {Ruttor, Andreas and Opper, Manfred},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {669--676},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/ruttor10a/ruttor10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/ruttor10a.html},\n  abstract = \t {We present an approximate inference approach to parameter estimation in a spatio-temporal stochastic process of the reaction-diffusion type. The continuous space limit of an inference method for Markov jump processes leads to an approximation which is related to a spatial Gaussian process. An efficient solution in feature space using a Fourier basis is applied to inference on simulational data.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/ruttor10a/ruttor10a.pdf",
        "supp": "",
        "pdf_size": 165646,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4127664150026755854&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "TU Berlin, Germany; TU Berlin, Germany",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Technical University of Berlin",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.tu-berlin.de",
        "aff_unique_abbr": "TU Berlin",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "6b33a2d770",
        "title": "Approximation of hidden Markov models by mixtures of experts with application to particle filtering",
        "site": "https://proceedings.mlr.press/v9/olsson10a.html",
        "author": "Jimmy Olsson; Jonas Str\u00f6jby",
        "abstract": "Selecting conveniently the proposal kernel and the adjustment multiplier weights of the auxiliary particle filter may increase significantly the accuracy and computational efficiency of the method. However, in practice the optimal proposal kernel and multiplier weights are seldom known. In this paper we present a simulation-based method for constructing offline an approximation of these quantities that makes the filter close to fully adapted at a reasonable computational cost. The approximation is constructed as a mixture of experts optimised through an efficient stochastic approximation algorithm. The method is illustrated on two simulated examples.",
        "bibtex": "@InProceedings{pmlr-v9-olsson10a,\n  title = \t {Approximation of hidden Markov models by mixtures of experts with application to particle filtering},\n  author = \t {Olsson, Jimmy and Str\u00f6jby, Jonas},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {573--580},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/olsson10a/olsson10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/olsson10a.html},\n  abstract = \t {Selecting conveniently the proposal kernel and the adjustment multiplier weights of the auxiliary particle filter may increase significantly the accuracy and computational efficiency of the method. However, in practice the optimal proposal kernel and multiplier weights are seldom known. In this paper we present a simulation-based method for constructing offline an approximation of these quantities that makes the filter close to fully adapted at a reasonable computational cost. The approximation is constructed as a mixture of experts optimised through an efficient stochastic approximation algorithm. The method is illustrated on two simulated examples.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/olsson10a/olsson10a.pdf",
        "supp": "",
        "pdf_size": 3060225,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12553868444946275260&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Division of Mathematical Statistics, Centre for Mathematical Sciences, Lund University; Division of Mathematical Statistics, Centre for Mathematical Sciences, Lund University",
        "aff_domain": "maths.lth.se;maths.lth.se",
        "email": "maths.lth.se;maths.lth.se",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Lund University",
        "aff_unique_dep": "Division of Mathematical Statistics, Centre for Mathematical Sciences",
        "aff_unique_url": "https://www.lunduniversity.lu.se",
        "aff_unique_abbr": "LU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Sweden"
    },
    {
        "id": "281a35f139",
        "title": "Bayesian Gaussian Process Latent Variable Model",
        "site": "https://proceedings.mlr.press/v9/titsias10a.html",
        "author": "Michalis Titsias; Neil D. Lawrence",
        "abstract": "We introduce a variational inference framework for training the Gaussian process latent variable model and thus performing Bayesian nonlinear dimensionality reduction. This method allows us to variationally integrate out the input variables of the Gaussian process and compute a lower bound on the exact marginal likelihood of the nonlinear latent variable model. The maximization of the variational lower bound provides a Bayesian training procedure that is robust to overfitting and can automatically select the dimensionality of the nonlinear latent space. We demonstrate our method on real world datasets. The focus in this paper is on dimensionality reduction problems, but the methodology is more general. For example, our algorithm is immediately applicable for training Gaussian process models in the presence of missing or uncertain inputs.",
        "bibtex": "@InProceedings{pmlr-v9-titsias10a,\n  title = \t {Bayesian Gaussian Process Latent Variable Model},\n  author = \t {Titsias, Michalis and Lawrence, Neil D.},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {844--851},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/titsias10a/titsias10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/titsias10a.html},\n  abstract = \t {We introduce a variational inference framework for training the Gaussian process latent variable model and thus performing Bayesian nonlinear dimensionality reduction. This method allows us to variationally integrate out the input variables of the Gaussian process and compute a lower bound on the exact marginal likelihood of the nonlinear latent variable model. The maximization of the variational lower bound provides a Bayesian training procedure that is robust to overfitting and can automatically select the dimensionality of the nonlinear latent space. We demonstrate our method on real world datasets. The focus in this paper is on dimensionality reduction problems, but the methodology is more general. For example, our algorithm is immediately applicable for training Gaussian process models in the presence of missing or uncertain inputs.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/titsias10a/titsias10a.pdf",
        "supp": "",
        "pdf_size": 409199,
        "gs_citation": 657,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1502456449160061709&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "School of Computer Science, University of Manchester; School of Computer Science, University of Manchester",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Manchester",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.manchester.ac.uk",
        "aff_unique_abbr": "UoM",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Manchester",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2bae8c0ebc",
        "title": "Bayesian Generalized Kernel Models",
        "site": "https://proceedings.mlr.press/v9/zhang10d.html",
        "author": "Zhihua Zhang; Guang Dai; Donghui Wang; Michael I. Jordan",
        "abstract": "We propose a fully Bayesian approach for generalized kernel models (GKMs), which are extensions of generalized linear models in the feature space induced by a reproducing kernel. We place a mixture of a point-mass distribution and Silverman\u2019s g-prior on the regression vector of GKMs. This mixture prior allows a fraction of the regression vector to be zero.  Thus, it serves for sparse modeling and Bayesian computation.  For inference, we exploit data augmentation methodology to develop a Markov chain Monte Carlo (MCMC) algorithm in which the reversible jump method is used for model selection and a Bayesian model averaging method is used for posterior prediction.",
        "bibtex": "@InProceedings{pmlr-v9-zhang10d,\n  title = \t {Bayesian Generalized Kernel Models},\n  author = \t {Zhang, Zhihua and Dai, Guang and Wang, Donghui and Jordan, Michael I.},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {972--979},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/zhang10d/zhang10d.pdf},\n  url = \t {https://proceedings.mlr.press/v9/zhang10d.html},\n  abstract = \t {We propose a fully Bayesian approach for generalized kernel models (GKMs), which are extensions of generalized linear models in the feature space induced by a reproducing kernel. We place a mixture of a point-mass distribution and Silverman\u2019s g-prior on the regression vector of GKMs. This mixture prior allows a fraction of the regression vector to be zero.  Thus, it serves for sparse modeling and Bayesian computation.  For inference, we exploit data augmentation methodology to develop a Markov chain Monte Carlo (MCMC) algorithm in which the reversible jump method is used for model selection and a Bayesian model averaging method is used for posterior prediction.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/zhang10d/zhang10d.pdf",
        "supp": "",
        "pdf_size": 959276,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5871486341996743870&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "College of Comp. Sci. and Tech., Zhejiang University, Zhejiang 310027, China; College of Comp. Sci. and Tech., Zhejiang University, Zhejiang 310027, China; College of Comp. Sci. and Tech., Zhejiang University, Zhejiang 310027, China; Depts. of EECS and Statistics, University of California, Berkeley, Berkeley, CA 94720, USA",
        "aff_domain": "cs.zju.edu.cn;gmail.com;gmail.com;cs.berkeley.edu",
        "email": "cs.zju.edu.cn;gmail.com;gmail.com;cs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Zhejiang University;University of California, Berkeley",
        "aff_unique_dep": "College of Computer Science and Technology;Department of Electrical Engineering and Computer Sciences, Department of Statistics",
        "aff_unique_url": "http://www.zju.edu.cn;https://www.berkeley.edu",
        "aff_unique_abbr": "ZJU;UC Berkeley",
        "aff_campus_unique_index": "0;0;0;1",
        "aff_campus_unique": "Zhejiang;Berkeley",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "fd601107fd",
        "title": "Bayesian Online Learning for Multi-label and Multi-variate Performance Measures",
        "site": "https://proceedings.mlr.press/v9/zhang10b.html",
        "author": "Xinhua Zhang; Thore Graepel; Ralf Herbrich",
        "abstract": "Many real world applications employ multi-variate performance measures and each example can belong to multiple classes. The currently most popular approaches train an SVM for each class, followed by ad hoc thresholding.  Probabilistic models using Bayesian decision theory are also commonly adopted.  In this paper, we propose a Bayesian online multi-label classification framework (BOMC) which learns a probabilistic linear classifier. The likelihood is modeled by a graphical model similar to TrueSkill$^\\text{TM}$, and inference is based on Gaussian density filtering with expectation propagation.  Using samples from the posterior, we label the testing data by maximizing the expected $F_1$-score. Our experiments on Reuters1-v2 dataset show BOMC compares favorably to the state-of-the-art online learners in macro-averaged $F_1$-score and training time.",
        "bibtex": "@InProceedings{pmlr-v9-zhang10b,\n  title = \t {Bayesian Online Learning for Multi-label and Multi-variate Performance Measures},\n  author = \t {Zhang, Xinhua and Graepel, Thore and Herbrich, Ralf},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {956--963},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/zhang10b/zhang10b.pdf},\n  url = \t {https://proceedings.mlr.press/v9/zhang10b.html},\n  abstract = \t {Many real world applications employ multi-variate performance measures and each example can belong to multiple classes. The currently most popular approaches train an SVM for each class, followed by ad hoc thresholding.  Probabilistic models using Bayesian decision theory are also commonly adopted.  In this paper, we propose a Bayesian online multi-label classification framework (BOMC) which learns a probabilistic linear classifier. The likelihood is modeled by a graphical model similar to TrueSkill$^\\text{TM}$, and inference is based on Gaussian density filtering with expectation propagation.  Using samples from the posterior, we label the testing data by maximizing the expected $F_1$-score. Our experiments on Reuters1-v2 dataset show BOMC compares favorably to the state-of-the-art online learners in macro-averaged $F_1$-score and training time.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/zhang10b/zhang10b.pdf",
        "supp": "",
        "pdf_size": 1105270,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13233314257221883320&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "University of Alberta; Microsoft Research, Cambridge; Microsoft Research, Cambridge",
        "aff_domain": "gmail.com;microsoft.com;microsoft.com",
        "email": "gmail.com;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of Alberta;Microsoft Research",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ualberta.ca;https://www.microsoft.com/en-us/research/group/cambridge",
        "aff_unique_abbr": "UAlberta;MSR",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "Canada;United Kingdom"
    },
    {
        "id": "afd7eb6fd1",
        "title": "Bayesian structure discovery in Bayesian networks with less space",
        "site": "https://proceedings.mlr.press/v9/parviainen10a.html",
        "author": "Pekka Parviainen; Mikko Koivisto",
        "abstract": "Current exact algorithms for score-based structure discovery in Bayesian networks on $n$ nodes run in time and space within a polynomial factor of $2^n$. For practical use, the space requirement is the bottleneck, which motivates trading space against time. Here, previous results on finding an optimal network structure in less space are extended in two directions. First, we consider the problem of computing the posterior probability of a given arc set. Second, we operate with the general partial order framework and its specialization to bucket orders,  introduced recently for related permutation problems. The main technical contribution is the development of a fast algorithm for a novel zeta transform variant, which may be of independent interest.",
        "bibtex": "@InProceedings{pmlr-v9-parviainen10a,\n  title = \t {Bayesian structure discovery in Bayesian networks with less space},\n  author = \t {Parviainen, Pekka and Koivisto, Mikko},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {589--596},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/parviainen10a/parviainen10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/parviainen10a.html},\n  abstract = \t {Current exact algorithms for score-based structure discovery in Bayesian networks on $n$ nodes run in time and space within a polynomial factor of $2^n$. For practical use, the space requirement is the bottleneck, which motivates trading space against time. Here, previous results on finding an optimal network structure in less space are extended in two directions. First, we consider the problem of computing the posterior probability of a given arc set. Second, we operate with the general partial order framework and its specialization to bucket orders,  introduced recently for related permutation problems. The main technical contribution is the development of a fast algorithm for a novel zeta transform variant, which may be of independent interest.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/parviainen10a/parviainen10a.pdf",
        "supp": "",
        "pdf_size": 514489,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3294359532339780947&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Helsinki Institute for Information Technology HIIT and Department of Computer Science, University of Helsinki, Finland; Helsinki Institute for Information Technology HIIT and Department of Computer Science, University of Helsinki, Finland",
        "aff_domain": "cs.helsinki.fi;cs.helsinki.fi",
        "email": "cs.helsinki.fi;cs.helsinki.fi",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Helsinki",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.helsinki.fi",
        "aff_unique_abbr": "UH",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Finland"
    },
    {
        "id": "efba364365",
        "title": "Bayesian variable order Markov models",
        "site": "https://proceedings.mlr.press/v9/dimitrakakis10a.html",
        "author": "Christos Dimitrakakis",
        "abstract": "We present a simple, effective generalisation of variable order   Markov models to full online Bayesian estimation. The mechanism used   is close to that employed in context tree weighting. The main   contribution is the addition of a prior, conditioned on context, on   the Markov order. The resulting construction uses a simple recursion   and can be updated efficiently. This allows the model to make   predictions using more complex contexts, as more data is acquired,   if necessary.  In addition, our model can be alternatively seen as a   mixture of tree experts.  Experimental results show that the   predictive model exhibits consistently good performance in a variety   of domains.",
        "bibtex": "@InProceedings{pmlr-v9-dimitrakakis10a,\n  title = \t {Bayesian variable order Markov models},\n  author = \t {Dimitrakakis, Christos},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {161--168},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/dimitrakakis10a/dimitrakakis10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/dimitrakakis10a.html},\n  abstract = \t {We present a simple, effective generalisation of variable order   Markov models to full online Bayesian estimation. The mechanism used   is close to that employed in context tree weighting. The main   contribution is the addition of a prior, conditioned on context, on   the Markov order. The resulting construction uses a simple recursion   and can be updated efficiently. This allows the model to make   predictions using more complex contexts, as more data is acquired,   if necessary.  In addition, our model can be alternatively seen as a   mixture of tree experts.  Experimental results show that the   predictive model exhibits consistently good performance in a variety   of domains.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/dimitrakakis10a/dimitrakakis10a.pdf",
        "supp": "",
        "pdf_size": 578359,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14023422514651867411&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "University of Amsterdam",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Amsterdam",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uva.nl",
        "aff_unique_abbr": "UvA",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Netherlands"
    },
    {
        "id": "ebbedc86f7",
        "title": "Boosted Optimization for Network Classification",
        "site": "https://proceedings.mlr.press/v9/hancock10a.html",
        "author": "Timothy Hancock; Hiroshi Mamitsuka",
        "abstract": "In this paper we propose a new classification algorithm designed for application on complex networks motivated by algorithmic similarities between boosting learning and message passing.  We consider a network classifier as a logistic regression where the variables define the nodes and the interaction effects define the edges.  From this definition we represent the problem as a factor graph of local exponential loss functions.  Using the factor graph representation it is possible to interpret the network classifier as an ensemble of individual node classifiers.  We then combine ideas from boosted learning with network optimization algorithms to define two novel algorithms, Boosted Expectation Propagation (BEP) and Boosted Message Passing (BMP).  These algorithms optimize the global network classifier performance by locally weighting each node classifier by the error of the surrounding network structure.  We compare the performance of BEP and BMP to logistic regression as well state of the art penalized logistic regression models on simulated grid structured networks.  The results show that using local boosting to optimize the performance of a network classifier increases classification performance and is especially powerful in cases when the whole network structure must be considered for accurate classification.",
        "bibtex": "@InProceedings{pmlr-v9-hancock10a,\n  title = \t {Boosted Optimization for Network Classification},\n  author = \t {Hancock, Timothy and Mamitsuka, Hiroshi},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {305--312},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/hancock10a/hancock10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/hancock10a.html},\n  abstract = \t {In this paper we propose a new classification algorithm designed for application on complex networks motivated by algorithmic similarities between boosting learning and message passing.  We consider a network classifier as a logistic regression where the variables define the nodes and the interaction effects define the edges.  From this definition we represent the problem as a factor graph of local exponential loss functions.  Using the factor graph representation it is possible to interpret the network classifier as an ensemble of individual node classifiers.  We then combine ideas from boosted learning with network optimization algorithms to define two novel algorithms, Boosted Expectation Propagation (BEP) and Boosted Message Passing (BMP).  These algorithms optimize the global network classifier performance by locally weighting each node classifier by the error of the surrounding network structure.  We compare the performance of BEP and BMP to logistic regression as well state of the art penalized logistic regression models on simulated grid structured networks.  The results show that using local boosting to optimize the performance of a network classifier increases classification performance and is especially powerful in cases when the whole network structure must be considered for accurate classification.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/hancock10a/hancock10a.pdf",
        "supp": "",
        "pdf_size": 436571,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16310976002920907783&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Bioinformatics Center, Institute for Chemical Research, Kyoto University, Japan; Bioinformatics Center, Institute for Chemical Research, Kyoto University, Japan",
        "aff_domain": "; ",
        "email": "; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Kyoto University",
        "aff_unique_dep": "Institute for Chemical Research",
        "aff_unique_url": "https://www.kyoto-u.ac.jp",
        "aff_unique_abbr": "Kyoto U",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Kyoto",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "c8bdf2cb9e",
        "title": "Coherent Inference on Optimal Play in Game Trees",
        "site": "https://proceedings.mlr.press/v9/hennig10a.html",
        "author": "Philipp Hennig; David Stern; Thore Graepel",
        "abstract": "Round-based games are an instance of discrete planning problems. Some of the best contemporary game tree search algorithms use random roll-outs as data. Relying on a good policy, they learn on-policy values by propagating information upwards in the tree, but not between sibling nodes. Here, we present a generative model and a corresponding approximate message passing scheme for inference on the optimal, off-policy value of nodes in smooth AND/OR trees, given random roll-outs. The crucial insight is that the distribution of values in game trees is not completely arbitrary. We define a generative model of the on-policy values using a latent score for each state, representing the value under the random roll-out policy. Inference on the values under the optimal policy separates into an inductive, pre-data step and a deductive, post-data part. Both can be solved approximately with Expectation Propagation, allowing off-policy value inference for any node in the (exponentially big) tree in linear time.",
        "bibtex": "@InProceedings{pmlr-v9-hennig10a,\n  title = \t {Coherent Inference on Optimal Play in Game Trees},\n  author = \t {Hennig, Philipp and Stern, David and Graepel, Thore},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {326--333},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/hennig10a/hennig10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/hennig10a.html},\n  abstract = \t {Round-based games are an instance of discrete planning problems. Some of the best contemporary game tree search algorithms use random roll-outs as data. Relying on a good policy, they learn on-policy values by propagating information upwards in the tree, but not between sibling nodes. Here, we present a generative model and a corresponding approximate message passing scheme for inference on the optimal, off-policy value of nodes in smooth AND/OR trees, given random roll-outs. The crucial insight is that the distribution of values in game trees is not completely arbitrary. We define a generative model of the on-policy values using a latent score for each state, representing the value under the random roll-out policy. Inference on the values under the optimal policy separates into an inductive, pre-data step and a deductive, post-data part. Both can be solved approximately with Expectation Propagation, allowing off-policy value inference for any node in the (exponentially big) tree in linear time.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/hennig10a/hennig10a.pdf",
        "supp": "",
        "pdf_size": 1581191,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10594315876272518364&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Cavendish Laboratory; Microsoft Research Ltd.; Microsoft Research Ltd.",
        "aff_domain": "; ; ",
        "email": "; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of Cambridge;Microsoft Research",
        "aff_unique_dep": "Cavendish Laboratory;",
        "aff_unique_url": "https://www.cavendishlaboratory.cam.ac.uk;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "Cavendish Lab;MSR",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "9288efb440",
        "title": "Collaborative Filtering on a Budget",
        "site": "https://proceedings.mlr.press/v9/karatzoglou10a.html",
        "author": "Alexandros Karatzoglou; Alex Smola; Markus Weimer",
        "abstract": "Matrix factorization is a successful technique for building   collaborative filtering systems. While it works well on a large   range of problems, it is also known for requiring significant   amounts of storage for each user or item to be added to the   database.  This is a problem whenever the collaborative filtering   task is larger than the medium-sized Netflix Prize data.    In this paper, we propose a new model for representing and   compressing matrix factors via hashing.  This allows for   essentially unbounded storage (at a graceful storage / performance   trade-off) for users and items to be represented in a pre-defined   memory footprint.  It allows us to scale recommender systems to very   large numbers of users or conversely, obtain very good performance   even for tiny models (e.g. 400kB of data suffice for a   representation of the EachMovie problem).    We provide both experimental results and approximation bounds for   our compressed representation and we show how this approach can be   extended to multipartite problems.",
        "bibtex": "@InProceedings{pmlr-v9-karatzoglou10a,\n  title = \t {Collaborative Filtering on a Budget},\n  author = \t {Karatzoglou, Alexandros and Smola, Alex and Weimer, Markus},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {389--396},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/karatzoglou10a/karatzoglou10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/karatzoglou10a.html},\n  abstract = \t {Matrix factorization is a successful technique for building   collaborative filtering systems. While it works well on a large   range of problems, it is also known for requiring significant   amounts of storage for each user or item to be added to the   database.  This is a problem whenever the collaborative filtering   task is larger than the medium-sized Netflix Prize data.    In this paper, we propose a new model for representing and   compressing matrix factors via hashing.  This allows for   essentially unbounded storage (at a graceful storage / performance   trade-off) for users and items to be represented in a pre-defined   memory footprint.  It allows us to scale recommender systems to very   large numbers of users or conversely, obtain very good performance   even for tiny models (e.g. 400kB of data suffice for a   representation of the EachMovie problem).    We provide both experimental results and approximation bounds for   our compressed representation and we show how this approach can be   extended to multipartite problems.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/karatzoglou10a/karatzoglou10a.pdf",
        "supp": "",
        "pdf_size": 786471,
        "gs_citation": 58,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15228114658077452591&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Telefonica Research, Barcelona, Spain; Yahoo! Research, Santa Clara, CA, USA; Yahoo! Labs, Santa Clara, CA, USA",
        "aff_domain": "tid.es;smola.org;acm.org",
        "email": "tid.es;smola.org;acm.org",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Telefonica Research;Yahoo! Research;Yahoo!",
        "aff_unique_dep": ";;Yahoo! Labs",
        "aff_unique_url": "https://www.telefonica.com/research;https://research.yahoo.com;https://yahoo.com",
        "aff_unique_abbr": "Telefonica Research;Yahoo! Res;Yahoo!",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Barcelona;Santa Clara",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "Spain;United States"
    },
    {
        "id": "6e0ea34997",
        "title": "Collaborative Filtering via Rating Concentration",
        "site": "https://proceedings.mlr.press/v9/huang10a.html",
        "author": "Bert Huang; Tony Jebara",
        "abstract": "While most popular collaborative filtering methods use low-rank matrix factorization and parametric density assumptions, this article proposes an approach based on distribution-free concentration inequalities. Using agnostic hierarchical sampling assumptions, functions of observed ratings are provably close to their expectations over query ratings, on average. A joint probability distribution over queries of interest is estimated using maximum entropy regularization. The distribution resides in a convex hull of allowable candidate distributions which satisfy concentration inequalities that stem from the sampling assumptions. The method accurately estimates rating distributions on synthetic and real data and is competitive with low rank and parametric methods which make more aggressive assumptions about the problem.",
        "bibtex": "@InProceedings{pmlr-v9-huang10a,\n  title = \t {Collaborative Filtering via Rating Concentration},\n  author = \t {Huang, Bert and Jebara, Tony},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {334--341},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/huang10a/huang10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/huang10a.html},\n  abstract = \t {While most popular collaborative filtering methods use low-rank matrix factorization and parametric density assumptions, this article proposes an approach based on distribution-free concentration inequalities. Using agnostic hierarchical sampling assumptions, functions of observed ratings are provably close to their expectations over query ratings, on average. A joint probability distribution over queries of interest is estimated using maximum entropy regularization. The distribution resides in a convex hull of allowable candidate distributions which satisfy concentration inequalities that stem from the sampling assumptions. The method accurately estimates rating distributions on synthetic and real data and is competitive with low rank and parametric methods which make more aggressive assumptions about the problem.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/huang10a/huang10a.pdf",
        "supp": "",
        "pdf_size": 534213,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4429665722455686495&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Computer Science Department, Columbia University, New York, NY 10027; Computer Science Department, Columbia University, New York, NY 10027",
        "aff_domain": "cs.columbia.edu;cs.columbia.edu",
        "email": "cs.columbia.edu;cs.columbia.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Columbia University",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.columbia.edu",
        "aff_unique_abbr": "Columbia",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "New York",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "0e35e3093f",
        "title": "Combining Experiments to Discover Linear Cyclic Models with Latent Variables",
        "site": "https://proceedings.mlr.press/v9/eberhardt10a.html",
        "author": "Frederick Eberhardt; Patrik Hoyer; Richard Scheines",
        "abstract": "We present an algorithm to infer causal relations between a set of measured variables on the basis of experiments on these variables. The algorithm assumes that the causal relations are linear, but is otherwise completely general: It provides consistent estimates when the true causal structure contains feedback loops and latent variables, while the experiments can involve surgical or \u2019soft\u2019 interventions on one or multiple variables at a time. The algorithm is \u2019online\u2019 in the sense that it combines the results from any set of available experiments, can incorporate background knowledge and resolves conflicts that arise from combining results from different experiments. In addition we provide a necessary and sufficient condition that (i) determines when the algorithm can uniquely return the true graph, and (ii) can be used to select the next best experiment until this condition is satisfied. We demonstrate the method by applying it to simulated data and the flow cytometry data of Sachs et al (2005).",
        "bibtex": "@InProceedings{pmlr-v9-eberhardt10a,\n  title = \t {Combining Experiments to Discover Linear Cyclic Models with Latent Variables},\n  author = \t {Eberhardt, Frederick and Hoyer, Patrik and Scheines, Richard},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {185--192},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/eberhardt10a/eberhardt10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/eberhardt10a.html},\n  abstract = \t {We present an algorithm to infer causal relations between a set of measured variables on the basis of experiments on these variables. The algorithm assumes that the causal relations are linear, but is otherwise completely general: It provides consistent estimates when the true causal structure contains feedback loops and latent variables, while the experiments can involve surgical or \u2019soft\u2019 interventions on one or multiple variables at a time. The algorithm is \u2019online\u2019 in the sense that it combines the results from any set of available experiments, can incorporate background knowledge and resolves conflicts that arise from combining results from different experiments. In addition we provide a necessary and sufficient condition that (i) determines when the algorithm can uniquely return the true graph, and (ii) can be used to select the next best experiment until this condition is satisfied. We demonstrate the method by applying it to simulated data and the flow cytometry data of Sachs et al (2005).}\n}",
        "pdf": "http://proceedings.mlr.press/v9/eberhardt10a/eberhardt10a.pdf",
        "supp": "",
        "pdf_size": 1862880,
        "gs_citation": 55,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10151961108659020499&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 20,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "99878b481d",
        "title": "Conditional Density Estimation via Least-Squares Density Ratio Estimation",
        "site": "https://proceedings.mlr.press/v9/sugiyama10a.html",
        "author": "Masashi Sugiyama; Ichiro Takeuchi; Taiji Suzuki; Takafumi Kanamori; Hirotaka Hachiya; Daisuke Okanohara",
        "abstract": "Estimating the conditional mean of an input-output relation is the goal of regression. However, regression analysis is not sufficiently informative if the conditional distribution has multi-modality, is highly asymmetric, or contains heteroscedastic noise. In such scenarios, estimating the conditional distribution itself would be more useful. In this paper, we propose a novel method of conditional density estimation that is suitable for multi-dimensional continuous variables. The basic idea of the proposed method is to express the conditional density in terms of the density ratio and the ratio is directly estimated without going through density estimation. Experiments using benchmark and robot transition datasets illustrate the usefulness of the proposed approach.",
        "bibtex": "@InProceedings{pmlr-v9-sugiyama10a,\n  title = \t {Conditional Density Estimation via Least-Squares Density Ratio Estimation},\n  author = \t {Sugiyama, Masashi and Takeuchi, Ichiro and Suzuki, Taiji and Kanamori, Takafumi and Hachiya, Hirotaka and Okanohara, Daisuke},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {781--788},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/sugiyama10a/sugiyama10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/sugiyama10a.html},\n  abstract = \t {Estimating the conditional mean of an input-output relation is the goal of regression. However, regression analysis is not sufficiently informative if the conditional distribution has multi-modality, is highly asymmetric, or contains heteroscedastic noise. In such scenarios, estimating the conditional distribution itself would be more useful. In this paper, we propose a novel method of conditional density estimation that is suitable for multi-dimensional continuous variables. The basic idea of the proposed method is to express the conditional density in terms of the density ratio and the ratio is directly estimated without going through density estimation. Experiments using benchmark and robot transition datasets illustrate the usefulness of the proposed approach.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/sugiyama10a/sugiyama10a.pdf",
        "supp": "",
        "pdf_size": 665827,
        "gs_citation": 83,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3666423711476713875&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "e06ad4738a",
        "title": "Contextual Multi-Armed Bandits",
        "site": "https://proceedings.mlr.press/v9/lu10a.html",
        "author": "Tyler Lu; David Pal; Martin Pal",
        "abstract": "We study contextual multi-armed bandit problems where the context comes from a metric space and the payoff satisfies a Lipschitz condition with respect to the metric.  Abstractly, a contextual multi-armed bandit problem models a situation where, in a sequence of independent trials, an online algorithm chooses, based on a given context (side information), an action from a set of possible actions so as to maximize the total payoff of the chosen actions.  The payoff depends on both the action chosen and the context. In contrast, context-free multi-armed bandit problems, a focus of much previous research, model situations where no side information is available and the payoff depends only on the action chosen.  Our problem is motivated by sponsored web search, where the task is to display ads to a user of an Internet search engine based on her search query so as to maximize the click-through rate (CTR) of the ads displayed.  We cast this problem as a contextual multi-armed bandit problem where queries and ads form metric spaces and the payoff function is Lipschitz with respect to both the metrics. For any $\\epsilon > 0$ we present an algorithm with regret $O(T^{\\frac{a+b+1}{a+b+2} + \\epsilon})$ where $a, b$ are the covering dimensions of the query space and the ad space respectively. We prove a lower bound $\\Omega(T^{\\frac{\\tilde{a}+\\tilde{b}+1}{\\tilde{a}+\\tilde{b}+2} - \\epsilon})$ for the regret of any algorithm where $\\tilde{a}, \\tilde{b}$ are packing dimensions of the query spaces and the ad space respectively. For finite spaces or convex bounded subsets of Euclidean spaces, this gives an almost matching upper and lower bound.",
        "bibtex": "@InProceedings{pmlr-v9-lu10a,\n  title = \t {Contextual Multi-Armed Bandits},\n  author = \t {Lu, Tyler and Pal, David and Pal, Martin},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {485--492},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/lu10a/lu10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/lu10a.html},\n  abstract = \t {We study contextual multi-armed bandit problems where the context comes from a metric space and the payoff satisfies a Lipschitz condition with respect to the metric.  Abstractly, a contextual multi-armed bandit problem models a situation where, in a sequence of independent trials, an online algorithm chooses, based on a given context (side information), an action from a set of possible actions so as to maximize the total payoff of the chosen actions.  The payoff depends on both the action chosen and the context. In contrast, context-free multi-armed bandit problems, a focus of much previous research, model situations where no side information is available and the payoff depends only on the action chosen.  Our problem is motivated by sponsored web search, where the task is to display ads to a user of an Internet search engine based on her search query so as to maximize the click-through rate (CTR) of the ads displayed.  We cast this problem as a contextual multi-armed bandit problem where queries and ads form metric spaces and the payoff function is Lipschitz with respect to both the metrics. For any $\\epsilon > 0$ we present an algorithm with regret $O(T^{\\frac{a+b+1}{a+b+2} + \\epsilon})$ where $a, b$ are the covering dimensions of the query space and the ad space respectively. We prove a lower bound $\\Omega(T^{\\frac{\\tilde{a}+\\tilde{b}+1}{\\tilde{a}+\\tilde{b}+2} - \\epsilon})$ for the regret of any algorithm where $\\tilde{a}, \\tilde{b}$ are packing dimensions of the query spaces and the ad space respectively. For finite spaces or convex bounded subsets of Euclidean spaces, this gives an almost matching upper and lower bound.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/lu10a/lu10a.pdf",
        "supp": "",
        "pdf_size": 1380038,
        "gs_citation": 380,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1849219688706163198&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Computer Science, University of Toronto; Department of Computing Science, University of Alberta; Google, Inc.",
        "aff_domain": "cs.toronto.edu;cs.ualberta.ca;google.com",
        "email": "cs.toronto.edu;cs.ualberta.ca;google.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University of Toronto;University of Alberta;Google",
        "aff_unique_dep": "Department of Computer Science;Department of Computing Science;",
        "aff_unique_url": "https://www.utoronto.ca;https://www.ualberta.ca;https://www.google.com",
        "aff_unique_abbr": "U of T;UAlberta;Google",
        "aff_campus_unique_index": "0;2",
        "aff_campus_unique": "Toronto;;Mountain View",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "Canada;United States"
    },
    {
        "id": "3aa5438113",
        "title": "Convex Structure Learning in Log-Linear Models: Beyond Pairwise Potentials",
        "site": "https://proceedings.mlr.press/v9/schmidt10a.html",
        "author": "Mark Schmidt; Kevin Murphy",
        "abstract": "Previous work has examined structure learning in log-linear models with $\\ell_1$-regularization, largely focusing on the case of pairwise potentials.  In this work we consider the case of models with potentials of arbitrary order, but that satisfy a hierarchical constraint.  We enforce the hierarchical constraint using group $\\ell_1$-regularization with overlapping groups, and an active set method that enforces hierarchical inclusion allows us to tractably consider the exponential number of higher-order potentials.  We use a spectral projected gradient method as a sub-routine for solving the overlapping group $\\ell_1$-regularization problem, and make use of a sparse version of Dykstra\u2019s algorithm to compute the projection.  Our experiments indicate that this model gives equal or better test set likelihood compared to previous models.",
        "bibtex": "@InProceedings{pmlr-v9-schmidt10a,\n  title = \t {Convex Structure Learning in Log-Linear Models: Beyond Pairwise Potentials},\n  author = \t {Schmidt, Mark and Murphy, Kevin},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {709--716},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/schmidt10a/schmidt10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/schmidt10a.html},\n  abstract = \t {Previous work has examined structure learning in log-linear models with $\\ell_1$-regularization, largely focusing on the case of pairwise potentials.  In this work we consider the case of models with potentials of arbitrary order, but that satisfy a hierarchical constraint.  We enforce the hierarchical constraint using group $\\ell_1$-regularization with overlapping groups, and an active set method that enforces hierarchical inclusion allows us to tractably consider the exponential number of higher-order potentials.  We use a spectral projected gradient method as a sub-routine for solving the overlapping group $\\ell_1$-regularization problem, and make use of a sparse version of Dykstra\u2019s algorithm to compute the projection.  Our experiments indicate that this model gives equal or better test set likelihood compared to previous models.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/schmidt10a/schmidt10a.pdf",
        "supp": "",
        "pdf_size": 1030804,
        "gs_citation": 80,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1719548296462246736&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science, University of British Columbia; Department of Computer Science, University of British Columbia",
        "aff_domain": "cs.ubc.ca;cs.ubc.ca",
        "email": "cs.ubc.ca;cs.ubc.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of British Columbia",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ubc.ca",
        "aff_unique_abbr": "UBC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Vancouver",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "16622ad473",
        "title": "Convexity of Proper Composite Binary Losses",
        "site": "https://proceedings.mlr.press/v9/reid10a.html",
        "author": "Mark Reid; Robert Williamson",
        "abstract": "A composite loss assigns a penalty to a real-valued prediction by associating the prediction with a probability via a link function then applying a class probability estimation (CPE) loss. If the risk for a composite loss is always minimised by predicting the value associated with the true class probability the composite loss is proper. We provide a novel, explicit and complete characterisation of the convexity of any proper composite loss in terms of its link and its \u201cweight function\u201d associated with its proper CPE loss.",
        "bibtex": "@InProceedings{pmlr-v9-reid10a,\n  title = \t {Convexity of Proper Composite Binary Losses},\n  author = \t {Reid, Mark and Williamson, Robert},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {637--644},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/reid10a/reid10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/reid10a.html},\n  abstract = \t {A composite loss assigns a penalty to a real-valued prediction by associating the prediction with a probability via a link function then applying a class probability estimation (CPE) loss. If the risk for a composite loss is always minimised by predicting the value associated with the true class probability the composite loss is proper. We provide a novel, explicit and complete characterisation of the convexity of any proper composite loss in terms of its link and its \u201cweight function\u201d associated with its proper CPE loss.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/reid10a/reid10a.pdf",
        "supp": "",
        "pdf_size": 1331719,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2630526326963779964&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "The Australian National University and NICTA; The Australian National University and NICTA",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Australian National University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.anu.edu.au",
        "aff_unique_abbr": "ANU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "1ed0c06387",
        "title": "Dense Message Passing for Sparse Principal Component Analysis",
        "site": "https://proceedings.mlr.press/v9/sharp10a.html",
        "author": "Kevin Sharp; Magnus Rattray",
        "abstract": "We describe a novel inference algorithm for sparse Bayesian PCA with a  zero-norm prior on the model parameters. Bayesian inference is very  challenging in probabilistic models of this type. MCMC procedures are  too slow to be practical in a very high-dimensional setting and standard  mean-field variational Bayes algorithms are ineffective.  We adopt a  dense message passing algorithm similar to algorithms developed in the  statistical  physics community and previously applied to inference  problems in coding and sparse classification. The algorithm achieves  near-optimal performance on synthetic data for which a statistical  mechanics theory of optimal learning can be derived. We also study two  gene expression datasets used in previous studies of sparse PCA. We find our  method  performs better than one published algorithm and comparably to a second.",
        "bibtex": "@InProceedings{pmlr-v9-sharp10a,\n  title = \t {Dense Message Passing for Sparse Principal Component Analysis},\n  author = \t {Sharp, Kevin and Rattray, Magnus},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {725--732},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/sharp10a/sharp10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/sharp10a.html},\n  abstract = \t {We describe a novel inference algorithm for sparse Bayesian PCA with a  zero-norm prior on the model parameters. Bayesian inference is very  challenging in probabilistic models of this type. MCMC procedures are  too slow to be practical in a very high-dimensional setting and standard  mean-field variational Bayes algorithms are ineffective.  We adopt a  dense message passing algorithm similar to algorithms developed in the  statistical  physics community and previously applied to inference  problems in coding and sparse classification. The algorithm achieves  near-optimal performance on synthetic data for which a statistical  mechanics theory of optimal learning can be derived. We also study two  gene expression datasets used in previous studies of sparse PCA. We find our  method  performs better than one published algorithm and comparably to a second.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/sharp10a/sharp10a.pdf",
        "supp": "",
        "pdf_size": 3813963,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13764946097789477333&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "School of Computer Science, University of Manchester, Manchester M13 9PL; School of Computer Science, University of Manchester, Manchester M13 9PL",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Manchester",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.manchester.ac.uk",
        "aff_unique_abbr": "UoM",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Manchester",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "f2bc21b9cb",
        "title": "Dependent Indian Buffet Processes",
        "site": "https://proceedings.mlr.press/v9/williamson10a.html",
        "author": "Sinead Williamson; Peter Orbanz; Zoubin Ghahramani",
        "abstract": "Latent variable models represent hidden structure in observational data.To account for the distribution of the observational data changing over time, space or some other covariate, we need generalizations of latent variable models that explicitly capture this dependency on the covariate. A variety of such generalizations has been proposed for latent variable models based on the Dirichlet process. We address dependency on covariates in binary latent feature models, by introducing a dependent Indian buffet process. The model generates, for each value of the covariate, a binary random matrix with an unbounded number of columns.  Evolution of the binary matrices over the covariate set is controlled by a hierarchical Gaussian process model. The choice of covariance functions controls the dependence structure and exchangeability properties of the model. We derive a Markov Chain Monte Carlo sampling algorithm for  Bayesian inference, and provide experiments on both synthetic and real-world data. The experimental results show that explicit modeling of dependencies significantly improves accuracy of predictions.",
        "bibtex": "@InProceedings{pmlr-v9-williamson10a,\n  title = \t {Dependent Indian Buffet Processes},\n  author = \t {Williamson, Sinead and Orbanz, Peter and Ghahramani, Zoubin},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {924--931},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/williamson10a/williamson10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/williamson10a.html},\n  abstract = \t {Latent variable models represent hidden structure in observational data.To account for the distribution of the observational data changing over time, space or some other covariate, we need generalizations of latent variable models that explicitly capture this dependency on the covariate. A variety of such generalizations has been proposed for latent variable models based on the Dirichlet process. We address dependency on covariates in binary latent feature models, by introducing a dependent Indian buffet process. The model generates, for each value of the covariate, a binary random matrix with an unbounded number of columns.  Evolution of the binary matrices over the covariate set is controlled by a hierarchical Gaussian process model. The choice of covariance functions controls the dependence structure and exchangeability properties of the model. We derive a Markov Chain Monte Carlo sampling algorithm for  Bayesian inference, and provide experiments on both synthetic and real-world data. The experimental results show that explicit modeling of dependencies significantly improves accuracy of predictions.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/williamson10a/williamson10a.pdf",
        "supp": "",
        "pdf_size": 756725,
        "gs_citation": 67,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3123864095459251247&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "2682c6dbae",
        "title": "Descent Methods for Tuning Parameter Refinement",
        "site": "https://proceedings.mlr.press/v9/lorbert10a.html",
        "author": "Alexander Lorbert; Peter Ramadge",
        "abstract": "This paper addresses multidimensional tuning parameter selection in the context of \u201ctrain-validate-test\u201d and $K$-fold cross validation. A coarse grid search over tuning parameter space is used to initialize a descent method which then jointly optimizes over variables and tuning parameters.  We study four regularized regression methods and develop the update equations for the corresponding descent algorithms.  Experiments on both simulated and real-world datasets show that the method results in significant tuning parameter refinement.",
        "bibtex": "@InProceedings{pmlr-v9-lorbert10a,\n  title = \t {Descent Methods for Tuning Parameter Refinement},\n  author = \t {Lorbert, Alexander and Ramadge, Peter},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {469--476},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/lorbert10a/lorbert10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/lorbert10a.html},\n  abstract = \t {This paper addresses multidimensional tuning parameter selection in the context of \u201ctrain-validate-test\u201d and $K$-fold cross validation. A coarse grid search over tuning parameter space is used to initialize a descent method which then jointly optimizes over variables and tuning parameters.  We study four regularized regression methods and develop the update equations for the corresponding descent algorithms.  Experiments on both simulated and real-world datasets show that the method results in significant tuning parameter refinement.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/lorbert10a/lorbert10a.pdf",
        "supp": "",
        "pdf_size": 660362,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15649613995765903792&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Dept. of Electrical Engineering, Princeton University; Dept. of Electrical Engineering, Princeton University",
        "aff_domain": "; ",
        "email": "; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Princeton University",
        "aff_unique_dep": "Dept. of Electrical Engineering",
        "aff_unique_url": "https://www.princeton.edu",
        "aff_unique_abbr": "Princeton",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "060f39bae0",
        "title": "Detecting Weak but Hierarchically-Structured Patterns in Networks",
        "site": "https://proceedings.mlr.press/v9/singh10a.html",
        "author": "Aarti Singh; Robert Nowak; Robert Calderbank",
        "abstract": "The ability to detect weak distributed activation patterns in networks is critical to several applications, such as identifying the onset of anomalous activity or incipient congestion in the Internet, or faint traces of a biochemical spread by a sensor network.  This is a challenging problem since weak distributed patterns can be invisible in per node statistics as well as a global network-wide aggregate. Most prior work considers situations in which the activation/non-activation of each node is statistically independent, but this is unrealistic in many problems.  In this paper, we consider structured patterns arising from statistical dependencies in the activation process.  Our contributions are three-fold. First, we propose a sparsifying transform that succinctly represents structured activation patterns that conform to a hierarchical dependency graph.  Second, we establish that the proposed transform facilitates detection of very weak activation patterns that cannot be detected with existing methods. Third, we show that the structure of the hierarchical dependency graph governing the activation process, and hence the network transform, can be learnt from very few (logarithmic in network size) independent snapshots of network activity.",
        "bibtex": "@InProceedings{pmlr-v9-singh10a,\n  title = \t {Detecting Weak but Hierarchically-Structured Patterns in Networks},\n  author = \t {Singh, Aarti and Nowak, Robert and Calderbank, Robert},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {749--756},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/singh10a/singh10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/singh10a.html},\n  abstract = \t {The ability to detect weak distributed activation patterns in networks is critical to several applications, such as identifying the onset of anomalous activity or incipient congestion in the Internet, or faint traces of a biochemical spread by a sensor network.  This is a challenging problem since weak distributed patterns can be invisible in per node statistics as well as a global network-wide aggregate. Most prior work considers situations in which the activation/non-activation of each node is statistically independent, but this is unrealistic in many problems.  In this paper, we consider structured patterns arising from statistical dependencies in the activation process.  Our contributions are three-fold. First, we propose a sparsifying transform that succinctly represents structured activation patterns that conform to a hierarchical dependency graph.  Second, we establish that the proposed transform facilitates detection of very weak activation patterns that cannot be detected with existing methods. Third, we show that the structure of the hierarchical dependency graph governing the activation process, and hence the network transform, can be learnt from very few (logarithmic in network size) independent snapshots of network activity.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/singh10a/singh10a.pdf",
        "supp": "",
        "pdf_size": 1565341,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18316199157322767740&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "Machine Learning Department, Carnegie Mellon University; Electrical and Computer Engineering, University of Wisconsin - Madison; Electrical Engineering, PACM, Princeton University",
        "aff_domain": "cmu.edu;engr.wisc.edu;princeton.edu",
        "email": "cmu.edu;engr.wisc.edu;princeton.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Carnegie Mellon University;University of Wisconsin - Madison;Princeton University",
        "aff_unique_dep": "Machine Learning Department;Electrical and Computer Engineering;Electrical Engineering, Program in Applied and Computational Mathematics",
        "aff_unique_url": "https://www.cmu.edu;https://www.wisc.edu;https://www.princeton.edu",
        "aff_unique_abbr": "CMU;UW-Madison;Princeton",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Madison",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d76f902933",
        "title": "Deterministic Bayesian inference for the $p*$ model",
        "site": "https://proceedings.mlr.press/v9/austad10a.html",
        "author": "Haakon Austad; Nial Friel",
        "abstract": "The $p*$ model is widely used in social network analysis. The likelihood of a network under this model is impossible to calculate for all but trivially small networks. Various approximation have been presented in  the literature, and the pseudolikelihood approximation is the most popular. The aim of this paper is to introduce two likelihood approximations which have the pseudolikelihood estimator as a special case. We show, for the examples that we have considered, that both approximations result in improved estimation of model parameters with respect to the standard methodological approaches. We provide a deterministic  approach and also illustrate how Bayesian model choice can be carried out in this setting.",
        "bibtex": "@InProceedings{pmlr-v9-austad10a,\n  title = \t {Deterministic Bayesian inference for the p* model},\n  author = \t {Austad, Haakon and Friel, Nial},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {41--48},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/austad10a/austad10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/austad10a.html},\n  abstract = \t {The $p*$ model is widely used in social network analysis. The likelihood of a network under this model is impossible to calculate for all but trivially small networks. Various approximation have been presented in  the literature, and the pseudolikelihood approximation is the most popular. The aim of this paper is to introduce two likelihood approximations which have the pseudolikelihood estimator as a special case. We show, for the examples that we have considered, that both approximations result in improved estimation of model parameters with respect to the standard methodological approaches. We provide a deterministic  approach and also illustrate how Bayesian model choice can be carried out in this setting.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/austad10a/austad10a.pdf",
        "supp": "",
        "pdf_size": 662832,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8210092942488349328&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Norwegian University of Science and Technology; University College Dublin",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Norwegian University of Science and Technology;University College Dublin",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ntnu.no;https://www.ucd.ie",
        "aff_unique_abbr": "NTNU;UCD",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Norway;Ireland"
    },
    {
        "id": "ddabe9eeee",
        "title": "Dirichlet Process Mixtures of Generalized Linear Models",
        "site": "https://proceedings.mlr.press/v9/hannah10a.html",
        "author": "Lauren Hannah; David Blei; Warren Powell",
        "abstract": "We propose Dirichlet Process mixtures of Generalized Linear Models (DP-GLMs), a new method of nonparametric regression that accommodates continuous and categorical inputs, models a response variable locally by a generalized linear model.  We give conditions for the existence and asymptotic unbiasedness of the DP-GLM regression mean function estimate;  we then give a practical example for when those conditions hold. We evaluate DP-GLM on several data sets, comparing it to modern methods of nonparametric regression including regression trees and Gaussian processes.",
        "bibtex": "@InProceedings{pmlr-v9-hannah10a,\n  title = \t {Dirichlet Process Mixtures of Generalized Linear Models},\n  author = \t {Hannah, Lauren and Blei, David and Powell, Warren},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {313--320},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/hannah10a/hannah10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/hannah10a.html},\n  abstract = \t {We propose Dirichlet Process mixtures of Generalized Linear Models (DP-GLMs), a new method of nonparametric regression that accommodates continuous and categorical inputs, models a response variable locally by a generalized linear model.  We give conditions for the existence and asymptotic unbiasedness of the DP-GLM regression mean function estimate;  we then give a practical example for when those conditions hold. We evaluate DP-GLM on several data sets, comparing it to modern methods of nonparametric regression including regression trees and Gaussian processes.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/hannah10a/hannah10a.pdf",
        "supp": "",
        "pdf_size": 1288210,
        "gs_citation": 220,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14194343943760494377&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 19,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "b0d86138e0",
        "title": "Discriminative Topic Segmentation of Text and Speech",
        "site": "https://proceedings.mlr.press/v9/mohri10a.html",
        "author": "Mehryar Mohri; Pedro Moreno; Eugene Weinstein",
        "abstract": "We explore automated discovery of topically-coherent segments in speech or text sequences. We give two new discriminative topic segmentation algorithms which employ a new measure of text similarity based on word co-occurrence. Both algorithms function by finding extrema in the similarity signal over the text, with the latter algorithm using a compact support-vector based description of a window of text or speech observations in word similarity space to overcome noise introduced by speech recognition errors and off-topic content. In experiments over speech and text news streams, we show that these algorithms outperform previous methods. We observe that topic segmentation of speech recognizer output is a more difficult problem than that of text streams; however, we demonstrate that by using a lattice of competing hypotheses rather than just the one-best hypothesis as input to the segmentation algorithm, the performance of the algorithm can be improved.",
        "bibtex": "@InProceedings{pmlr-v9-mohri10a,\n  title = \t {Discriminative Topic Segmentation of Text and Speech},\n  author = \t {Mohri, Mehryar and Moreno, Pedro and Weinstein, Eugene},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {533--540},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/mohri10a/mohri10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/mohri10a.html},\n  abstract = \t {We explore automated discovery of topically-coherent segments in speech or text sequences. We give two new discriminative topic segmentation algorithms which employ a new measure of text similarity based on word co-occurrence. Both algorithms function by finding extrema in the similarity signal over the text, with the latter algorithm using a compact support-vector based description of a window of text or speech observations in word similarity space to overcome noise introduced by speech recognition errors and off-topic content. In experiments over speech and text news streams, we show that these algorithms outperform previous methods. We observe that topic segmentation of speech recognizer output is a more difficult problem than that of text streams; however, we demonstrate that by using a lattice of competing hypotheses rather than just the one-best hypothesis as input to the segmentation algorithm, the performance of the algorithm can be improved.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/mohri10a/mohri10a.pdf",
        "supp": "",
        "pdf_size": 305385,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2622161289549941298&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Courant Institute and Google; Google; Google",
        "aff_domain": "cs.nyu.edu;google.com;google.com",
        "email": "cs.nyu.edu;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Courant Institute of Mathematical Sciences;Google",
        "aff_unique_dep": "Mathematical Sciences;",
        "aff_unique_url": "https://courant.nyu.edu;https://www.google.com",
        "aff_unique_abbr": "Courant;Google",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d23cef6f37",
        "title": "Efficient Learning of Deep Boltzmann Machines",
        "site": "https://proceedings.mlr.press/v9/salakhutdinov10a.html",
        "author": "Ruslan Salakhutdinov; Hugo Larochelle",
        "abstract": "We present a new approximate inference algorithm for Deep Boltzmann Machines (DBM\u2019s), a generative model with many layers of hidden variables. The algorithm learns a separate \u201crecognition\u201d model that is used to quickly initialize, in a single bottom-up pass, the values of the latent variables in all hidden layers. We show that using such a recognition model, followed by a combined top-down and bottom-up pass, it is possible to efficiently learn a good generative model of high-dimensional highly-structured sensory input. We show that the additional computations required by incorporating a top-down feedback plays a critical role in the performance of a DBM, both as a generative and discriminative model. Moreover, inference is only at most three times slower compared to the approximate inference in a Deep Belief Network (DBN), making large-scale learning of DBM\u2019s practical. Finally, we demonstrate that the DBM\u2019s trained using the proposed approximate inference algorithm perform well compared to DBN\u2019s and SVM\u2019s on the MNIST handwritten digit, OCR English letters, and NORB visual object recognition tasks.",
        "bibtex": "@InProceedings{pmlr-v9-salakhutdinov10a,\n  title = \t {Efficient Learning of Deep Boltzmann Machines},\n  author = \t {Salakhutdinov, Ruslan and Larochelle, Hugo},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {693--700},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/salakhutdinov10a/salakhutdinov10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/salakhutdinov10a.html},\n  abstract = \t {We present a new approximate inference algorithm for Deep Boltzmann Machines (DBM\u2019s), a generative model with many layers of hidden variables. The algorithm learns a separate \u201crecognition\u201d model that is used to quickly initialize, in a single bottom-up pass, the values of the latent variables in all hidden layers. We show that using such a recognition model, followed by a combined top-down and bottom-up pass, it is possible to efficiently learn a good generative model of high-dimensional highly-structured sensory input. We show that the additional computations required by incorporating a top-down feedback plays a critical role in the performance of a DBM, both as a generative and discriminative model. Moreover, inference is only at most three times slower compared to the approximate inference in a Deep Belief Network (DBN), making large-scale learning of DBM\u2019s practical. Finally, we demonstrate that the DBM\u2019s trained using the proposed approximate inference algorithm perform well compared to DBN\u2019s and SVM\u2019s on the MNIST handwritten digit, OCR English letters, and NORB visual object recognition tasks.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/salakhutdinov10a/salakhutdinov10a.pdf",
        "supp": "",
        "pdf_size": 267020,
        "gs_citation": 1199,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11098646544805421266&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 29,
        "aff": "Brain and Cognitive Sciences and CSAIL, Massachusetts Institute of Technology; Department of Computer Science, University of Toronto",
        "aff_domain": "mit.edu;cs.toronto.edu",
        "email": "mit.edu;cs.toronto.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Massachusetts Institute of Technology;University of Toronto",
        "aff_unique_dep": "Brain and Cognitive Sciences and Computer Science and Artificial Intelligence Laboratory;Department of Computer Science",
        "aff_unique_url": "https://web.mit.edu;https://www.utoronto.ca",
        "aff_unique_abbr": "MIT;U of T",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Cambridge;Toronto",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United States;Canada"
    },
    {
        "id": "ad343d25f3",
        "title": "Efficient Multioutput Gaussian Processes through Variational Inducing Kernels",
        "site": "https://proceedings.mlr.press/v9/alvarez10a.html",
        "author": "Mauricio \u00c1lvarez; David Luengo; Michalis Titsias; Neil D. Lawrence",
        "abstract": "Interest in multioutput kernel methods is increasing, whether under the guise of multitask learning, multisensor networks or structured output data. From the Gaussian process perspective a multioutput Mercer kernel is a covariance function over correlated output functions. One way to construct such kernels is based on convolution processes (CP). A key problem for this approach is efficient inference. Alvarez and Lawrence recently presented a sparse approximation for CPs that enabled efficient inference. In this paper, we extend this work in two directions: we introduce the concept of variational inducing functions to handle potential non-smooth functions involved in the kernel CP construction and we consider an alternative approach to approximate inference based on variational methods, extending the work by Titsias (2009) to the multiple output case. We demonstrate our approaches on prediction of school marks, compiler performance and financial time series.",
        "bibtex": "@InProceedings{pmlr-v9-alvarez10a,\n  title = \t {Efficient Multioutput Gaussian Processes through Variational Inducing Kernels},\n  author = \t {\u00c1lvarez, Mauricio and Luengo, David and Titsias, Michalis and Lawrence, Neil D.},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {25--32},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/alvarez10a/alvarez10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/alvarez10a.html},\n  abstract = \t {Interest in multioutput kernel methods is increasing, whether under the guise of multitask learning, multisensor networks or structured output data. From the Gaussian process perspective a multioutput Mercer kernel is a covariance function over correlated output functions. One way to construct such kernels is based on convolution processes (CP). A key problem for this approach is efficient inference. Alvarez and Lawrence recently presented a sparse approximation for CPs that enabled efficient inference. In this paper, we extend this work in two directions: we introduce the concept of variational inducing functions to handle potential non-smooth functions involved in the kernel CP construction and we consider an alternative approach to approximate inference based on variational methods, extending the work by Titsias (2009) to the multiple output case. We demonstrate our approaches on prediction of school marks, compiler performance and financial time series.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/alvarez10a/alvarez10a.pdf",
        "supp": "",
        "pdf_size": 965004,
        "gs_citation": 162,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1649054339554580219&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "School of Computer Science, University of Manchester, Manchester, UK, M13 9PL + Depto. Teor\u0131a de Senal y Comunicaciones, Universidad Carlos III de Madrid, 28911 Leganes, Spain; Depto. Teor\u0131a de Senal y Comunicaciones, Universidad Carlos III de Madrid, 28911 Leganes, Spain; School of Computer Science, University of Manchester, Manchester, UK, M13 9PL; School of Computer Science, University of Manchester, Manchester, UK, M13 9PL",
        "aff_domain": "cs.man.ac.uk;ieee.org;cs.man.ac.uk;cs.man.ac.uk",
        "email": "cs.man.ac.uk;ieee.org;cs.man.ac.uk;cs.man.ac.uk",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1;0;0",
        "aff_unique_norm": "University of Manchester;Universidad Carlos III de Madrid",
        "aff_unique_dep": "School of Computer Science;Depto. Teor\u0131a de Senal y Comunicaciones",
        "aff_unique_url": "https://www.manchester.ac.uk;https://www.uc3m.es",
        "aff_unique_abbr": "UoM;UC3M",
        "aff_campus_unique_index": "0+1;1;0;0",
        "aff_campus_unique": "Manchester;Leganes",
        "aff_country_unique_index": "0+1;1;0;0",
        "aff_country_unique": "United Kingdom;Spain"
    },
    {
        "id": "6a2120da82",
        "title": "Efficient Reductions for Imitation Learning",
        "site": "https://proceedings.mlr.press/v9/ross10a.html",
        "author": "Stephane Ross; Drew Bagnell",
        "abstract": "Imitation Learning, while applied successfully on many large real-world problems, is typically addressed as a standard supervised learning problem, where it is assumed the training and testing data are i.i.d..  This is not true in imitation learning as the learned policy influences the future test inputs (states) upon which it will be tested. We show that this leads to compounding errors and a regret bound that grows quadratically in the time horizon of the task. We propose two alternative algorithms for imitation learning where training occurs over several episodes of interaction. These two approaches share in common that the learner\u2019s policy is slowly modified from executing the expert\u2019s policy to the learned policy. We show that this leads to stronger performance guarantees and demonstrate the improved performance on two challenging problems: training a learner to play 1) a 3D racing game (Super Tux Kart) and 2) Mario Bros.; given input images from the games and corresponding actions taken by a human expert and near-optimal planner respectively.",
        "bibtex": "@InProceedings{pmlr-v9-ross10a,\n  title = \t {Efficient Reductions for Imitation Learning},\n  author = \t {Ross, Stephane and Bagnell, Drew},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {661--668},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/ross10a/ross10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/ross10a.html},\n  abstract = \t {Imitation Learning, while applied successfully on many large real-world problems, is typically addressed as a standard supervised learning problem, where it is assumed the training and testing data are i.i.d..  This is not true in imitation learning as the learned policy influences the future test inputs (states) upon which it will be tested. We show that this leads to compounding errors and a regret bound that grows quadratically in the time horizon of the task. We propose two alternative algorithms for imitation learning where training occurs over several episodes of interaction. These two approaches share in common that the learner\u2019s policy is slowly modified from executing the expert\u2019s policy to the learned policy. We show that this leads to stronger performance guarantees and demonstrate the improved performance on two challenging problems: training a learner to play 1) a 3D racing game (Super Tux Kart) and 2) Mario Bros.; given input images from the games and corresponding actions taken by a human expert and near-optimal planner respectively.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/ross10a/ross10a.pdf",
        "supp": "",
        "pdf_size": 1032877,
        "gs_citation": 1198,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7759057686629248779&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 21,
        "aff": "Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Robotics Institute",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e1a51771e0",
        "title": "Elliptical slice sampling",
        "site": "https://proceedings.mlr.press/v9/murray10a.html",
        "author": "Iain Murray; Ryan Adams; David MacKay",
        "abstract": "Many probabilistic models introduce strong dependencies between variables using a latent multivariate Gaussian distribution or a Gaussian process. We present a new Markov chain Monte Carlo algorithm for performing inference in models with multivariate Gaussian priors. Its key properties are: 1) it has simple, generic code applicable to many models, 2) it has no free parameters, 3) it works well for a variety of Gaussian process based models. These properties make our method ideal for use while model building, removing the need to spend time deriving and tuning updates for more complex algorithms.",
        "bibtex": "@InProceedings{pmlr-v9-murray10a,\n  title = \t {Elliptical slice sampling},\n  author = \t {Murray, Iain and Adams, Ryan and MacKay, David},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {541--548},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/murray10a/murray10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/murray10a.html},\n  abstract = \t {Many probabilistic models introduce strong dependencies between variables using a latent multivariate Gaussian distribution or a Gaussian process. We present a new Markov chain Monte Carlo algorithm for performing inference in models with multivariate Gaussian priors. Its key properties are: 1) it has simple, generic code applicable to many models, 2) it has no free parameters, 3) it works well for a variety of Gaussian process based models. These properties make our method ideal for use while model building, removing the need to spend time deriving and tuning updates for more complex algorithms.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/murray10a/murray10a.pdf",
        "supp": "",
        "pdf_size": 848058,
        "gs_citation": 598,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9214240611165080710&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "e28230c861",
        "title": "Empirical Bernstein Boosting",
        "site": "https://proceedings.mlr.press/v9/shivaswamy10a.html",
        "author": "Pannagadatta Shivaswamy; Tony Jebara",
        "abstract": "Concentration inequalities that incorporate variance information (such as Bernstein\u2019s or Bennett\u2019s inequality) are often significantly tighter than counterparts (such as Hoeffding\u2019s inequality) that disregard variance. Nevertheless, many state of the art machine learning algorithms for classification problems like AdaBoost and support vector machines (SVMs) extensively use Hoeffding\u2019s inequalities to justify empirical risk minimization and its variants. This article proposes a novel boosting algorithm based on a recently introduced principle\u2013sample variance penalization\u2013which is motivated from an empirical version of Bernstein\u2019s inequality.  This framework leads to an efficient algorithm that is as easy to implement as AdaBoost while producing a strict generalization. Experiments on a large number of datasets show significant performance gains over AdaBoost. This paper shows that sample variance penalization could be a viable alternative to empirical risk minimization.",
        "bibtex": "@InProceedings{pmlr-v9-shivaswamy10a,\n  title = \t {Empirical Bernstein Boosting},\n  author = \t {Shivaswamy, Pannagadatta and Jebara, Tony},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {733--740},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/shivaswamy10a/shivaswamy10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/shivaswamy10a.html},\n  abstract = \t {Concentration inequalities that incorporate variance information (such as Bernstein\u2019s or Bennett\u2019s inequality) are often significantly tighter than counterparts (such as Hoeffding\u2019s inequality) that disregard variance. Nevertheless, many state of the art machine learning algorithms for classification problems like AdaBoost and support vector machines (SVMs) extensively use Hoeffding\u2019s inequalities to justify empirical risk minimization and its variants. This article proposes a novel boosting algorithm based on a recently introduced principle\u2013sample variance penalization\u2013which is motivated from an empirical version of Bernstein\u2019s inequality.  This framework leads to an efficient algorithm that is as easy to implement as AdaBoost while producing a strict generalization. Experiments on a large number of datasets show significant performance gains over AdaBoost. This paper shows that sample variance penalization could be a viable alternative to empirical risk minimization.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/shivaswamy10a/shivaswamy10a.pdf",
        "supp": "",
        "pdf_size": 517237,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1007412355015552808&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science, Columbia University, New York NY 10027; Department of Computer Science, Columbia University, New York NY 10027",
        "aff_domain": "cs.columbia.edu;cs.columbia.edu",
        "email": "cs.columbia.edu;cs.columbia.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Columbia University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.columbia.edu",
        "aff_unique_abbr": "Columbia",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "New York",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ca734883db",
        "title": "Exclusive Lasso for Multi-task Feature Selection",
        "site": "https://proceedings.mlr.press/v9/zhou10a.html",
        "author": "Yang Zhou; Rong Jin; Steven Chu\u2013Hong Hoi",
        "abstract": "We propose a novel group regularization which we call exclusive lasso. Unlike the group lasso regularizer that assumes co-varying variables in groups, the proposed exclusive lasso regularizer models the scenario when variables in the same group compete with each other. Analysis is presented to illustrate the properties of the proposed regularizer. We present a framework of kernel-based multi-task feature selection algorithm based on the proposed exclusive lasso regularizer. An efficient algorithm is derived to solve the related optimization problem. Experiments with document categorization show that our approach outperforms state-of-the-art algorithms for multi-task feature selection.",
        "bibtex": "@InProceedings{pmlr-v9-zhou10a,\n  title = \t {Exclusive Lasso for Multi-task Feature Selection},\n  author = \t {Zhou, Yang and Jin, Rong and Hoi, Steven Chu\u2013Hong},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {988--995},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/zhou10a/zhou10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/zhou10a.html},\n  abstract = \t {We propose a novel group regularization which we call exclusive lasso. Unlike the group lasso regularizer that assumes co-varying variables in groups, the proposed exclusive lasso regularizer models the scenario when variables in the same group compete with each other. Analysis is presented to illustrate the properties of the proposed regularizer. We present a framework of kernel-based multi-task feature selection algorithm based on the proposed exclusive lasso regularizer. An efficient algorithm is derived to solve the related optimization problem. Experiments with document categorization show that our approach outperforms state-of-the-art algorithms for multi-task feature selection.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/zhou10a/zhou10a.pdf",
        "supp": "",
        "pdf_size": 508136,
        "gs_citation": 288,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3157540089503508956&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Department of Computer Science and Engineering, Michigan State University; Department of Computer Science and Engineering, Michigan State University; School of Computer Engineering, Nanyang Technological University",
        "aff_domain": "msu.edu;msu.edu;ntu.edu.sg",
        "email": "msu.edu;msu.edu;ntu.edu.sg",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Michigan State University;Nanyang Technological University",
        "aff_unique_dep": "Department of Computer Science and Engineering;School of Computer Engineering",
        "aff_unique_url": "https://www.msu.edu;https://www.ntu.edu.sg",
        "aff_unique_abbr": "MSU;NTU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "United States;Singapore"
    },
    {
        "id": "6279150530",
        "title": "Exploiting Covariate Similarity in Sparse Regression via the Pairwise Elastic Net",
        "site": "https://proceedings.mlr.press/v9/lorbert10b.html",
        "author": "Alexander Lorbert; David Eis; Victoria Kostina; David Blei; Peter Ramadge",
        "abstract": "A new approach to regression regularization called the Pairwise Elastic Net is proposed. Like the Elastic Net, it simultaneously performs automatic variable selection and continuous shrinkage. In addition, the Pairwise Elastic Net encourages the grouping of strongly correlated predictors based on a pairwise similarity measure. We give examples of how the Pairwise Elastic Net can be used to achieve the objectives of Ridge regression, the Lasso, the Elastic Net, and Group Lasso. Finally, we present a coordinate descent algorithm to solve the Pairwise Elastic Net.",
        "bibtex": "@InProceedings{pmlr-v9-lorbert10b,\n  title = \t {Exploiting Covariate Similarity in Sparse Regression via the Pairwise Elastic Net},\n  author = \t {Lorbert, Alexander and Eis, David and Kostina, Victoria and Blei, David and Ramadge, Peter},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {477--484},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/lorbert10b/lorbert10b.pdf},\n  url = \t {https://proceedings.mlr.press/v9/lorbert10b.html},\n  abstract = \t {A new approach to regression regularization called the Pairwise Elastic Net is proposed. Like the Elastic Net, it simultaneously performs automatic variable selection and continuous shrinkage. In addition, the Pairwise Elastic Net encourages the grouping of strongly correlated predictors based on a pairwise similarity measure. We give examples of how the Pairwise Elastic Net can be used to achieve the objectives of Ridge regression, the Lasso, the Elastic Net, and Group Lasso. Finally, we present a coordinate descent algorithm to solve the Pairwise Elastic Net.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/lorbert10b/lorbert10b.pdf",
        "supp": "",
        "pdf_size": 708207,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16611508881377650582&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "Dept. of Electrical Engineering; Dept. of Electrical Engineering; Dept. of Electrical Engineering; Dept. of Computer Science; Dept. of Electrical Engineering",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University Affiliation Not Specified",
        "aff_unique_dep": "Department of Electrical Engineering",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "0cfd00dc65",
        "title": "Exploiting Feature Covariance in High-Dimensional Online Learning",
        "site": "https://proceedings.mlr.press/v9/ma10a.html",
        "author": "Justin Ma; Alex Kulesza; Mark Dredze; Koby Crammer; Lawrence Saul; Fernando Pereira",
        "abstract": "Some online algorithms for linear classification model the uncertainty in their weights over the course of learning.  Modeling the full covariance structure of the weights can provide a significant advantage for classification.  However, for high-dimensional, large-scale data, even though there may be many second-order feature interactions, it is computationally infeasible to maintain this covariance structure. To extend second-order methods to high-dimensional data, we develop low-rank approximations of the covariance structure. We evaluate our approach on both synthetic and real-world data sets using the confidence-weighted online learning framework. We show improvements over diagonal covariance matrices for both low and high-dimensional data.",
        "bibtex": "@InProceedings{pmlr-v9-ma10a,\n  title = \t {Exploiting Feature Covariance in High-Dimensional Online Learning},\n  author = \t {Ma, Justin and Kulesza, Alex and Dredze, Mark and Crammer, Koby and Saul, Lawrence and Pereira, Fernando},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {493--500},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/ma10a/ma10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/ma10a.html},\n  abstract = \t {Some online algorithms for linear classification model the uncertainty in their weights over the course of learning.  Modeling the full covariance structure of the weights can provide a significant advantage for classification.  However, for high-dimensional, large-scale data, even though there may be many second-order feature interactions, it is computationally infeasible to maintain this covariance structure. To extend second-order methods to high-dimensional data, we develop low-rank approximations of the covariance structure. We evaluate our approach on both synthetic and real-world data sets using the confidence-weighted online learning framework. We show improvements over diagonal covariance matrices for both low and high-dimensional data.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/ma10a/ma10a.pdf",
        "supp": "",
        "pdf_size": 529858,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9220110121434873316&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 26,
        "aff": "UC San Diego; University of Pennsylvania; Johns Hopkins University; The Technion; UC San Diego; Google",
        "aff_domain": "cs.ucsd.edu;cis.upenn.edu;cs.jhu.edu;ee.technion.ac.il;cs.ucsd.edu;google.com",
        "email": "cs.ucsd.edu;cis.upenn.edu;cs.jhu.edu;ee.technion.ac.il;cs.ucsd.edu;google.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3;0;4",
        "aff_unique_norm": "University of California, San Diego;University of Pennsylvania;Johns Hopkins University;Technion - Israel Institute of Technology;Google",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://www.ucsd.edu;https://www.upenn.edu;https://www.jhu.edu;https://www.technion.ac.il/en/;https://www.google.com",
        "aff_unique_abbr": "UCSD;UPenn;JHU;Technion;Google",
        "aff_campus_unique_index": "0;0;2",
        "aff_campus_unique": "San Diego;;Mountain View",
        "aff_country_unique_index": "0;0;0;1;0;0",
        "aff_country_unique": "United States;Israel"
    },
    {
        "id": "451f6023fc",
        "title": "Exploiting Within-Clique Factorizations in Junction-Tree Algorithms",
        "site": "https://proceedings.mlr.press/v9/mcauley10a.html",
        "author": "Julian McAuley; Tiberio Caetano",
        "abstract": "We show that the expected computational complexity of the Junction-Tree Algorithm for maximum a posteriori inference in graphical models can be improved. Our results apply whenever the potentials over maximal cliques of the triangulated graph are factored over subcliques. This is common in many real applications, as we illustrate with several examples. The new algorithms are easily implemented, and experiments show substantial speed-ups over the classical Junction-Tree Algorithm. This enlarges the class of models for which exact inference is efficient.",
        "bibtex": "@InProceedings{pmlr-v9-mcauley10a,\n  title = \t {Exploiting Within-Clique Factorizations in Junction-Tree Algorithms},\n  author = \t {McAuley, Julian and Caetano, Tiberio},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {525--532},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/mcauley10a/mcauley10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/mcauley10a.html},\n  abstract = \t {We show that the expected computational complexity of the Junction-Tree Algorithm for maximum a posteriori inference in graphical models can be improved. Our results apply whenever the potentials over maximal cliques of the triangulated graph are factored over subcliques. This is common in many real applications, as we illustrate with several examples. The new algorithms are easily implemented, and experiments show substantial speed-ups over the classical Junction-Tree Algorithm. This enlarges the class of models for which exact inference is efficient.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/mcauley10a/mcauley10a.pdf",
        "supp": "",
        "pdf_size": 2841364,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9009549111048398156&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 19,
        "aff": "NICTA - Australian National University; NICTA - Australian National University",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Australian National University",
        "aff_unique_dep": "NICTA",
        "aff_unique_url": "https://www.anu.edu.au",
        "aff_unique_abbr": "ANU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "06354f2c7f",
        "title": "Factored 3-Way Restricted Boltzmann Machines For Modeling Natural Images",
        "site": "https://proceedings.mlr.press/v9/ranzato10a.html",
        "author": "Marc\u2019Aurelio Ranzato; Alex Krizhevsky; Geoffrey Hinton",
        "abstract": "Deep belief nets have been successful in modeling handwritten characters, but it has proved more difficult to apply them to real images. The problem lies in the restricted Boltzmann machine (RBM) which is used as a module for learning deep belief nets one layer at a time. The Gaussian-Binary RBMs that have been used to model real-valued data are not a good way to model the covariance structure of natural images. We propose a factored 3-way RBM that uses the states of its hidden units to represent abnormalities in the local covariance structure of an image. This provides a probabilistic framework for the widely used simple/complex cell architecture. Our model learns binary features that work very well for object recognition on the \u201ctiny images\u201d data set. Even better features are obtained by then using standard binary RBM\u2019s to learn a deeper model.",
        "bibtex": "@InProceedings{pmlr-v9-ranzato10a,\n  title = \t {Factored 3-Way Restricted Boltzmann Machines For Modeling Natural Images},\n  author = \t {Ranzato, Marc\u2019Aurelio and Krizhevsky, Alex and Hinton, Geoffrey},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {621--628},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/ranzato10a/ranzato10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/ranzato10a.html},\n  abstract = \t {Deep belief nets have been successful in modeling handwritten characters, but it has proved more difficult to apply them to real images. The problem lies in the restricted Boltzmann machine (RBM) which is used as a module for learning deep belief nets one layer at a time. The Gaussian-Binary RBMs that have been used to model real-valued data are not a good way to model the covariance structure of natural images. We propose a factored 3-way RBM that uses the states of its hidden units to represent abnormalities in the local covariance structure of an image. This provides a probabilistic framework for the widely used simple/complex cell architecture. Our model learns binary features that work very well for object recognition on the \u201ctiny images\u201d data set. Even better features are obtained by then using standard binary RBM\u2019s to learn a deeper model.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/ranzato10a/ranzato10a.pdf",
        "supp": "",
        "pdf_size": 3289861,
        "gs_citation": 299,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14413593275167682647&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "a35493b23e",
        "title": "Factorized Orthogonal Latent Spaces",
        "site": "https://proceedings.mlr.press/v9/salzmann10a.html",
        "author": "Mathieu Salzmann; Carl Henrik Ek; Raquel Urtasun; Trevor Darrell",
        "abstract": "Existing approaches to multi-view learning are particularly effective when the views are either independent (i.e, multi-kernel approaches) or fully dependent (i.e., shared latent spaces). However, in real scenarios, these assumptions are almost never truly satisfied. Recently, two methods have attempted to tackle this problem by factorizing the information and learn separate latent spaces for modeling the shared (i.e., correlated) and private (i.e., independent) parts of the data. However, these approaches are very sensitive to parameters setting or initialization. In this paper we propose a robust approach to factorizing the latent space into shared and private spaces by introducing orthogonality constraints, which penalize redundant latent representations. Furthermore, unlike previous approaches, we simultaneously learn the structure and dimensionality of the latent spaces by relying on a regularizer that encourages the latent space of each data stream to be low dimensional. To demonstrate the benefits of our approach, we apply it to two existing shared latent space models that assume full dependence of the views, the sGPLVM and the sKIE, and show that our constraints improve the performance of these models on the task of pose estimation from monocular images.",
        "bibtex": "@InProceedings{pmlr-v9-salzmann10a,\n  title = \t {Factorized Orthogonal Latent Spaces},\n  author = \t {Salzmann, Mathieu and Ek, Carl Henrik and Urtasun, Raquel and Darrell, Trevor},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {701--708},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/salzmann10a/salzmann10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/salzmann10a.html},\n  abstract = \t {Existing approaches to multi-view learning are particularly effective when the views are either independent (i.e, multi-kernel approaches) or fully dependent (i.e., shared latent spaces). However, in real scenarios, these assumptions are almost never truly satisfied. Recently, two methods have attempted to tackle this problem by factorizing the information and learn separate latent spaces for modeling the shared (i.e., correlated) and private (i.e., independent) parts of the data. However, these approaches are very sensitive to parameters setting or initialization. In this paper we propose a robust approach to factorizing the latent space into shared and private spaces by introducing orthogonality constraints, which penalize redundant latent representations. Furthermore, unlike previous approaches, we simultaneously learn the structure and dimensionality of the latent spaces by relying on a regularizer that encourages the latent space of each data stream to be low dimensional. To demonstrate the benefits of our approach, we apply it to two existing shared latent space models that assume full dependence of the views, the sGPLVM and the sKIE, and show that our constraints improve the performance of these models on the task of pose estimation from monocular images.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/salzmann10a/salzmann10a.pdf",
        "supp": "",
        "pdf_size": 673614,
        "gs_citation": 154,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3180135923543251093&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "094f2bb2de",
        "title": "Fast Active-set-type Algorithms for $l1$-regularized Linear Regression",
        "site": "https://proceedings.mlr.press/v9/kim10a.html",
        "author": "Jingu Kim; Haesun Park",
        "abstract": "In this paper, we investigate new active-set-type methods for l1-regularized linear regression that overcome some difficulties of existing active set methods. By showing a relationship between $l1$-regularized linear regression and the linear complementarity problem with bounds, we present a fast active-set-type method, called block principal pivoting. This method accelerates computation by allowing exchanges of several variables among working sets. We further provide an improvement of this method, discuss its properties, and also explain a connection to the structure learning of Gaussian graphical models. Experimental comparisons on synthetic and real data sets show that the proposed method is significantly faster than existing active set methods and competitive against recently developed iterative methods.",
        "bibtex": "@InProceedings{pmlr-v9-kim10a,\n  title = \t {Fast Active-set-type Algorithms for L1-regularized Linear Regression},\n  author = \t {Kim, Jingu and Park, Haesun},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {397--404},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/kim10a/kim10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/kim10a.html},\n  abstract = \t {In this paper, we investigate new active-set-type methods for l1-regularized linear regression that overcome some difficulties of existing active set methods. By showing a relationship between $l1$-regularized linear regression and the linear complementarity problem with bounds, we present a fast active-set-type method, called block principal pivoting. This method accelerates computation by allowing exchanges of several variables among working sets. We further provide an improvement of this method, discuss its properties, and also explain a connection to the structure learning of Gaussian graphical models. Experimental comparisons on synthetic and real data sets show that the proposed method is significantly faster than existing active set methods and competitive against recently developed iterative methods.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/kim10a/kim10a.pdf",
        "supp": "",
        "pdf_size": 603075,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16289336496445404643&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "School of Computational Science and Engineering, College of Computing, Georgia Institute of Technology; School of Computational Science and Engineering, College of Computing, Georgia Institute of Technology",
        "aff_domain": "cc.gatech.edu;cc.gatech.edu",
        "email": "cc.gatech.edu;cc.gatech.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Georgia Institute of Technology",
        "aff_unique_dep": "School of Computational Science and Engineering",
        "aff_unique_url": "https://www.gatech.edu",
        "aff_unique_abbr": "Georgia Tech",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Atlanta",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ff0a78338c",
        "title": "Feature Selection using Multiple Streams",
        "site": "https://proceedings.mlr.press/v9/dhillon10a.html",
        "author": "Paramveer Dhillon; Dean Foster; Lyle Ungar",
        "abstract": "Feature selection for supervised learning can be greatly improved by making use of the fact that features often come in classes. For example, in gene  expression data, the genes which serve as features may be  divided into classes based on their membership in gene families or pathways. When labeling words with senses for word sense disambiguation, features fall into classes including adjacent words, their parts of speech, and the topic and venue of the document the word is in. We present a streamwise feature selection method that allows  dynamic generation and selection of features, while taking advantage of the different feature classes, and the fact that they are of different sizes and have different (but unknown) fractions of good features. Experimental results show that our approach provides significant improvement in performance and is computationally less expensive than comparable \u201cbatch\u201d methods that do not take advantage of the feature classes and expect all features to be known in advance.",
        "bibtex": "@InProceedings{pmlr-v9-dhillon10a,\n  title = \t {Feature Selection using Multiple Streams},\n  author = \t {Dhillon, Paramveer and Foster, Dean and Ungar, Lyle},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {153--160},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/dhillon10a/dhillon10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/dhillon10a.html},\n  abstract = \t {Feature selection for supervised learning can be greatly improved by making use of the fact that features often come in classes. For example, in gene  expression data, the genes which serve as features may be  divided into classes based on their membership in gene families or pathways. When labeling words with senses for word sense disambiguation, features fall into classes including adjacent words, their parts of speech, and the topic and venue of the document the word is in. We present a streamwise feature selection method that allows  dynamic generation and selection of features, while taking advantage of the different feature classes, and the fact that they are of different sizes and have different (but unknown) fractions of good features. Experimental results show that our approach provides significant improvement in performance and is computationally less expensive than comparable \u201cbatch\u201d methods that do not take advantage of the feature classes and expect all features to be known in advance.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/dhillon10a/dhillon10a.pdf",
        "supp": "",
        "pdf_size": 511161,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1094086825924546376&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Computer and Information Science, University of Pennsylvania, Philadelphia, PA, U.S.A; Statistics, Wharton School, University of Pennsylvania, Philadelphia, PA, U.S.A; Computer and Information Science, University of Pennsylvania, Philadelphia, PA, U.S.A",
        "aff_domain": "; ; ",
        "email": "; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Pennsylvania",
        "aff_unique_dep": "Computer and Information Science",
        "aff_unique_url": "https://www.upenn.edu",
        "aff_unique_abbr": "UPenn",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Philadelphia",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "34dd866edb",
        "title": "Fluid Dynamics Models for Low Rank Discriminant Analysis",
        "site": "https://proceedings.mlr.press/v9/noh10a.html",
        "author": "Yung\u2013Kyun Noh; Byoung\u2013Tak Zhang; Daniel Lee",
        "abstract": "We consider the problem of reducing the dimensionality of labeled data for classification. Unfortunately, the optimal approach of finding the low-dimensional projection with minimal Bayes classification error is intractable, so most standard algorithms optimize a tractable heuristic function in the projected subspace. Here, we investigate a physics-based model where we consider the labeled data as interacting fluid distributions. We derive the forces arising in the fluids from information theoretic potential functions, and consider appropriate low rank constraints on the resulting acceleration and velocity flow fields.  We show how to apply the Gauss principle of least constraint in fluids to obtain tractable solutions for low rank projections. Our fluid dynamic approach is demonstrated to better approximate the Bayes optimal solution on Gaussian systems, including infinite dimensional Gaussian processes.",
        "bibtex": "@InProceedings{pmlr-v9-noh10a,\n  title = \t {Fluid Dynamics Models for Low Rank Discriminant Analysis},\n  author = \t {Noh, Yung\u2013Kyun and Zhang, Byoung\u2013Tak and Lee, Daniel},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {565--572},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/noh10a/noh10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/noh10a.html},\n  abstract = \t {We consider the problem of reducing the dimensionality of labeled data for classification. Unfortunately, the optimal approach of finding the low-dimensional projection with minimal Bayes classification error is intractable, so most standard algorithms optimize a tractable heuristic function in the projected subspace. Here, we investigate a physics-based model where we consider the labeled data as interacting fluid distributions. We derive the forces arising in the fluids from information theoretic potential functions, and consider appropriate low rank constraints on the resulting acceleration and velocity flow fields.  We show how to apply the Gauss principle of least constraint in fluids to obtain tractable solutions for low rank projections. Our fluid dynamic approach is demonstrated to better approximate the Bayes optimal solution on Gaussian systems, including infinite dimensional Gaussian processes.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/noh10a/noh10a.pdf",
        "supp": "",
        "pdf_size": 1488639,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16766568229212179375&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "GRASP Lab, University of Pennsylvania, Philadelphia, PA 19104, USA+Biointelligence Lab, Seoul National University, Seoul 151-742, Korea; Biointelligence Lab, Seoul National University, Seoul 151-742, Korea; GRASP Lab, University of Pennsylvania, Philadelphia, PA 19104, USA",
        "aff_domain": "seas.upenn.edu;bi.snu.ac.kr;seas.upenn.edu",
        "email": "seas.upenn.edu;bi.snu.ac.kr;seas.upenn.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1;0",
        "aff_unique_norm": "University of Pennsylvania;Seoul National University",
        "aff_unique_dep": "GRASP Lab;Biointelligence Lab",
        "aff_unique_url": "https://www.upenn.edu;https://www.snu.ac.kr",
        "aff_unique_abbr": "UPenn;SNU",
        "aff_campus_unique_index": "0+1;1;0",
        "aff_campus_unique": "Philadelphia;Seoul",
        "aff_country_unique_index": "0+1;1;0",
        "aff_country_unique": "United States;South Korea"
    },
    {
        "id": "3a0cf6e8cc",
        "title": "Focused Belief Propagation for Query-Specific Inference",
        "site": "https://proceedings.mlr.press/v9/chechetka10a.html",
        "author": "Anton Chechetka; Carlos Guestrin",
        "abstract": "With the increasing popularity of large-scale probabilistic graphical models, even \u201clightweight\u201d approximate inference methods are becoming infeasible. Fortunately, often large parts of the model are of no immediate interest to the end user. Given the variable that the user actually cares about, we show how to quantify edge importance in graphical models and to significantly speed up inference by focusing computation on important parts of the model. Our algorithm empirically demonstrates convergence speedup by multiple times over state of the art",
        "bibtex": "@InProceedings{pmlr-v9-chechetka10a,\n  title = \t {Focused Belief Propagation for Query-Specific Inference},\n  author = \t {Chechetka, Anton and Guestrin, Carlos},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {89--96},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/chechetka10a/chechetka10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/chechetka10a.html},\n  abstract = \t {With the increasing popularity of large-scale probabilistic graphical models, even \u201clightweight\u201d approximate inference methods are becoming infeasible. Fortunately, often large parts of the model are of no immediate interest to the end user. Given the variable that the user actually cares about, we show how to quantify edge importance in graphical models and to significantly speed up inference by focusing computation on important parts of the model. Our algorithm empirically demonstrates convergence speedup by multiple times over state of the art}\n}",
        "pdf": "http://proceedings.mlr.press/v9/chechetka10a/chechetka10a.pdf",
        "supp": "",
        "pdf_size": 1540266,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16790501354852011941&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Carnegie Mellon University; Carnegie Mellon University",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c5f53704ef",
        "title": "Gaussian processes with monotonicity information",
        "site": "https://proceedings.mlr.press/v9/riihimaki10a.html",
        "author": "Jaakko Riihim\u00e4ki; Aki Vehtari",
        "abstract": "A method for using monotonicity information in multivariate Gaussian process regression and classification is proposed. Monotonicity information is introduced with virtual derivative observations, and the resulting posterior is approximated with expectation propagation. Behaviour of the method is illustrated with artificial regression examples, and the method is used in a real world health care classification problem to include monotonicity information with respect to one of the covariates.",
        "bibtex": "@InProceedings{pmlr-v9-riihimaki10a,\n  title = \t {Gaussian processes with monotonicity information},\n  author = \t {Riihim\u00e4ki, Jaakko and Vehtari, Aki},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {645--652},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/riihimaki10a/riihimaki10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/riihimaki10a.html},\n  abstract = \t {A method for using monotonicity information in multivariate Gaussian process regression and classification is proposed. Monotonicity information is introduced with virtual derivative observations, and the resulting posterior is approximated with expectation propagation. Behaviour of the method is illustrated with artificial regression examples, and the method is used in a real world health care classification problem to include monotonicity information with respect to one of the covariates.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/riihimaki10a/riihimaki10a.pdf",
        "supp": "",
        "pdf_size": 451437,
        "gs_citation": 263,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6782795359533366843&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Dept. of Biomedical Engineering and Computational Science, Aalto University; Dept. of Biomedical Engineering and Computational Science, Aalto University",
        "aff_domain": "tkk.fi;tkk.fi",
        "email": "tkk.fi;tkk.fi",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Aalto University",
        "aff_unique_dep": "Dept. of Biomedical Engineering and Computational Science",
        "aff_unique_url": "https://www.aalto.fi",
        "aff_unique_abbr": "Aalto",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Finland"
    },
    {
        "id": "3a998406c8",
        "title": "Graphical Gaussian modelling of multivariate time series with latent variables",
        "site": "https://proceedings.mlr.press/v9/eichler10a.html",
        "author": "Michael Eichler",
        "abstract": "In time series analysis, inference about cause-effect relationships among multiple times series is commonly based on the concept of Granger causality, which exploits temporal structure to achieve causal ordering of dependent variables. One major problem in the application of Granger causality for the identification of causal relationships is the possible presence of latent variables that affect the measured components and thus lead to so-called spurious causalities. In this paper, we describe a new graphical approach for modelling the dependence structure of multivariate stationary time series that are affected by latent variables. To this end, we introduce dynamic maximal ancestral graphs (dMAGs), in which each time series is represented by a single vertex. For Gaussian processes, this approach leads to vector autoregressive models with errors that are not independent but correlated according to the dashed edges in the graph. We discuss identifiability of the parameters and show that these models can be viewed as graphical ARMA models that satisfy the Granger causality restrictions encoded by the associated dynamic maximal ancestral graph.",
        "bibtex": "@InProceedings{pmlr-v9-eichler10a,\n  title = \t {Graphical Gaussian modelling of multivariate time series with latent variables},\n  author = \t {Eichler, Michael},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {193--200},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/eichler10a/eichler10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/eichler10a.html},\n  abstract = \t {In time series analysis, inference about cause-effect relationships among multiple times series is commonly based on the concept of Granger causality, which exploits temporal structure to achieve causal ordering of dependent variables. One major problem in the application of Granger causality for the identification of causal relationships is the possible presence of latent variables that affect the measured components and thus lead to so-called spurious causalities. In this paper, we describe a new graphical approach for modelling the dependence structure of multivariate stationary time series that are affected by latent variables. To this end, we introduce dynamic maximal ancestral graphs (dMAGs), in which each time series is represented by a single vertex. For Gaussian processes, this approach leads to vector autoregressive models with errors that are not independent but correlated according to the dashed edges in the graph. We discuss identifiability of the parameters and show that these models can be viewed as graphical ARMA models that satisfy the Granger causality restrictions encoded by the associated dynamic maximal ancestral graph.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/eichler10a/eichler10a.pdf",
        "supp": "",
        "pdf_size": 546960,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4070689569353940354&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Quantitative Economics, Maastricht University, NL",
        "aff_domain": "maastrichtuniversity.nl",
        "email": "maastrichtuniversity.nl",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Maastricht University",
        "aff_unique_dep": "Department of Quantitative Economics",
        "aff_unique_url": "https://www.maastrichtuniversity.nl",
        "aff_unique_abbr": "MU",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Netherlands"
    },
    {
        "id": "a918ee7f09",
        "title": "Guarantees for Approximate Incremental SVMs",
        "site": "https://proceedings.mlr.press/v9/usunier10a.html",
        "author": "Nicolas Usunier; Antoine Bordes; L\u00e9on Bottou",
        "abstract": "Assume a teacher provides examples one by one. An approximate incremental SVM computes a sequence of classifiers that are close to the true SVM solutions computed on the successive incremental training sets. We show that simple algorithms can satisfy an averaged accuracy criterion with a computational cost that scales as well as the best SVM algorithms with the number of examples. Finally, we exhibit some experiments highlighting the benefits of joining fast incremental optimization and curriculum and active learning (Schon and Cohn, 2000; Bordes et al., 2005; Bengio et al., 2009).",
        "bibtex": "@InProceedings{pmlr-v9-usunier10a,\n  title = \t {Guarantees for Approximate Incremental SVMs},\n  author = \t {Usunier, Nicolas and Bordes, Antoine and Bottou, L\u00e9on},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {884--891},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/usunier10a/usunier10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/usunier10a.html},\n  abstract = \t {Assume a teacher provides examples one by one. An approximate incremental SVM computes a sequence of classifiers that are close to the true SVM solutions computed on the successive incremental training sets. We show that simple algorithms can satisfy an averaged accuracy criterion with a computational cost that scales as well as the best SVM algorithms with the number of examples. Finally, we exhibit some experiments highlighting the benefits of joining fast incremental optimization and curriculum and active learning (Schon and Cohn, 2000; Bordes et al., 2005; Bengio et al., 2009).}\n}",
        "pdf": "http://proceedings.mlr.press/v9/usunier10a/usunier10a.pdf",
        "supp": "",
        "pdf_size": 1394789,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2476408697405658524&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "LIP6, Universit\u00e9 Paris 6, Paris, France; LIP6, Universit\u00e9 Paris 6, Paris, France; NEC Laboratories America, Princeton, USA",
        "aff_domain": "lip6.fr;lip6.fr;bottou.org",
        "email": "lip6.fr;lip6.fr;bottou.org",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Universit\u00e9 Paris 6;NEC Laboratories America",
        "aff_unique_dep": "LIP6;",
        "aff_unique_url": "https://www.upmc.fr;https://www.nec-labs.com",
        "aff_unique_abbr": "UP6;NEC Labs",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Paris;Princeton",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "France;United States"
    },
    {
        "id": "fc11060167",
        "title": "HOP-MAP: Efficient Message Passing with High Order Potentials",
        "site": "https://proceedings.mlr.press/v9/tarlow10a.html",
        "author": "Daniel Tarlow; Inmar Givoni; Richard Zemel",
        "abstract": "There is a growing interest in building probabilistic models with high order potentials (HOPs), or interactions, among discrete variables.  Message passing inference in such models generally takes time exponential in the size of the interaction, but in some cases maximum a posteriori (MAP) inference can be carried out efficiently.   We build upon such results,  introducing two new classes, including composite HOPs that allow us to flexibly combine tractable HOPs using simple logical switching rules. We present efficient message update algorithms for the new HOPs, and we improve upon the efficiency of message updates for a general class of existing HOPs. Importantly, we present both new and existing HOPs in a common representation;  performing inference with any combination of these HOPs requires no change of representations or new derivations.",
        "bibtex": "@InProceedings{pmlr-v9-tarlow10a,\n  title = \t {HOP-MAP: Efficient Message Passing with High Order Potentials},\n  author = \t {Tarlow, Daniel and Givoni, Inmar and Zemel, Richard},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {812--819},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/tarlow10a/tarlow10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/tarlow10a.html},\n  abstract = \t {There is a growing interest in building probabilistic models with high order potentials (HOPs), or interactions, among discrete variables.  Message passing inference in such models generally takes time exponential in the size of the interaction, but in some cases maximum a posteriori (MAP) inference can be carried out efficiently.   We build upon such results,  introducing two new classes, including composite HOPs that allow us to flexibly combine tractable HOPs using simple logical switching rules. We present efficient message update algorithms for the new HOPs, and we improve upon the efficiency of message updates for a general class of existing HOPs. Importantly, we present both new and existing HOPs in a common representation;  performing inference with any combination of these HOPs requires no change of representations or new derivations.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/tarlow10a/tarlow10a.pdf",
        "supp": "",
        "pdf_size": 844305,
        "gs_citation": 173,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6472653526928024610&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "b4a5f10d41",
        "title": "Half Transductive Ranking",
        "site": "https://proceedings.mlr.press/v9/bai10a.html",
        "author": "Bing Bai; Jason Weston; David Grangier; Ronan Collobert; Corinna Cortes; Mehryar Mohri",
        "abstract": "We study the standard retrieval task of ranking a fixed set of items given a previously unseen query and pose it as the half transductive ranking problem. The task is transductive as the set of items is fixed. Transductive representations (where the vector representation of each example is learned) allow the generation of highly nonlinear embeddings that capture object relationships without relying on a specific choice of features, and require only relatively simple optimization. Unfortunately, they have no direct out-of-sample extension. Inductive approaches on the other hand allow for the representation of unknown queries. We describe algorithms for this setting which have the advantages of both transductive and inductive approaches, and can be applied in unsupervised (either reconstruction-based or graph-based) and supervised ranking setups. We show empirically that our methods give strong performance on all three tasks.",
        "bibtex": "@InProceedings{pmlr-v9-bai10a,\n  title = \t {Half Transductive Ranking},\n  author = \t {Bai, Bing and Weston, Jason and Grangier, David and Collobert, Ronan and Cortes, Corinna and Mohri, Mehryar},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {49--56},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/bai10a/bai10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/bai10a.html},\n  abstract = \t {We study the standard retrieval task of ranking a fixed set of items given a previously unseen query and pose it as the half transductive ranking problem. The task is transductive as the set of items is fixed. Transductive representations (where the vector representation of each example is learned) allow the generation of highly nonlinear embeddings that capture object relationships without relying on a specific choice of features, and require only relatively simple optimization. Unfortunately, they have no direct out-of-sample extension. Inductive approaches on the other hand allow for the representation of unknown queries. We describe algorithms for this setting which have the advantages of both transductive and inductive approaches, and can be applied in unsupervised (either reconstruction-based or graph-based) and supervised ranking setups. We show empirically that our methods give strong performance on all three tasks.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/bai10a/bai10a.pdf",
        "supp": "",
        "pdf_size": 870277,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1175607680460345894&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 22,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "c8ccc479a6",
        "title": "Hartigan\u2019s Method: k-means Clustering without Voronoi",
        "site": "https://proceedings.mlr.press/v9/telgarsky10a.html",
        "author": "Matus Telgarsky; Andrea Vattani",
        "abstract": "Hartigan\u2019s method for $k$-means clustering is the following greedy heuristic: select a point, and optimally reassign it.  This paper develops two other formulations of the heuristic, one leading to a number of consistency properties, the other showing that the data partition is always quite separated from the induced Voronoi partition.  A characterization of the volume of this separation is provided.  Empirical tests verify not only good optimization performance relative to Lloyd\u2019s method, but also good running time.",
        "bibtex": "@InProceedings{pmlr-v9-telgarsky10a,\n  title = \t {Hartigan's Method: k-means Clustering without Voronoi},\n  author = \t {Telgarsky, Matus and Vattani, Andrea},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {820--827},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/telgarsky10a/telgarsky10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/telgarsky10a.html},\n  abstract = \t {Hartigan\u2019s method for $k$-means clustering is the following greedy heuristic: select a point, and optimally reassign it.  This paper develops two other formulations of the heuristic, one leading to a number of consistency properties, the other showing that the data partition is always quite separated from the induced Voronoi partition.  A characterization of the volume of this separation is provided.  Empirical tests verify not only good optimization performance relative to Lloyd\u2019s method, but also good running time.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/telgarsky10a/telgarsky10a.pdf",
        "supp": "",
        "pdf_size": 817098,
        "gs_citation": 121,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15092157900985748249&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "University of California, San Diego; University of California, San Diego",
        "aff_domain": "cs.ucsd.edu;cs.ucsd.edu",
        "email": "cs.ucsd.edu;cs.ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b28eeab3d0",
        "title": "Identifying Cause and Effect on Discrete Data using Additive Noise Models",
        "site": "https://proceedings.mlr.press/v9/peters10a.html",
        "author": "Jonas Peters; Dominik Janzing; Bernhard Sch\u00f6lkopf",
        "abstract": "Inferring the causal structure of a set of random variables from a finite sample of the joint distribution is an important problem in science. Recently, methods using additive noise models have been suggested to approach the case of continuous variables. In many situations, however, the variables of interest are discrete or even have only finitely many states. In this work we extend the notion of additive noise models to these cases. Whenever the joint distribution $\\mathbf{P}^{(X,Y)}$ admits such a model in one direction, e.g. $Y = f(X)+N$, $N \\perp \\!\\!\\! \\perp X$, it does not admit the reversed model  $X=g(Y)+\\tilde{N}$, $\\tilde{N} \\perp \\!\\!\\! \\perp Y$ as long as the model is chosen in a generic way. Based on these deliberations we propose an efficient new algorithm that is able to distinguish between cause and effect for a finite sample of discrete variables. We show that this algorithm works both on synthetic and real data sets.",
        "bibtex": "@InProceedings{pmlr-v9-peters10a,\n  title = \t {Identifying Cause and Effect on Discrete Data using Additive Noise Models},\n  author = \t {Peters, Jonas and Janzing, Dominik and Sch\u00f6lkopf, Bernhard},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {597--604},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/peters10a/peters10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/peters10a.html},\n  abstract = \t {Inferring the causal structure of a set of random variables from a finite sample of the joint distribution is an important problem in science. Recently, methods using additive noise models have been suggested to approach the case of continuous variables. In many situations, however, the variables of interest are discrete or even have only finitely many states. In this work we extend the notion of additive noise models to these cases. Whenever the joint distribution $\\mathbf{P}^{(X,Y)}$ admits such a model in one direction, e.g. $Y = f(X)+N$, $N \\perp \\!\\!\\! \\perp X$, it does not admit the reversed model  $X=g(Y)+\\tilde{N}$, $\\tilde{N} \\perp \\!\\!\\! \\perp Y$ as long as the model is chosen in a generic way. Based on these deliberations we propose an efficient new algorithm that is able to distinguish between cause and effect for a finite sample of discrete variables. We show that this algorithm works both on synthetic and real data sets.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/peters10a/peters10a.pdf",
        "supp": "",
        "pdf_size": 801752,
        "gs_citation": 92,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15460195782528535698&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "MPI for Biological Cybernetics; MPI for Biological Cybernetics; MPI for Biological Cybernetics",
        "aff_domain": "tuebingen.mpg.de;tuebingen.mpg.de;tuebingen.mpg.de",
        "email": "tuebingen.mpg.de;tuebingen.mpg.de;tuebingen.mpg.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Max Planck Institute for Biological Cybernetics",
        "aff_unique_dep": "Biological Cybernetics",
        "aff_unique_url": "https://www.biological-cybernetics.de",
        "aff_unique_abbr": "MPIBC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "79631ea01d",
        "title": "Impossibility Theorems for Domain Adaptation",
        "site": "https://proceedings.mlr.press/v9/david10a.html",
        "author": "Shai Ben David; Tyler Lu; Teresa Luu; David Pal",
        "abstract": "The domain adaptation problem in machine learning occurs when the test data generating distribution differs from the one that generates the training data. It is clear that the success of learning under such circumstances depends on similarities between the two data distributions. We study assumptions about the relationship between the two distributions that one needed for domain adaptation learning to succeed.  We analyze the assumptions in an agnostic PAC-style learning model for a the setting in which the learner can access a labeled training data sample and an unlabeled sample generated by the test data distribution. We focus on three assumptions: (i) Similarity between the unlabeled distributions, (ii) Existence of a classifier in the hypothesis class with low error on both training and testing distributions, and (iii) The covariate shift assumption. I.e., the assumption that the conditioned label distribution (for each data point) is the same for both the training and test distributions.  We show that without either assumption (i) or (ii), the combination of the remaining assumptions is not sufficient to guarantee successful learning. Our negative results hold with respect to any domain adaptation learning algorithm, as long as it does not have access to target labeled examples.  In particular, we provide formal proofs that the popular covariate shift assumption is rather weak and does not relieve the necessity of the other assumptions.  We also discuss the intuitively appealing paradigm of reweighing the labeled training sample according to the target unlabeled distribution. We show that, somewhat counter intuitively, that paradigm cannot be trusted in the following sense. There are DA tasks that are indistinguishable, as far as the input training data goes, but in which reweighing leads to significant improvement in one task, while causing dramatic deterioration of the learning success in the other.",
        "bibtex": "@InProceedings{pmlr-v9-david10a,\n  title = \t {Impossibility Theorems for Domain Adaptation},\n  author = \t {David, Shai Ben and Lu, Tyler and Luu, Teresa and Pal, David},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {129--136},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/david10a/david10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/david10a.html},\n  abstract = \t {The domain adaptation problem in machine learning occurs when the test data generating distribution differs from the one that generates the training data. It is clear that the success of learning under such circumstances depends on similarities between the two data distributions. We study assumptions about the relationship between the two distributions that one needed for domain adaptation learning to succeed.  We analyze the assumptions in an agnostic PAC-style learning model for a the setting in which the learner can access a labeled training data sample and an unlabeled sample generated by the test data distribution. We focus on three assumptions: (i) Similarity between the unlabeled distributions, (ii) Existence of a classifier in the hypothesis class with low error on both training and testing distributions, and (iii) The covariate shift assumption. I.e., the assumption that the conditioned label distribution (for each data point) is the same for both the training and test distributions.  We show that without either assumption (i) or (ii), the combination of the remaining assumptions is not sufficient to guarantee successful learning. Our negative results hold with respect to any domain adaptation learning algorithm, as long as it does not have access to target labeled examples.  In particular, we provide formal proofs that the popular covariate shift assumption is rather weak and does not relieve the necessity of the other assumptions.  We also discuss the intuitively appealing paradigm of reweighing the labeled training sample according to the target unlabeled distribution. We show that, somewhat counter intuitively, that paradigm cannot be trusted in the following sense. There are DA tasks that are indistinguishable, as far as the input training data goes, but in which reweighing leads to significant improvement in one task, while causing dramatic deterioration of the learning success in the other.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/david10a/david10a.pdf",
        "supp": "",
        "pdf_size": 492618,
        "gs_citation": 387,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16402586863811012991&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "School of Computer Science, University of Waterloo; School of Computer Science, University of Waterloo; Dept. of Computer Science, University of Toronto; Department of Computing Science, University of Alberta",
        "aff_domain": "cs.uwaterloo.ca;cs.uwaterloo.ca;cs.toronto.edu;cs.ualberta.ca",
        "email": "cs.uwaterloo.ca;cs.uwaterloo.ca;cs.toronto.edu;cs.ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2",
        "aff_unique_norm": "University of Waterloo;University of Toronto;University of Alberta",
        "aff_unique_dep": "School of Computer Science;Department of Computer Science;Department of Computing Science",
        "aff_unique_url": "https://uwaterloo.ca;https://www.utoronto.ca;https://www.ualberta.ca",
        "aff_unique_abbr": "UWaterloo;U of T;UAlberta",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Waterloo;Toronto;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "675f1d51fd",
        "title": "Improving posterior marginal approximations in latent Gaussian models",
        "site": "https://proceedings.mlr.press/v9/cseke10a.html",
        "author": "Botond Cseke; Tom Heskes",
        "abstract": "We consider the problem of correcting the posterior marginal approximations computed by expectation propagation and Laplace approximation in latent Gaussian models and propose correction methods that are similar in spirit to the Laplace approximation of Tierney and Kadane (1986). We show that in the case of sparse Gaussian models, the computational complexity of expectation propagation can be made comparable to that of the Laplace approximation by using a parallel updating scheme. In some cases, expectation propagation gives excellent estimates, where the Laplace approximation fails. Inspired by bounds on the marginal corrections, we arrive at factorized approximations, which can be applied on top of both expectation propagation and Laplace. These give nearly indistinguishable results from the non-factorized approximations in a fraction of the time.",
        "bibtex": "@InProceedings{pmlr-v9-cseke10a,\n  title = \t {Improving posterior marginal approximations in latent Gaussian models},\n  author = \t {Cseke, Botond and Heskes, Tom},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {121--128},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/cseke10a/cseke10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/cseke10a.html},\n  abstract = \t {We consider the problem of correcting the posterior marginal approximations computed by expectation propagation and Laplace approximation in latent Gaussian models and propose correction methods that are similar in spirit to the Laplace approximation of Tierney and Kadane (1986). We show that in the case of sparse Gaussian models, the computational complexity of expectation propagation can be made comparable to that of the Laplace approximation by using a parallel updating scheme. In some cases, expectation propagation gives excellent estimates, where the Laplace approximation fails. Inspired by bounds on the marginal corrections, we arrive at factorized approximations, which can be applied on top of both expectation propagation and Laplace. These give nearly indistinguishable results from the non-factorized approximations in a fraction of the time.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/cseke10a/cseke10a.pdf",
        "supp": "",
        "pdf_size": 1400474,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=850145644382107253&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Radboud University Nijmegen, Institute for Computing and Information Sciences; Radboud University Nijmegen, Institute for Computing and Information Sciences",
        "aff_domain": "science.ru.nl;science.ru.nl",
        "email": "science.ru.nl;science.ru.nl",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Radboud University",
        "aff_unique_dep": "Institute for Computing and Information Sciences",
        "aff_unique_url": "https://www.ru.nl/",
        "aff_unique_abbr": "RU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Nijmegen",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Netherlands"
    },
    {
        "id": "d38673b123",
        "title": "Incremental Sparsification for Real-time Online Model Learning",
        "site": "https://proceedings.mlr.press/v9/nguyen_tuong10a.html",
        "author": "Duy Nguyen\u2013Tuong; Jan Peters",
        "abstract": "Online model learning in real-time is required by many applications, for example, robot tracking control. It poses a difficult problem, as fast and incremental online regression with large data sets is the essential component and cannot be realized by straightforward usage of off-the-shelf machine learning methods such as Gaussian process regression or support vector regression. In this paper, we propose a framework for online, incremental sparsification with a fixed budget designed for large scale real-time model learning. The proposed approach combines a sparsification method based on an independency measure with a large scale database. In combination with an incremental learning approach such as sequential support vector regression, we obtain a regression method which is applicable in real-time online learning. It exhibits competitive learning accuracy when compared with standard regression techniques. Implementation on a real robot emphasizes the applicability of the proposed approach in real-time online model learning for real world systems.",
        "bibtex": "@InProceedings{pmlr-v9-nguyen_tuong10a,\n  title = \t {Incremental Sparsification for Real-time Online Model Learning},\n  author = \t {Nguyen\u2013Tuong, Duy and Peters, Jan},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {557--564},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/nguyen_tuong10a/nguyen_tuong10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/nguyen_tuong10a.html},\n  abstract = \t {Online model learning in real-time is required by many applications, for example, robot tracking control. It poses a difficult problem, as fast and incremental online regression with large data sets is the essential component and cannot be realized by straightforward usage of off-the-shelf machine learning methods such as Gaussian process regression or support vector regression. In this paper, we propose a framework for online, incremental sparsification with a fixed budget designed for large scale real-time model learning. The proposed approach combines a sparsification method based on an independency measure with a large scale database. In combination with an incremental learning approach such as sequential support vector regression, we obtain a regression method which is applicable in real-time online learning. It exhibits competitive learning accuracy when compared with standard regression techniques. Implementation on a real robot emphasizes the applicability of the proposed approach in real-time online model learning for real world systems.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/nguyen_tuong10a/nguyen_tuong10a.pdf",
        "supp": "",
        "pdf_size": 875302,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1869765784594141439&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Max Planck Institute for Biological Cybernetics; Max Planck Institute for Biological Cybernetics",
        "aff_domain": "tuebingen.mpg.de;tuebingen.mpg.de",
        "email": "tuebingen.mpg.de;tuebingen.mpg.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Max Planck Institute for Biological Cybernetics",
        "aff_unique_dep": "Biological Cybernetics",
        "aff_unique_url": "https://www.biocybernetics.mpg.de",
        "aff_unique_abbr": "MPIBC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "42ba029fb0",
        "title": "Inductive Principles for Restricted Boltzmann Machine Learning",
        "site": "https://proceedings.mlr.press/v9/marlin10a.html",
        "author": "Benjamin Marlin; Kevin Swersky; Bo Chen; Nando Freitas",
        "abstract": "Recent research has seen the proposal of several new inductive principles designed specifically to avoid the problems associated with maximum likelihood learning in models with intractable partition functions. In this paper, we study learning methods for binary restricted Boltzmann machines (RBMs) based on ratio matching and generalized score matching. We compare these new RBM learning methods to a range of existing learning methods including stochastic maximum likelihood, contrastive divergence, and pseudo-likelihood. We perform an extensive empirical evaluation across multiple tasks and data sets.",
        "bibtex": "@InProceedings{pmlr-v9-marlin10a,\n  title = \t {Inductive Principles for Restricted Boltzmann Machine Learning},\n  author = \t {Marlin, Benjamin and Swersky, Kevin and Chen, Bo and Freitas, Nando},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {509--516},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/marlin10a/marlin10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/marlin10a.html},\n  abstract = \t {Recent research has seen the proposal of several new inductive principles designed specifically to avoid the problems associated with maximum likelihood learning in models with intractable partition functions. In this paper, we study learning methods for binary restricted Boltzmann machines (RBMs) based on ratio matching and generalized score matching. We compare these new RBM learning methods to a range of existing learning methods including stochastic maximum likelihood, contrastive divergence, and pseudo-likelihood. We perform an extensive empirical evaluation across multiple tasks and data sets.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/marlin10a/marlin10a.pdf",
        "supp": "",
        "pdf_size": 841167,
        "gs_citation": 210,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7006522565861675272&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Department of Computer Science, University of British Columbia; Department of Computer Science, University of British Columbia; Department of Computer Science, University of British Columbia; Department of Computer Science, University of British Columbia",
        "aff_domain": "cs.ubc.ca;cs.ubc.ca;cs.ubc.ca;cs.ubc.ca",
        "email": "cs.ubc.ca;cs.ubc.ca;cs.ubc.ca;cs.ubc.ca",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of British Columbia",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ubc.ca",
        "aff_unique_abbr": "UBC",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Vancouver",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "d330f1a2c6",
        "title": "Inference and Learning in Networks of Queues",
        "site": "https://proceedings.mlr.press/v9/sutton10a.html",
        "author": "Charles Sutton; Michael I. Jordan",
        "abstract": "Probabilistic models of the performance of computer systems are useful both for predicting system performance in new conditions, and for diagnosing past performance problems. The most popular performance models are networks of queues. However, no current methods exist for parameter estimation or inference in networks of queues with missing data. In this paper, we present a novel viewpoint that combines queueing networks and graphical models, allowing Markov chain Monte Carlo to be applied. We demonstrate the effectiveness of our sampler on real-world data from a benchmark Web application.",
        "bibtex": "@InProceedings{pmlr-v9-sutton10a,\n  title = \t {Inference and Learning in Networks of Queues},\n  author = \t {Sutton, Charles and Jordan, Michael I.},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {796--803},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/sutton10a/sutton10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/sutton10a.html},\n  abstract = \t {Probabilistic models of the performance of computer systems are useful both for predicting system performance in new conditions, and for diagnosing past performance problems. The most popular performance models are networks of queues. However, no current methods exist for parameter estimation or inference in networks of queues with missing data. In this paper, we present a novel viewpoint that combines queueing networks and graphical models, allowing Markov chain Monte Carlo to be applied. We demonstrate the effectiveness of our sampler on real-world data from a benchmark Web application.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/sutton10a/sutton10a.pdf",
        "supp": "",
        "pdf_size": 857929,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3098050045955360176&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "School of Informatics, University of Edinburgh; Computer Science Division, University of California, Berkeley",
        "aff_domain": "inf.ed.ac.uk;eecs.berkeley.edu",
        "email": "inf.ed.ac.uk;eecs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Edinburgh;University of California, Berkeley",
        "aff_unique_dep": "School of Informatics;Computer Science Division",
        "aff_unique_url": "https://www.ed.ac.uk;https://www.berkeley.edu",
        "aff_unique_abbr": "Edinburgh;UC Berkeley",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Edinburgh;Berkeley",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "a372b53623",
        "title": "Inference of Sparse Networks with Unobserved Variables. Application to Gene Regulatory Networks",
        "site": "https://proceedings.mlr.press/v9/slavov10a.html",
        "author": "Nikolai Slavov",
        "abstract": "Networks are becoming a unifying framework for modeling complex systems and network inference problems are frequently encountered in many fields. Here, I develop and apply a generative approach to network inference (RCweb) for the case when the network is sparse and the latent (not observed) variables affect the observed ones. From all possible factor analysis (FA) decompositions explaining the variance in the data, RCweb selects the FA decomposition that is consistent with a sparse underlying network. The sparsity constraint is imposed by a novel method that significantly outperforms (in terms of accuracy, robustness to noise, complexity scaling and computational efficiency) methods using $\\ell 1$ norm relaxation such as K-SVD and $\\ell 1$-based sparse principle component analysis (PCA). Results from simulated models demonstrate that RCweb recovers exactly the model structures for sparsity as low (as non-sparse) as 50% and with ratio of unobserved to observed variables as high as 2. RCweb is robust to noise, with gradual decrease in the parameter ranges as the noise level increases.",
        "bibtex": "@InProceedings{pmlr-v9-slavov10a,\n  title = \t {Inference of Sparse Networks with Unobserved Variables. Application to Gene Regulatory Networks},\n  author = \t {Slavov, Nikolai},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {757--764},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/slavov10a/slavov10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/slavov10a.html},\n  abstract = \t {Networks are becoming a unifying framework for modeling complex systems and network inference problems are frequently encountered in many fields. Here, I develop and apply a generative approach to network inference (RCweb) for the case when the network is sparse and the latent (not observed) variables affect the observed ones. From all possible factor analysis (FA) decompositions explaining the variance in the data, RCweb selects the FA decomposition that is consistent with a sparse underlying network. The sparsity constraint is imposed by a novel method that significantly outperforms (in terms of accuracy, robustness to noise, complexity scaling and computational efficiency) methods using $\\ell 1$ norm relaxation such as K-SVD and $\\ell 1$-based sparse principle component analysis (PCA). Results from simulated models demonstrate that RCweb recovers exactly the model structures for sparsity as low (as non-sparse) as 50% and with ratio of unobserved to observed variables as high as 2. RCweb is robust to noise, with gradual decrease in the parameter ranges as the noise level increases.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/slavov10a/slavov10a.pdf",
        "supp": "",
        "pdf_size": 1158652,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8800713572319856782&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Princeton University, NJ 08544",
        "aff_domain": "princeton.edu",
        "email": "princeton.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Princeton University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.princeton.edu",
        "aff_unique_abbr": "Princeton",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "fc2535c941",
        "title": "Infinite Predictor Subspace Models for Multitask Learning",
        "site": "https://proceedings.mlr.press/v9/rai10a.html",
        "author": "Piyush Rai; Hal Daum\u00e9 III",
        "abstract": "Given several related learning tasks, we propose a nonparametric Bayesian model that captures task relatedness by assuming that the task parameters (i.e., predictors) share a latent subspace. More specifically, the intrinsic dimensionality of the task subspace is not assumed to be known a priori. We use an infinite latent feature model to automatically infer this number (depending on and limited by only the number of tasks). Furthermore, our approach is applicable when the underlying task parameter subspace is inherently sparse, drawing parallels with l1 regularization and LASSO-style models. We also propose an augmented model which can make use of (labeled, and additionally unlabeled if available) inputs to assist learning this subspace, leading to further improvements in the performance. Experimental results demonstrate the efficacy of both the proposed approaches, especially when the number of examples per task is small. Finally, we discuss an extension of the proposed framework where a nonparametric mixture of linear subspaces can be used to learn a manifold over the task parameters, and also deal with the issue of negative transfer from unrelated tasks.",
        "bibtex": "@InProceedings{pmlr-v9-rai10a,\n  title = \t {Infinite Predictor Subspace Models for Multitask Learning},\n  author =       {Rai, Piyush and Daum\\'e, III, Hal},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {613--620},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/rai10a/rai10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/rai10a.html},\n  abstract = \t {Given several related learning tasks, we propose a nonparametric Bayesian model that captures task relatedness by assuming that the task parameters (i.e., predictors) share a latent subspace. More specifically, the intrinsic dimensionality of the task subspace is not assumed to be known a priori. We use an infinite latent feature model to automatically infer this number (depending on and limited by only the number of tasks). Furthermore, our approach is applicable when the underlying task parameter subspace is inherently sparse, drawing parallels with l1 regularization and LASSO-style models. We also propose an augmented model which can make use of (labeled, and additionally unlabeled if available) inputs to assist learning this subspace, leading to further improvements in the performance. Experimental results demonstrate the efficacy of both the proposed approaches, especially when the number of examples per task is small. Finally, we discuss an extension of the proposed framework where a nonparametric mixture of linear subspaces can be used to learn a manifold over the task parameters, and also deal with the issue of negative transfer from unrelated tasks.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/rai10a/rai10a.pdf",
        "supp": "",
        "pdf_size": 500902,
        "gs_citation": 87,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4564162466313763721&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "School of Computing, University of Utah; School of Computing, University of Utah",
        "aff_domain": "cs.utah.edu;cs.utah.edu",
        "email": "cs.utah.edu;cs.utah.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Utah",
        "aff_unique_dep": "School of Computing",
        "aff_unique_url": "https://www.utah.edu",
        "aff_unique_abbr": "U of U",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Utah",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "98eec5afa2",
        "title": "Kernel Partial Least Squares is Universally Consistent",
        "site": "https://proceedings.mlr.press/v9/blanchard10a.html",
        "author": "Gilles Blanchard; Nicole Kr\u00e4mer",
        "abstract": "We prove the statistical consistency of kernel Partial Least Squares Regression applied to a bounded regression learning problem on a reproducing kernel Hilbert space. Partial Least Squares  stands out of well-known classical approaches as e.g. Ridge Regression or Principal Components Regression, as it is not defined as the solution of a global cost minimization procedure over a fixed model nor is it a linear estimator. Instead, approximate solutions are constructed by projections onto a nested set of data-dependent subspaces. To prove consistency, we exploit the known fact that Partial Least Squares is equivalent to the conjugate gradient algorithm in combination with early stopping. The choice of the stopping rule (number of iterations) is a crucial point. We study two empirical stopping rules. The first one monitors the estimation error in each iteration step of Partial Least Squares, and the second one estimates the empirical complexity in terms of a condition number. Both stopping rules lead to universally consistent estimators provided the kernel is universal.",
        "bibtex": "@InProceedings{pmlr-v9-blanchard10a,\n  title = \t {Kernel Partial Least Squares is Universally Consistent},\n  author = \t {Blanchard, Gilles and Kr\u00e4mer, Nicole},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {57--64},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/blanchard10a/blanchard10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/blanchard10a.html},\n  abstract = \t {We prove the statistical consistency of kernel Partial Least Squares Regression applied to a bounded regression learning problem on a reproducing kernel Hilbert space. Partial Least Squares  stands out of well-known classical approaches as e.g. Ridge Regression or Principal Components Regression, as it is not defined as the solution of a global cost minimization procedure over a fixed model nor is it a linear estimator. Instead, approximate solutions are constructed by projections onto a nested set of data-dependent subspaces. To prove consistency, we exploit the known fact that Partial Least Squares is equivalent to the conjugate gradient algorithm in combination with early stopping. The choice of the stopping rule (number of iterations) is a crucial point. We study two empirical stopping rules. The first one monitors the estimation error in each iteration step of Partial Least Squares, and the second one estimates the empirical complexity in terms of a condition number. Both stopping rules lead to universally consistent estimators provided the kernel is universal.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/blanchard10a/blanchard10a.pdf",
        "supp": "",
        "pdf_size": 908708,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2280617070107837837&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "f59cc7e1ac",
        "title": "Learning Bayesian Network Structure using LP Relaxations",
        "site": "https://proceedings.mlr.press/v9/jaakkola10a.html",
        "author": "Tommi Jaakkola; David Sontag; Amir Globerson; Marina Meila",
        "abstract": "We propose to solve the combinatorial problem of finding the highest scoring Bayesian network structure from data. This structure learning problem can be viewed as an inference problem where the variables specify the choice of parents for each node in the graph. The key combinatorial difficulty arises from the global constraint that the graph structure has to be acyclic. We cast the structure learning problem as a linear program over the polytope defined by valid acyclic structures. In relaxing this problem, we maintain an outer bound approximation to the polytope and iteratively tighten it by searching over a new class of valid constraints. If an integral solution is found, it is guaranteed to be the optimal Bayesian network. When the relaxation is not tight, the fast dual algorithms we develop remain useful in combination with a branch and bound method. Empirical results suggest that the method is competitive or faster than alternative exact methods based on dynamic programming.",
        "bibtex": "@InProceedings{pmlr-v9-jaakkola10a,\n  title = \t {Learning Bayesian Network Structure using LP Relaxations},\n  author = \t {Jaakkola, Tommi and Sontag, David and Globerson, Amir and Meila, Marina},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {358--365},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/jaakkola10a/jaakkola10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/jaakkola10a.html},\n  abstract = \t {We propose to solve the combinatorial problem of finding the highest scoring Bayesian network structure from data. This structure learning problem can be viewed as an inference problem where the variables specify the choice of parents for each node in the graph. The key combinatorial difficulty arises from the global constraint that the graph structure has to be acyclic. We cast the structure learning problem as a linear program over the polytope defined by valid acyclic structures. In relaxing this problem, we maintain an outer bound approximation to the polytope and iteratively tighten it by searching over a new class of valid constraints. If an integral solution is found, it is guaranteed to be the optimal Bayesian network. When the relaxation is not tight, the fast dual algorithms we develop remain useful in combination with a branch and bound method. Empirical results suggest that the method is competitive or faster than alternative exact methods based on dynamic programming.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/jaakkola10a/jaakkola10a.pdf",
        "supp": "",
        "pdf_size": 833359,
        "gs_citation": 309,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2590180176192586495&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "31b72e536e",
        "title": "Learning Causal Structure from Overlapping Variable Sets",
        "site": "https://proceedings.mlr.press/v9/triantafillou10a.html",
        "author": "Sofia Triantafillou; Ioannis Tsamardinos; Ioannis Tollis",
        "abstract": "We present an algorithm name cSAT+ for learning the causal structure in a domain from datasets measuring different variables sets. The algorithm outputs a graph with edges corresponding to all possible pairwise causal relations between two variables, named Pairwise Causal Graph (PCG). Examples of interesting inferences include the induction of the absence or presence of some causal relation between two variables never measured together. cSAT+ converts the problem to a series of SAT problems, obtaining leverage from the efficiency of state-of-the-art solvers. In our empirical evaluation, it is shown to outperform ION, the first algorithm solving a similar but more general problem, by two orders of magnitude.",
        "bibtex": "@InProceedings{pmlr-v9-triantafillou10a,\n  title = \t {Learning Causal Structure from Overlapping Variable Sets},\n  author = \t {Triantafillou, Sofia and Tsamardinos, Ioannis and Tollis, Ioannis},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {860--867},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/triantafillou10a/triantafillou10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/triantafillou10a.html},\n  abstract = \t {We present an algorithm name cSAT+ for learning the causal structure in a domain from datasets measuring different variables sets. The algorithm outputs a graph with edges corresponding to all possible pairwise causal relations between two variables, named Pairwise Causal Graph (PCG). Examples of interesting inferences include the induction of the absence or presence of some causal relation between two variables never measured together. cSAT+ converts the problem to a series of SAT problems, obtaining leverage from the efficiency of state-of-the-art solvers. In our empirical evaluation, it is shown to outperform ION, the first algorithm solving a similar but more general problem, by two orders of magnitude.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/triantafillou10a/triantafillou10a.pdf",
        "supp": "",
        "pdf_size": 959580,
        "gs_citation": 68,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=864378282269150754&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Computer Science Department, University of Crete; Computer Science Department, University of Crete; Computer Science Department, University of Crete",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Crete",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.uoc.gr",
        "aff_unique_abbr": "UoC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Greece"
    },
    {
        "id": "7668384eed",
        "title": "Learning Exponential Families in High-Dimensions: Strong Convexity and Sparsity",
        "site": "https://proceedings.mlr.press/v9/kakade10a.html",
        "author": "Sham Kakade; Ohad Shamir; Karthik Sindharan; Ambuj Tewari",
        "abstract": "The versatility of exponential families, along with their attendant convexity properties, make them a popular and effective statistical model. A central issue is learning these models in high-dimensions when the optimal parameter  vector is sparse. This work characterizes a certain strong convexity property of general exponential families, which allows their generalization ability to be quantified. In particular, we show how this property can be used to analyze generic exponential families under L1 regularization.",
        "bibtex": "@InProceedings{pmlr-v9-kakade10a,\n  title = \t {Learning Exponential Families in High-Dimensions: Strong Convexity and Sparsity},\n  author = \t {Kakade, Sham and Shamir, Ohad and Sindharan, Karthik and Tewari, Ambuj},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {381--388},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/kakade10a/kakade10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/kakade10a.html},\n  abstract = \t {The versatility of exponential families, along with their attendant convexity properties, make them a popular and effective statistical model. A central issue is learning these models in high-dimensions when the optimal parameter  vector is sparse. This work characterizes a certain strong convexity property of general exponential families, which allows their generalization ability to be quantified. In particular, we show how this property can be used to analyze generic exponential families under L1 regularization.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/kakade10a/kakade10a.pdf",
        "supp": "",
        "pdf_size": 660093,
        "gs_citation": 88,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5039717833388393717&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 24,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "11390a3937",
        "title": "Learning Nonlinear Dynamic Models from Non-sequenced Data",
        "site": "https://proceedings.mlr.press/v9/huang10c.html",
        "author": "Tzu\u2013Kuo Huang; Le Song; Jeff Schneider",
        "abstract": "Virtually all methods of learning dynamic systems from data start from the same basic assumption: the learning algorithm will be given a sequence, or trajectory, of data generated from the dynamic system.  We consider the case where the data is not sequenced.  The training data points come from the system\u2019s operation but with no temporal ordering.  The data are simply drawn as individual disconnected points.  While making this assumption may seem absurd at first glance, many scientific modeling tasks have exactly this property.  Previous work proposed methods for learning linear, discrete time models under these assumptions by optimizing approximate likelihood functions.  In this paper, we extend those methods to nonlinear models using kernel methods.  We go on to propose a new approach to solving the problem that focuses on achieving temporal smoothness in the learned dynamics.  The result is a convex criterion that can be easily optimized and often outperforms the earlier methods.  We test these methods on several synthetic data sets including one generated from the Lorenz attractor.",
        "bibtex": "@InProceedings{pmlr-v9-huang10c,\n  title = \t {Learning Nonlinear Dynamic Models from Non-sequenced Data},\n  author = \t {Huang, Tzu\u2013Kuo and Song, Le and Schneider, Jeff},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {350--357},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/huang10c/huang10c.pdf},\n  url = \t {https://proceedings.mlr.press/v9/huang10c.html},\n  abstract = \t {Virtually all methods of learning dynamic systems from data start from the same basic assumption: the learning algorithm will be given a sequence, or trajectory, of data generated from the dynamic system.  We consider the case where the data is not sequenced.  The training data points come from the system\u2019s operation but with no temporal ordering.  The data are simply drawn as individual disconnected points.  While making this assumption may seem absurd at first glance, many scientific modeling tasks have exactly this property.  Previous work proposed methods for learning linear, discrete time models under these assumptions by optimizing approximate likelihood functions.  In this paper, we extend those methods to nonlinear models using kernel methods.  We go on to propose a new approach to solving the problem that focuses on achieving temporal smoothness in the learned dynamics.  The result is a convex criterion that can be easily optimized and often outperforms the earlier methods.  We test these methods on several synthetic data sets including one generated from the Lorenz attractor.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/huang10c/huang10c.pdf",
        "supp": "",
        "pdf_size": 648917,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1986174658899083294&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "7084ab9e10",
        "title": "Learning Policy Improvements with Path Integrals",
        "site": "https://proceedings.mlr.press/v9/theodorou10a.html",
        "author": "Evangelos Theodorou; Jonas Buchli; Stefan Schaal",
        "abstract": "With the goal to generate more scalable algorithms with higher efficiency and fewer open parameters, reinforcement learning (RL) has recently moved towards combining classical techniques from optimal control and dynamic programming with modern learning techniques from statistical estimation theory. In this vein, this paper suggests to use the framework of stochastic optimal control with path integrals to derive a novel approach to RL with parametrized policies. While solidly grounded in value function estimation and optimal control based on the stochastic Hamilton-Jacobi-Bellman (HJB) equations, policy improvements can be transformed into an approximation problem of a path integral which has no open parameters other than the exploration noise. The resulting algorithm can be conceived of as model-based, semi-model-based, or even model free, depending on how the learning problem is structured.   Our new algorithm demonstrates interesting similarities with previous RL research in the framework of probability matching and provides intuition why the slightly heuristically motivated probability matching approach can actually perform well. Empirical evaluations demonstrate significant performance improvements over gradient-based policy learning and scalability to high-dimensional control problems.  We believe that Policy  Improvement with Path Integrals (PI$^2$) offers currently one of the most efficient, numerically robust, and easy to implement algorithms for RL based on trajectory roll-outs.",
        "bibtex": "@InProceedings{pmlr-v9-theodorou10a,\n  title = \t {Learning Policy Improvements with Path Integrals},\n  author = \t {Theodorou, Evangelos and Buchli, Jonas and Schaal, Stefan},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {828--835},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/theodorou10a/theodorou10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/theodorou10a.html},\n  abstract = \t {With the goal to generate more scalable algorithms with higher efficiency and fewer open parameters, reinforcement learning (RL) has recently moved towards combining classical techniques from optimal control and dynamic programming with modern learning techniques from statistical estimation theory. In this vein, this paper suggests to use the framework of stochastic optimal control with path integrals to derive a novel approach to RL with parametrized policies. While solidly grounded in value function estimation and optimal control based on the stochastic Hamilton-Jacobi-Bellman (HJB) equations, policy improvements can be transformed into an approximation problem of a path integral which has no open parameters other than the exploration noise. The resulting algorithm can be conceived of as model-based, semi-model-based, or even model free, depending on how the learning problem is structured.   Our new algorithm demonstrates interesting similarities with previous RL research in the framework of probability matching and provides intuition why the slightly heuristically motivated probability matching approach can actually perform well. Empirical evaluations demonstrate significant performance improvements over gradient-based policy learning and scalability to high-dimensional control problems.  We believe that Policy  Improvement with Path Integrals (PI$^2$) offers currently one of the most efficient, numerically robust, and easy to implement algorithms for RL based on trajectory roll-outs.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/theodorou10a/theodorou10a.pdf",
        "supp": "",
        "pdf_size": 2119270,
        "gs_citation": 88,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=646577347284203227&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "University of Southern California; ETH Zurich; University of Southern California",
        "aff_domain": "usc.edu;ethz.ch;usc.edu",
        "email": "usc.edu;ethz.ch;usc.edu",
        "github": "",
        "project": "http://www-clmc.usc.edu",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Southern California;ETH Zurich",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.usc.edu;https://www.ethz.ch",
        "aff_unique_abbr": "USC;ETHZ",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;Switzerland"
    },
    {
        "id": "2d3b45703e",
        "title": "Learning the Structure of Deep Sparse Graphical Models",
        "site": "https://proceedings.mlr.press/v9/adams10a.html",
        "author": "Ryan P. Adams; Hanna Wallach; Zoubin Ghahramani",
        "abstract": "Deep belief networks are a powerful way to model complex probability   distributions.  However, it is difficult to learn the structure of a   belief network, particularly one with hidden units.  The Indian   buffet process has been used as a nonparametric Bayesian prior on   the structure of a directed belief network with a single infinitely   wide hidden layer. Here, we introduce the cascading Indian   buffet process (CIBP), which provides a prior on the structure of a   layered, directed belief network that is unbounded in both depth and   width, yet allows tractable inference.  We use the CIBP prior with   the nonlinear Gaussian belief network framework to allow each unit   to vary its behavior between discrete and continuous   representations.  We use Markov chain Monte Carlo for inference in   this model and explore the structures learned on image data.",
        "bibtex": "@InProceedings{pmlr-v9-adams10a,\n  title = \t {Learning the Structure of Deep Sparse Graphical Models},\n  author = \t {Adams, Ryan P. and Wallach, Hanna and Ghahramani, Zoubin},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1--8},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/adams10a/adams10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/adams10a.html},\n  abstract = \t {Deep belief networks are a powerful way to model complex probability   distributions.  However, it is difficult to learn the structure of a   belief network, particularly one with hidden units.  The Indian   buffet process has been used as a nonparametric Bayesian prior on   the structure of a directed belief network with a single infinitely   wide hidden layer. Here, we introduce the cascading Indian   buffet process (CIBP), which provides a prior on the structure of a   layered, directed belief network that is unbounded in both depth and   width, yet allows tractable inference.  We use the CIBP prior with   the nonlinear Gaussian belief network framework to allow each unit   to vary its behavior between discrete and continuous   representations.  We use Markov chain Monte Carlo for inference in   this model and explore the structures learned on image data.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/adams10a/adams10a.pdf",
        "supp": "",
        "pdf_size": 1327108,
        "gs_citation": 100,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3765553615562796443&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 20,
        "aff": "University of Toronto; University of Massachusetts Amherst; University of Cambridge",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University of Toronto;University of Massachusetts Amherst;University of Cambridge",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.utoronto.ca;https://www.umass.edu;https://www.cam.ac.uk",
        "aff_unique_abbr": "U of T;UMass Amherst;Cambridge",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Amherst;Cambridge",
        "aff_country_unique_index": "0;1;2",
        "aff_country_unique": "Canada;United States;United Kingdom"
    },
    {
        "id": "0b4a03ef2a",
        "title": "Learning with Blocks: Composite Likelihood and Contrastive Divergence",
        "site": "https://proceedings.mlr.press/v9/asuncion10a.html",
        "author": "Arthur Asuncion; Qiang Liu; Alexander Ihler; Padhraic Smyth",
        "abstract": "Composite likelihood methods provide a wide spectrum of computationally efficient techniques for statistical tasks such as parameter estimation and model selection. In this paper, we present a formal connection between the optimization of composite likelihoods and the well-known contrastive divergence algorithm. In particular, we show that composite likelihoods can be stochastically optimized by performing a variant of contrastive divergence with random-scan blocked Gibbs sampling. By using higher-order composite likelihoods, our proposed learning framework makes it possible to trade off computation time for increased accuracy. Furthermore, one can choose composite likelihood blocks that match the model\u2019s dependence structure, making the optimization of higher-order composite likelihoods computationally efficient.  We empirically analyze the performance of blocked contrastive divergence on various models, including visible Boltzmann machines, conditional random fields, and exponential random graph models, and we demonstrate that using higher-order blocks improves both the accuracy of parameter estimates and the rate of convergence.",
        "bibtex": "@InProceedings{pmlr-v9-asuncion10a,\n  title = \t {Learning with Blocks: Composite Likelihood and Contrastive Divergence},\n  author = \t {Asuncion, Arthur and Liu, Qiang and Ihler, Alexander and Smyth, Padhraic},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {33--40},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/asuncion10a/asuncion10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/asuncion10a.html},\n  abstract = \t {Composite likelihood methods provide a wide spectrum of computationally efficient techniques for statistical tasks such as parameter estimation and model selection. In this paper, we present a formal connection between the optimization of composite likelihoods and the well-known contrastive divergence algorithm. In particular, we show that composite likelihoods can be stochastically optimized by performing a variant of contrastive divergence with random-scan blocked Gibbs sampling. By using higher-order composite likelihoods, our proposed learning framework makes it possible to trade off computation time for increased accuracy. Furthermore, one can choose composite likelihood blocks that match the model\u2019s dependence structure, making the optimization of higher-order composite likelihoods computationally efficient.  We empirically analyze the performance of blocked contrastive divergence on various models, including visible Boltzmann machines, conditional random fields, and exponential random graph models, and we demonstrate that using higher-order blocks improves both the accuracy of parameter estimates and the rate of convergence.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/asuncion10a/asuncion10a.pdf",
        "supp": "",
        "pdf_size": 991412,
        "gs_citation": 51,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7430597648871978051&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "Department of Computer Science, University of California, Irvine; Department of Computer Science, University of California, Irvine; Department of Computer Science, University of California, Irvine; Department of Computer Science, University of California, Irvine",
        "aff_domain": "ics.uci.edu;ics.uci.edu;ics.uci.edu;ics.uci.edu",
        "email": "ics.uci.edu;ics.uci.edu;ics.uci.edu;ics.uci.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of California, Irvine",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.uci.edu",
        "aff_unique_abbr": "UCI",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Irvine",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "412578fe5c",
        "title": "Locally Linear Denoising on Image Manifolds",
        "site": "https://proceedings.mlr.press/v9/gong10a.html",
        "author": "Dian Gong; Fei Sha; G\u00e9rard Medioni",
        "abstract": "We study the problem of image denoising where images are assumed to be samples from low dimensional  (sub)manifolds. We propose the algorithm of locally linear denoising. The algorithm approximates manifolds with locally linear patches by constructing nearest neighbor graphs. Each image is then locally denoised within its neighborhoods. A global optimal denoising result is then identified by aligning those local estimates. The algorithm has a closed-form solution that is efficient to compute. We evaluated and compared the algorithm to alternative methods on two image data sets. We demonstrated the effectiveness of the proposed algorithm, which yields visually appealing denoising results, incurs smaller reconstruction errors and results in lower error rates when the denoised data are used in supervised learning tasks.",
        "bibtex": "@InProceedings{pmlr-v9-gong10a,\n  title = \t {Locally Linear Denoising on Image Manifolds},\n  author = \t {Gong, Dian and Sha, Fei and Medioni, G\u00e9rard},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {265--272},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/gong10a/gong10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/gong10a.html},\n  abstract = \t {We study the problem of image denoising where images are assumed to be samples from low dimensional  (sub)manifolds. We propose the algorithm of locally linear denoising. The algorithm approximates manifolds with locally linear patches by constructing nearest neighbor graphs. Each image is then locally denoised within its neighborhoods. A global optimal denoising result is then identified by aligning those local estimates. The algorithm has a closed-form solution that is efficient to compute. We evaluated and compared the algorithm to alternative methods on two image data sets. We demonstrated the effectiveness of the proposed algorithm, which yields visually appealing denoising results, incurs smaller reconstruction errors and results in lower error rates when the denoised data are used in supervised learning tasks.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/gong10a/gong10a.pdf",
        "supp": "",
        "pdf_size": 589297,
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18427887613906470909&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "c773b7a70a",
        "title": "Mass Fatality Incident Identification based on nuclear DNA evidence",
        "site": "https://proceedings.mlr.press/v9/corradi10a.html",
        "author": "Fabio Corradi",
        "abstract": "This paper focuses on the use of nuclear DNA Short Tandem Repeat traits for the identification of the victims of a Mass Fatality Incident. The goal of the analysis is  the assessment of the identification probabilities concerning the  recovered victims.  Identification hypotheses are evaluated conditionally to the DNA evidence observed both on the recovered victims and on the relatives of the missing persons disappeared in the tragical event. After specifying  a set of conditional independence assertions suitable for the problem, an inference strategy is provided, treating some points to achieve computational efficiency.  Finally, the proposal is tested through the simulation of a Mass Fatality Incident and the results are  examined in details.",
        "bibtex": "@InProceedings{pmlr-v9-corradi10a,\n  title = \t {Mass Fatality Incident Identification based on nuclear DNA evidence},\n  author = \t {Corradi, Fabio},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {105--112},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/corradi10a/corradi10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/corradi10a.html},\n  abstract = \t {This paper focuses on the use of nuclear DNA Short Tandem Repeat traits for the identification of the victims of a Mass Fatality Incident. The goal of the analysis is  the assessment of the identification probabilities concerning the  recovered victims.  Identification hypotheses are evaluated conditionally to the DNA evidence observed both on the recovered victims and on the relatives of the missing persons disappeared in the tragical event. After specifying  a set of conditional independence assertions suitable for the problem, an inference strategy is provided, treating some points to achieve computational efficiency.  Finally, the proposal is tested through the simulation of a Mass Fatality Incident and the results are  examined in details.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/corradi10a/corradi10a.pdf",
        "supp": "",
        "pdf_size": 448992,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10741927045660270289&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Statistics - University of Florence - Italy",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Florence",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "https://www.unifi.it",
        "aff_unique_abbr": "UNIFI",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Italy"
    },
    {
        "id": "965c97487c",
        "title": "Matrix-Variate Dirichlet Process Mixture Models",
        "site": "https://proceedings.mlr.press/v9/zhang10e.html",
        "author": "Zhihua Zhang; Guang Dai; Michael I. Jordan",
        "abstract": "We are concerned with a multivariate response regression problem where the interest is in considering correlations both across response variates and across response samples. In this paper we develop a new Bayesian nonparametric model for such a setting based on Dirichlet process priors. Building on an additive kernel model, we allow each sample to have its own regression matrix. Although this overcomplete representation could in principle suffer from severe overfitting problems, we are able to provide effective control over the model via a matrix-variate Dirichlet process prior on the regression matrices. Our model is able to share statistical strength among regression matrices due to the clustering property of the Dirichlet process. We make use of a Markov chain Monte Carlo algorithm for inference and prediction. Compared with other Bayesian kernel models, our model has advantages in both computational and statistical efficiency.",
        "bibtex": "@InProceedings{pmlr-v9-zhang10e,\n  title = \t {Matrix-Variate Dirichlet Process Mixture Models},\n  author = \t {Zhang, Zhihua and Dai, Guang and Jordan, Michael I.},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {980--987},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/zhang10e/zhang10e.pdf},\n  url = \t {https://proceedings.mlr.press/v9/zhang10e.html},\n  abstract = \t {We are concerned with a multivariate response regression problem where the interest is in considering correlations both across response variates and across response samples. In this paper we develop a new Bayesian nonparametric model for such a setting based on Dirichlet process priors. Building on an additive kernel model, we allow each sample to have its own regression matrix. Although this overcomplete representation could in principle suffer from severe overfitting problems, we are able to provide effective control over the model via a matrix-variate Dirichlet process prior on the regression matrices. Our model is able to share statistical strength among regression matrices due to the clustering property of the Dirichlet process. We make use of a Markov chain Monte Carlo algorithm for inference and prediction. Compared with other Bayesian kernel models, our model has advantages in both computational and statistical efficiency.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/zhang10e/zhang10e.pdf",
        "supp": "",
        "pdf_size": 884239,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1898470953589889803&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "College of Comp. Sci. and Tech., Zhejiang University, Zhejiang 310027, China; College of Comp. Sci. and Tech., Zhejiang University, Zhejiang 310027, China; Depts. of EECS and Statistics, University of California, Berkeley, Berkeley, CA 94720, USA",
        "aff_domain": "cs.zju.edu.cn;gmail.com;cs.berkeley.edu",
        "email": "cs.zju.edu.cn;gmail.com;cs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Zhejiang University;University of California, Berkeley",
        "aff_unique_dep": "College of Computer Science and Technology;Department of Electrical Engineering and Computer Sciences, Department of Statistics",
        "aff_unique_url": "http://www.zju.edu.cn;https://www.berkeley.edu",
        "aff_unique_abbr": "ZJU;UC Berkeley",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Zhejiang;Berkeley",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "dad1930598",
        "title": "Maximum-likelihood learning of cumulative distribution functions on graphs",
        "site": "https://proceedings.mlr.press/v9/huang10b.html",
        "author": "Jim Huang; Nebojsa Jojic",
        "abstract": "For many applications, a probability model can be easily expressed as a cumulative distribution functions (CDF) as compared to the use of probability density or mass functions (PDF/PMFs).  Cumulative distribution networks (CDNs) have recently been proposed as a class of graphical models for CDFs. One advantage of CDF models is the simplicity of representing multivariate heavy-tailed distributions. Examples of fields that can benefit from the use of graphical models for CDFs include climatology and epidemiology, where data may follow extreme value statistics and exhibit spatial correlations so that dependencies between model variables must be accounted for. The problem of learning from data in such settings may nevertheless consist of optimizing the log-likelihood function with respect to model parameters where we are required to optimize a log-PDF/PMF and not a log-CDF.   We present a message-passing algorithm called the gradient-derivative-product (GDP) algorithm that allows us to learn the model in terms of the log-likelihood function whereby messages correspond to local gradients of the likelihood with respect to model parameters.  We will demonstrate the GDP algorithm on real-world rainfall and H1N1 mortality data and we will show that CDNs provide a natural choice of parameterizations for the heavy-tailed multivariate distributions that arise in these problems.",
        "bibtex": "@InProceedings{pmlr-v9-huang10b,\n  title = \t {Maximum-likelihood learning of cumulative distribution functions on graphs},\n  author = \t {Huang, Jim and Jojic, Nebojsa},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {342--349},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/huang10b/huang10b.pdf},\n  url = \t {https://proceedings.mlr.press/v9/huang10b.html},\n  abstract = \t {For many applications, a probability model can be easily expressed as a cumulative distribution functions (CDF) as compared to the use of probability density or mass functions (PDF/PMFs).  Cumulative distribution networks (CDNs) have recently been proposed as a class of graphical models for CDFs. One advantage of CDF models is the simplicity of representing multivariate heavy-tailed distributions. Examples of fields that can benefit from the use of graphical models for CDFs include climatology and epidemiology, where data may follow extreme value statistics and exhibit spatial correlations so that dependencies between model variables must be accounted for. The problem of learning from data in such settings may nevertheless consist of optimizing the log-likelihood function with respect to model parameters where we are required to optimize a log-PDF/PMF and not a log-CDF.   We present a message-passing algorithm called the gradient-derivative-product (GDP) algorithm that allows us to learn the model in terms of the log-likelihood function whereby messages correspond to local gradients of the likelihood with respect to model parameters.  We will demonstrate the GDP algorithm on real-world rainfall and H1N1 mortality data and we will show that CDNs provide a natural choice of parameterizations for the heavy-tailed multivariate distributions that arise in these problems.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/huang10b/huang10b.pdf",
        "supp": "",
        "pdf_size": 827851,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18222439964500574311&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "8569446cb5",
        "title": "Model-Free Monte Carlo-like Policy Evaluation",
        "site": "https://proceedings.mlr.press/v9/fonteneau10a.html",
        "author": "Raphael Fonteneau; Susan Murphy; Louis Wehenkel; Damien Ernst",
        "abstract": "We propose an algorithm for estimating the finite-horizon expected return of a closed loop control policy from an a priori given (off-policy) sample of one-step transitions. It averages cumulated rewards along a set of \u201cbroken trajectories\u201d made of one-step transitions selected from the sample on the basis of the control policy. Under some Lipschitz continuity assumptions on the system dynamics, reward function and control policy, we provide bounds on the bias and variance of the estimator that depend only on the Lipschitz constants, on the number of broken trajectories used in the estimator, and on the  sparsity of the sample of one-step transitions.",
        "bibtex": "@InProceedings{pmlr-v9-fonteneau10a,\n  title = \t {Model-Free Monte Carlo-like Policy Evaluation},\n  author = \t {Fonteneau, Raphael and Murphy, Susan and Wehenkel, Louis and Ernst, Damien},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {217--224},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/fonteneau10a/fonteneau10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/fonteneau10a.html},\n  abstract = \t {We propose an algorithm for estimating the finite-horizon expected return of a closed loop control policy from an a priori given (off-policy) sample of one-step transitions. It averages cumulated rewards along a set of \u201cbroken trajectories\u201d made of one-step transitions selected from the sample on the basis of the control policy. Under some Lipschitz continuity assumptions on the system dynamics, reward function and control policy, we provide bounds on the bias and variance of the estimator that depend only on the Lipschitz constants, on the number of broken trajectories used in the estimator, and on the  sparsity of the sample of one-step transitions.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/fonteneau10a/fonteneau10a.pdf",
        "supp": "",
        "pdf_size": 1042288,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10043740071829585918&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "f41dc3e2b4",
        "title": "Modeling annotator expertise: Learning when everybody knows a bit of something",
        "site": "https://proceedings.mlr.press/v9/yan10a.html",
        "author": "Yan Yan; Romer Rosales; Glenn Fung; Mark Schmidt; Gerardo Hermosillo; Luca Bogoni; Linda Moy; Jennifer Dy",
        "abstract": "Supervised learning from multiple labeling sources is an increasingly important problem in machine learning and data mining. This paper develops a probabilistic approach to this problem when annotators may be unreliable (labels are noisy), but also their expertise varies depending on the data they observe (annotators may have knowledge about different parts of the input space). That is, an annotator may not be consistently accurate (or inaccurate) across the task domain. The presented approach produces classification and annotator models that allow us to provide estimates of the true labels and annotator variable expertise. We provide an analysis of the proposed model under various scenarios and show experimentally that annotator expertise can indeed vary in real tasks and that the presented approach provides clear advantages over previously introduced multi-annotator methods, which only consider general annotator characteristics.",
        "bibtex": "@InProceedings{pmlr-v9-yan10a,\n  title = \t {Modeling annotator expertise: Learning when everybody knows a bit of something},\n  author = \t {Yan, Yan and Rosales, Romer and Fung, Glenn and Schmidt, Mark and Hermosillo, Gerardo and Bogoni, Luca and Moy, Linda and Dy, Jennifer},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {932--939},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/yan10a/yan10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/yan10a.html},\n  abstract = \t {Supervised learning from multiple labeling sources is an increasingly important problem in machine learning and data mining. This paper develops a probabilistic approach to this problem when annotators may be unreliable (labels are noisy), but also their expertise varies depending on the data they observe (annotators may have knowledge about different parts of the input space). That is, an annotator may not be consistently accurate (or inaccurate) across the task domain. The presented approach produces classification and annotator models that allow us to provide estimates of the true labels and annotator variable expertise. We provide an analysis of the proposed model under various scenarios and show experimentally that annotator expertise can indeed vary in real tasks and that the presented approach provides clear advantages over previously introduced multi-annotator methods, which only consider general annotator characteristics.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/yan10a/yan10a.pdf",
        "supp": "",
        "pdf_size": 461218,
        "gs_citation": 263,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14793961623617433948&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Northeastern Univ., Boston, MA USA; Siemens Healthcare, Malvern, PA USA; Univ. of British Columbia, Vancouver, BC Canada; Siemens Healthcare, Malvern, PA USA; Siemens Healthcare, Malvern, PA USA; Siemens Healthcare, Malvern, PA USA; New York Univ., New York, NY USA; Northeastern Univ., Boston, MA USA",
        "aff_domain": "; ; ; ; ; ; ; ",
        "email": "; ; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 8,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;1;1;1;3;0",
        "aff_unique_norm": "Northeastern University;Siemens Healthcare;University of British Columbia;New York University",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.northeastern.edu;https://www.siemens-healthineers.com;https://www.ubc.ca;https://www.nyu.edu",
        "aff_unique_abbr": "NEU;;UBC;NYU",
        "aff_campus_unique_index": "0;1;2;1;1;1;3;0",
        "aff_campus_unique": "Boston;Malvern;Vancouver;New York",
        "aff_country_unique_index": "0;0;1;0;0;0;0;0",
        "aff_country_unique": "United States;Canada"
    },
    {
        "id": "60d3cf5338",
        "title": "Multi-Task Learning using Generalized t Process",
        "site": "https://proceedings.mlr.press/v9/zhang10c.html",
        "author": "Yu Zhang; Dit\u2013Yan Yeung",
        "abstract": "Multi-task learning seeks to improve the generalization performance of a learning task with the help of other related learning tasks.  Among the multi-task learning methods proposed thus far, Bonilla et al.\u2019s method provides a novel multi-task extension of Gaussian process (GP) by using a task covariance matrix to model the relationships between tasks. However, learning the task covariance matrix directly has both computational and representational drawbacks. In this paper, we propose a Bayesian extension by modeling the task covariance matrix as a random matrix with an inverse-Wishart prior and integrating it out to achieve Bayesian model averaging. To make the computation feasible, we first give an alternative weight-space view of Bonilla et al.\u2019s multi-task GP model and then integrate out the task covariance matrix in the model, leading to a multi-task generalized t process (MTGTP). For the likelihood, we use a generalized t noise model which, together with the generalized t process prior, brings about the robustness advantage as well as an analytical form for the marginal likelihood.  In order to specify the inverse-Wishart prior, we use the maximum mean discrepancy (MMD) statistic to estimate the parameter matrix of the inverse-Wishart prior. Moreover, we investigate some theoretical properties of MTGTP, such as its asymptotic analysis and learning curve. Comparative experimental studies on two common multi-task learning applications show very promising results.",
        "bibtex": "@InProceedings{pmlr-v9-zhang10c,\n  title = \t {Multi-Task Learning using Generalized t Process},\n  author = \t {Zhang, Yu and Yeung, Dit\u2013Yan},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {964--971},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/zhang10c/zhang10c.pdf},\n  url = \t {https://proceedings.mlr.press/v9/zhang10c.html},\n  abstract = \t {Multi-task learning seeks to improve the generalization performance of a learning task with the help of other related learning tasks.  Among the multi-task learning methods proposed thus far, Bonilla et al.\u2019s method provides a novel multi-task extension of Gaussian process (GP) by using a task covariance matrix to model the relationships between tasks. However, learning the task covariance matrix directly has both computational and representational drawbacks. In this paper, we propose a Bayesian extension by modeling the task covariance matrix as a random matrix with an inverse-Wishart prior and integrating it out to achieve Bayesian model averaging. To make the computation feasible, we first give an alternative weight-space view of Bonilla et al.\u2019s multi-task GP model and then integrate out the task covariance matrix in the model, leading to a multi-task generalized t process (MTGTP). For the likelihood, we use a generalized t noise model which, together with the generalized t process prior, brings about the robustness advantage as well as an analytical form for the marginal likelihood.  In order to specify the inverse-Wishart prior, we use the maximum mean discrepancy (MMD) statistic to estimate the parameter matrix of the inverse-Wishart prior. Moreover, we investigate some theoretical properties of MTGTP, such as its asymptotic analysis and learning curve. Comparative experimental studies on two common multi-task learning applications show very promising results.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/zhang10c/zhang10c.pdf",
        "supp": "",
        "pdf_size": 1656888,
        "gs_citation": 74,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11154623656975782225&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, China; Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, China",
        "aff_domain": "cse.ust.hk;cse.ust.hk",
        "email": "cse.ust.hk;cse.ust.hk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Hong Kong University of Science and Technology",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.ust.hk",
        "aff_unique_abbr": "HKUST",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hong Kong",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "d15fa58a26",
        "title": "Multiclass-Multilabel Classification with More Classes than Examples",
        "site": "https://proceedings.mlr.press/v9/dekel10a.html",
        "author": "Ofer Dekel; Ohad Shamir",
        "abstract": "We discuss multiclass-multilabel classification problems in which   the set of possible labels is extremely large. Most existing   multiclass-multilabel learning algorithms expect to observe a   reasonably large sample from each class, and fail if they receive   only a handful of examples with a given label. We propose and   analyze the following two-stage approach: first use an arbitrary   (perhaps heuristic) classification algorithm to construct an initial   classifier, then apply a simple but principled method to augment   this classifier by removing harmful labels from its output.  A   careful theoretical analysis allows us to justify our approach under   some reasonable conditions (such as label sparsity and power-law   distribution of label frequencies), even when the training set does   not provide a statistically accurate representation of most   classes. Surprisingly, our theoretical analysis continues to hold   even when the number of classes exceeds the sample size. We   demonstrate the merits of our approach on the ambitious task of   categorizing the entire web using the 1.5 million categories   defined on Wikipedia.",
        "bibtex": "@InProceedings{pmlr-v9-dekel10a,\n  title = \t {Multiclass-Multilabel Classification with More Classes than Examples},\n  author = \t {Dekel, Ofer and Shamir, Ohad},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {137--144},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/dekel10a/dekel10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/dekel10a.html},\n  abstract = \t {We discuss multiclass-multilabel classification problems in which   the set of possible labels is extremely large. Most existing   multiclass-multilabel learning algorithms expect to observe a   reasonably large sample from each class, and fail if they receive   only a handful of examples with a given label. We propose and   analyze the following two-stage approach: first use an arbitrary   (perhaps heuristic) classification algorithm to construct an initial   classifier, then apply a simple but principled method to augment   this classifier by removing harmful labels from its output.  A   careful theoretical analysis allows us to justify our approach under   some reasonable conditions (such as label sparsity and power-law   distribution of label frequencies), even when the training set does   not provide a statistically accurate representation of most   classes. Surprisingly, our theoretical analysis continues to hold   even when the number of classes exceeds the sample size. We   demonstrate the merits of our approach on the ambitious task of   categorizing the entire web using the 1.5 million categories   defined on Wikipedia.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/dekel10a/dekel10a.pdf",
        "supp": "",
        "pdf_size": 307596,
        "gs_citation": 98,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14874747618370339537&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 11,
        "aff": "Microsoft Research; School of Computer Science and Engineering, The Hebrew University",
        "aff_domain": "; ",
        "email": "; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Microsoft Corporation;The Hebrew University",
        "aff_unique_dep": "Microsoft Research;School of Computer Science and Engineering",
        "aff_unique_url": "https://www.microsoft.com/en-us/research;http://www.huji.ac.il",
        "aff_unique_abbr": "MSR;HUJI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United States;Israel"
    },
    {
        "id": "7feeef7759",
        "title": "Multitask Learning for Brain-Computer Interfaces",
        "site": "https://proceedings.mlr.press/v9/alamgir10a.html",
        "author": "Morteza Alamgir; Moritz Grosse\u2013Wentrup; Yasemin Altun",
        "abstract": "Brain-computer interfaces (BCIs) are limited in their applicability in everyday settings by the current necessity to record subject-specific calibration data prior to actual use of the BCI for communication. In this paper, we utilize the framework of multitask learning to construct a BCI that can be used without any subject-specific calibration process. We discuss how this out-of-the-box BCI can be further improved in a computationally efficient manner as subject-specific data becomes available. The feasibility of the approach is demonstrated on two sets of experimental EEG data recorded during a standard two-class motor imagery paradigm from a total of 19 healthy subjects. Specifically, we show that satisfactory classification results can be achieved with zero training data, and combining prior recordings with subject-specific calibration data substantially outperforms using subject-specific data only. Our results further show that transfer between recordings under slightly different experimental setups is feasible.",
        "bibtex": "@InProceedings{pmlr-v9-alamgir10a,\n  title = \t {Multitask Learning for Brain-Computer Interfaces},\n  author = \t {Alamgir, Morteza and Grosse\u2013Wentrup, Moritz and Altun, Yasemin},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {17--24},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/alamgir10a/alamgir10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/alamgir10a.html},\n  abstract = \t {Brain-computer interfaces (BCIs) are limited in their applicability in everyday settings by the current necessity to record subject-specific calibration data prior to actual use of the BCI for communication. In this paper, we utilize the framework of multitask learning to construct a BCI that can be used without any subject-specific calibration process. We discuss how this out-of-the-box BCI can be further improved in a computationally efficient manner as subject-specific data becomes available. The feasibility of the approach is demonstrated on two sets of experimental EEG data recorded during a standard two-class motor imagery paradigm from a total of 19 healthy subjects. Specifically, we show that satisfactory classification results can be achieved with zero training data, and combining prior recordings with subject-specific calibration data substantially outperforms using subject-specific data only. Our results further show that transfer between recordings under slightly different experimental setups is feasible.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/alamgir10a/alamgir10a.pdf",
        "supp": "",
        "pdf_size": 759372,
        "gs_citation": 133,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14274224640631396802&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Max Planck Institute for Biological Cybernetics; Max Planck Institute for Biological Cybernetics; Max Planck Institute for Biological Cybernetics",
        "aff_domain": "tuebingen.mpg.de;ieee.org;tuebingen.mpg.de",
        "email": "tuebingen.mpg.de;ieee.org;tuebingen.mpg.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Max Planck Institute for Biological Cybernetics",
        "aff_unique_dep": "Biological Cybernetics",
        "aff_unique_url": "https://www.biocybernetics.mpg.de",
        "aff_unique_abbr": "MPIBC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "e7cc331ff5",
        "title": "Near-Optimal Evasion of Convex-Inducing Classifiers",
        "site": "https://proceedings.mlr.press/v9/nelson10a.html",
        "author": "Blaine Nelson; Benjamin Rubinstein; Ling Huang; Anthony Joseph; Shing\u2013hon Lau; Steven Lee; Satish Rao; Anthony Tran; Doug Tygar",
        "abstract": "Classifiers are often used to detect miscreant activities. We study how an adversary can efficiently query a classifier to elicit information that allows the adversary to evade detection at near-minimal cost. We generalize results of Lowd and Meek (2005) to convex-inducing classifiers. We present algorithms that construct undetected instances of near-minimal cost using only polynomially many queries in the dimension of the space and without reverse engineering the decision boundary.",
        "bibtex": "@InProceedings{pmlr-v9-nelson10a,\n  title = \t {Near-Optimal Evasion of Convex-Inducing Classifiers},\n  author = \t {Nelson, Blaine and Rubinstein, Benjamin and Huang, Ling and Joseph, Anthony and Lau, Shing\u2013hon and Lee, Steven and Rao, Satish and Tran, Anthony and Tygar, Doug},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {549--556},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/nelson10a/nelson10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/nelson10a.html},\n  abstract = \t {Classifiers are often used to detect miscreant activities. We study how an adversary can efficiently query a classifier to elicit information that allows the adversary to evade detection at near-minimal cost. We generalize results of Lowd and Meek (2005) to convex-inducing classifiers. We present algorithms that construct undetected instances of near-minimal cost using only polynomially many queries in the dimension of the space and without reverse engineering the decision boundary.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/nelson10a/nelson10a.pdf",
        "supp": "",
        "pdf_size": 733357,
        "gs_citation": 60,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7340340859974671829&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "Computer Science Division, UC Berkeley; Computer Science Division, UC Berkeley; Intel Labs Berkeley; Computer Science Division, UC Berkeley+Intel Labs Berkeley; School of Computer Science, CMU; Computer Science Division, UC Berkeley; Computer Science Division, UC Berkeley; Computer Science Division, UC Berkeley; Computer Science Division, UC Berkeley",
        "aff_domain": ";;;;;;;;",
        "email": ";;;;;;;;",
        "github": "",
        "project": "",
        "author_num": 9,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0+1;2;0;0;0;0",
        "aff_unique_norm": "University of California, Berkeley;Intel Labs;Carnegie Mellon University",
        "aff_unique_dep": "Computer Science Division;;School of Computer Science",
        "aff_unique_url": "https://www.berkeley.edu;https://www.intel.com/research/labs;https://www.cmu.edu",
        "aff_unique_abbr": "UC Berkeley;Intel Labs;CMU",
        "aff_campus_unique_index": "0;0;0;0+0;1;0;0;0;0",
        "aff_campus_unique": "Berkeley;Pittsburgh",
        "aff_country_unique_index": "0;0;0;0+0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "0ecf4d8652",
        "title": "Negative Results for Active Learning with Convex Losses",
        "site": "https://proceedings.mlr.press/v9/hanneke10a.html",
        "author": "Steve Hanneke; Liu Yang",
        "abstract": "We study the problem of active learning with convex loss functions.  We prove that even under bounded noise constraints, the minimax rates for proper active learning are often no better than passive learning.",
        "bibtex": "@InProceedings{pmlr-v9-hanneke10a,\n  title = \t {Negative Results for Active Learning with Convex Losses},\n  author = \t {Hanneke, Steve and Yang, Liu},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {321--325},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/hanneke10a/hanneke10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/hanneke10a.html},\n  abstract = \t {We study the problem of active learning with convex loss functions.  We prove that even under bounded noise constraints, the minimax rates for proper active learning are often no better than passive learning.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/hanneke10a/hanneke10a.pdf",
        "supp": "",
        "pdf_size": 161267,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6688389374273983072&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Statistics, Carnegie Mellon University; Machine Learning Department, Carnegie Mellon University",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b57b8a52df",
        "title": "Neural conditional random fields",
        "site": "https://proceedings.mlr.press/v9/do10a.html",
        "author": "Trinh\u2013Minh\u2013Tri Do; Thierry Artieres",
        "abstract": "We propose a non-linear graphical model for structured prediction. It combines the power of deep neural networks to extract high level features with the graphical framework of Markov networks, yielding a powerful and scalable probabilistic model that we apply to signal labeling tasks.",
        "bibtex": "@InProceedings{pmlr-v9-do10a,\n  title = \t {Neural conditional random fields},\n  author = \t {Do, Trinh\u2013Minh\u2013Tri and Artieres, Thierry},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {177--184},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/do10a/do10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/do10a.html},\n  abstract = \t {We propose a non-linear graphical model for structured prediction. It combines the power of deep neural networks to extract high level features with the graphical framework of Markov networks, yielding a powerful and scalable probabilistic model that we apply to signal labeling tasks.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/do10a/do10a.pdf",
        "supp": "",
        "pdf_size": 950083,
        "gs_citation": 162,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5252879352268942310&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Idiap Research Institute, Martigny, Switzerland; LIP6, Universite Pierre et Marie Curie, Paris, France",
        "aff_domain": "idiap.ch;lip6.fr",
        "email": "idiap.ch;lip6.fr",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Idiap Research Institute;Universite Pierre et Marie Curie",
        "aff_unique_dep": ";LIP6",
        "aff_unique_url": "https://www.idiap.ch;https://www.upmc.fr",
        "aff_unique_abbr": "Idiap;UPMC",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Martigny;Paris",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Switzerland;France"
    },
    {
        "id": "2916cd3662",
        "title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models",
        "site": "https://proceedings.mlr.press/v9/gutmann10a.html",
        "author": "Michael Gutmann; Aapo Hyv\u00e4rinen",
        "abstract": "We present a new estimation principle for parameterized statistical models. The idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise, using the model log-density function in the regression nonlinearity.  We show that this leads to a consistent (convergent) estimator of the parameters, and analyze the asymptotic variance.  In particular, the method is shown to directly work for unnormalized models, i.e. models where the density function does not integrate to one. The normalization constant can be estimated just like any other parameter. For a tractable ICA model, we compare the method with other estimation methods that can be used to learn unnormalized models, including score matching, contrastive divergence, and maximum-likelihood where the normalization constant is estimated with importance sampling. Simulations show that noise-contrastive estimation offers the best trade-off between computational and statistical efficiency. The method is then applied to the modeling of natural images: We show that the method can successfully estimate a large-scale two-layer model and a Markov random field.",
        "bibtex": "@InProceedings{pmlr-v9-gutmann10a,\n  title = \t {Noise-contrastive estimation: A new estimation principle for unnormalized statistical models},\n  author = \t {Gutmann, Michael and Hyv\u00e4rinen, Aapo},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {297--304},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/gutmann10a.html},\n  abstract = \t {We present a new estimation principle for parameterized statistical models. The idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise, using the model log-density function in the regression nonlinearity.  We show that this leads to a consistent (convergent) estimator of the parameters, and analyze the asymptotic variance.  In particular, the method is shown to directly work for unnormalized models, i.e. models where the density function does not integrate to one. The normalization constant can be estimated just like any other parameter. For a tractable ICA model, we compare the method with other estimation methods that can be used to learn unnormalized models, including score matching, contrastive divergence, and maximum-likelihood where the normalization constant is estimated with importance sampling. Simulations show that noise-contrastive estimation offers the best trade-off between computational and statistical efficiency. The method is then applied to the modeling of natural images: We show that the method can successfully estimate a large-scale two-layer model and a Markov random field.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf",
        "supp": "",
        "pdf_size": 633631,
        "gs_citation": 3047,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6380930159682781969&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Dept of Computer Science and HIIT, University of Helsinki; Dept of Mathematics & Statistics, Dept of Computer Science and HIIT, University of Helsinki",
        "aff_domain": "helsinki.fi;helsinki.fi",
        "email": "helsinki.fi;helsinki.fi",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Helsinki",
        "aff_unique_dep": "Dept of Computer Science and HIIT",
        "aff_unique_url": "https://www.helsinki.fi",
        "aff_unique_abbr": "UH",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Finland"
    },
    {
        "id": "154f6a3b7f",
        "title": "Nonlinear functional regression: a functional RKHS approach",
        "site": "https://proceedings.mlr.press/v9/kadri10a.html",
        "author": "Hachem Kadri; Emmanuel Duflos; Philippe Preux; St\u00e9phane Canu; Manuel Davy",
        "abstract": "This paper deals with functional regression, in which the input attributes as well as the response are functions. To deal with this problem, we develop a functional reproducing kernel Hilbert space approach; here, a kernel is an operator acting on a function and yielding a function. We demonstrate basic properties of these functional RKHS, as well as a representer theorem for this setting; we investigate the construction of kernels; we provide some experimental insight.",
        "bibtex": "@InProceedings{pmlr-v9-kadri10a,\n  title = \t {Nonlinear functional regression: a functional RKHS approach},\n  author = \t {Kadri, Hachem and Duflos, Emmanuel and Preux, Philippe and Canu, St\u00e9phane and Davy, Manuel},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {374--380},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/kadri10a/kadri10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/kadri10a.html},\n  abstract = \t {This paper deals with functional regression, in which the input attributes as well as the response are functions. To deal with this problem, we develop a functional reproducing kernel Hilbert space approach; here, a kernel is an operator acting on a function and yielding a function. We demonstrate basic properties of these functional RKHS, as well as a representer theorem for this setting; we investigate the construction of kernels; we provide some experimental insight.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/kadri10a/kadri10a.pdf",
        "supp": "",
        "pdf_size": 655207,
        "gs_citation": 80,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2695270819281165160&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "INRIA Lille - Nord Europe; INRIA Lille/Ecole Centrale de Lille; INRIA Lille/Universit\u00e9 de Lille; INSA de Rouen; Ecole Centrale de Lille",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;2",
        "aff_unique_norm": "INRIA;INSA de Rouen;Ecole Centrale de Lille",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.inria.fr;https://www.insa-rouen.fr;https://www.ec-lille.fr",
        "aff_unique_abbr": "INRIA;INSA Rouen;ECL",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Lille - Nord Europe;Lille;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "a702e9716b",
        "title": "Nonparametric Bayesian Matrix Factorization by Power-EP",
        "site": "https://proceedings.mlr.press/v9/ding10a.html",
        "author": "Nan Ding; Yuan Qi; Rongjing Xiang; Ian Molloy; Ninghui Li",
        "abstract": "Many real-world applications can be modeled by matrix factorization. By approximating an observed data matrix  as the product of two latent matrices,  matrix factorization can reveal hidden structures embedded in data. A common challenge to use matrix factorization is determining the dimensionality of the latent matrices from data. Indian Buffet Processes (IBPs) enable us to apply the nonparametric Bayesian machinery to address this challenge. However, it remains a difficult task to learn nonparametric Bayesian matrix factorization models. In this paper, we propose a novel variational Bayesian method based on new equivalence classes of infinite matrices for learning these models. Furthermore, inspired by the success of nonnegative matrix factorization on many learning problems, we impose nonnegativity constraints on the latent matrices and mix variational inference with expectation propagation. This mixed inference method is unified in a power expectation propagation framework. Experimental results on image decomposition demonstrate the superior computational efficiency and the higher prediction accuracy of our methods compared to alternative Monte Carlo and variational inference methods for IBP models. We also apply the new methods to collaborative filtering and role mining and show the improved predictive performance over other matrix factorization methods.",
        "bibtex": "@InProceedings{pmlr-v9-ding10a,\n  title = \t {Nonparametric Bayesian Matrix Factorization by Power-EP},\n  author = \t {Ding, Nan and Qi, Yuan and Xiang, Rongjing and Molloy, Ian and Li, Ninghui},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {169--176},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/ding10a/ding10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/ding10a.html},\n  abstract = \t {Many real-world applications can be modeled by matrix factorization. By approximating an observed data matrix  as the product of two latent matrices,  matrix factorization can reveal hidden structures embedded in data. A common challenge to use matrix factorization is determining the dimensionality of the latent matrices from data. Indian Buffet Processes (IBPs) enable us to apply the nonparametric Bayesian machinery to address this challenge. However, it remains a difficult task to learn nonparametric Bayesian matrix factorization models. In this paper, we propose a novel variational Bayesian method based on new equivalence classes of infinite matrices for learning these models. Furthermore, inspired by the success of nonnegative matrix factorization on many learning problems, we impose nonnegativity constraints on the latent matrices and mix variational inference with expectation propagation. This mixed inference method is unified in a power expectation propagation framework. Experimental results on image decomposition demonstrate the superior computational efficiency and the higher prediction accuracy of our methods compared to alternative Monte Carlo and variational inference methods for IBP models. We also apply the new methods to collaborative filtering and role mining and show the improved predictive performance over other matrix factorization methods.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/ding10a/ding10a.pdf",
        "supp": "",
        "pdf_size": 789392,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5408879301243447393&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science, Purdue University, West Lafayette, IN 47907+Departments of Computer Science and Statistics, Purdue University, West Lafayette, IN 47907; Departments of Computer Science and Statistics, Purdue University, West Lafayette, IN 47907; Department of Computer Science, Purdue University, West Lafayette, IN 47907; Department of Computer Science, Purdue University, West Lafayette, IN 47907; Department of Computer Science, Purdue University, West Lafayette, IN 47907",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+0;0;0;0;0",
        "aff_unique_norm": "Purdue University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.purdue.edu",
        "aff_unique_abbr": "Purdue",
        "aff_campus_unique_index": "0+0;0;0;0;0",
        "aff_campus_unique": "West Lafayette",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ed50a8b7f0",
        "title": "Nonparametric Tree Graphical Models",
        "site": "https://proceedings.mlr.press/v9/song10a.html",
        "author": "Le Song; Arthur Gretton; Carlos Guestrin",
        "abstract": "We introduce a nonparametric representation for graphical model on trees which expresses  marginals as Hilbert space embeddings and conditionals as embedding operators.  This formulation allows us to define a graphical model solely  on the basis of the feature space representation of its variables. Thus, this nonparametric model can be applied to general domains where kernels are defined, handling challenging cases such as discrete variables whose domains are huge, or very complex, non-Gaussian continuous distributions. We also derive kernel belief propagation, a Hilbert-space algorithm for performing inference in our model.  We show that our method outperforms state-of-the-art techniques  in a cross-lingual document retrieval task and  a camera rotation estimation problem.",
        "bibtex": "@InProceedings{pmlr-v9-song10a,\n  title = \t {Nonparametric Tree Graphical Models},\n  author = \t {Song, Le and Gretton, Arthur and Guestrin, Carlos},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {765--772},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/song10a/song10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/song10a.html},\n  abstract = \t {We introduce a nonparametric representation for graphical model on trees which expresses  marginals as Hilbert space embeddings and conditionals as embedding operators.  This formulation allows us to define a graphical model solely  on the basis of the feature space representation of its variables. Thus, this nonparametric model can be applied to general domains where kernels are defined, handling challenging cases such as discrete variables whose domains are huge, or very complex, non-Gaussian continuous distributions. We also derive kernel belief propagation, a Hilbert-space algorithm for performing inference in our model.  We show that our method outperforms state-of-the-art techniques  in a cross-lingual document retrieval task and  a camera rotation estimation problem.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/song10a/song10a.pdf",
        "supp": "",
        "pdf_size": 1328576,
        "gs_citation": 69,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15514826350287806598&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "529ebc95a3",
        "title": "Nonparametric prior for adaptive sparsity",
        "site": "https://proceedings.mlr.press/v9/raykar10a.html",
        "author": "Vikas Raykar; Linda Zhao",
        "abstract": "For high-dimensional problems various parametric priors have been proposed to promote sparse solutions. While parametric  priors has shown considerable success they are not very robust in adapting to varying degrees of sparsity. In this work we propose a discrete mixture prior which is partially nonparametric. The right structure for the prior and the amount of sparsity is estimated directly from the data. Our experiments show that the proposed prior adapts to sparsity much better than its parametric counterparts. We  apply the proposed method to classification of high dimensional microarray datasets.",
        "bibtex": "@InProceedings{pmlr-v9-raykar10a,\n  title = \t {Nonparametric prior for adaptive sparsity},\n  author = \t {Raykar, Vikas and Zhao, Linda},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {629--636},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/raykar10a/raykar10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/raykar10a.html},\n  abstract = \t {For high-dimensional problems various parametric priors have been proposed to promote sparse solutions. While parametric  priors has shown considerable success they are not very robust in adapting to varying degrees of sparsity. In this work we propose a discrete mixture prior which is partially nonparametric. The right structure for the prior and the amount of sparsity is estimated directly from the data. Our experiments show that the proposed prior adapts to sparsity much better than its parametric counterparts. We  apply the proposed method to classification of high dimensional microarray datasets.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/raykar10a/raykar10a.pdf",
        "supp": "",
        "pdf_size": 704500,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6085546267585931270&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Siemens Healthcare, Malvern PA, USA; University of Pennsylvania, Philadelphia PA, USA",
        "aff_domain": "siemens.com;wharton.upenn.edu",
        "email": "siemens.com;wharton.upenn.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Siemens Healthcare;University of Pennsylvania",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.siemens-healthineers.com;https://www.upenn.edu",
        "aff_unique_abbr": ";UPenn",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Malvern;Philadelphia",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c9e473c281",
        "title": "On Combining Graph-based Variance Reduction schemes",
        "site": "https://proceedings.mlr.press/v9/gogate10a.html",
        "author": "Vibhav Gogate; Rina Dechter",
        "abstract": "In this paper, we consider two variance reduction schemes that exploit the structure of the primal graph of the graphical model: Rao-Blackwellised w-cutset sampling and AND/OR sampling. We show that the two schemes are orthogonal and can be combined to further reduce the variance. Our combination yields a new family of estimators which trade time and space with variance. We demonstrate experimentally that the new estimators are superior, often yielding an order of magnitude improvement over previous schemes on several benchmarks.",
        "bibtex": "@InProceedings{pmlr-v9-gogate10a,\n  title = \t {On Combining Graph-based Variance Reduction schemes},\n  author = \t {Gogate, Vibhav and Dechter, Rina},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {257--264},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/gogate10a/gogate10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/gogate10a.html},\n  abstract = \t {In this paper, we consider two variance reduction schemes that exploit the structure of the primal graph of the graphical model: Rao-Blackwellised w-cutset sampling and AND/OR sampling. We show that the two schemes are orthogonal and can be combined to further reduce the variance. Our combination yields a new family of estimators which trade time and space with variance. We demonstrate experimentally that the new estimators are superior, often yielding an order of magnitude improvement over previous schemes on several benchmarks.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/gogate10a/gogate10a.pdf",
        "supp": "",
        "pdf_size": 309432,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12483587336576064033&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "University of Washington, Seattle; University of California, Irvine",
        "aff_domain": "cs.washington.edu;ics.uci.edu",
        "email": "cs.washington.edu;ics.uci.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Washington;University of California, Irvine",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.washington.edu;https://www.uci.edu",
        "aff_unique_abbr": "UW;UCI",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Seattle;Irvine",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "26b760d0a8",
        "title": "On the Convergence Properties of Contrastive Divergence",
        "site": "https://proceedings.mlr.press/v9/sutskever10a.html",
        "author": "Ilya Sutskever; Tijmen Tieleman",
        "abstract": "Contrastive Divergence (CD) is a popular method for estimating the parameters of Markov Random Fields (MRFs) by rapidly approximating an intractable term in the gradient of the log probability. Despite CD\u2019s empirical success, little is known about its theoretical convergence properties. In this paper, we analyze the CD$_1$ update rule for Restricted Boltzmann Machines (RBMs) with binary variables. We show that this update is not the gradient of any function, and construct a counterintuitive \u201cregularization function\u201d that causes CD learning to cycle indefinitely.  Nonetheless, we show that the regularized CD update has a fixed point for a large class of regularization functions using Brower\u2019s fixed point theorem.",
        "bibtex": "@InProceedings{pmlr-v9-sutskever10a,\n  title = \t {On the Convergence Properties of Contrastive Divergence},\n  author = \t {Sutskever, Ilya and Tieleman, Tijmen},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {789--795},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/sutskever10a/sutskever10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/sutskever10a.html},\n  abstract = \t {Contrastive Divergence (CD) is a popular method for estimating the parameters of Markov Random Fields (MRFs) by rapidly approximating an intractable term in the gradient of the log probability. Despite CD\u2019s empirical success, little is known about its theoretical convergence properties. In this paper, we analyze the CD$_1$ update rule for Restricted Boltzmann Machines (RBMs) with binary variables. We show that this update is not the gradient of any function, and construct a counterintuitive \u201cregularization function\u201d that causes CD learning to cycle indefinitely.  Nonetheless, we show that the regularized CD update has a fixed point for a large class of regularization functions using Brower\u2019s fixed point theorem.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/sutskever10a/sutskever10a.pdf",
        "supp": "",
        "pdf_size": 589298,
        "gs_citation": 161,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17384961002752391320&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "University of Toronto; University of Toronto",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Toronto",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.utoronto.ca",
        "aff_unique_abbr": "U of T",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "56d8743092",
        "title": "On the Impact of Kernel Approximation on Learning Accuracy",
        "site": "https://proceedings.mlr.press/v9/cortes10a.html",
        "author": "Corinna Cortes; Mehryar Mohri; Ameet Talwalkar",
        "abstract": "Kernel approximation is commonly used to scale kernel-based algorithms to applications containing as many as several million instances. This paper analyzes the effect of such approximations in the kernel matrix on the hypothesis generated by several widely used learning algorithms. We give stability bounds based on the norm of the kernel approximation for these algorithms, including SVMs, KRR, and graph Laplacian-based regularization algorithms. These bounds help determine the degree of approximation that can be tolerated in the estimation of the kernel matrix. Our analysis is general and applies to arbitrary approximations of the kernel matrix. However, we also give a specific analysis of the Nystrom low-rank approximation in this context and report the results of experiments evaluating the quality of the Nystrom low-rank kernel approximation when used with ridge regression.",
        "bibtex": "@InProceedings{pmlr-v9-cortes10a,\n  title = \t {On the Impact of Kernel Approximation on Learning Accuracy},\n  author = \t {Cortes, Corinna and Mohri, Mehryar and Talwalkar, Ameet},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {113--120},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/cortes10a/cortes10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/cortes10a.html},\n  abstract = \t {Kernel approximation is commonly used to scale kernel-based algorithms to applications containing as many as several million instances. This paper analyzes the effect of such approximations in the kernel matrix on the hypothesis generated by several widely used learning algorithms. We give stability bounds based on the norm of the kernel approximation for these algorithms, including SVMs, KRR, and graph Laplacian-based regularization algorithms. These bounds help determine the degree of approximation that can be tolerated in the estimation of the kernel matrix. Our analysis is general and applies to arbitrary approximations of the kernel matrix. However, we also give a specific analysis of the Nystrom low-rank approximation in this context and report the results of experiments evaluating the quality of the Nystrom low-rank kernel approximation when used with ridge regression.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/cortes10a/cortes10a.pdf",
        "supp": "",
        "pdf_size": 330199,
        "gs_citation": 195,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2217891510935956770&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Google Research; Courant Institute and Google Research; Courant Institute",
        "aff_domain": "google.com;cs.nyu.edu;cs.nyu.edu",
        "email": "google.com;cs.nyu.edu;cs.nyu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Google;Courant Institute;Courant Institute of Mathematical Sciences",
        "aff_unique_dep": "Google Research;Courant Institute;Mathematical Sciences",
        "aff_unique_url": "https://research.google;https://courant.nyu.edu;https://courant.nyu.edu",
        "aff_unique_abbr": "Google Research;Courant;Courant",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Mountain View;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "4897787fa5",
        "title": "On the relation between universality, characteristic kernels and RKHS embedding of measures",
        "site": "https://proceedings.mlr.press/v9/sriperumbudur10a.html",
        "author": "Bharath Sriperumbudur; Kenji Fukumizu; Gert Lanckriet",
        "abstract": "Universal kernels have been shown to play an important role in the achievability of the Bayes risk by many kernel-based algorithms that include binary classification, regression, etc. In this paper, we propose a notion of universality that generalizes the notions introduced by Steinwart and Micchelli et al. and study the necessary and sufficient conditions for a kernel to be universal. We show that all these notions of universality are closely linked to the injective embedding of a certain class of Borel measures into a reproducing kernel Hilbert space (RKHS). By exploiting this relation between universality and the embedding of Borel measures into an RKHS, we establish the relation between universal and characteristic kernels. The latter have been proposed in the context of the RKHS embedding of probability measures, used in statistical applications like homogeneity testing, independence testing, etc.",
        "bibtex": "@InProceedings{pmlr-v9-sriperumbudur10a,\n  title = \t {On the relation between universality, characteristic kernels and RKHS embedding of measures},\n  author = \t {Sriperumbudur, Bharath and Fukumizu, Kenji and Lanckriet, Gert},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {773--780},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/sriperumbudur10a/sriperumbudur10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/sriperumbudur10a.html},\n  abstract = \t {Universal kernels have been shown to play an important role in the achievability of the Bayes risk by many kernel-based algorithms that include binary classification, regression, etc. In this paper, we propose a notion of universality that generalizes the notions introduced by Steinwart and Micchelli et al. and study the necessary and sufficient conditions for a kernel to be universal. We show that all these notions of universality are closely linked to the injective embedding of a certain class of Borel measures into a reproducing kernel Hilbert space (RKHS). By exploiting this relation between universality and the embedding of Borel measures into an RKHS, we establish the relation between universal and characteristic kernels. The latter have been proposed in the context of the RKHS embedding of probability measures, used in statistical applications like homogeneity testing, independence testing, etc.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/sriperumbudur10a/sriperumbudur10a.pdf",
        "supp": "",
        "pdf_size": 711602,
        "gs_citation": 99,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12948192763817673414&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 11,
        "aff": "Dept. of ECE, UC San Diego; The Institute of Statistical Mathematics; Dept. of ECE, UC San Diego",
        "aff_domain": "ucsd.edu;ism.ac.jp;ece.ucsd.edu",
        "email": "ucsd.edu;ism.ac.jp;ece.ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of California, San Diego;The Institute of Statistical Mathematics",
        "aff_unique_dep": "Dept. of Electrical and Computer Engineering;",
        "aff_unique_url": "https://www.ucsd.edu;https://www.ism.ac.jp",
        "aff_unique_abbr": "UCSD;ISM",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "San Diego;",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;Japan"
    },
    {
        "id": "23178479fa",
        "title": "Online Anomaly Detection under Adversarial Impact",
        "site": "https://proceedings.mlr.press/v9/kloft10a.html",
        "author": "Marius Kloft; Pavel Laskov",
        "abstract": "Security analysis of learning algorithms is gaining increasing importance, especially since they have become target of deliberate obstruction in certain applications. Some security-hardened algorithms have been previously proposed for supervised learning; however, very little is known about the behavior of anomaly detection methods in such scenarios. In this contribution, we analyze the performance of a particular method\u2014online centroid anomaly detection\u2014in the presence of adversarial noise. Our analysis addresses three key security-related issues: derivation of an optimal attack, analysis of its efficiency and constraints. Experimental evaluation carried out on real HTTP and exploit traces confirms the tightness of our theoretical bounds.",
        "bibtex": "@InProceedings{pmlr-v9-kloft10a,\n  title = \t {Online Anomaly Detection under Adversarial Impact},\n  author = \t {Kloft, Marius and Laskov, Pavel},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {405--412},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/kloft10a/kloft10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/kloft10a.html},\n  abstract = \t {Security analysis of learning algorithms is gaining increasing importance, especially since they have become target of deliberate obstruction in certain applications. Some security-hardened algorithms have been previously proposed for supervised learning; however, very little is known about the behavior of anomaly detection methods in such scenarios. In this contribution, we analyze the performance of a particular method\u2014online centroid anomaly detection\u2014in the presence of adversarial noise. Our analysis addresses three key security-related issues: derivation of an optimal attack, analysis of its efficiency and constraints. Experimental evaluation carried out on real HTTP and exploit traces confirms the tightness of our theoretical bounds.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/kloft10a/kloft10a.pdf",
        "supp": "",
        "pdf_size": 1643198,
        "gs_citation": 209,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16758088174583889687&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "University of California, Berkeley, USA; University of T\u00fcbingen, Germany",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of California, Berkeley;University of T\u00fcbingen",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.berkeley.edu;https://www.uni-tuebingen.de/",
        "aff_unique_abbr": "UC Berkeley;Uni T\u00fcbingen",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United States;Germany"
    },
    {
        "id": "0c7d430287",
        "title": "Online Passive-Aggressive Algorithms on a Budget",
        "site": "https://proceedings.mlr.press/v9/wang10b.html",
        "author": "Zhuang Wang; Slobodan Vucetic",
        "abstract": "In this paper a kernel-based online learning algorithm, which has both constant space and update time, is proposed. The approach is based on the popular online Passive-Aggressive (PA) algorithm. When used in conjunction with kernel function, the number of support vectors in PA grows without bounds when learning from noisy data streams. This implies unlimited memory and ever increasing model update and prediction time. To address this issue, the proposed budgeted PA algorithm maintains only a fixed number of support vectors. By introducing an additional constraint to the original PA optimization problem, a closed-form solution was derived for the support vector removal and model update. Using the hinge loss we developed several budgeted PA algorithms that can trade between accuracy and update cost. We also developed the ramp loss versions of both original and budgeted PA and showed that the resulting algorithms can be interpreted as the combination of active learning and hinge loss PA. All proposed algorithms were comprehensively tested on 7 benchmark data sets. The experiments showed that they are superior to the existing budgeted online algorithms. Even with modest budgets, the budgeted PA achieved very competitive accuracies to the non-budgeted PA and kernel perceptron algorithms.",
        "bibtex": "@InProceedings{pmlr-v9-wang10b,\n  title = \t {Online Passive-Aggressive Algorithms on a Budget},\n  author = \t {Wang, Zhuang and Vucetic, Slobodan},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {908--915},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/wang10b/wang10b.pdf},\n  url = \t {https://proceedings.mlr.press/v9/wang10b.html},\n  abstract = \t {In this paper a kernel-based online learning algorithm, which has both constant space and update time, is proposed. The approach is based on the popular online Passive-Aggressive (PA) algorithm. When used in conjunction with kernel function, the number of support vectors in PA grows without bounds when learning from noisy data streams. This implies unlimited memory and ever increasing model update and prediction time. To address this issue, the proposed budgeted PA algorithm maintains only a fixed number of support vectors. By introducing an additional constraint to the original PA optimization problem, a closed-form solution was derived for the support vector removal and model update. Using the hinge loss we developed several budgeted PA algorithms that can trade between accuracy and update cost. We also developed the ramp loss versions of both original and budgeted PA and showed that the resulting algorithms can be interpreted as the combination of active learning and hinge loss PA. All proposed algorithms were comprehensively tested on 7 benchmark data sets. The experiments showed that they are superior to the existing budgeted online algorithms. Even with modest budgets, the budgeted PA achieved very competitive accuracies to the non-budgeted PA and kernel perceptron algorithms.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/wang10b/wang10b.pdf",
        "supp": "",
        "pdf_size": 897845,
        "gs_citation": 98,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15168809135336263393&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Dept. of Computer and Information Sciences, Temple University, USA; Dept. of Computer and Information Sciences, Temple University, USA",
        "aff_domain": "temple.edu;temple.edu",
        "email": "temple.edu;temple.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Temple University",
        "aff_unique_dep": "Dept. of Computer and Information Sciences",
        "aff_unique_url": "https://www.temple.edu",
        "aff_unique_abbr": "Temple",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "58680792fa",
        "title": "Optimal Allocation Strategies for the Dark Pool Problem",
        "site": "https://proceedings.mlr.press/v9/agarwal10a.html",
        "author": "Alekh Agarwal; Peter Bartlett; Max Dama",
        "abstract": "We study the problem of allocating stocks to dark pools. We propose and analyze an optimal approach for allocations, if continuous-valued allocations are allowed. We also propose a modification for the case when only integer-valued allocations are possible. We extend the previous work on this problem (Ganchev et al., 2009) to adversarial scenarios, while also improving over their results in the iid setup. The resulting algorithms are efficient, and perform well in simulations under stochastic and adversarial inputs.",
        "bibtex": "@InProceedings{pmlr-v9-agarwal10a,\n  title = \t {Optimal Allocation Strategies for the Dark Pool Problem},\n  author = \t {Agarwal, Alekh and Bartlett, Peter and Dama, Max},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {9--16},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/agarwal10a/agarwal10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/agarwal10a.html},\n  abstract = \t {We study the problem of allocating stocks to dark pools. We propose and analyze an optimal approach for allocations, if continuous-valued allocations are allowed. We also propose a modification for the case when only integer-valued allocations are possible. We extend the previous work on this problem (Ganchev et al., 2009) to adversarial scenarios, while also improving over their results in the iid setup. The resulting algorithms are efficient, and perform well in simulations under stochastic and adversarial inputs.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/agarwal10a/agarwal10a.pdf",
        "supp": "",
        "pdf_size": 1717141,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6710143830979978493&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "University of California, Berkeley; University of California, Berkeley; University of California, Berkeley",
        "aff_domain": "cs.berkeley.edu;cs.berkeley.edu;berkeley.edu",
        "email": "cs.berkeley.edu;cs.berkeley.edu;berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "087003c666",
        "title": "Parallelizable Sampling of Markov Random Fields",
        "site": "https://proceedings.mlr.press/v9/martens10a.html",
        "author": "James Martens; Ilya Sutskever",
        "abstract": "Markov Random Fields (MRFs) are an important class of probabilistic models which are used for density estimation, classification, denoising, and for constructing Deep Belief Networks.  Every application of an MRF requires addressing its inference problem, which can be done using deterministic inference methods or using stochastic Markov Chain Monte Carlo methods.  In this paper we introduce a new Markov Chain transition operator that updates all the variables of a pairwise MRF in parallel by using auxiliary Gaussian variables. The proposed MCMC operator is extremely simple to implement and to parallelize. This is achieved by a formal equivalence result between arbitrary pairwise MRFs and a particular type of Restricted Boltzmann Machine.  This result also implies that the later can be learned in place of the former without any loss of modeling power, a possibility we explore in experiments.",
        "bibtex": "@InProceedings{pmlr-v9-martens10a,\n  title = \t {Parallelizable Sampling of Markov Random Fields},\n  author = \t {Martens, James and Sutskever, Ilya},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {517--524},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/martens10a/martens10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/martens10a.html},\n  abstract = \t {Markov Random Fields (MRFs) are an important class of probabilistic models which are used for density estimation, classification, denoising, and for constructing Deep Belief Networks.  Every application of an MRF requires addressing its inference problem, which can be done using deterministic inference methods or using stochastic Markov Chain Monte Carlo methods.  In this paper we introduce a new Markov Chain transition operator that updates all the variables of a pairwise MRF in parallel by using auxiliary Gaussian variables. The proposed MCMC operator is extremely simple to implement and to parallelize. This is achieved by a formal equivalence result between arbitrary pairwise MRFs and a particular type of Restricted Boltzmann Machine.  This result also implies that the later can be learned in place of the former without any loss of modeling power, a possibility we explore in experiments.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/martens10a/martens10a.pdf",
        "supp": "",
        "pdf_size": 540842,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14006585725378956333&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "University of Toronto; University of Toronto",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Toronto",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.utoronto.ca",
        "aff_unique_abbr": "U of T",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "0226c10cf8",
        "title": "Parametric Herding",
        "site": "https://proceedings.mlr.press/v9/chen10a.html",
        "author": "Yutian Chen; Max Welling",
        "abstract": "A parametric version of herding is formulated. The nonlinear mapping between consecutive time slices is learned by a form of self-supervised training. The resulting dynamical system generates pseudo-samples that resemble the original data. We show how this parametric herding can be successfully used to compress a dataset consisting of binary digits. It is also verified that high compression rates translate into good prediction performance on unseen test data.",
        "bibtex": "@InProceedings{pmlr-v9-chen10a,\n  title = \t {Parametric Herding},\n  author = \t {Chen, Yutian and Welling, Max},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {97--104},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/chen10a/chen10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/chen10a.html},\n  abstract = \t {A parametric version of herding is formulated. The nonlinear mapping between consecutive time slices is learned by a form of self-supervised training. The resulting dynamical system generates pseudo-samples that resemble the original data. We show how this parametric herding can be successfully used to compress a dataset consisting of binary digits. It is also verified that high compression rates translate into good prediction performance on unseen test data.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/chen10a/chen10a.pdf",
        "supp": "",
        "pdf_size": 487983,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17768973702544044387&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Bren School of Information and Computer Science, University of California, Irvine; Bren School of Information and Computer Science, University of California, Irvine",
        "aff_domain": "ics.uci.edu;ics.uci.edu",
        "email": "ics.uci.edu;ics.uci.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Irvine",
        "aff_unique_dep": "Bren School of Information and Computer Science",
        "aff_unique_url": "https://www.uci.edu",
        "aff_unique_abbr": "UCI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Irvine",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2cdfbb9ac5",
        "title": "Polynomial-Time Exact Inference in NP-Hard Binary MRFs via Reweighted Perfect Matching",
        "site": "https://proceedings.mlr.press/v9/schraudolph10a.html",
        "author": "Nic Schraudolph",
        "abstract": "We develop a new form of reweighting (Wainwright et al., 2005b) to leverage the relationship between Ising spin glasses and perfect matchings into a novel technique for the exact computation of MAP states in hitherto intractable binary Markov random fields. Our method solves an $n \\times n$ lattice with external field and random couplings much faster, and for larger $n$, than the best competing algorithms. It empirically scales as $O(n^3)$ even though this problem is NP-hard and non-approximable in polynomial time. We discuss limitations of our current implementation and propose ways to overcome them.",
        "bibtex": "@InProceedings{pmlr-v9-schraudolph10a,\n  title = \t {Polynomial-Time Exact Inference in NP-Hard Binary MRFs via Reweighted Perfect Matching},\n  author = \t {Schraudolph, Nic},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {717--724},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/schraudolph10a/schraudolph10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/schraudolph10a.html},\n  abstract = \t {We develop a new form of reweighting (Wainwright et al., 2005b) to leverage the relationship between Ising spin glasses and perfect matchings into a novel technique for the exact computation of MAP states in hitherto intractable binary Markov random fields. Our method solves an $n \\times n$ lattice with external field and random couplings much faster, and for larger $n$, than the best competing algorithms. It empirically scales as $O(n^3)$ even though this problem is NP-hard and non-approximable in polynomial time. We discuss limitations of our current implementation and propose ways to overcome them.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/schraudolph10a/schraudolph10a.pdf",
        "supp": "",
        "pdf_size": 1165786,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11927293096361596621&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "62a1f68599",
        "title": "Posterior distributions are computable from predictive distributions",
        "site": "https://proceedings.mlr.press/v9/freer10a.html",
        "author": "Cameron Freer; Daniel Roy",
        "abstract": "As we devise more complicated prior distributions, will inference algorithms keep up?  We highlight a negative result in computable probability theory by Ackerman, Freer, and Roy (2010) that shows that there exist computable priors with noncomputable posteriors.  In addition to providing a brief survey of computable probability theory geared towards the A.I. and statistics community, we give a new result characterizing when conditioning is computable in the setting of exchangeable sequences, and provide a computational perspective on work by Orbanz (2010) on conjugate nonparametric models.  In particular, using a computable extension of de Finetti\u2019s theorem (Freer and Roy 2009), we describe how to transform a posterior predictive rule for generating an exchangeable sequence into an algorithm for computing the posterior distribution of the directing random measure.",
        "bibtex": "@InProceedings{pmlr-v9-freer10a,\n  title = \t {Posterior distributions are computable from predictive distributions},\n  author = \t {Freer, Cameron and Roy, Daniel},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {233--240},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/freer10a/freer10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/freer10a.html},\n  abstract = \t {As we devise more complicated prior distributions, will inference algorithms keep up?  We highlight a negative result in computable probability theory by Ackerman, Freer, and Roy (2010) that shows that there exist computable priors with noncomputable posteriors.  In addition to providing a brief survey of computable probability theory geared towards the A.I. and statistics community, we give a new result characterizing when conditioning is computable in the setting of exchangeable sequences, and provide a computational perspective on work by Orbanz (2010) on conjugate nonparametric models.  In particular, using a computable extension of de Finetti\u2019s theorem (Freer and Roy 2009), we describe how to transform a posterior predictive rule for generating an exchangeable sequence into an algorithm for computing the posterior distribution of the directing random measure.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/freer10a/freer10a.pdf",
        "supp": "",
        "pdf_size": 1106483,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8292878410851907209&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Mathematics, Massachusetts Institute of Technology + Computer Science and A.I. Laboratory, Massachusetts Institute of Technology; Department of Mathematics, Massachusetts Institute of Technology + Computer Science and A.I. Laboratory, Massachusetts Institute of Technology",
        "aff_domain": "math.mit.edu;csail.mit.edu",
        "email": "math.mit.edu;csail.mit.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+0;0+0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Department of Mathematics",
        "aff_unique_url": "https://web.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0+0;0+0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "5cf0682208",
        "title": "Preface",
        "site": "https://proceedings.mlr.press/v9/teh10a.html",
        "author": "Yee Whye Teh; Mike Titterington",
        "abstract": "Preface to the Proceedings of the Thirteenth International  Conference on Artificial Intelligence and Statistics May 13-15, 2010,  Chia Laguna Resort, Sardinia, Italy.",
        "bibtex": "@InProceedings{pmlr-v9-teh10a,\n  title = \t {Preface},\n  author = \t {Teh, Yee Whye and Titterington, Mike},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {i--v},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/teh10a/teh10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/teh10a.html},\n  abstract = \t {Preface to the Proceedings of the Thirteenth International  Conference on Artificial Intelligence and Statistics May 13-15, 2010,  Chia Laguna Resort, Sardinia, Italy.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/teh10a/teh10a.pdf",
        "supp": "",
        "pdf_size": 41326,
        "gs_citation": 96,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14141043608347785016&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "90f6272ddd",
        "title": "REGO: Rank-based Estimation of Renyi Information using Euclidean Graph Optimization",
        "site": "https://proceedings.mlr.press/v9/poczos10a.html",
        "author": "Barnabas Poczos; Sergey Kirshner; Csaba Szepesv\u00e1ri",
        "abstract": "We propose a new method for a non-parametric estimation of Renyi and Shannon information for a multivariate distribution using a corresponding copula, a multivariate distribution over normalized ranks of the data. As the information of the distribution is the same as the negative entropy of its copula, our method estimates this information by solving a Euclidean graph optimization problem on the empirical estimate of the distribution\u2019s copula. Owing to the properties of the copula, we show that the resulting estimator of Renyi information is strongly consistent and robust. Further, we demonstrate its applicability in the image registration in addition to simulated experiments.",
        "bibtex": "@InProceedings{pmlr-v9-poczos10a,\n  title = \t {REGO: Rank-based Estimation of Renyi Information using Euclidean Graph Optimization},\n  author = \t {Poczos, Barnabas and Kirshner, Sergey and Szepesv\u00e1ri, Csaba},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {605--612},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/poczos10a/poczos10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/poczos10a.html},\n  abstract = \t {We propose a new method for a non-parametric estimation of Renyi and Shannon information for a multivariate distribution using a corresponding copula, a multivariate distribution over normalized ranks of the data. As the information of the distribution is the same as the negative entropy of its copula, our method estimates this information by solving a Euclidean graph optimization problem on the empirical estimate of the distribution\u2019s copula. Owing to the properties of the copula, we show that the resulting estimator of Renyi information is strongly consistent and robust. Further, we demonstrate its applicability in the image registration in addition to simulated experiments.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/poczos10a/poczos10a.pdf",
        "supp": "",
        "pdf_size": 634024,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14746022320992143482&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "d0681eb64c",
        "title": "Real-time Multiattribute Bayesian Preference Elicitation with Pairwise Comparison Queries",
        "site": "https://proceedings.mlr.press/v9/guo10b.html",
        "author": "Shengbo Guo; Scott Sanner",
        "abstract": "Preference elicitation (PE) is an important component of interactive decision support systems that aim to make optimal recommendations to users by actively querying their preferences.  In this paper, we outline five principles important for PE in real-world problems: (1) real-time, (2) multiattribute, (3) low cognitive load, (4) robust to noise, and (5) scalable.  In light of these requirements, we introduce an approximate PE framework based on TrueSkill for performing efficient closed-form Bayesian updates and query selection for a multiattribute utility belief state \u2014 a novel PE approach that naturally facilitates the efficient evaluation of value of information (VOI) heuristics for use in query selection strategies.  Our best VOI query strategy satisfies all five principles (in contrast to related work) and performs on par with the most accurate (and often computationally intensive) algorithms on experiments with synthetic and real-world datasets.",
        "bibtex": "@InProceedings{pmlr-v9-guo10b,\n  title = \t {Real-time Multiattribute Bayesian Preference Elicitation with Pairwise Comparison Queries},\n  author = \t {Guo, Shengbo and Sanner, Scott},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {289--296},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/guo10b/guo10b.pdf},\n  url = \t {https://proceedings.mlr.press/v9/guo10b.html},\n  abstract = \t {Preference elicitation (PE) is an important component of interactive decision support systems that aim to make optimal recommendations to users by actively querying their preferences.  In this paper, we outline five principles important for PE in real-world problems: (1) real-time, (2) multiattribute, (3) low cognitive load, (4) robust to noise, and (5) scalable.  In light of these requirements, we introduce an approximate PE framework based on TrueSkill for performing efficient closed-form Bayesian updates and query selection for a multiattribute utility belief state \u2014 a novel PE approach that naturally facilitates the efficient evaluation of value of information (VOI) heuristics for use in query selection strategies.  Our best VOI query strategy satisfies all five principles (in contrast to related work) and performs on par with the most accurate (and often computationally intensive) algorithms on experiments with synthetic and real-world datasets.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/guo10b/guo10b.pdf",
        "supp": "",
        "pdf_size": 984035,
        "gs_citation": 104,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12365869494412992682&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Australian National University - NICTA; NICTA - Australian National University",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Australian National University",
        "aff_unique_dep": "NICTA",
        "aff_unique_url": "https://www.anu.edu.au",
        "aff_unique_abbr": "ANU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "a739883b38",
        "title": "Reduced-Rank Hidden Markov Models",
        "site": "https://proceedings.mlr.press/v9/siddiqi10a.html",
        "author": "Sajid Siddiqi; Byron Boots; Geoffrey Gordon",
        "abstract": "Hsu et al. (2009) recently proposed an efficient, accurate spectral learning algorithm for Hidden Markov Models (HMMs). In this paper we relax their assumptions and prove a tighter finite-sample error bound for the case of Reduced-Rank HMMs, i.e., HMMs with low-rank transition matrices. Since rank-$k$ RR-HMMs are a larger class of models than $k$-state HMMs while being equally efficient to work with, this relaxation greatly increases the learning algorithm\u2019s scope. In addition, we generalize the algorithm and bounds to models where multiple observations are needed to disambiguate state, and to models that emit multivariate real-valued observations. Finally we prove consistency for learning Predictive State Representations, an even larger class of models. Experiments on synthetic data and a toy video, as well as on difficult robot vision data, yield accurate models that compare favorably with alternatives in simulation quality and prediction accuracy.",
        "bibtex": "@InProceedings{pmlr-v9-siddiqi10a,\n  title = \t {Reduced-Rank Hidden Markov Models},\n  author = \t {Siddiqi, Sajid and Boots, Byron and Gordon, Geoffrey},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {741--748},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/siddiqi10a/siddiqi10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/siddiqi10a.html},\n  abstract = \t {Hsu et al. (2009) recently proposed an efficient, accurate spectral learning algorithm for Hidden Markov Models (HMMs). In this paper we relax their assumptions and prove a tighter finite-sample error bound for the case of Reduced-Rank HMMs, i.e., HMMs with low-rank transition matrices. Since rank-$k$ RR-HMMs are a larger class of models than $k$-state HMMs while being equally efficient to work with, this relaxation greatly increases the learning algorithm\u2019s scope. In addition, we generalize the algorithm and bounds to models where multiple observations are needed to disambiguate state, and to models that emit multivariate real-valued observations. Finally we prove consistency for learning Predictive State Representations, an even larger class of models. Experiments on synthetic data and a toy video, as well as on difficult robot vision data, yield accurate models that compare favorably with alternatives in simulation quality and prediction accuracy.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/siddiqi10a/siddiqi10a.pdf",
        "supp": "",
        "pdf_size": 2326051,
        "gs_citation": 148,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1139475603384443868&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 23,
        "aff": "Robotics Institute, Carnegie Mellon University; Machine Learning Department, Carnegie Mellon University; Machine Learning Department, Carnegie Mellon University",
        "aff_domain": "google.com;cs.cmu.edu;cs.cmu.edu",
        "email": "google.com;cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Robotics Institute",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Pittsburgh;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "37aab11495",
        "title": "Reducing Label Complexity by Learning From Bags",
        "site": "https://proceedings.mlr.press/v9/sabato10a.html",
        "author": "Sivan Sabato; Nathan Srebro; Naftali Tishby",
        "abstract": "We consider a supervised learning setting in which the main cost of learning is the number of training labels and one can obtain a single label for a bag of examples, indicating only if a positive example exists in the bag, as in Multi-Instance Learning. We thus propose to create a training sample of bags, and to use the obtained labels to learn to classify individual examples. We provide a theoretical analysis showing how to select the bag size as a function of the problem parameters, and prove that if the original labels are distributed unevenly, the number of required labels drops considerably when learning from bags. We demonstrate that finding a low-error separating hyperplane from bags is feasible in this setting using a simple iterative procedure similar to latent SVM. Experiments on synthetic and real data sets demonstrate the success of the approach.",
        "bibtex": "@InProceedings{pmlr-v9-sabato10a,\n  title = \t {Reducing Label Complexity by Learning From Bags},\n  author = \t {Sabato, Sivan and Srebro, Nathan and Tishby, Naftali},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {685--692},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/sabato10a/sabato10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/sabato10a.html},\n  abstract = \t {We consider a supervised learning setting in which the main cost of learning is the number of training labels and one can obtain a single label for a bag of examples, indicating only if a positive example exists in the bag, as in Multi-Instance Learning. We thus propose to create a training sample of bags, and to use the obtained labels to learn to classify individual examples. We provide a theoretical analysis showing how to select the bag size as a function of the problem parameters, and prove that if the original labels are distributed unevenly, the number of required labels drops considerably when learning from bags. We demonstrate that finding a low-error separating hyperplane from bags is feasible in this setting using a simple iterative procedure similar to latent SVM. Experiments on synthetic and real data sets demonstrate the success of the approach.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/sabato10a/sabato10a.pdf",
        "supp": "",
        "pdf_size": 414630,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4320568951603804964&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "443741ef73",
        "title": "Regret Bounds for Gaussian Process Bandit Problems",
        "site": "https://proceedings.mlr.press/v9/grunewalder10a.html",
        "author": "Steffen Gr\u00fcnew\u00e4lder; Jean\u2013Yves Audibert; Manfred Opper; John Shawe\u2013Taylor",
        "abstract": "Bandit algorithms are concerned with trading exploration with exploitation where a number of options are available but we can only learn their quality by experimenting with them. We consider the scenario in which the reward distribution for arms is modeled by a Gaussian process and there is no noise in the observed reward. Our main result is to bound the regret experienced by algorithms relative to the a posteriori optimal strategy of playing the best arm throughout based on benign assumptions about the covariance function defining the Gaussian process. We further complement these upper bounds with corresponding lower bounds for particular covariance functions demonstrating that in general there is at most a logarithmic looseness in our upper bounds.",
        "bibtex": "@InProceedings{pmlr-v9-grunewalder10a,\n  title = \t {Regret Bounds for Gaussian Process Bandit Problems},\n  author = \t {Gr\u00fcnew\u00e4lder, Steffen and Audibert, Jean\u2013Yves and Opper, Manfred and Shawe\u2013Taylor, John},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {273--280},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/grunewalder10a/grunewalder10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/grunewalder10a.html},\n  abstract = \t {Bandit algorithms are concerned with trading exploration with exploitation where a number of options are available but we can only learn their quality by experimenting with them. We consider the scenario in which the reward distribution for arms is modeled by a Gaussian process and there is no noise in the observed reward. Our main result is to bound the regret experienced by algorithms relative to the a posteriori optimal strategy of playing the best arm throughout based on benign assumptions about the covariance function defining the Gaussian process. We further complement these upper bounds with corresponding lower bounds for particular covariance functions demonstrating that in general there is at most a logarithmic looseness in our upper bounds.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/grunewalder10a/grunewalder10a.pdf",
        "supp": "",
        "pdf_size": 1909222,
        "gs_citation": 113,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9977030144470762050&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff": "University College London; Universite Paris-Est & INRIA/ENS/CNRS; TU-Berlin; University College London",
        "aff_domain": "cs.ucl.ac.uk;imagine.enpc.fr;cs.tu-berlin.de;cs.ucl.ac.uk",
        "email": "cs.ucl.ac.uk;imagine.enpc.fr;cs.tu-berlin.de;cs.ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "University College London;Universite Paris-Est;Technische Universit\u00e4t Berlin",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.ucl.ac.uk;https://www.univ-Paris12.fr;https://www.tu-berlin.de",
        "aff_unique_abbr": "UCL;UPE;TU Berlin",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Berlin",
        "aff_country_unique_index": "0;1;2;0",
        "aff_country_unique": "United Kingdom;France;Germany"
    },
    {
        "id": "02974d673e",
        "title": "Relating Function Class Complexity and Cluster Structure in the Function Domain with Applications to Transduction",
        "site": "https://proceedings.mlr.press/v9/lever10a.html",
        "author": "Guy Lever",
        "abstract": "We relate function class complexity to structure in the function domain. This facilitates risk analysis relative to cluster structure in the input space which is particularly effective in semi-supervised learning. In particular we quantify the complexity of function classes defined over a graph in terms of the graph structure.",
        "bibtex": "@InProceedings{pmlr-v9-lever10a,\n  title = \t {Relating Function Class Complexity and Cluster Structure in the Function Domain with Applications to Transduction},\n  author = \t {Lever, Guy},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {437--444},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/lever10a/lever10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/lever10a.html},\n  abstract = \t {We relate function class complexity to structure in the function domain. This facilitates risk analysis relative to cluster structure in the input space which is particularly effective in semi-supervised learning. In particular we quantify the complexity of function classes defined over a graph in terms of the graph structure.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/lever10a/lever10a.pdf",
        "supp": "",
        "pdf_size": 1227096,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3057920294426844894&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "University College London",
        "aff_domain": "cs.ucl.ac.uk",
        "email": "cs.ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University College London",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ucl.ac.uk",
        "aff_unique_abbr": "UCL",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "43e4d98142",
        "title": "Risk Bounds for Levy Processes in the PAC-Learning Framework",
        "site": "https://proceedings.mlr.press/v9/zhang10a.html",
        "author": "Chao Zhang; Dacheng Tao",
        "abstract": "Levy processes play an important role in the stochastic process theory. However, since samples are non-i.i.d., statistical learning results based on the i.i.d. scenarios cannot be utilized to study the risk bounds for Levy processes. In this paper, we present risk bounds for non-i.i.d. samples drawn from Levy processes in the PAC-learning framework. In particular, by using a concentration inequality for infinitely divisible distributions, we first prove that the function of risk error is Lipschitz continuous with a high probability, and then by using a specific concentration inequality for Levy processes, we obtain the risk bounds for non-i.i.d. samples drawn from Levy processes without Gaussian components. Based on the resulted risk bounds, we analyze the factors that affect the convergence of the risk bounds and then prove the convergence.",
        "bibtex": "@InProceedings{pmlr-v9-zhang10a,\n  title = \t {Risk Bounds for Levy Processes in the PAC-Learning Framework},\n  author = \t {Zhang, Chao and Tao, Dacheng},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {948--955},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/zhang10a/zhang10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/zhang10a.html},\n  abstract = \t {Levy processes play an important role in the stochastic process theory. However, since samples are non-i.i.d., statistical learning results based on the i.i.d. scenarios cannot be utilized to study the risk bounds for Levy processes. In this paper, we present risk bounds for non-i.i.d. samples drawn from Levy processes in the PAC-learning framework. In particular, by using a concentration inequality for infinitely divisible distributions, we first prove that the function of risk error is Lipschitz continuous with a high probability, and then by using a specific concentration inequality for Levy processes, we obtain the risk bounds for non-i.i.d. samples drawn from Levy processes without Gaussian components. Based on the resulted risk bounds, we analyze the factors that affect the convergence of the risk bounds and then prove the convergence.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/zhang10a/zhang10a.pdf",
        "supp": "",
        "pdf_size": 510196,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4246884609158458722&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "753569e7ae",
        "title": "Semi-Supervised Learning via Generalized Maximum Entropy",
        "site": "https://proceedings.mlr.press/v9/erkan10a.html",
        "author": "Ayse Erkan; Yasemin Altun",
        "abstract": "Various supervised inference methods can be analyzed as convex duals of the generalized maximum entropy (MaxEnt) framework. Generalized MaxEnt aims to find a distribution that maximizes an entropy function while respecting prior information represented as potential functions in miscellaneous forms of constraints and/or penalties. We extend this framework to semi-supervised learning by incorporating unlabeled data via modifications to these potential functions reflecting structural assumptions on the data geometry. The proposed approach leads to a family of discriminative semi-supervised algorithms, that are convex, scalable, inherently multi-class, easy to implement, and that can be kernelized naturally. Experimental evaluation of special cases shows the competitiveness of our methodology.",
        "bibtex": "@InProceedings{pmlr-v9-erkan10a,\n  title = \t {Semi-Supervised Learning via Generalized Maximum Entropy},\n  author = \t {Erkan, Ayse and Altun, Yasemin},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {209--216},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/erkan10a/erkan10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/erkan10a.html},\n  abstract = \t {Various supervised inference methods can be analyzed as convex duals of the generalized maximum entropy (MaxEnt) framework. Generalized MaxEnt aims to find a distribution that maximizes an entropy function while respecting prior information represented as potential functions in miscellaneous forms of constraints and/or penalties. We extend this framework to semi-supervised learning by incorporating unlabeled data via modifications to these potential functions reflecting structural assumptions on the data geometry. The proposed approach leads to a family of discriminative semi-supervised algorithms, that are convex, scalable, inherently multi-class, easy to implement, and that can be kernelized naturally. Experimental evaluation of special cases shows the competitiveness of our methodology.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/erkan10a/erkan10a.pdf",
        "supp": "",
        "pdf_size": 831046,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10543389170167159253&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Max Planck Institute for Biological Cybernetics, T\u00a8ubingen, Germany + New York University, New York, NY, USA; Max Planck Institute for Biological Cybernetics, T\u00a8ubingen, Germany",
        "aff_domain": "; ",
        "email": "; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0",
        "aff_unique_norm": "Max Planck Institute for Biological Cybernetics;New York University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.biocybernetics.mpg.de;https://www.nyu.edu",
        "aff_unique_abbr": "MPIBC;NYU",
        "aff_campus_unique_index": "0+1;0",
        "aff_campus_unique": "T\u00fcbingen;New York",
        "aff_country_unique_index": "0+1;0",
        "aff_country_unique": "Germany;United States"
    },
    {
        "id": "5c885fa81e",
        "title": "Semi-Supervised Learning with Max-Margin Graph Cuts",
        "site": "https://proceedings.mlr.press/v9/kveton10a.html",
        "author": "Branislav Kveton; Michal Valko; Ali Rahimi; Ling Huang",
        "abstract": "This paper proposes a novel algorithm for semi-supervised learning. This algorithm learns graph cuts that maximize the margin with respect to the labels induced by the harmonic function solution. We motivate the approach, compare it to existing work, and prove a bound on its generalization error. The quality of our solutions is evaluated on a synthetic problem and three UCI ML repository datasets. In most cases, we outperform manifold regularization of support vector machines, which is a state-of-the-art approach to semi-supervised max-margin learning.",
        "bibtex": "@InProceedings{pmlr-v9-kveton10a,\n  title = \t {Semi-Supervised Learning with Max-Margin Graph Cuts},\n  author = \t {Kveton, Branislav and Valko, Michal and Rahimi, Ali and Huang, Ling},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {421--428},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/kveton10a/kveton10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/kveton10a.html},\n  abstract = \t {This paper proposes a novel algorithm for semi-supervised learning. This algorithm learns graph cuts that maximize the margin with respect to the labels induced by the harmonic function solution. We motivate the approach, compare it to existing work, and prove a bound on its generalization error. The quality of our solutions is evaluated on a synthetic problem and three UCI ML repository datasets. In most cases, we outperform manifold regularization of support vector machines, which is a state-of-the-art approach to semi-supervised max-margin learning.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/kveton10a/kveton10a.pdf",
        "supp": "",
        "pdf_size": 2031654,
        "gs_citation": 47,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3099038816242034192&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 19,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "ec22df4ca2",
        "title": "Sequential Monte Carlo Samplers for Dirichlet Process Mixtures",
        "site": "https://proceedings.mlr.press/v9/ulker10a.html",
        "author": "Yener Ulker; Bilge G\u00fcnsel; Taylan Cemgil",
        "abstract": "In this paper, we develop a novel online algorithm based on the Sequential Monte Carlo(SMC) samplers framework for posterior inference in Dirichlet Process Mixtures (DPM). Our method generalizes  many sequential importance sampling approaches. It provides a computationally efficient improvement to particle filtering that is less prone to getting stuck in isolated modes. The proposed method is a particular SMC sampler that enables us to design sophisticated clustering update schemes, such as updating past trajectories of the particles in light of recent observations, and still ensures convergence to the true DPM target distribution asymptotically.  Performance has been evaluated in a Bayesian Infinite Gaussian mixture density estimation problem and it is shown that the proposed algorithm outperforms conventional Monte Carlo approaches in terms of estimation variance and average log-marginal likelihood.",
        "bibtex": "@InProceedings{pmlr-v9-ulker10a,\n  title = \t {Sequential Monte Carlo Samplers for Dirichlet Process Mixtures},\n  author = \t {Ulker, Yener and G\u00fcnsel, Bilge and Cemgil, Taylan},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {876--883},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/ulker10a/ulker10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/ulker10a.html},\n  abstract = \t {In this paper, we develop a novel online algorithm based on the Sequential Monte Carlo(SMC) samplers framework for posterior inference in Dirichlet Process Mixtures (DPM). Our method generalizes  many sequential importance sampling approaches. It provides a computationally efficient improvement to particle filtering that is less prone to getting stuck in isolated modes. The proposed method is a particular SMC sampler that enables us to design sophisticated clustering update schemes, such as updating past trajectories of the particles in light of recent observations, and still ensures convergence to the true DPM target distribution asymptotically.  Performance has been evaluated in a Bayesian Infinite Gaussian mixture density estimation problem and it is shown that the proposed algorithm outperforms conventional Monte Carlo approaches in terms of estimation variance and average log-marginal likelihood.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/ulker10a/ulker10a.pdf",
        "supp": "",
        "pdf_size": 469164,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9160435776213506980&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Dept. of Electronics and Communications Eng., Istanbul Technical University; Dept. of Electronics and Communications Eng., Istanbul Technical University; Dept. of Computer Engineering, Bogazici University",
        "aff_domain": "itu.edu.tr;itu.edu.tr;boun.edu.tr",
        "email": "itu.edu.tr;itu.edu.tr;boun.edu.tr",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Istanbul Technical University;Bogazici University",
        "aff_unique_dep": "Dept. of Electronics and Communications Eng.;Dept. of Computer Engineering",
        "aff_unique_url": "https://www.itu.edu.tr;https://www.boun.edu.tr",
        "aff_unique_abbr": "ITU;BU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Turkey"
    },
    {
        "id": "6aa8841d52",
        "title": "Simple Exponential Family PCA",
        "site": "https://proceedings.mlr.press/v9/li10b.html",
        "author": "Jun Li; Dacheng Tao",
        "abstract": "Bayesian principal component analysis (BPCA), a probabilistic reformulation of PCA with Bayesian model selection, is a systematic approach to determining the number of essential principal components (PCs) for data representation.  However, it assumes that data are Gaussian distributed and thus it cannot handle all types of practical observations, e.g. integers and binary values.  In this paper, we propose simple exponential family PCA (SePCA), a generalised family of probabilistic principal component analysers. SePCA employs exponential family distributions to handle general types of observations. By using Bayesian inference, SePCA also automatically discovers the number of essential PCs. We discuss techniques for fitting the model, develop the corresponding mixture model, and show the effectiveness of the model based on experiments.",
        "bibtex": "@InProceedings{pmlr-v9-li10b,\n  title = \t {Simple Exponential Family PCA},\n  author = \t {Li, Jun and Tao, Dacheng},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {453--460},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/li10b/li10b.pdf},\n  url = \t {https://proceedings.mlr.press/v9/li10b.html},\n  abstract = \t {Bayesian principal component analysis (BPCA), a probabilistic reformulation of PCA with Bayesian model selection, is a systematic approach to determining the number of essential principal components (PCs) for data representation.  However, it assumes that data are Gaussian distributed and thus it cannot handle all types of practical observations, e.g. integers and binary values.  In this paper, we propose simple exponential family PCA (SePCA), a generalised family of probabilistic principal component analysers. SePCA employs exponential family distributions to handle general types of observations. By using Bayesian inference, SePCA also automatically discovers the number of essential PCs. We discuss techniques for fitting the model, develop the corresponding mixture model, and show the effectiveness of the model based on experiments.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/li10b/li10b.pdf",
        "supp": "",
        "pdf_size": 354686,
        "gs_citation": 106,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4036363132332188596&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "School of Computer Engineering, Nanyang Technological University; School of Computer Engineering, Nanyang Technological University",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Nanyang Technological University",
        "aff_unique_dep": "School of Computer Engineering",
        "aff_unique_url": "https://www.ntu.edu.sg",
        "aff_unique_abbr": "NTU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "141ed823a3",
        "title": "Solving the Uncapacitated Facility Location Problem Using Message Passing Algorithms",
        "site": "https://proceedings.mlr.press/v9/lazic10a.html",
        "author": "Nevena Lazic; Brendan Frey; Parham Aarabi",
        "abstract": "The Uncapacitated Facility Location Problem (UFLP) is one of the most widely studied discrete location problems, whose applications arise in a variety of settings.  We tackle the UFLP using probabilistic inference in a graphical model - an approach that has received little attention in the past.  We show that the fixed points of max-product linear programming (MPLP), a convexified version of the max-product algorithm, can be used to construct a solution with a 3-approximation guarantee for metric UFLP instances.  In addition, we characterize some scenarios under which the MPLP solution is guaranteed to be globally optimal.  We evaluate the performance of both max-sum and MPLP empirically on metric and non-metric problems, demonstrating the advantages of the 3-approximation construction and algorithm applicability to non-metric instances.",
        "bibtex": "@InProceedings{pmlr-v9-lazic10a,\n  title = \t {Solving the Uncapacitated Facility Location Problem Using Message Passing Algorithms},\n  author = \t {Lazic, Nevena and Frey, Brendan and Aarabi, Parham},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {429--436},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/lazic10a/lazic10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/lazic10a.html},\n  abstract = \t {The Uncapacitated Facility Location Problem (UFLP) is one of the most widely studied discrete location problems, whose applications arise in a variety of settings.  We tackle the UFLP using probabilistic inference in a graphical model - an approach that has received little attention in the past.  We show that the fixed points of max-product linear programming (MPLP), a convexified version of the max-product algorithm, can be used to construct a solution with a 3-approximation guarantee for metric UFLP instances.  In addition, we characterize some scenarios under which the MPLP solution is guaranteed to be globally optimal.  We evaluate the performance of both max-sum and MPLP empirically on metric and non-metric problems, demonstrating the advantages of the 3-approximation construction and algorithm applicability to non-metric instances.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/lazic10a/lazic10a.pdf",
        "supp": "",
        "pdf_size": 820806,
        "gs_citation": 78,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7669171261036908937&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "ed38c66d59",
        "title": "State-Space Inference and Learning with Gaussian Processes",
        "site": "https://proceedings.mlr.press/v9/turner10a.html",
        "author": "Ryan Turner; Marc Deisenroth; Carl Rasmussen",
        "abstract": "State-space inference and learning with Gaussian processes (GPs) is an unsolved problem. We propose a new, general methodology for inference and learning in nonlinear state-space models that are described probabilistically by non-parametric GP models. We apply the expectation maximization algorithm to iterate between inference in the latent state-space and learning the parameters of the underlying GP dynamics model.",
        "bibtex": "@InProceedings{pmlr-v9-turner10a,\n  title = \t {State-Space Inference and Learning with Gaussian Processes},\n  author = \t {Turner, Ryan and Deisenroth, Marc and Rasmussen, Carl},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {868--875},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/turner10a/turner10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/turner10a.html},\n  abstract = \t {State-space inference and learning with Gaussian processes (GPs) is an unsolved problem. We propose a new, general methodology for inference and learning in nonlinear state-space models that are described probabilistically by non-parametric GP models. We apply the expectation maximization algorithm to iterate between inference in the latent state-space and learning the parameters of the underlying GP dynamics model.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/turner10a/turner10a.pdf",
        "supp": "",
        "pdf_size": 945467,
        "gs_citation": 126,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5592313579866443446&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 19,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "957e571487",
        "title": "Structured Prediction Cascades",
        "site": "https://proceedings.mlr.press/v9/weiss10a.html",
        "author": "David Weiss; Benjamin Taskar",
        "abstract": "Structured prediction tasks pose a fundamental trade-off between the need for model complexity to increase predictive power and the limited computational resources for inference in the exponentially-sized output spaces such models require. We formulate and develop structured prediction cascades: a sequence of increasingly complex models that progressively filter the space of possible outputs. We represent an exponentially large set of filtered outputs using max marginals and propose a novel convex loss function that balances filtering error with filtering efficiency. We provide generalization bounds for these loss functions and evaluate our approach on handwriting recognition and part-of-speech tagging. We find that the learned cascades are capable of reducing the complexity of inference by up to five orders of magnitude, enabling the use of models which incorporate higher order features and yield higher accuracy.",
        "bibtex": "@InProceedings{pmlr-v9-weiss10a,\n  title = \t {Structured Prediction Cascades},\n  author = \t {Weiss, David and Taskar, Benjamin},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {916--923},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/weiss10a/weiss10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/weiss10a.html},\n  abstract = \t {Structured prediction tasks pose a fundamental trade-off between the need for model complexity to increase predictive power and the limited computational resources for inference in the exponentially-sized output spaces such models require. We formulate and develop structured prediction cascades: a sequence of increasingly complex models that progressively filter the space of possible outputs. We represent an exponentially large set of filtered outputs using max marginals and propose a novel convex loss function that balances filtering error with filtering efficiency. We provide generalization bounds for these loss functions and evaluate our approach on handwriting recognition and part-of-speech tagging. We find that the learned cascades are capable of reducing the complexity of inference by up to five orders of magnitude, enabling the use of models which incorporate higher order features and yield higher accuracy.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/weiss10a/weiss10a.pdf",
        "supp": "",
        "pdf_size": 1731277,
        "gs_citation": 151,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4557187161765709287&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "University of Pennsylvania; University of Pennsylvania",
        "aff_domain": "; ",
        "email": "; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Pennsylvania",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.upenn.edu",
        "aff_unique_abbr": "UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b7a2414c46",
        "title": "Structured Sparse Principal Component Analysis",
        "site": "https://proceedings.mlr.press/v9/jenatton10a.html",
        "author": "Rodolphe Jenatton; Guillaume Obozinski; Francis Bach",
        "abstract": "We present an extension of sparse PCA, or sparse dictionary learning, where the sparsity patterns of all dictionary elements are structured and constrained to belong to a prespecified set of shapes. This structured sparse PCA is based on a structured regularization recently introduced by Jenatton et al. (2009). While classical sparse priors only deal with cardinality, the regularization we use encodes higher-order information about the data. We propose an efficient and simple optimization procedure to solve this problem. Experiments with two practical tasks, the denoising of sparse structured signals and face recognition, demonstrate the benefits of the proposed structured approach over unstructured approaches.",
        "bibtex": "@InProceedings{pmlr-v9-jenatton10a,\n  title = \t {Structured Sparse Principal Component Analysis},\n  author = \t {Jenatton, Rodolphe and Obozinski, Guillaume and Bach, Francis},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {366--373},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/jenatton10a/jenatton10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/jenatton10a.html},\n  abstract = \t {We present an extension of sparse PCA, or sparse dictionary learning, where the sparsity patterns of all dictionary elements are structured and constrained to belong to a prespecified set of shapes. This structured sparse PCA is based on a structured regularization recently introduced by Jenatton et al. (2009). While classical sparse priors only deal with cardinality, the regularization we use encodes higher-order information about the data. We propose an efficient and simple optimization procedure to solve this problem. Experiments with two practical tasks, the denoising of sparse structured signals and face recognition, demonstrate the benefits of the proposed structured approach over unstructured approaches.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/jenatton10a/jenatton10a.pdf",
        "supp": "",
        "pdf_size": 589772,
        "gs_citation": 404,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6943000575122669337&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "INRIA Willow Project, Laboratoire d\u2019Informatique de l\u2019Ecole Normale Sup\u00e9rieure (INRIA/ENS/CNRS UMR8548) - 23, avenue d\u2019Italie, 75214 Paris, France; INRIA Willow Project, Laboratoire d\u2019Informatique de l\u2019Ecole Normale Sup\u00e9rieure (INRIA/ENS/CNRS UMR8548) - 23, avenue d\u2019Italie, 75214 Paris, France; INRIA Willow Project, Laboratoire d\u2019Informatique de l\u2019Ecole Normale Sup\u00e9rieure (INRIA/ENS/CNRS UMR8548) - 23, avenue d\u2019Italie, 75214 Paris, France",
        "aff_domain": "inria.fr;inria.fr;inria.fr",
        "email": "inria.fr;inria.fr;inria.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "INRIA",
        "aff_unique_dep": "Laboratoire d\u2019Informatique de l\u2019Ecole Normale Sup\u00e9rieure",
        "aff_unique_url": "https://www.inria.fr",
        "aff_unique_abbr": "INRIA",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Paris",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "7da2311d19",
        "title": "Sufficient Dimension Reduction via Squared-loss Mutual Information Estimation",
        "site": "https://proceedings.mlr.press/v9/suzuki10a.html",
        "author": "Taiji Suzuki; Masashi Sugiyama",
        "abstract": "The goal of sufficient dimension reduction in supervised learning is to find the low dimensional subspace of input features that is \"sufficient\" for predicting output values. In this paper, we propose a novel sufficient dimension reduction method using a squared-loss variant of mutual information as a dependency measure. We utilize an analytic approximator of squared-loss mutual information based on density ratio estimation, which is shown to possess suitable convergence properties. We then develop a natural gradient algorithm for sufficient subspace search. Numerical experiments show that the proposed method compares favorably with existing dimension reduction approaches.",
        "bibtex": "@InProceedings{pmlr-v9-suzuki10a,\n  title = \t {Sufficient Dimension Reduction via Squared-loss Mutual Information Estimation},\n  author = \t {Suzuki, Taiji and Sugiyama, Masashi},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {804--811},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/suzuki10a/suzuki10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/suzuki10a.html},\n  abstract = \t {The goal of sufficient dimension reduction in supervised learning is to find the low dimensional subspace of input features that is \"sufficient\" for predicting output values. In this paper, we propose a novel sufficient dimension reduction method using a squared-loss variant of mutual information as a dependency measure. We utilize an analytic approximator of squared-loss mutual information based on density ratio estimation, which is shown to possess suitable convergence properties. We then develop a natural gradient algorithm for sufficient subspace search. Numerical experiments show that the proposed method compares favorably with existing dimension reduction approaches.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/suzuki10a/suzuki10a.pdf",
        "supp": "",
        "pdf_size": 605193,
        "gs_citation": 78,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7316620851007783747&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "The University of Tokyo; Tokyo Institute of Technology",
        "aff_domain": "stat.t.u-tokyo.ac.jp;cs.titech.ac.jp",
        "email": "stat.t.u-tokyo.ac.jp;cs.titech.ac.jp",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Tokyo;Tokyo Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.u-tokyo.ac.jp;https://www.titech.ac.jp",
        "aff_unique_abbr": "UTokyo;Titech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "3ca10fea15",
        "title": "Sufficient covariates and linear propensity analysis",
        "site": "https://proceedings.mlr.press/v9/guo10a.html",
        "author": "Hui Guo; Philip Dawid",
        "abstract": "Working within the decision-theoretic framework for causal inference, we study the properties of \u201csufficient covariates\", which support causal inference from observational data, and possibilities for their reduction. In particular we illustrate the role of a propensity variable by means of a simple model, and explain why such a reduction typically does not increase (and may reduce) estimation efficiency.",
        "bibtex": "@InProceedings{pmlr-v9-guo10a,\n  title = \t {Sufficient covariates and linear propensity analysis},\n  author = \t {Guo, Hui and Dawid, Philip},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {281--288},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/guo10a/guo10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/guo10a.html},\n  abstract = \t {Working within the decision-theoretic framework for causal inference, we study the properties of \u201csufficient covariates\", which support causal inference from observational data, and possibilities for their reduction. In particular we illustrate the role of a propensity variable by means of a simple model, and explain why such a reduction typically does not increase (and may reduce) estimation efficiency.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/guo10a/guo10a.pdf",
        "supp": "",
        "pdf_size": 812958,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15094641593670543120&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Statistical Laboratory, University of Cambridge; Statistical Laboratory, University of Cambridge",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Cambridge",
        "aff_unique_dep": "Statistical Laboratory",
        "aff_unique_url": "https://www.cam.ac.uk",
        "aff_unique_abbr": "Cambridge",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "c0542a3ec8",
        "title": "Supervised Dimension Reduction Using Bayesian Mixture Modeling",
        "site": "https://proceedings.mlr.press/v9/mao10a.html",
        "author": "Kai Mao; Feng Liang; Sayan Mukherjee",
        "abstract": "We develop a Bayesian framework for supervised dimension reduction using a flexible nonparametric Bayesian mixture modeling approach. Our method retrieves the dimension reduction or d.r. subspace by utilizing a dependent Dirichlet process that allows for natural clustering for the data in terms of both the response and predictor variables. Formal probabilistic models with likelihoods and priors are given and efficient posterior sampling of the d.r. subspace can be obtained by a Gibbs sampler. As the posterior draws are linear subspaces which are points on a Grassmann manifold, we output the posterior mean d.r. subspace with respect to geodesics on the Grassmannian. The utility of our approach is illustrated on a set of simulated and real examples.  Some Key Words: supervised dimension reduction, inverse regression, Dirichlet process, factor models, Grassman manifold.",
        "bibtex": "@InProceedings{pmlr-v9-mao10a,\n  title = \t {Supervised Dimension Reduction Using Bayesian Mixture Modeling},\n  author = \t {Mao, Kai and Liang, Feng and Mukherjee, Sayan},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {501--508},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/mao10a/mao10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/mao10a.html},\n  abstract = \t {We develop a Bayesian framework for supervised dimension reduction using a flexible nonparametric Bayesian mixture modeling approach. Our method retrieves the dimension reduction or d.r. subspace by utilizing a dependent Dirichlet process that allows for natural clustering for the data in terms of both the response and predictor variables. Formal probabilistic models with likelihoods and priors are given and efficient posterior sampling of the d.r. subspace can be obtained by a Gibbs sampler. As the posterior draws are linear subspaces which are points on a Grassmann manifold, we output the posterior mean d.r. subspace with respect to geodesics on the Grassmannian. The utility of our approach is illustrated on a set of simulated and real examples.  Some Key Words: supervised dimension reduction, inverse regression, Dirichlet process, factor models, Grassman manifold.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/mao10a/mao10a.pdf",
        "supp": "",
        "pdf_size": 373957,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3994409319853816704&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "66083956a2",
        "title": "Tempered Markov Chain Monte Carlo for training of Restricted Boltzmann Machines",
        "site": "https://proceedings.mlr.press/v9/desjardins10a.html",
        "author": "Guillaume Desjardins; Aaron Courville; Yoshua Bengio; Pascal Vincent; Olivier Delalleau",
        "abstract": "Alternating Gibbs sampling is the most common scheme used for sampling from Restricted Boltzmann Machines (RBM), a crucial component in deep architectures such as Deep Belief Networks. However, we find that it often does a very poor job of rendering the diversity of modes captured by the trained model. We suspect that this hinders the advantage that could in principle be brought by training algorithms relying on Gibbs sampling for uncovering spurious modes, such as the Persistent Contrastive Divergence algorithm. To alleviate this problem, we explore the use of tempered Markov Chain Monte-Carlo for sampling in RBMs.  We find both through visualization of samples and measures of likelihood on a toy dataset that it helps both sampling and learning.",
        "bibtex": "@InProceedings{pmlr-v9-desjardins10a,\n  title = \t {Tempered Markov Chain Monte Carlo for training of Restricted Boltzmann Machines},\n  author = \t {Desjardins, Guillaume and Courville, Aaron and Bengio, Yoshua and Vincent, Pascal and Delalleau, Olivier},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {145--152},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/desjardins10a/desjardins10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/desjardins10a.html},\n  abstract = \t {Alternating Gibbs sampling is the most common scheme used for sampling from Restricted Boltzmann Machines (RBM), a crucial component in deep architectures such as Deep Belief Networks. However, we find that it often does a very poor job of rendering the diversity of modes captured by the trained model. We suspect that this hinders the advantage that could in principle be brought by training algorithms relying on Gibbs sampling for uncovering spurious modes, such as the Persistent Contrastive Divergence algorithm. To alleviate this problem, we explore the use of tempered Markov Chain Monte-Carlo for sampling in RBMs.  We find both through visualization of samples and measures of likelihood on a toy dataset that it helps both sampling and learning.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/desjardins10a/desjardins10a.pdf",
        "supp": "",
        "pdf_size": 1660030,
        "gs_citation": 160,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7194634392171839437&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Universit\u00b4e de Montr\u00b4eal; Universit\u00b4e de Montr\u00b4eal; Universit\u00b4e de Montr\u00b4eal; Universit\u00b4e de Montr\u00b4eal; Universit\u00b4e de Montr\u00b4eal",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Universit\u00e9 de Montr\u00e9al",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.umontreal.ca",
        "aff_unique_abbr": "UdeM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "dbb36a82bd",
        "title": "The Feature Selection Path in Kernel Methods",
        "site": "https://proceedings.mlr.press/v9/li10a.html",
        "author": "Fuxin Li; Cristian Sminchisescu",
        "abstract": "The problem of automatic feature selection/weighting in kernel methods is examined. We work on a formulation that optimizes both the weights of features and the parameters of the kernel model simultaneously, using $L_1$ regularization for feature selection. Under quite general choices of kernels, we prove that there exists a unique regularization path for this problem, that runs from 0 to a stationary point of the non-regularized problem. We propose an ODE-based homotopy method to follow this trajectory. By following the path, our algorithm is able to automatically discard irrelevant features and to automatically go back and forth to avoid local optima. Experiments on synthetic and real datasets show that the method achieves low prediction error and is efficient in separating relevant from irrelevant features.",
        "bibtex": "@InProceedings{pmlr-v9-li10a,\n  title = \t {The Feature Selection Path in Kernel Methods},\n  author = \t {Li, Fuxin and Sminchisescu, Cristian},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {445--452},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/li10a/li10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/li10a.html},\n  abstract = \t {The problem of automatic feature selection/weighting in kernel methods is examined. We work on a formulation that optimizes both the weights of features and the parameters of the kernel model simultaneously, using $L_1$ regularization for feature selection. Under quite general choices of kernels, we prove that there exists a unique regularization path for this problem, that runs from 0 to a stationary point of the non-regularized problem. We propose an ODE-based homotopy method to follow this trajectory. By following the path, our algorithm is able to automatically discard irrelevant features and to automatically go back and forth to avoid local optima. Experiments on synthetic and real datasets show that the method achieves low prediction error and is efficient in separating relevant from irrelevant features.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/li10a/li10a.pdf",
        "supp": "",
        "pdf_size": 597200,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4593526464520720959&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Institute of Numerical Simulation, University of Bonn; Institute of Numerical Simulation, University of Bonn",
        "aff_domain": "ins.uni-bonn.de;ins.uni-bonn.de",
        "email": "ins.uni-bonn.de;ins.uni-bonn.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Bonn",
        "aff_unique_dep": "Institute of Numerical Simulation",
        "aff_unique_url": "https://www.uni-bonn.de",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "7982e8fe14",
        "title": "The Group Dantzig Selector",
        "site": "https://proceedings.mlr.press/v9/liu10a.html",
        "author": "Han Liu; Jian Zhang; Xiaoye Jiang; Jun Liu",
        "abstract": "We introduce a new method \u2013 the  group Dantzig selector \u2013 for high dimensional sparse regression with  group structure, which has a convincing theory about why utilizing the group structure can be beneficial. Under a  group restricted isometry condition, we obtain a significantly improved nonasymptotic $\\ell_2$-norm bound over the basis pursuit or the Dantzig selector which ignores the group structure.   To gain more insight, we also introduce a surprisingly simple and intuitive  \u201csparsity oracle condition\u201d to obtain a block $\\ell_1$-norm bound, which is easily accessible to a broad audience in machine learning community. Encouraging numerical results are also provided to support our theory.",
        "bibtex": "@InProceedings{pmlr-v9-liu10a,\n  title = \t {The Group Dantzig Selector},\n  author = \t {Liu, Han and Zhang, Jian and Jiang, Xiaoye and Liu, Jun},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {461--468},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/liu10a/liu10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/liu10a.html},\n  abstract = \t {We introduce a new method \u2013 the  group Dantzig selector \u2013 for high dimensional sparse regression with  group structure, which has a convincing theory about why utilizing the group structure can be beneficial. Under a  group restricted isometry condition, we obtain a significantly improved nonasymptotic $\\ell_2$-norm bound over the basis pursuit or the Dantzig selector which ignores the group structure.   To gain more insight, we also introduce a surprisingly simple and intuitive  \u201csparsity oracle condition\u201d to obtain a block $\\ell_1$-norm bound, which is easily accessible to a broad audience in machine learning community. Encouraging numerical results are also provided to support our theory.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/liu10a/liu10a.pdf",
        "supp": "",
        "pdf_size": 813171,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4221740404978570341&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Machine Learning Department, Carnegie Mellon University; Statistics Department, Purdue University; Computational Mathematics, Stanford University; Computer Science Department, Arizona State University",
        "aff_domain": "; ; ; ",
        "email": "; ; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "Carnegie Mellon University;Purdue University;Stanford University;Arizona State University",
        "aff_unique_dep": "Machine Learning Department;Statistics Department;Department of Computational Mathematics;Computer Science Department",
        "aff_unique_url": "https://www.cmu.edu;https://www.purdue.edu;https://www.stanford.edu;https://www.asu.edu",
        "aff_unique_abbr": "CMU;Purdue;Stanford;ASU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Stanford",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ad43b36c08",
        "title": "Towards Understanding Situated Natural Language",
        "site": "https://proceedings.mlr.press/v9/bordes10a.html",
        "author": "Antoine Bordes; Nicolas Usunier; Ronan Collobert; Jason Weston",
        "abstract": "We present a general framework and learning algorithm for the task of concept labeling: each word in a given sentence has to be tagged with the unique physical entity (e.g. person, object or location) or abstract concept it refers to. Our method allows both world knowledge and linguistic information to be used during learning and prediction. We show experimentally that we can learn to use world knowledge to resolve ambiguities in language, such as word senses or reference resolution, without the use of handcrafted rules or features.",
        "bibtex": "@InProceedings{pmlr-v9-bordes10a,\n  title = \t {Towards Understanding Situated Natural Language},\n  author = \t {Bordes, Antoine and Usunier, Nicolas and Collobert, Ronan and Weston, Jason},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {65--72},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/bordes10a/bordes10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/bordes10a.html},\n  abstract = \t {We present a general framework and learning algorithm for the task of concept labeling: each word in a given sentence has to be tagged with the unique physical entity (e.g. person, object or location) or abstract concept it refers to. Our method allows both world knowledge and linguistic information to be used during learning and prediction. We show experimentally that we can learn to use world knowledge to resolve ambiguities in language, such as word senses or reference resolution, without the use of handcrafted rules or features.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/bordes10a/bordes10a.pdf",
        "supp": "",
        "pdf_size": 1033291,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17849812119124669978&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 19,
        "aff": "LIP6, Universit\u00e9 Paris 6, Paris, France; LIP6, Universit\u00e9 Paris 6, Paris, France; NEC Laboratories America, Princeton, USA; Google, New York, USA",
        "aff_domain": "lip6.fr;lip6.fr;collobert.com;google.com",
        "email": "lip6.fr;lip6.fr;collobert.com;google.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2",
        "aff_unique_norm": "Universit\u00e9 Paris 6;NEC Laboratories America;Google",
        "aff_unique_dep": "LIP6;;",
        "aff_unique_url": "https://www.upmc.fr;https://www.nec-labs.com;https://www.google.com",
        "aff_unique_abbr": "UP6;NEC Labs;Google",
        "aff_campus_unique_index": "0;0;1;2",
        "aff_campus_unique": "Paris;Princeton;New York",
        "aff_country_unique_index": "0;0;1;1",
        "aff_country_unique": "France;United States"
    },
    {
        "id": "db01b8b0b9",
        "title": "Ultra-high Dimensional Multiple Output Learning With Simultaneous Orthogonal Matching Pursuit: Screening Approach",
        "site": "https://proceedings.mlr.press/v9/kolar10a.html",
        "author": "Mladen Kolar; Eric Xing",
        "abstract": "We propose a novel application of the Simultaneous Orthogonal Matching Pursuit (S-OMP) procedure to perform variable selection in ultra-high dimensional multiple output regression problems, which is the first attempt to utilize multiple outputs to perform fast removal of the irrelevant variables. As our main theoretical contribution, we show that the S-OMP can be used to reduce an ultra-high number of variables to below the sample size, without losing relevant variables.  We also provide formal evidence that the modified Bayesian information criterion (BIC) can be used to efficiently select the number of iterations in the S-OMP. Once the number of variables has been reduced to a manageable size, we show that a more computationally demanding procedure can be used to identify the relevant variables for each of the regression outputs. We further provide evidence on the benefit of variable selection using the regression outputs jointly, as opposed to performing variable selection for each output separately. The finite sample performance of the S-OMP has been demonstrated on extensive simulation studies.",
        "bibtex": "@InProceedings{pmlr-v9-kolar10a,\n  title = \t {Ultra-high Dimensional Multiple Output Learning With Simultaneous Orthogonal Matching Pursuit: Screening Approach},\n  author = \t {Kolar, Mladen and Xing, Eric},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {413--420},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/kolar10a/kolar10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/kolar10a.html},\n  abstract = \t {We propose a novel application of the Simultaneous Orthogonal Matching Pursuit (S-OMP) procedure to perform variable selection in ultra-high dimensional multiple output regression problems, which is the first attempt to utilize multiple outputs to perform fast removal of the irrelevant variables. As our main theoretical contribution, we show that the S-OMP can be used to reduce an ultra-high number of variables to below the sample size, without losing relevant variables.  We also provide formal evidence that the modified Bayesian information criterion (BIC) can be used to efficiently select the number of iterations in the S-OMP. Once the number of variables has been reduced to a manageable size, we show that a more computationally demanding procedure can be used to identify the relevant variables for each of the regression outputs. We further provide evidence on the benefit of variable selection using the regression outputs jointly, as opposed to performing variable selection for each output separately. The finite sample performance of the S-OMP has been demonstrated on extensive simulation studies.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/kolar10a/kolar10a.pdf",
        "supp": "",
        "pdf_size": 567997,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9803682895554717631&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA, USA; Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA, USA",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Machine Learning Department",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3fb8bd35c5",
        "title": "Understanding the difficulty of training deep feedforward neural networks",
        "site": "https://proceedings.mlr.press/v9/glorot10a.html",
        "author": "Xavier Glorot; Yoshua Bengio",
        "abstract": "Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.",
        "bibtex": "@InProceedings{pmlr-v9-glorot10a,\n  title = \t {Understanding the difficulty of training deep feedforward neural networks},\n  author = \t {Glorot, Xavier and Bengio, Yoshua},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {249--256},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/glorot10a.html},\n  abstract = \t {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf",
        "supp": "",
        "pdf_size": 1647622,
        "gs_citation": 27559,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17889055433985220047&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff": "DIRO, Universitu00e9 de Montru00e9al, Montru00e9al, Quu00e9bec, Canada; DIRO, Universitu00e9 de Montru00e9al, Montru00e9al, Quu00e9bec, Canada",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Universit\u00e9 de Montr\u00e9al",
        "aff_unique_dep": "DIRO",
        "aff_unique_url": "https://www.umontreal.ca",
        "aff_unique_abbr": "UdeM",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Montr\u00e9al",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "4c5541f565",
        "title": "Unsupervised Aggregation for Classification Problems with Large Numbers of Categories",
        "site": "https://proceedings.mlr.press/v9/titov10a.html",
        "author": "Ivan Titov; Alexandre Klementiev; Kevin Small; Dan Roth",
        "abstract": "Classification problems with a very large or unbounded set of output categories are common in many areas such as natural language and image processing. In order to improve accuracy on these tasks, it is natural for a  decision-maker to  combine predictions from various  sources.  However, supervised data needed to fit an aggregation model  is often difficult to obtain, especially if needed for multiple domains. Therefore, we propose a generative model for unsupervised aggregation which exploits the agreement signal to estimate the expertise of individual judges.  Due to the large output space size, this aggregation model cannot encode expertise of constituent judges with respect to every category for all problems. Consequently, we extend it by incorporating the notion of category types  to account for variability  of the judge expertise depending on the type.  The viability of our approach is demonstrated both on synthetic experiments and on a practical task of syntactic parser aggregation.",
        "bibtex": "@InProceedings{pmlr-v9-titov10a,\n  title = \t {Unsupervised Aggregation for Classification Problems with Large Numbers of Categories},\n  author = \t {Titov, Ivan and Klementiev, Alexandre and Small, Kevin and Roth, Dan},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {836--843},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/titov10a/titov10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/titov10a.html},\n  abstract = \t {Classification problems with a very large or unbounded set of output categories are common in many areas such as natural language and image processing. In order to improve accuracy on these tasks, it is natural for a  decision-maker to  combine predictions from various  sources.  However, supervised data needed to fit an aggregation model  is often difficult to obtain, especially if needed for multiple domains. Therefore, we propose a generative model for unsupervised aggregation which exploits the agreement signal to estimate the expertise of individual judges.  Due to the large output space size, this aggregation model cannot encode expertise of constituent judges with respect to every category for all problems. Consequently, we extend it by incorporating the notion of category types  to account for variability  of the judge expertise depending on the type.  The viability of our approach is demonstrated both on synthetic experiments and on a practical task of syntactic parser aggregation.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/titov10a/titov10a.pdf",
        "supp": "",
        "pdf_size": 867353,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15406665192120415405&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "efbd83c212",
        "title": "Using Descendants as Instrumental Variables for the Identification of Direct Causal Effects in Linear SEMs",
        "site": "https://proceedings.mlr.press/v9/chan10a.html",
        "author": "Hei Chan; Manabu Kuroki",
        "abstract": "In this paper, we present an extended set of graphical criteria for the identification of direct causal effects in linear Structural Equation Models (SEMs). Previous methods of graphical identification of direct causal effects in linear SEMs include methods such as the single-door criterion, the instrumental variable and the IV-pair, and the accessory set. However, there remain graphical models where a direct causal effect can be identified and these graphical criteria all fail. As a result, we introduce a new set of graphical criteria which uses descendants of either the cause variable or the effect variable as \u201cpath-specific instrumental variables\u201d for the identification of the direct causal effect as long as certain conditions are satisfied. These conditions are based on edge removal and the existing graphical criteria of instrumental variables, and the identifiability of certain other total effects, and thus can be easily checked.",
        "bibtex": "@InProceedings{pmlr-v9-chan10a,\n  title = \t {Using Descendants as Instrumental Variables for the Identification of Direct Causal Effects in Linear SEMs},\n  author = \t {Chan, Hei and Kuroki, Manabu},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {73--80},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/chan10a/chan10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/chan10a.html},\n  abstract = \t {In this paper, we present an extended set of graphical criteria for the identification of direct causal effects in linear Structural Equation Models (SEMs). Previous methods of graphical identification of direct causal effects in linear SEMs include methods such as the single-door criterion, the instrumental variable and the IV-pair, and the accessory set. However, there remain graphical models where a direct causal effect can be identified and these graphical criteria all fail. As a result, we introduce a new set of graphical criteria which uses descendants of either the cause variable or the effect variable as \u201cpath-specific instrumental variables\u201d for the identification of the direct causal effect as long as certain conditions are satisfied. These conditions are based on edge removal and the existing graphical criteria of instrumental variables, and the identifiability of certain other total effects, and thus can be easily checked.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/chan10a/chan10a.pdf",
        "supp": "",
        "pdf_size": 1281768,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2517835677305252147&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Institute of Statistical Mathematics, Tokyo, Japan; Department of Systems Innovation, Graduate School of Engineering Science, Osaka University, Osaka, Japan",
        "aff_domain": "gmail.com;sigmath.es.osaka-u.ac.jp",
        "email": "gmail.com;sigmath.es.osaka-u.ac.jp",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Institute of Statistical Mathematics;Osaka University",
        "aff_unique_dep": ";Department of Systems Innovation",
        "aff_unique_url": "https://www.ism.ac.jp;https://www.osaka-u.ac.jp",
        "aff_unique_abbr": "ISM;Osaka U",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Tokyo;Osaka",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "dffe4db1be",
        "title": "Variational methods for Reinforcement Learning",
        "site": "https://proceedings.mlr.press/v9/furmston10a.html",
        "author": "Thomas Furmston; David Barber",
        "abstract": "We consider reinforcement learning as solving a Markov decision process with unknown transition distribution. Based on interaction with the environment, an estimate of the transition matrix is obtained from which the optimal decision policy is formed. The classical maximum likelihood point estimate of the transition model does not reflect the uncertainty in the estimate of the transition model and the resulting policies may consequently lack a sufficient degree of exploration. We consider a Bayesian alternative that maintains a distribution over the transition so that the resulting policy takes into account the limited experience of the environment. The resulting algorithm is formally intractable and we discuss two approximate solution methods, Variational Bayes and Expectation Propagation.",
        "bibtex": "@InProceedings{pmlr-v9-furmston10a,\n  title = \t {Variational methods for Reinforcement Learning},\n  author = \t {Furmston, Thomas and Barber, David},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {241--248},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/furmston10a/furmston10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/furmston10a.html},\n  abstract = \t {We consider reinforcement learning as solving a Markov decision process with unknown transition distribution. Based on interaction with the environment, an estimate of the transition matrix is obtained from which the optimal decision policy is formed. The classical maximum likelihood point estimate of the transition model does not reflect the uncertainty in the estimate of the transition model and the resulting policies may consequently lack a sufficient degree of exploration. We consider a Bayesian alternative that maintains a distribution over the transition so that the resulting policy takes into account the limited experience of the environment. The resulting algorithm is formally intractable and we discuss two approximate solution methods, Variational Bayes and Expectation Propagation.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/furmston10a/furmston10a.pdf",
        "supp": "",
        "pdf_size": 837362,
        "gs_citation": 69,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1005173032141533617&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "5596f7a365",
        "title": "Why Does Unsupervised Pre-training Help Deep Learning?",
        "site": "https://proceedings.mlr.press/v9/erhan10a.html",
        "author": "Dumitru Erhan; Aaron Courville; Yoshua Bengio; Pascal Vincent",
        "abstract": "Much recent research has been devoted to learning algorithms for deep architectures such as Deep Belief Networks and stacks of auto-encoder   variants with impressive results being obtained in several areas, mostly   on vision and language datasets.  The best results obtained on supervised   learning tasks often involve an unsupervised learning component, usually   in an unsupervised pre-training phase. The main question investigated   here is the following: why does unsupervised pre-training work so well?   Through extensive experimentation, we explore several possible   explanations discussed in the literature including its action as a   regularizer (Erhan et al. 2009) and as an aid to optimization   (Bengio et al. 2007).  Our results build on the work of   Erhan et al. 2009, showing that unsupervised pre-training appears to   play predominantly a regularization role in subsequent supervised   training. However our results in an online setting, with a virtually unlimited   data stream, point to a somewhat more nuanced interpretation of the roles   of optimization and regularization in the unsupervised pre-training   effect.",
        "bibtex": "@InProceedings{pmlr-v9-erhan10a,\n  title = \t {Why Does Unsupervised Pre-training Help Deep Learning?},\n  author = \t {Erhan, Dumitru and Courville, Aaron and Bengio, Yoshua and Vincent, Pascal},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {201--208},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/erhan10a/erhan10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/erhan10a.html},\n  abstract = \t {Much recent research has been devoted to learning algorithms for deep architectures such as Deep Belief Networks and stacks of auto-encoder   variants with impressive results being obtained in several areas, mostly   on vision and language datasets.  The best results obtained on supervised   learning tasks often involve an unsupervised learning component, usually   in an unsupervised pre-training phase. The main question investigated   here is the following: why does unsupervised pre-training work so well?   Through extensive experimentation, we explore several possible   explanations discussed in the literature including its action as a   regularizer (Erhan et al. 2009) and as an aid to optimization   (Bengio et al. 2007).  Our results build on the work of   Erhan et al. 2009, showing that unsupervised pre-training appears to   play predominantly a regularization role in subsequent supervised   training. However our results in an online setting, with a virtually unlimited   data stream, point to a somewhat more nuanced interpretation of the roles   of optimization and regularization in the unsupervised pre-training   effect.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/erhan10a/erhan10a.pdf",
        "supp": "",
        "pdf_size": 1453507,
        "gs_citation": 3749,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13018263321881826087&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 42,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "d29adca200",
        "title": "Why are DBNs sparse?",
        "site": "https://proceedings.mlr.press/v9/chatterjee10a.html",
        "author": "Shaunak Chatterjee; Stuart Russell",
        "abstract": "Real stochastic processes operate in continuous time and can be modeled by sets of stochastic differential equations.  On the other hand, several popular model families, including hidden Markov models and dynamic Bayesian networks (DBNs), use discrete time steps.  This paper explores methods for converting DBNs with infinitesimal time steps into DBNs with finite time steps, to enable efficient simulation and filtering over long periods.  An exact conversion\u2014summing out all intervening time slices between two steps\u2014results in a completely connected DBN, yet nearly all human-constructed DBNs are sparse.  We show how this sparsity arises from well-founded approximations resulting from differences among the natural time scales of the variables in the DBN. We define an automated procedure for constructing a provably accurate, approximate DBN model for any desired time step. We illustrate the method by generating a series of approximations to a simple pH model for the human body, demonstrating speedups of several orders of magnitude compared to the original model.",
        "bibtex": "@InProceedings{pmlr-v9-chatterjee10a,\n  title = \t {Why are DBNs sparse?},\n  author = \t {Chatterjee, Shaunak and Russell, Stuart},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {81--88},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/chatterjee10a/chatterjee10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/chatterjee10a.html},\n  abstract = \t {Real stochastic processes operate in continuous time and can be modeled by sets of stochastic differential equations.  On the other hand, several popular model families, including hidden Markov models and dynamic Bayesian networks (DBNs), use discrete time steps.  This paper explores methods for converting DBNs with infinitesimal time steps into DBNs with finite time steps, to enable efficient simulation and filtering over long periods.  An exact conversion\u2014summing out all intervening time slices between two steps\u2014results in a completely connected DBN, yet nearly all human-constructed DBNs are sparse.  We show how this sparsity arises from well-founded approximations resulting from differences among the natural time scales of the variables in the DBN. We define an automated procedure for constructing a provably accurate, approximate DBN model for any desired time step. We illustrate the method by generating a series of approximations to a simple pH model for the human body, demonstrating speedups of several orders of magnitude compared to the original model.}\n}",
        "pdf": "http://proceedings.mlr.press/v9/chatterjee10a/chatterjee10a.pdf",
        "supp": "",
        "pdf_size": 1700662,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2680269957074775079&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Computer Science Division, University of California, Berkeley, CA 94720; Computer Science Division, University of California, Berkeley, CA 94720",
        "aff_domain": "cs.berkeley.edu;cs.berkeley.edu",
        "email": "cs.berkeley.edu;cs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "Computer Science Division",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    }
]