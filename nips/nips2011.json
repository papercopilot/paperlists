[
    {
        "id": "d288687ffa",
        "title": "$\\theta$-MRF: Capturing Spatial and Semantic Structure in the Parameters for Scene Understanding",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/d9fc5b73a8d78fad3d6dffe419384e70-Abstract.html",
        "author": "Congcong Li; Ashutosh Saxena; Tsuhan Chen",
        "abstract": "For most scene understanding tasks (such as object detection or depth estimation), the classifiers need to consider contextual information in addition to the local features. We can capture such contextual information by taking as input the features/attributes from all the regions in the image. However, this contextual dependence also varies with the spatial location of the region of interest, and we therefore need a different set of parameters for each spatial location. This results in a very large number of parameters. In this work, we model the independence properties between the parameters for each location and for each task, by defining a Markov Random Field (MRF) over the parameters. In particular, two sets of parameters are encouraged to have similar values if they are spatially close or semantically close. Our method is, in principle, complementary to other ways of capturing context such as the ones that use a graphical model over the labels instead. In extensive evaluation over two different settings, of multi-class object detection and of multiple scene understanding tasks (scene categorization, depth estimation, geometric labeling), our method beats the state-of-the-art methods in all the four tasks.",
        "bibtex": "@inproceedings{NIPS2011_d9fc5b73,\n author = {Li, Congcong and Saxena, Ashutosh and Chen, Tsuhan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {\\textbackslash theta-MRF: Capturing Spatial and Semantic Structure in the Parameters for Scene Understanding},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/d9fc5b73a8d78fad3d6dffe419384e70-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/d9fc5b73a8d78fad3d6dffe419384e70-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/d9fc5b73a8d78fad3d6dffe419384e70-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1486191,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9807611401208595770&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Cornell University; Cornell University; Cornell University",
        "aff_domain": "cornell.edu;cs.cornell.edu;ece.cornell.edu",
        "email": "cornell.edu;cs.cornell.edu;ece.cornell.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Cornell University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cornell.edu",
        "aff_unique_abbr": "Cornell",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "885e836e32",
        "title": "A Brain-Machine Interface Operating with a Real-Time Spiking Neural Network Control Algorithm",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/25df35de87aa441b88f22a6c2a830a17-Abstract.html",
        "author": "Julie Dethier; Paul Nuyujukian; Chris Eliasmith; Terrence C. Stewart; Shauki A. Elasaad; Krishna V. Shenoy; Kwabena A. Boahen",
        "abstract": "Motor prostheses aim to restore function to disabled patients. Despite compelling proof of concept systems, barriers to clinical translation remain. One challenge is to develop a low-power, fully-implantable system that dissipates only minimal power so as not to damage tissue. To this end, we implemented a Kalman-filter based decoder via a spiking neural network (SNN) and tested it in brain-machine interface (BMI) experiments with a rhesus monkey. The Kalman filter was trained to predict the arm\u2019s velocity and mapped on to the SNN using the Neural Engineer- ing Framework (NEF). A 2,000-neuron embedded Matlab SNN implementation runs in real-time and its closed-loop performance is quite comparable to that of the standard Kalman filter. The success of this closed-loop decoder holds promise for hardware SNN implementations of statistical signal processing algorithms on neuromorphic chips, which may offer power savings necessary to overcome a major obstacle to the successful clinical translation of neural motor prostheses.",
        "bibtex": "@inproceedings{NIPS2011_25df35de,\n author = {Dethier, Julie and Nuyujukian, Paul and Eliasmith, Chris and Stewart, Terrence and Elasaad, Shauki and Shenoy, Krishna V and Boahen, Kwabena A},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Brain-Machine Interface Operating with a Real-Time Spiking Neural Network Control Algorithm},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/25df35de87aa441b88f22a6c2a830a17-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/25df35de87aa441b88f22a6c2a830a17-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/25df35de87aa441b88f22a6c2a830a17-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 574494,
        "gs_citation": 51,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2798538729067125785&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "Department of Bioengineering, Stanford University, CA 94305 + Research Fellow F.R.S.-FNRS, Systmod Unit, University of Liege, Belgium; Department of Bioengineering, School of Medicine, Stanford University, CA 94305; Centre for Theoretical Neuroscience, University of Waterloo, Canada; Centre for Theoretical Neuroscience, University of Waterloo, Canada; Department of Bioengineering, Stanford University, CA 94305; Department of Electrical Engineering, Department of Bioengineering, Department of Neurobiology, Stanford University, CA 94305; Department of Bioengineering, Stanford University, CA 94305",
        "aff_domain": "stanford.edu;npl.stanford.edu;uwaterloo.ca;uwaterloo.ca;stanford.edu;stanford.edu;stanford.edu",
        "email": "stanford.edu;npl.stanford.edu;uwaterloo.ca;uwaterloo.ca;stanford.edu;stanford.edu;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2;3;3;0;4;0",
        "aff_unique_norm": "Department of Bioengineering, Stanford University, CA 94305;Research Fellow F.R.S.-FNRS, Systmod Unit, University of Liege, Belgium;Department of Bioengineering, School of Medicine, Stanford University, CA 94305;University of Waterloo;Department of Electrical Engineering, Department of Bioengineering, Department of Neurobiology, Stanford University, CA 94305",
        "aff_unique_dep": ";;;Centre for Theoretical Neuroscience;",
        "aff_unique_url": ";;;https://uwaterloo.ca;",
        "aff_unique_abbr": ";;;;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": ";1;1",
        "aff_country_unique": ";Canada"
    },
    {
        "id": "04fb09c67b",
        "title": "A Collaborative Mechanism for Crowdsourcing Prediction Problems",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/4edaa105d5f53590338791951e38c3ad-Abstract.html",
        "author": "Jacob D. Abernethy; Rafael M. Frongillo",
        "abstract": "Machine Learning competitions such as the Netflix Prize have proven reasonably successful as a method of \u201ccrowdsourcing\u201d prediction tasks. But these compe- titions have a number of weaknesses, particularly in the incentive structure they create for the participants. We propose a new approach, called a Crowdsourced Learning Mechanism, in which participants collaboratively \u201clearn\u201d a hypothesis for a given prediction task. The approach draws heavily from the concept of a prediction market, where traders bet on the likelihood of a future event. In our framework, the mechanism continues to publish the current hypothesis, and par- ticipants can modify this hypothesis by wagering on an update. The critical in- centive property is that a participant will profit an amount that scales according to how much her update improves performance on a released test set.",
        "bibtex": "@inproceedings{NIPS2011_4edaa105,\n author = {Abernethy, Jacob D and Frongillo, Rafael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Collaborative Mechanism for Crowdsourcing Prediction Problems},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/4edaa105d5f53590338791951e38c3ad-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/4edaa105d5f53590338791951e38c3ad-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/4edaa105d5f53590338791951e38c3ad-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 289103,
        "gs_citation": 82,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11041588608298199808&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Division of Computer Science, University of California at Berkeley; Division of Computer Science, University of California at Berkeley",
        "aff_domain": "cs.berkeley.edu;cs.berkeley.edu",
        "email": "cs.berkeley.edu;cs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Division of Computer Science, University of California at Berkeley",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "5f53ab6ed2",
        "title": "A Convergence Analysis of Log-Linear Training",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/e836d813fd184325132fca8edcdfb40e-Abstract.html",
        "author": "Simon Wiesler; Hermann Ney",
        "abstract": "Log-linear models are widely used probability models for statistical pattern recognition. Typically, log-linear models are trained according to a convex criterion. In recent years, the interest in log-linear models has greatly increased. The optimization of log-linear model parameters is costly and therefore an important topic, in particular for large-scale applications. Different optimization algorithms have been evaluated empirically in many papers. In this work, we analyze the optimization problem analytically and show that the training of log-linear models can be highly ill-conditioned. We verify our findings on two handwriting tasks. By making use of our convergence analysis, we obtain good results on a large-scale continuous handwriting recognition task with a simple and generic approach.",
        "bibtex": "@inproceedings{NIPS2011_e836d813,\n author = {Wiesler, Simon and Ney, Hermann},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Convergence Analysis of Log-Linear Training},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/e836d813fd184325132fca8edcdfb40e-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/e836d813fd184325132fca8edcdfb40e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/e836d813fd184325132fca8edcdfb40e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 250108,
        "gs_citation": 109,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17779848107743178227&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Computer Science Department, RWTH Aachen University; Computer Science Department, RWTH Aachen University",
        "aff_domain": "cs.rwth-aachen.de;cs.rwth-aachen.de",
        "email": "cs.rwth-aachen.de;cs.rwth-aachen.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Computer Science Department, RWTH Aachen University",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "aa8d6d039f",
        "title": "A Denoising View of Matrix Completion",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/38db3aed920cf82ab059bfccbd02be6a-Abstract.html",
        "author": "Weiran Wang; Miguel \u00c1. Carreira-Perpi\u00f1\u00e1n; Zhengdong Lu",
        "abstract": "In matrix completion, we are given a matrix where the values of only some of the entries are present, and we want to reconstruct the missing ones. Much work has focused on the assumption that the data matrix has low rank. We propose a more general assumption based on denoising, so that we expect that the value of a missing entry can be predicted from the values of neighboring points. We propose a nonparametric version of denoising based on local, iterated averaging with mean-shift, possibly constrained to preserve local low-rank manifold structure. The few user parameters required (the denoising scale, number of neighbors and local dimensionality) and the number of iterations can be estimated by cross-validating the reconstruction error. Using our algorithms as a postprocessing step on an initial reconstruction (provided by e.g. a low-rank method), we show consistent improvements with synthetic, image and motion-capture data.",
        "bibtex": "@inproceedings{NIPS2011_38db3aed,\n author = {Wang, Weiran and Carreira-Perpi\\~{n}\\'{a}n, Miguel and Lu, Zhengdong},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Denoising View of Matrix Completion},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/38db3aed920cf82ab059bfccbd02be6a-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/38db3aed920cf82ab059bfccbd02be6a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/38db3aed920cf82ab059bfccbd02be6a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1383088,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16172587889974091073&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "EECS, University of California, Merced; EECS, University of California, Merced; Microsoft Research Asia, Beijing",
        "aff_domain": "ucmerced.edu;ucmerced.edu;microsoft.com",
        "email": "ucmerced.edu;ucmerced.edu;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of California, Merced;Microsoft Research Asia, Beijing",
        "aff_unique_dep": "EECS;",
        "aff_unique_url": "https://www.ucmerced.edu;",
        "aff_unique_abbr": "UC Merced;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Merced;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "2dc151cce1",
        "title": "A Global Structural EM Algorithm for a Model of Cancer Progression",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/9b8619251a19057cff70779273e95aa6-Abstract.html",
        "author": "Ali Tofigh; Erik Sj\u0326lund; Mattias H\u0326glund; Jens Lagergren",
        "abstract": "Cancer has complex patterns of progression that include converging as  well as diverging progressional pathways. Vogelstein's path model of  colon cancer was a pioneering contribution to cancer research.  Since  then, several attempts have been made at obtaining mathematical models  of cancer progression, devising learning algorithms, and applying  these to cross-sectional data. Beerenwinkel {\\em et al.}  provided,  what they coined, EM-like algorithms for Oncogenetic Trees (OTs) and  mixtures of such. Given the small size of current and future data  sets, it is important to minimize the number of parameters of a  model. For this reason, we too focus on tree-based models and  introduce Hidden-variable Oncogenetic Trees (HOTs). In contrast to  OTs, HOTs allow for errors in the data and thereby provide more  realistic modeling. We also design global structural EM algorithms for  learning HOTs and mixtures of HOTs (HOT-mixtures). The algorithms are  global in the sense that, during the M-step, they find a structure  that yields a global maximum of the expected complete log-likelihood  rather than merely one that improves it. The algorithm for single HOTs  performs very well on reasonable-sized data sets, while that for  HOT-mixtures requires data sets of sizes obtainable only with  tomorrow's more cost-efficient technologies.",
        "bibtex": "@inproceedings{NIPS2011_9b861925,\n author = {Tofigh, Ali and Sj\u0326lund, Erik and H\u0326glund, Mattias and Lagergren, Jens},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Global Structural EM Algorithm for a Model of Cancer Progression},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/9b8619251a19057cff70779273e95aa6-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/9b8619251a19057cff70779273e95aa6-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/9b8619251a19057cff70779273e95aa6-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/9b8619251a19057cff70779273e95aa6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 276177,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6439193788679072122&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "School of Computer Science, McGill Centre for Bioinformatics, McGill University, Canada; Stockholm Bioinformatics Center, Stockholm University, Sweden; Department of Oncology, Lund University, Sweden; Science for Life Lab, Swedish e-Science Research Center, Stockholm Bioinformatics Center, School of Computer Science and Communication, KTH Royal Institute of Technology, Sweden",
        "aff_domain": "mcgill.ca;sbc.su.se;med.lu.se;csc.kth.se",
        "email": "mcgill.ca;sbc.su.se;med.lu.se;csc.kth.se",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "School of Computer Science, McGill Centre for Bioinformatics, McGill University, Canada;Stockholm Bioinformatics Center, Stockholm University, Sweden;Department of Oncology, Lund University, Sweden;Science for Life Lab, Swedish e-Science Research Center, Stockholm Bioinformatics Center, School of Computer Science and Communication, KTH Royal Institute of Technology, Sweden",
        "aff_unique_dep": ";;;",
        "aff_unique_url": ";;;",
        "aff_unique_abbr": ";;;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "9c26551387",
        "title": "A Machine Learning Approach to Predict Chemical Reactions",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/b337e84de8752b27eda3a12363109e80-Abstract.html",
        "author": "Matthew A. Kayala; Pierre F. Baldi",
        "abstract": "Being able to predict the course of arbitrary chemical reactions is essential to the theory and applications of organic chemistry. Previous approaches are not high-throughput, are not generalizable or scalable, or lack sufficient data to be effective. We describe single mechanistic reactions as concerted electron movements from an electron orbital source to an electron orbital sink. We use an existing rule-based expert system to derive a dataset consisting of 2,989 productive mechanistic steps and 6.14 million non-productive mechanistic steps. We then pose identifying productive mechanistic steps as a ranking problem: rank potential orbital interactions such that the top ranked interactions yield the major products. The machine learning implementation follows a two-stage approach, in which we first train atom level reactivity filters to prune 94.0% of non-productive reactions with less than a 0.1% false negative rate. Then, we train an ensemble of ranking models on pairs of interacting orbitals to learn a relative productivity function over single mechanistic reactions in a given system. Without the use of explicit transformation patterns, the ensemble perfectly ranks the productive mechanisms at the top 89.1% of the time, rising to 99.9% of the time when top ranked lists with at most four non-productive reactions are considered. The final system allows multi-step reaction prediction. Furthermore, it is generalizable, making reasonable predictions over reactants and conditions which the rule-based expert system does not handle.",
        "bibtex": "@inproceedings{NIPS2011_b337e84d,\n author = {Kayala, Matthew and Baldi, Pierre},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Machine Learning Approach to Predict Chemical Reactions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/b337e84de8752b27eda3a12363109e80-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/b337e84de8752b27eda3a12363109e80-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/b337e84de8752b27eda3a12363109e80-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 264497,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11728267679946052437&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Institute of Genomics and Bioinformatics, School of Information and Computer Sciences, University of California, Irvine; Institute of Genomics and Bioinformatics, School of Information and Computer Sciences, University of California, Irvine",
        "aff_domain": "ics.uci.edu;ics.uci.edu",
        "email": "ics.uci.edu;ics.uci.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Institute of Genomics and Bioinformatics, School of Information and Computer Sciences, University of California, Irvine",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "ee072cb735",
        "title": "A Model for Temporal Dependencies in Event Streams",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/c9f95a0a5af052bffce5c89917335f67-Abstract.html",
        "author": "Asela Gunawardana; Christopher Meek; Puyang Xu",
        "abstract": "We introduce the Piecewise-Constant Conditional Intensity Model, a model for learning temporal dependencies in event streams.  We describe a closed-form Bayesian approach to learning these models, and describe an importance sampling algorithm for forecasting future events using these models, using a proposal distribution based on Poisson superposition.  We then use synthetic data, supercomputer event logs, and web search query logs to illustrate that our learning algorithm can efficiently learn nonlinear temporal dependencies, and that our importance sampling algorithm can effectively forecast future events.",
        "bibtex": "@inproceedings{NIPS2011_c9f95a0a,\n author = {Gunawardana, Asela and Meek, Christopher and Xu, Puyang},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Model for Temporal Dependencies in Event Streams},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/c9f95a0a5af052bffce5c89917335f67-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/c9f95a0a5af052bffce5c89917335f67-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/c9f95a0a5af052bffce5c89917335f67-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 687258,
        "gs_citation": 130,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6940847061662449832&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Microsoft Research; Microsoft Research; ECE Dept. & CLSP, Johns Hopkins University",
        "aff_domain": "microsoft.com;microsoft.com;jhu.edu",
        "email": "microsoft.com;microsoft.com;jhu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Microsoft Corporation;ECE Dept. & CLSP, Johns Hopkins University",
        "aff_unique_dep": "Microsoft Research;",
        "aff_unique_url": "https://www.microsoft.com/en-us/research;",
        "aff_unique_abbr": "MSR;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "7dd7253478",
        "title": "A More Powerful Two-Sample Test in High Dimensions using Random Projection",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/5487315b1286f907165907aa8fc96619-Abstract.html",
        "author": "Miles Lopes; Laurent Jacob; Martin J. Wainwright",
        "abstract": "We consider the hypothesis testing problem of detecting a shift between the means of two multivariate normal distributions in the high-dimensional setting, allowing for the data dimension p to exceed the sample size n. Our contribution is a new test statistic for the two-sample test of means that integrates a random projection with the classical Hotelling T squared statistic. Working within a high- dimensional framework that allows (p,n) to tend to infinity, we first derive an asymptotic power function for our test, and then provide sufficient conditions for it to achieve greater power than other  state-of-the-art tests. Using ROC curves generated from simulated data, we demonstrate superior performance against competing tests in the parameter regimes anticipated by our theoretical results. Lastly, we illustrate an advantage of our procedure with comparisons on a high-dimensional gene expression dataset involving the discrimination of different types of cancer.",
        "bibtex": "@inproceedings{NIPS2011_5487315b,\n author = {Lopes, Miles and Jacob, Laurent and Wainwright, Martin J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A More Powerful Two-Sample Test in High Dimensions using Random Projection},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/5487315b1286f907165907aa8fc96619-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/5487315b1286f907165907aa8fc96619-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/5487315b1286f907165907aa8fc96619-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 409528,
        "gs_citation": 187,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4465665090565953337&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Departments of Statistics; Departments of Statistics; Departments of Statistics + EECS",
        "aff_domain": "stat.berkeley.edu;stat.berkeley.edu;stat.berkeley.edu",
        "email": "stat.berkeley.edu;stat.berkeley.edu;stat.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0+1",
        "aff_unique_norm": "University Affiliation Not Specified;EECS",
        "aff_unique_dep": "Departments of Statistics;Electrical Engineering and Computer Science",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "3f64e35158",
        "title": "A Non-Parametric Approach to Dynamic Programming",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/4311359ed4969e8401880e3c1836fbe1-Abstract.html",
        "author": "Oliver B. Kroemer; Jan R. Peters",
        "abstract": "In this paper, we consider the problem of policy evaluation for continuous-state systems. We present a non-parametric approach to policy evaluation, which uses kernel density estimation to represent the system. The true form of the value function for this model can be determined, and can be computed using Galerkin's method. Furthermore, we also present a unified view of several well-known policy evaluation methods. In particular, we show that the same Galerkin method can be used to derive Least-Squares Temporal Difference learning, Kernelized Temporal Difference learning, and a discrete-state Dynamic Programming solution, as well as our proposed method. In a numerical evaluation of these algorithms, the proposed approach performed better than the other methods.",
        "bibtex": "@inproceedings{NIPS2011_4311359e,\n author = {Kroemer, Oliver and Peters, Jan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Non-Parametric Approach to Dynamic Programming},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/4311359ed4969e8401880e3c1836fbe1-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/4311359ed4969e8401880e3c1836fbe1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/4311359ed4969e8401880e3c1836fbe1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 428019,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17700079924824112388&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Intelligent Autonomous Systems, Technische Universit\u00e4t Darmstadt + Robot Learning Lab, Max Planck Institute for Intelligent Systems; Intelligent Autonomous Systems, Technische Universit\u00e4t Darmstadt + Robot Learning Lab, Max Planck Institute for Intelligent Systems",
        "aff_domain": "ias.tu-darmstadt.de;ias.tu-darmstadt.de",
        "email": "ias.tu-darmstadt.de;ias.tu-darmstadt.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "Technische Universit\u00e4t Darmstadt;Robot Learning Lab, Max Planck Institute for Intelligent Systems",
        "aff_unique_dep": "Intelligent Autonomous Systems;",
        "aff_unique_url": "https://www.tu-darmstadt.de;",
        "aff_unique_abbr": "TUD;",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany;"
    },
    {
        "id": "cbd276e13c",
        "title": "A Reinforcement Learning Theory for Homeostatic Regulation",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/9778d5d219c5080b9a6a17bef029331c-Abstract.html",
        "author": "Mehdi Keramati; Boris S. Gutkin",
        "abstract": "Reinforcement learning models address animal's behavioral adaptation to its changing \"external\" environment, and are based on the assumption that Pavlovian, habitual and goal-directed responses seek to maximize reward acquisition. Negative-feedback models of homeostatic regulation, on the other hand, are concerned with behavioral adaptation in response to the \"internal\" state of the animal, and assume that animals' behavioral objective is to minimize deviations of some key physiological variables from their hypothetical setpoints. Building upon the drive-reduction theory of reward, we propose a new analytical framework that integrates learning and regulatory systems, such that the two seemingly unrelated objectives of reward maximization and physiological-stability prove to be identical. The proposed theory shows behavioral adaptation to both internal and external states in a disciplined way. We further show that the proposed framework allows for a unified explanation of some behavioral phenomenon like motivational sensitivity of different associative learning mechanism, anticipatory responses, interaction among competing motivational systems, and risk aversion.",
        "bibtex": "@inproceedings{NIPS2011_9778d5d2,\n author = {Keramati, Mehdi and Gutkin, Boris},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Reinforcement Learning Theory for Homeostatic Regulation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/9778d5d219c5080b9a6a17bef029331c-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/9778d5d219c5080b9a6a17bef029331c-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/9778d5d219c5080b9a6a17bef029331c-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/9778d5d219c5080b9a6a17bef029331c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 160048,
        "gs_citation": 70,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4702877796250619872&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Group for Neural Theory, LNC, ENS, Paris, France; Group for Neural Theory, LNC, ENS, Paris, France",
        "aff_domain": "ens.fr;ens.fr",
        "email": "ens.fr;ens.fr",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Group for Neural Theory, LNC, ENS, Paris, France",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "ffbabed639",
        "title": "A Two-Stage Weighting Framework for Multi-Source Domain Adaptation",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/d709f38ef758b5066ef31b18039b8ce5-Abstract.html",
        "author": "Qian Sun; Rita Chattopadhyay; Sethuraman Panchanathan; Jieping Ye",
        "abstract": "Discriminative learning when training and test data belong to different distributions is a challenging and complex task. Often times we have very few or no labeled data from the test or target distribution but may have plenty of labeled data from multiple related sources with different distributions. The difference in distributions may be in both marginal and conditional probabilities. Most of the existing domain adaptation work focuses on the marginal  probability distribution difference between the domains, assuming that the conditional probabilities are similar. However in many real world applications, conditional probability distribution differences are as commonplace as marginal probability differences. In this paper we propose a two-stage domain adaptation methodology which combines weighted data from multiple sources based on marginal probability differences (first stage) as well as conditional probability differences (second stage), with the target domain data. The weights for minimizing the marginal probability differences are estimated independently, while the weights for minimizing conditional  probability differences are computed simultaneously by exploiting the potential interaction among multiple sources. We also provide a theoretical analysis on the generalization performance of the proposed multi-source domain adaptation formulation using the weighted Rademacher complexity measure. Empirical comparisons with existing state-of-the-art domain adaptation methods using three real-world datasets demonstrate the effectiveness of the proposed approach.",
        "bibtex": "@inproceedings{NIPS2011_d709f38e,\n author = {Sun, Qian and Chattopadhyay, Rita and Panchanathan, Sethuraman and Ye, Jieping},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Two-Stage Weighting Framework for Multi-Source Domain Adaptation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/d709f38ef758b5066ef31b18039b8ce5-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/d709f38ef758b5066ef31b18039b8ce5-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/d709f38ef758b5066ef31b18039b8ce5-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/d709f38ef758b5066ef31b18039b8ce5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 270272,
        "gs_citation": 245,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7400495735702830798&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Computer Science and Engineering, Arizona State University, AZ 85287; Computer Science and Engineering, Arizona State University, AZ 85287; Computer Science and Engineering, Arizona State University, AZ 85287; Computer Science and Engineering, Arizona State University, AZ 85287",
        "aff_domain": "asu.edu;asu.edu;asu.edu;asu.edu",
        "email": "asu.edu;asu.edu;asu.edu;asu.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Computer Science and Engineering, Arizona State University, AZ 85287",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "f1379ec897",
        "title": "A blind sparse deconvolution method for neural spike identification",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/e0ec453e28e061cc58ac43f91dc2f3f0-Abstract.html",
        "author": "Chaitanya Ekanadham; Daniel Tranchina; Eero P. Simoncelli",
        "abstract": "We consider the problem of estimating neural spikes from extracellular voltage recordings. Most current methods are based on clustering, which requires substantial human supervision and produces systematic errors by failing to properly handle temporally overlapping spikes. We formulate the problem as one of statistical inference, in which the recorded voltage is a noisy sum of the spike trains of each neuron convolved with its associated spike waveform.  Joint maximum-a-posteriori (MAP) estimation of the waveforms and spikes is then a blind deconvolution problem in which the coefficients are sparse. We develop a block-coordinate descent method for approximating the MAP solution.  We validate our method on data simulated according to the generative model, as well as on real data for which ground truth is available via simultaneous intracellular recordings. In both cases, our method substantially reduces the number of missed spikes and false positives when compared to a standard clustering algorithm, primarily by recovering temporally overlapping spikes.  The method offers a fully automated alternative to clustering methods that is less susceptible to systematic errors.",
        "bibtex": "@inproceedings{NIPS2011_e0ec453e,\n author = {Ekanadham, Chaitanya and Tranchina, Daniel and Simoncelli, Eero},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A blind sparse deconvolution method for neural spike identification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/e0ec453e28e061cc58ac43f91dc2f3f0-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/e0ec453e28e061cc58ac43f91dc2f3f0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/e0ec453e28e061cc58ac43f91dc2f3f0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 3007640,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15532537504095076013&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Courant Institute, New York University, New York, NY 10012; Courant Institute, New York University, New York, NY 10012; Courant Institute, Center for Neural Science, Howard Hughes Medical Institute, New York University, New York, NY 10012",
        "aff_domain": "math.nyu.edu; ; ",
        "email": "math.nyu.edu; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Courant Institute, New York University, New York, NY 10012;Courant Institute, Center for Neural Science, Howard Hughes Medical Institute, New York University, New York, NY 10012",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "85757b3027",
        "title": "A concave regularization technique for sparse mixture models",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/31857b449c407203749ae32dd0e7d64a-Abstract.html",
        "author": "Martin O. Larsson; Johan Ugander",
        "abstract": "Latent variable mixture models are a powerful tool for exploring the structure in large datasets. A common challenge for interpreting such models is a desire to impose sparsity, the natural assumption that each data point only contains few latent features. Since mixture distributions are constrained in their L1 norm, typical sparsity techniques based on L1 regularization become toothless, and concave regularization becomes necessary. Unfortunately concave regularization typically results in EM algorithms that must perform problematic non-concave M-step maximizations. In this work, we introduce a technique for circumventing this difficulty, using the so-called Mountain Pass Theorem to provide easily verifiable conditions under which the M-step is well-behaved despite the lacking concavity. We also develop a correspondence between logarithmic regularization and what we term the pseudo-Dirichlet distribution, a generalization of the ordinary Dirichlet distribution well-suited for inducing sparsity. We demonstrate our approach on a text corpus, inferring a sparse topic mixture model for 2,406 weblogs.",
        "bibtex": "@inproceedings{NIPS2011_31857b44,\n author = {Larsson, Martin and Ugander, Johan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A concave regularization technique for sparse mixture models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/31857b449c407203749ae32dd0e7d64a-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/31857b449c407203749ae32dd0e7d64a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/31857b449c407203749ae32dd0e7d64a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2627701,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9594261388616336343&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "School of Operations Research and Information Engineering, Cornell University; Center for Applied Mathematics, Cornell University",
        "aff_domain": "cornell.edu;cornell.edu",
        "email": "cornell.edu;cornell.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Cornell University;Center for Applied Mathematics, Cornell University",
        "aff_unique_dep": "School of Operations Research and Information Engineering;",
        "aff_unique_url": "https://www.cornell.edu;",
        "aff_unique_abbr": "Cornell;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "06a7a0e7f5",
        "title": "A rational model of causal inference with continuous causes",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/f91e24dfe80012e2a7984afa4480a6d6-Abstract.html",
        "author": "Thomas L. Griffiths; Michael James",
        "abstract": "Rational models of causal induction have been successful in accounting for people's judgments about the existence of causal relationships. However, these models have focused on explaining inferences from discrete data of the kind that can be summarized in a 2 \u2715 2 contingency table. This severely limits the scope of these models, since the world often provides non-binary data. We develop a new rational model of causal induction using continuous dimensions, which aims to diminish the gap between empirical and theoretical approaches and real-world causal induction. This model successfully predicts human judgments from previous studies better than models of discrete causal inference, and outperforms several other plausible models of causal induction with continuous causes in accounting for people's inferences in a new experiment.",
        "bibtex": "@inproceedings{NIPS2011_f91e24df,\n author = {Griffiths, Thomas and James, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A rational model of causal inference with continuous causes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/f91e24dfe80012e2a7984afa4480a6d6-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/f91e24dfe80012e2a7984afa4480a6d6-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/f91e24dfe80012e2a7984afa4480a6d6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 418390,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17589547573711559257&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Department of Psychology, University of California, Berkeley; Department of Psychology, University of California, Berkeley",
        "aff_domain": "berkeley.edu;berkeley.edu",
        "email": "berkeley.edu;berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Department of Psychology, University of California, Berkeley",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "8fdf2bd24e",
        "title": "A reinterpretation of the policy oscillation phenomenon in approximate policy iteration",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/d9731321ef4e063ebbee79298fa36f56-Abstract.html",
        "author": "Paul Wagner",
        "abstract": "A majority of approximate dynamic programming approaches to the reinforcement learning problem can be categorized into greedy value function methods and value-based policy gradient methods. The former approach, although fast, is well known to be susceptible to the policy oscillation phenomenon. We take a fresh view to this phenomenon by casting a considerable subset of the former approach as a limiting special case of the latter. We explain the phenomenon in terms of this view and illustrate the underlying mechanism with artificial examples. We also use it to derive the constrained natural actor-critic algorithm that can interpolate between the aforementioned approaches. In addition, it has been suggested in the literature that the oscillation phenomenon might be subtly connected to the grossly suboptimal performance in the Tetris benchmark problem of all attempted approximate dynamic programming methods. We report empirical evidence against such a connection and in favor of an alternative explanation. Finally, we report scores in the Tetris problem that improve on existing dynamic programming based results.",
        "bibtex": "@inproceedings{NIPS2011_d9731321,\n author = {Wagner, Paul},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A reinterpretation of the policy oscillation phenomenon in approximate policy iteration},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/d9731321ef4e063ebbee79298fa36f56-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/d9731321ef4e063ebbee79298fa36f56-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/d9731321ef4e063ebbee79298fa36f56-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 293653,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17464082621005088194&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Information and Computer Science, Aalto University School of Science",
        "aff_domain": "cis.hut.fi",
        "email": "cis.hut.fi",
        "github": "",
        "project": "http://users.ics.tkk.fi/pwagner/",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Department of Information and Computer Science, Aalto University School of Science",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "977c78d375",
        "title": "Accelerated Adaptive Markov Chain for Partition Function Computation",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/7fb8ceb3bd59c7956b1df66729296a4c-Abstract.html",
        "author": "Stefano Ermon; Carla P. Gomes; Ashish Sabharwal; Bart Selman",
        "abstract": "We propose a novel Adaptive Markov Chain Monte Carlo algorithm to  compute the partition function. In particular, we show how to  accelerate a flat histogram sampling technique by significantly  reducing the number of ``null moves'' in the chain, while maintaining  asymptotic convergence properties. Our experiments show that our  method converges quickly to highly accurate solutions on a range of  benchmark instances, outperforming other state-of-the-art methods such  as IJGP, TRW, and Gibbs sampling both in run-time and accuracy. We  also show how obtaining a so-called density of states distribution  allows for efficient weight learning in Markov Logic theories.",
        "bibtex": "@inproceedings{NIPS2011_7fb8ceb3,\n author = {Ermon, Stefano and Gomes, Carla P and Sabharwal, Ashish and Selman, Bart},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Accelerated Adaptive Markov Chain for Partition Function Computation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/7fb8ceb3bd59c7956b1df66729296a4c-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/7fb8ceb3bd59c7956b1df66729296a4c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/7fb8ceb3bd59c7956b1df66729296a4c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 826395,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1764308696967477179&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "c71df191e0",
        "title": "Action-Gap Phenomenon in Reinforcement Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/013d407166ec4fa56eb1e1f8cbe183b9-Abstract.html",
        "author": "Amir-massoud Farahmand",
        "abstract": "Many practitioners of reinforcement learning problems have observed that oftentimes  the performance of the agent reaches very close to the optimal performance even though the estimated (action-)value function is still far from the optimal one.  The goal of this paper is to explain and formalize this phenomenon by introducing the concept of the action-gap regularity.  As a typical result, we prove that for an agent following the greedy policy (\\hat{\\pi}) with respect to an action-value function (\\hat{Q}), the performance loss (E[V^",
        "bibtex": "@inproceedings{NIPS2011_013d4071,\n author = {Farahmand, Amir-massoud},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Action-Gap Phenomenon in Reinforcement Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/013d407166ec4fa56eb1e1f8cbe183b9-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/013d407166ec4fa56eb1e1f8cbe183b9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/013d407166ec4fa56eb1e1f8cbe183b9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 365388,
        "gs_citation": 69,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12614657151778615424&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "School of Computer Science, McGill University",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "www.SoloGen.net",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "McGill University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.mcgill.ca",
        "aff_unique_abbr": "McGill",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Montreal",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "e66727dcfa",
        "title": "Active Classification based on Value of Classifier",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/303ed4c69846ab36c2904d3ba8573050-Abstract.html",
        "author": "Tianshi Gao; Daphne Koller",
        "abstract": "Modern classification tasks usually involve many class labels and can be informed by a broad range of features. Many of these tasks are tackled by constructing a set of classifiers, which are then applied at test time and then pieced together in a fixed procedure determined in advance or at training time. We present an active classification process at the test time, where each classifier in a large ensemble is viewed as a potential observation that might inform our classification process. Observations are then selected dynamically based on previous observations, using a value-theoretic computation that balances an estimate of the expected classification gain from each observation as well as its computational cost. The expected classification gain is computed using a probabilistic model that uses the outcome from previous observations. This active classification process is applied at test time for each individual test instance, resulting in an efficient instance-specific decision path. We demonstrate the benefit of the active scheme on various real-world datasets, and show that it can achieve comparable or even higher classification accuracy at a fraction of the computational costs of traditional methods.",
        "bibtex": "@inproceedings{NIPS2011_303ed4c6,\n author = {Gao, Tianshi and Koller, Daphne},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Active Classification based on Value of Classifier},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/303ed4c69846ab36c2904d3ba8573050-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/303ed4c69846ab36c2904d3ba8573050-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/303ed4c69846ab36c2904d3ba8573050-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 141838,
        "gs_citation": 139,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14039400614118318144&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Electrical Engineering, Stanford University; Department of Computer Science, Stanford University",
        "aff_domain": "stanford.edu;cs.stanford.edu",
        "email": "stanford.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Department of Electrical Engineering",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b306f4e306",
        "title": "Active Learning Ranking from Pairwise Preferences with Almost Optimal Query Complexity",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/81448138f5f163ccdba4acc69819f280-Abstract.html",
        "author": "Nir Ailon",
        "abstract": "Given a set $V$ of  $n$ elements we wish to linearly order them using pairwise preference labels  which may be non-transitive (due to irrationality or arbitrary noise).  The goal is to linearly order the elements while disagreeing with  as few pairwise preference labels as possible.  Our performance is measured by two parameters:  The number of disagreements (loss) and the query complexity (number of pairwise preference labels).  Our algorithm adaptively queries  at most $O(n\\poly(\\log n,\\eps^{-1}))$ preference labels for a regret of  $\\eps$ times the optimal loss.  This is strictly better, and often significantly better than what  non-adaptive sampling could achieve.  Our main result helps settle an open problem posed by   learning-to-rank (from pairwise information) theoreticians and practitioners:  What is a provably correct way to sample preference labels?",
        "bibtex": "@inproceedings{NIPS2011_81448138,\n author = {Ailon, Nir},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Active Learning Ranking from Pairwise Preferences with Almost Optimal Query Complexity},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/81448138f5f163ccdba4acc69819f280-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/81448138f5f163ccdba4acc69819f280-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/81448138f5f163ccdba4acc69819f280-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 326279,
        "gs_citation": 60,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5261978668023322992&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Technion, Haifa, Israel",
        "aff_domain": "cs.technion.ac.il",
        "email": "cs.technion.ac.il",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Technion - Israel Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.technion.ac.il/en/",
        "aff_unique_abbr": "Technion",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Haifa",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "3d412a16cf",
        "title": "Active Learning with a Drifting Distribution",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/1e1d184167ca7676cf665225e236a3d2-Abstract.html",
        "author": "Liu Yang",
        "abstract": "We study the problem of active learning in a stream-based setting, allowing the distribution of the examples to change over time.  We prove upper bounds on the number of prediction mistakes and number of label requests for established disagreement-based active learning algorithms, both in the realizable case and under Tsybakov noise.  We further prove minimax lower bounds for this problem.",
        "bibtex": "@inproceedings{NIPS2011_1e1d1841,\n author = {Yang, Liu},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Active Learning with a Drifting Distribution},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/1e1d184167ca7676cf665225e236a3d2-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/1e1d184167ca7676cf665225e236a3d2-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/1e1d184167ca7676cf665225e236a3d2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 138228,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12810754578656411253&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Machine Learning Department, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu",
        "email": "cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Machine Learning Department",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e269ee72a3",
        "title": "Active Ranking using Pairwise Comparisons",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/6c14da109e294d1e8155be8aa4b1ce8e-Abstract.html",
        "author": "Kevin G. Jamieson; Robert Nowak",
        "abstract": "This paper examines the problem of ranking a collection of objects using pairwise comparisons (rankings of two objects).  In general, the ranking of $n$ objects can be identified by standard sorting methods using $n\\log_2 n$ pairwise comparisons. We are interested in natural situations in which relationships among the objects may allow for ranking using far fewer pairwise comparisons. {Specifically, we assume that the objects can be embedded into a $d$-dimensional Euclidean space and that the rankings reflect their relative distances from a common reference point in $\\R^d$. We show that under this assumption the number of possible rankings grows like $n^{2d}$ and demonstrate an algorithm that can identify a randomly selected ranking using just slightly more than $d\\log n$ adaptively selected pairwise comparisons, on average.}  If instead the comparisons are chosen at random, then almost all pairwise comparisons must be made in order to identify any ranking. In addition, we propose a robust, error-tolerant algorithm that only requires that the pairwise comparisons are probably correct. Experimental studies with synthetic and real datasets support the conclusions of our theoretical analysis.",
        "bibtex": "@inproceedings{NIPS2011_6c14da10,\n author = {Jamieson, Kevin G and Nowak, Robert},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Active Ranking using Pairwise Comparisons},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/6c14da109e294d1e8155be8aa4b1ce8e-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/6c14da109e294d1e8155be8aa4b1ce8e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/6c14da109e294d1e8155be8aa4b1ce8e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1271281,
        "gs_citation": 308,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7339479019602141797&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "University of Wisconsin; University of Wisconsin",
        "aff_domain": "wisc.edu;engr.wisc.edu",
        "email": "wisc.edu;engr.wisc.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Wisconsin",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.wisc.edu",
        "aff_unique_abbr": "UW",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c46dfb1ce5",
        "title": "Active dendrites: adaptation to spike-based communication",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/c06d06da9666a219db15cf575aff2824-Abstract.html",
        "author": "Balazs B. Ujfalussy; M\u00e1t\u00e9 Lengyel",
        "abstract": "Computational analyses of dendritic computations often assume stationary inputs to neurons, ignoring the pulsatile nature of spike-based communication between neurons and the moment-to-moment fluctuations caused by such spiking inputs. Conversely, circuit computations with spiking neurons are usually formalized without regard to the rich nonlinear nature of dendritic processing. Here we address the computational challenge faced by neurons that compute and represent analogue quantities but communicate with digital spikes, and show that reliable computation of even purely linear functions of inputs can require the interplay of strongly nonlinear subunits within the postsynaptic dendritic tree. Our theory predicts a matching of dendritic nonlinearities and synaptic weight distributions to the joint statistics of presynaptic inputs. This approach suggests normative roles for some puzzling forms of nonlinear dendritic dynamics and plasticity.",
        "bibtex": "@inproceedings{NIPS2011_c06d06da,\n author = {Ujfalussy, Balazs and Lengyel, M\\'{a}t\\'{e}},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Active dendrites: adaptation to spike-based communication},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/c06d06da9666a219db15cf575aff2824-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/c06d06da9666a219db15cf575aff2824-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/c06d06da9666a219db15cf575aff2824-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/c06d06da9666a219db15cf575aff2824-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1035184,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5203396912655172226&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Computational & Biological Learning Lab, Dept. of Engineering, University of Cambridge, UK + Computational Neuroscience Group, Dept. of Biophysics, MTA KFKI RMKI, Budapest, Hungary; Computational & Biological Learning Lab, Dept. of Engineering, University of Cambridge, UK",
        "aff_domain": "rmki.kfki.hu;eng.cam.ac.uk",
        "email": "rmki.kfki.hu;eng.cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0",
        "aff_unique_norm": "Computational & Biological Learning Lab, Dept. of Engineering, University of Cambridge, UK;Computational Neuroscience Group, Dept. of Biophysics, MTA KFKI RMKI, Budapest, Hungary",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "f27a961c2e",
        "title": "Active learning of neural response functions with Gaussian processes",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/0c048b3a434e49e655c1247efb389cec-Abstract.html",
        "author": "Mijung Park; Greg Horwitz; Jonathan W. Pillow",
        "abstract": "A sizable literature has focused on the problem of estimating a low-dimensional feature space capturing a neuron's stimulus sensitivity.  However, comparatively little work has addressed the problem of estimating the nonlinear function from feature space to a neuron's output spike rate. Here, we use a Gaussian process (GP) prior over the infinite-dimensional space of nonlinear functions to obtain Bayesian estimates of the \"nonlinearity\" in the linear-nonlinear-Poisson (LNP) encoding model. This offers flexibility, robustness, and computational tractability compared to traditional methods (e.g., parametric forms, histograms, cubic splines).  Most importantly, we develop a framework for optimal experimental design based on uncertainty sampling.  This involves adaptively selecting stimuli to characterize the nonlinearity with as little experimental data as possible, and relies on a method for rapidly updating hyperparameters using the Laplace approximation.  We apply these methods to data from color-tuned neurons in macaque V1.  We estimate nonlinearities in the 3D space of cone contrasts, which reveal that V1 combines cone inputs in a highly nonlinear manner. With simulated experiments, we show that optimal design substantially reduces the amount of data required to estimate this nonlinear combination rule.",
        "bibtex": "@inproceedings{NIPS2011_0c048b3a,\n author = {Park, Mijung and Horwitz, Greg and Pillow, Jonathan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Active learning of neural response functions with Gaussian processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/0c048b3a434e49e655c1247efb389cec-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/0c048b3a434e49e655c1247efb389cec-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/0c048b3a434e49e655c1247efb389cec-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/0c048b3a434e49e655c1247efb389cec-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1145953,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=307327696900955741&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Electrical and Computer Engineering, The University of Texas at Austin; Departments of Physiology and Biophysics, The University of Washington; Departments of Psychology and Neurobiology, The University of Texas at Austin",
        "aff_domain": "mail.utexas.edu;uw.edu;mail.utexas.edu",
        "email": "mail.utexas.edu;uw.edu;mail.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "The University of Texas at Austin;Departments of Physiology and Biophysics, The University of Washington;Departments of Psychology and Neurobiology, The University of Texas at Austin",
        "aff_unique_dep": "Electrical and Computer Engineering;;",
        "aff_unique_url": "https://www.utexas.edu;;",
        "aff_unique_abbr": "UT Austin;;",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Austin;",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "5b3880151f",
        "title": "Adaptive Hedge",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/64223ccf70bbb65a3a4aceac37e21016-Abstract.html",
        "author": "Tim V. Erven; Wouter M. Koolen; Steven D. Rooij; Peter Gr\u00fcnwald",
        "abstract": "Most methods for decision-theoretic online learning are based on the Hedge algorithm, which takes a parameter called the learning rate. In most previous analyses the learning rate was carefully tuned to obtain optimal worst-case performance, leading to suboptimal performance on easy instances, for example when there exists an action that is significantly better than all others. We propose a new way of setting the learning rate, which adapts to the difficulty of the learning problem: in the worst case our procedure still guarantees optimal performance, but on easy instances it achieves much smaller regret. In particular, our adaptive method achieves constant regret in a probabilistic setting, when there exists an action that on average obtains strictly smaller loss than all other actions. We also provide a simulation study comparing our approach to existing methods.",
        "bibtex": "@inproceedings{NIPS2011_64223ccf,\n author = {Erven, Tim and Koolen, Wouter M and Rooij, Steven and Gr\\\"{u}nwald, Peter},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Adaptive Hedge},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/64223ccf70bbb65a3a4aceac37e21016-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/64223ccf70bbb65a3a4aceac37e21016-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/64223ccf70bbb65a3a4aceac37e21016-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 516975,
        "gs_citation": 66,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14872717333190289412&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "VU University; Centrum Wiskunde & Informatica (CWI); CWI+Royal Holloway, University of London; Centrum Wiskunde & Informatica (CWI)",
        "aff_domain": "timvanerven.nl;cwi.nl;cs.rhul.ac.uk;cwi.nl",
        "email": "timvanerven.nl;cwi.nl;cs.rhul.ac.uk;cwi.nl",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1+2;1",
        "aff_unique_norm": "VU University;Centrum Wiskunde & Informatica;University of London",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";https://www.cwi.nl/;https://www.royalholloway.ac.uk",
        "aff_unique_abbr": ";CWI;RHUL",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Royal Holloway",
        "aff_country_unique_index": "1;1+2;1",
        "aff_country_unique": ";Netherlands;United Kingdom"
    },
    {
        "id": "f0a4d6017f",
        "title": "Additive Gaussian Processes",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/4c5bde74a8f110656874902f07378009-Abstract.html",
        "author": "David K. Duvenaud; Hannes Nickisch; Carl E. Rasmussen",
        "abstract": "We introduce a Gaussian process model of functions which are additive.  An additive function is one which decomposes into a sum of low-dimensional functions, each depending on only a subset of the input variables. Additive GPs generalize both Generalized Additive Models, and the standard GP models which use squared-exponential kernels.  Hyperparameter learning in this model can be seen as Bayesian Hierarchical Kernel Learning (HKL).  We introduce an expressive but tractable parameterization of the kernel function, which allows efficient evaluation of all input interaction terms, whose number is exponential in the input dimension.  The additional structure discoverable by this model results in increased interpretability, as well as state-of-the-art predictive power in regression tasks.",
        "bibtex": "@inproceedings{NIPS2011_4c5bde74,\n author = {Duvenaud, David K and Nickisch, Hannes and Rasmussen, Carl},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Additive Gaussian Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/4c5bde74a8f110656874902f07378009-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/4c5bde74a8f110656874902f07378009-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/4c5bde74a8f110656874902f07378009-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1513154,
        "gs_citation": 477,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5037411748325088580&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "Department of Engineering, Cambridge University; MPI for Intelligent Systems, T\u00a8ubingen, Germany; Department of Engineering, Cambridge University",
        "aff_domain": "cam.ac.uk;tue.mpg.de;cam.ac.uk",
        "email": "cam.ac.uk;tue.mpg.de;cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Department of Engineering, Cambridge University;Max Planck Institute for Intelligent Systems",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://www.mpi-is.mpg.de",
        "aff_unique_abbr": ";MPI-IS",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";T\u00fcbingen",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";Germany"
    },
    {
        "id": "7aed64a1e0",
        "title": "Advice Refinement in Knowledge-Based SVMs",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/d79aac075930c83c2f1e369a511148fe-Abstract.html",
        "author": "Gautam Kunapuli; Richard Maclin; Jude W. Shavlik",
        "abstract": "Knowledge-based support vector machines (KBSVMs) incorporate advice from domain experts, which can improve generalization significantly. A major limitation that has not been fully addressed occurs when the expert advice is imperfect, which can lead to poorer models. We propose a model that extends KBSVMs and is able to not only learn from data and advice, but also simultaneously improve the advice. The proposed approach is particularly effective for knowledge discovery in domains with few labeled examples. The proposed model contains bilinear constraints, and is solved using two iterative approaches: successive linear programming and a constrained concave-convex approach. Experimental results demonstrate that these algorithms yield useful refinements to expert advice, as well as improve the performance of the learning algorithm overall.",
        "bibtex": "@inproceedings{NIPS2011_d79aac07,\n author = {Kunapuli, Gautam and Maclin, Richard and Shavlik, Jude},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Advice Refinement in Knowledge-Based SVMs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/d79aac075930c83c2f1e369a511148fe-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/d79aac075930c83c2f1e369a511148fe-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/d79aac075930c83c2f1e369a511148fe-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 410230,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16605538799411890517&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff": "Univ. of Wisconsin-Madison; Univ. of Minnesota, Duluth; Univ. of Wisconsin-Madison",
        "aff_domain": "wisc.edu;d.umn.edu;cs.wisc.edu",
        "email": "wisc.edu;d.umn.edu;cs.wisc.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Wisconsin-Madison;Univ. of Minnesota, Duluth",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.wisc.edu;",
        "aff_unique_abbr": "UW-Madison;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Madison;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "4134b7c37b",
        "title": "Agnostic Selective Classification",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/4b6538a44a1dfdc2b83477cd76dee98e-Abstract.html",
        "author": "Yair Wiener; Ran El-Yaniv",
        "abstract": "For a learning problem whose associated excess loss class is $(\\beta,B)$-Bernstein, we show that it is theoretically possible to track the same classification performance  of the best (unknown) hypothesis in our class, provided that we are free to abstain from prediction in some region of our choice. The (probabilistic) volume of this rejected region of the domain is shown to be diminishing at rate $O(B\\theta (\\sqrt{1/m}))^\\beta)$, where $\\theta$ is Hanneke's disagreement coefficient. The strategy achieving this performance has computational barriers because it requires empirical error minimization in an agnostic setting. Nevertheless, we heuristically approximate this strategy and develop a novel selective classification algorithm  using constrained SVMs. We show empirically that the resulting algorithm consistently outperforms  the traditional rejection mechanism based on distance from decision boundary.",
        "bibtex": "@inproceedings{NIPS2011_4b6538a4,\n author = {Wiener, Yair and El-Yaniv, Ran},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Agnostic Selective Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/4b6538a44a1dfdc2b83477cd76dee98e-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/4b6538a44a1dfdc2b83477cd76dee98e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/4b6538a44a1dfdc2b83477cd76dee98e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 166186,
        "gs_citation": 91,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2997353021852078227&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Computer Science Department, Technion \u2013 Israel Institute of Technology; Computer Science Department, Technion \u2013 Israel Institute of Technology",
        "aff_domain": "cs.technion.ac.il;tx.technion.ac.il",
        "email": "cs.technion.ac.il;tx.technion.ac.il",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Computer Science Department, Technion \u2013 Israel Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "f0e0569121",
        "title": "Algorithms and hardness results for parallel large margin learning",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/b7ee6f5f9aa5cd17ca1aea43ce848496-Abstract.html",
        "author": "Phil Long; Rocco Servedio",
        "abstract": "We study the fundamental problem of learning an unknown large-margin halfspace in the context of parallel computation.    Our main positive result is a parallel algorithm for learning a large-margin halfspace that is based on interior point methods from convex optimization and fast parallel algorithms for matrix computations. We show that this algorithm learns an unknown gamma-margin halfspace over n dimensions using poly(n,1/gamma) processors and runs in time ~O(1/gamma) + O(log n). In contrast, naive parallel algorithms that learn a gamma-margin halfspace in time that depends polylogarithmically on n have Omega(1/gamma^2) runtime dependence on gamma.     Our main negative result deals with boosting, which is a standard approach to learning large-margin halfspaces. We give an information-theoretic proof that in the original PAC framework, in which a weak learning algorithm is provided as an oracle that is called by the booster, boosting cannot be parallelized: the ability to  call the weak learner multiple times in parallel within a single boosting stage does not reduce the overall number of successive stages of boosting that are required.",
        "bibtex": "@inproceedings{NIPS2011_b7ee6f5f,\n author = {Long, Phil and Servedio, Rocco},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Algorithms and hardness results for parallel large margin learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/b7ee6f5f9aa5cd17ca1aea43ce848496-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/b7ee6f5f9aa5cd17ca1aea43ce848496-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/b7ee6f5f9aa5cd17ca1aea43ce848496-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 230178,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4823739868608365351&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 26,
        "aff": "Google; Columbia University",
        "aff_domain": "google.com;cs.columbia.edu",
        "email": "google.com;cs.columbia.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Google;Columbia University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.google.com;https://www.columbia.edu",
        "aff_unique_abbr": "Google;Columbia",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Mountain View;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "45dbe42811",
        "title": "Algorithms for Hyper-Parameter Optimization",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/86e8f7ab32cfd12577bc2619bc635690-Abstract.html",
        "author": "James S. Bergstra; R\u00e9mi Bardenet; Yoshua Bengio; Bal\u00e1zs K\u00e9gl",
        "abstract": "Several recent advances to the state of the art in image classification benchmarks have come from better configurations of existing techniques rather than novel approaches to feature learning. Traditionally, hyper-parameter optimization has been the job of humans because they can be very efficient in regimes where only a few trials are possible. Presently, computer clusters and GPU processors make it possible to run more trials and we show that algorithmic approaches can find better results. We present hyper-parameter optimization results on tasks of training neural networks and deep belief networks (DBNs). We optimize hyper-parameters using random search and two new greedy sequential methods based on the expected improvement criterion. Random search has been shown to be sufficiently efficient for learning neural networks for several datasets, but we show it is unreliable for training DBNs. The sequential algorithms are applied to the most difficult DBN learning problems from [Larochelle et al., 2007] and find significantly better results than the best previously reported. This work contributes novel techniques for making response surface models P (y|x) in which many elements of hyper-parameter assignment (x) are known to be irrelevant given particular values of other elements.",
        "bibtex": "@inproceedings{NIPS2011_86e8f7ab,\n author = {Bergstra, James and Bardenet, R\\'{e}mi and Bengio, Yoshua and K\\'{e}gl, Bal\\'{a}zs},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Algorithms for Hyper-Parameter Optimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 498521,
        "gs_citation": 7578,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2237556841915885113&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "The Rowland Institute, Harvard University; Laboratoire de Recherche en Informatique, Universit \u00b4e Paris-Sud; D\u00b4ept. d\u2019Informatique et Recherche Op \u00b4erationelle, Universit \u00b4e de Montr \u00b4eal; Linear Accelerator Laboratory, Universit \u00b4e Paris-Sud, CNRS",
        "aff_domain": "rowland.harvard.edu;lri.fr;umontreal.ca;gmail.com",
        "email": "rowland.harvard.edu;lri.fr;umontreal.ca;gmail.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "The Rowland Institute, Harvard University;Laboratoire de Recherche en Informatique, Universit \u00b4e Paris-Sud;D\u00b4ept. d\u2019Informatique et Recherche Op \u00b4erationelle, Universit \u00b4e de Montr \u00b4eal;Linear Accelerator Laboratory, Universit \u00b4e Paris-Sud, CNRS",
        "aff_unique_dep": ";;;",
        "aff_unique_url": ";;;",
        "aff_unique_abbr": ";;;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "6dbc816336",
        "title": "An Application of Tree-Structured Expectation Propagation for Channel Decoding",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/58c54802a9fb9526cd0923353a34a7ae-Abstract.html",
        "author": "Pablo M. Olmos; Luis Salamanca; Juan Fuentes; Fernando P\u00e9rez-Cruz",
        "abstract": "We show an application of a tree structure for approximate inference in graphical models using the expectation propagation algorithm. These approximations are typically used over graphs with short-range cycles. We demonstrate that these approximations also help in sparse graphs with long-range loops, as the ones used in coding theory to approach channel capacity. For asymptotically large sparse graph, the expectation propagation algorithm together with the tree structure yields a completely disconnected approximation to the graphical model but, for for finite-length practical sparse graphs, the tree structure approximation to the code graph provides accurate estimates for the marginal of each variable.",
        "bibtex": "@inproceedings{NIPS2011_58c54802,\n author = {Olmos, Pablo and Salamanca, Luis and Fuentes, Juan and P\\'{e}rez-Cruz, Fernando},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {An Application of Tree-Structured Expectation Propagation for Channel Decoding},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/58c54802a9fb9526cd0923353a34a7ae-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/58c54802a9fb9526cd0923353a34a7ae-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/58c54802a9fb9526cd0923353a34a7ae-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1424712,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:J3UcLXNLwCwJ:scholar.google.com/&scioq=An+Application+of+Tree-Structured+Expectation+Propagation+for+Channel+Decoding&hl=en&as_sdt=0,5",
        "gs_version_total": 7,
        "aff": "Dept. of Signal Theory and Communications, University of Sevilla; Dept. of Signal Theory and Communications, University of Sevilla; Dept. of Signal Theory and Communications, University of Sevilla; Dept. of Signal Theory and Communications, University Carlos III in Madrid",
        "aff_domain": "us.es;us.es;us.es;tsc.uc3m.es",
        "email": "us.es;us.es;us.es;tsc.uc3m.es",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Dept. of Signal Theory and Communications, University of Sevilla;Dept. of Signal Theory and Communications, University Carlos III in Madrid",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "55cd6caf50",
        "title": "An Empirical Evaluation of Thompson Sampling",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/e53a0a2978c28872a4505bdb51db06dc-Abstract.html",
        "author": "Olivier Chapelle; Lihong Li",
        "abstract": "Thompson sampling is one of oldest heuristic to address the exploration / exploitation trade-off, but it is surprisingly not very popular in the literature. We present here some empirical results using Thompson sampling on simulated and real data, and show that it is highly competitive. And since this heuristic is very easy to implement, we argue that it should be part of the standard baselines to compare against.",
        "bibtex": "@inproceedings{NIPS2011_e53a0a29,\n author = {Chapelle, Olivier and Li, Lihong},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {An Empirical Evaluation of Thompson Sampling},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/e53a0a2978c28872a4505bdb51db06dc-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/e53a0a2978c28872a4505bdb51db06dc-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/e53a0a2978c28872a4505bdb51db06dc-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 239361,
        "gs_citation": 2001,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18098714511046734850&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Yahoo! Research; Yahoo! Research",
        "aff_domain": "yahoo-inc.com;yahoo-inc.com",
        "email": "yahoo-inc.com;yahoo-inc.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Yahoo!",
        "aff_unique_dep": "Yahoo! Research",
        "aff_unique_url": "https://research.yahoo.com",
        "aff_unique_abbr": "Yahoo!",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e175aed423",
        "title": "An Exact Algorithm for F-Measure Maximization",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/71ad16ad2c4d81f348082ff6c4b20768-Abstract.html",
        "author": "Krzysztof J. Dembczynski; Willem Waegeman; Weiwei Cheng; Eyke H\u00fcllermeier",
        "abstract": "The F-measure, originally introduced in information retrieval, is nowadays routinely used as a performance metric for problems such as binary classification, multi-label classification, and structured output prediction. Optimizing this measure remains a statistically and computationally challenging problem, since no closed-form maximizer exists. Current algorithms are approximate and typically rely on additional assumptions regarding the statistical distribution of the binary response variables. In this paper, we present an algorithm which is not only computationally efficient but also exact, regardless of the underlying distribution. The algorithm requires only a quadratic number of parameters of the joint distribution  (with respect to the number of binary responses). We illustrate its practical performance by means of experimental results for multi-label classification.",
        "bibtex": "@inproceedings{NIPS2011_71ad16ad,\n author = {Dembczynski, Krzysztof and Waegeman, Willem and Cheng, Weiwei and H\\\"{u}llermeier, Eyke},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {An Exact Algorithm for F-Measure Maximization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/71ad16ad2c4d81f348082ff6c4b20768-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/71ad16ad2c4d81f348082ff6c4b20768-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/71ad16ad2c4d81f348082ff6c4b20768-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 212864,
        "gs_citation": 158,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1009767309881414223&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Institute of Computing Science, Pozna \u00b4n University of Technology, Pozna \u00b4n, 60-695 Poland; Mathematical Modelling, Statistics and Bioinformatics, Ghent University, Ghent, 9000 Belgium; Mathematics and Computer Science, Philipps-Universit \u00a8at Marburg, Marburg, 35032 Germany; Mathematics and Computer Science, Philipps-Universit \u00a8at Marburg, Marburg, 35032 Germany",
        "aff_domain": "cs.put.poznan.pl;ugent.be;mathematik.uni-marburg.de;mathematik.uni-marburg.de",
        "email": "cs.put.poznan.pl;ugent.be;mathematik.uni-marburg.de;mathematik.uni-marburg.de",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;2",
        "aff_unique_norm": "Institute of Computing Science, Pozna \u00b4n University of Technology, Pozna \u00b4n, 60-695 Poland;Mathematical Modelling, Statistics and Bioinformatics, Ghent University, Ghent, 9000 Belgium;Mathematics and Computer Science, Philipps-Universit \u00a8at Marburg, Marburg, 35032 Germany",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "0134a92dc7",
        "title": "An Unsupervised Decontamination Procedure For Improving The Reliability Of Human Judgments",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/f33ba15effa5c10e873bf3842afb46a6-Abstract.html",
        "author": "Michael Mozer; Benjamin Link; Harold Pashler",
        "abstract": "Psychologists have long been struck by individuals' limitations in expressing their internal sensations, impressions, and evaluations via rating scales.  Instead of using an absolute scale, individuals rely on reference points from recent experience.  This",
        "bibtex": "@inproceedings{NIPS2011_f33ba15e,\n author = {Mozer, Michael C and Link, Benjamin and Pashler, Harold},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {An Unsupervised Decontamination Procedure For Improving The Reliability Of Human Judgments},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/f33ba15effa5c10e873bf3842afb46a6-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/f33ba15effa5c10e873bf3842afb46a6-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/f33ba15effa5c10e873bf3842afb46a6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 341161,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3183433517187843094&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "5b52730b4a",
        "title": "An ideal observer model for identifying the reference frame of objects",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/ffd52f3c7e12435a724a8f30fddadd9c-Abstract.html",
        "author": "Joseph L. Austerweil; Abram L. Friesen; Thomas L. Griffiths",
        "abstract": "The object people perceive in an image can depend on its orientation relative to the scene it is in (its reference frame). For example, the images of the symbols $\\times$ and $+$ differ by a 45 degree rotation. Although real scenes have multiple images and reference frames, psychologists have focused on scenes with only one reference frame. We propose an ideal observer model based on nonparametric Bayesian statistics for inferring the number of reference frames in a scene and their parameters. When an ambiguous image could be assigned to two conflicting reference frames, the model predicts two factors should influence the reference frame inferred for the image: The image should be more likely to share the reference frame of the closer object ({\\em proximity}) and it should be more likely to share the reference frame containing the most objects ({\\em alignment}). We confirm people use both cues using a novel methodology that allows for easy testing of human reference frame inference.",
        "bibtex": "@inproceedings{NIPS2011_ffd52f3c,\n author = {Austerweil, Joseph and Friesen, Abram L and Griffiths, Thomas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {An ideal observer model for identifying the reference frame of objects},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/ffd52f3c7e12435a724a8f30fddadd9c-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/ffd52f3c7e12435a724a8f30fddadd9c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/ffd52f3c7e12435a724a8f30fddadd9c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 116411,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=915218229670523093&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Psychology, University of California, Berkeley; Department of Computer Science and Engineering, University of Washington; Department of Psychology, University of California, Berkeley",
        "aff_domain": "gmail.com;cs.washington.edu;berkeley.edu",
        "email": "gmail.com;cs.washington.edu;berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Department of Psychology, University of California, Berkeley;University of Washington",
        "aff_unique_dep": ";Department of Computer Science and Engineering",
        "aff_unique_url": ";https://www.washington.edu",
        "aff_unique_abbr": ";UW",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Seattle",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "f855da63e0",
        "title": "Analysis and Improvement of Policy Gradient Estimation",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/85d8ce590ad8981ca2c8286f79f59954-Abstract.html",
        "author": "Tingting Zhao; Hirotaka Hachiya; Gang Niu; Masashi Sugiyama",
        "abstract": "Policy gradient is a useful model-free reinforcement learning approach,  but it tends to suffer from instability of gradient estimates.  In this paper, we analyze and improve the stability of policy gradient methods. We first prove that the variance of gradient estimates in the PGPE(policy gradients with parameter-based exploration) method is smaller than that of the classical REINFORCE method under a mild assumption.  We then derive the optimal baseline for PGPE, which contributes to further reducing the variance.  We also theoretically show that PGPE with the optimal baseline is more preferable than REINFORCE with the optimal baseline in terms of the variance of gradient estimates.  Finally, we demonstrate the usefulness of the improved PGPE method through experiments.",
        "bibtex": "@inproceedings{NIPS2011_85d8ce59,\n author = {Zhao, Tingting and Hachiya, Hirotaka and Niu, Gang and Sugiyama, Masashi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Analysis and Improvement of Policy Gradient Estimation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/85d8ce590ad8981ca2c8286f79f59954-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/85d8ce590ad8981ca2c8286f79f59954-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/85d8ce590ad8981ca2c8286f79f59954-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/85d8ce590ad8981ca2c8286f79f59954-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 346183,
        "gs_citation": 138,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9252648331480113088&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Tokyo Institute of Technology; Tokyo Institute of Technology; Tokyo Institute of Technology; Tokyo Institute of Technology",
        "aff_domain": "sg.cs.titech.ac.jp;sg.cs.titech.ac.jp;sg.cs.titech.ac.jp;cs.titech.ac.jp",
        "email": "sg.cs.titech.ac.jp;sg.cs.titech.ac.jp;sg.cs.titech.ac.jp;cs.titech.ac.jp",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Tokyo Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.titech.ac.jp",
        "aff_unique_abbr": "Titech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "875b19172a",
        "title": "Analytical Results for the Error in Filtering of Gaussian Processes",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/1c65cef3dfd1e00c0b03923a1c591db4-Abstract.html",
        "author": "Alex K. Susemihl; Ron Meir; Manfred Opper",
        "abstract": "Bayesian filtering of stochastic stimuli has received a great deal of attention re- cently. It has been applied to describe the way in which biological systems dy- namically represent and make decisions about the environment. There have been no exact results for the error in the biologically plausible setting of inference on point process, however. We present an exact analysis of the evolution of the mean- squared error in a state estimation task using Gaussian-tuned point processes as sensors. This allows us to study the dynamics of the error of an optimal Bayesian decoder, providing insights into the limits obtainable in this task. This is done for Markovian and a class of non-Markovian Gaussian processes. We find that there is an optimal tuning width for which the error is minimized. This leads to a char- acterization of the optimal encoding for the setting as a function of the statistics of the stimulus, providing a mathematically sound primer for an ecological theory of sensory processing.",
        "bibtex": "@inproceedings{NIPS2011_1c65cef3,\n author = {Susemihl, Alex K and Meir, Ron and Opper, Manfred},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Analytical Results for the Error in Filtering of Gaussian Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/1c65cef3dfd1e00c0b03923a1c591db4-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/1c65cef3dfd1e00c0b03923a1c591db4-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/1c65cef3dfd1e00c0b03923a1c591db4-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/1c65cef3dfd1e00c0b03923a1c591db4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1987642,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3464864488286162270&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Bernstein Center for Computational Neuroscience Berlin, Technische Universit \u00a8at Berlin; Department of Eletrical Engineering, Technion, Haifa; Bernstein Center for Computational Neuroscience Berlin, Technische Universit \u00a8at Berlin",
        "aff_domain": "bccn-berlin.de;ee.technion.ac.il;cs.tu-berlin.de",
        "email": "bccn-berlin.de;ee.technion.ac.il;cs.tu-berlin.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Bernstein Center for Computational Neuroscience Berlin, Technische Universit \u00a8at Berlin;Department of Eletrical Engineering, Technion, Haifa",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "809960fb78",
        "title": "Anatomically Constrained Decoding of Finger Flexion from Electrocorticographic Signals",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/120ca817ebe8caa71e92ac53049b2c6a-Abstract.html",
        "author": "Zuoguan Wang; Gerwin Schalk; Qiang Ji",
        "abstract": "Brain-computer interfaces (BCIs) use brain signals to convey a user's intent. Some BCI approaches begin by decoding kinematic parameters of movements from brain signals, and then proceed to using these signals, in absence of movements, to allow a user to control an output. Recent results have shown that electrocorticographic (ECoG) recordings from the surface of the brain in humans can give information about kinematic parameters (e.g., hand velocity or finger flexion). The decoding approaches in these demonstrations usually employed classical classification/regression algorithms that derive a linear mapping between brain signals and outputs. However, they typically only incorporate little prior information about the target kinematic parameter. In this paper, we show that different types of anatomical constraints that govern finger flexion can be exploited in this context. Specifically, we incorporate these constraints in the construction, structure, and the probabilistic functions of a switched non-parametric dynamic system (SNDS) model. We then apply the resulting SNDS decoder to infer the flexion of individual fingers from the same ECoG dataset used in a recent study. Our results show that the application of the proposed model, which incorporates anatomical constraints, improves decoding performance compared to the results in the previous work. Thus, the results presented in this paper may ultimately lead to neurally controlled hand prostheses with full fine-grained finger articulation.",
        "bibtex": "@inproceedings{NIPS2011_120ca817,\n author = {Wang, Zuoguan and Schalk, Gerwin and Ji, Qiang},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Anatomically Constrained Decoding of Finger Flexion from Electrocorticographic Signals},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/120ca817ebe8caa71e92ac53049b2c6a-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/120ca817ebe8caa71e92ac53049b2c6a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/120ca817ebe8caa71e92ac53049b2c6a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 603593,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1134693246193562968&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of ECSE, Rensselaer Polytechnic Inst., Troy, NY 12180; Wadsworth Center, NYS Dept of Health, Albany, NY 12201; Department of ECSE, Rensselaer Polytechnic Inst., Troy, NY 12180",
        "aff_domain": "rpi.edu;wadsworth.org;rpi.edu",
        "email": "rpi.edu;wadsworth.org;rpi.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Department of ECSE, Rensselaer Polytechnic Inst., Troy, NY 12180;Wadsworth Center, NYS Dept of Health, Albany, NY 12201",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "5244a0c05c",
        "title": "Approximating Semidefinite Programs in Sublinear Time",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/2f37d10131f2a483a8dd005b3d14b0d9-Abstract.html",
        "author": "Dan Garber; Elad Hazan",
        "abstract": "In recent years semidefinite optimization has become a tool of major importance in various optimization and machine learning problems. In many of these problems the amount of data in practice is so large that there is a constant need for faster algorithms. In this work we present the first  sublinear time approximation algorithm for semidefinite programs which we believe may be useful for such problems in which the size of data may cause even linear time algorithms to have prohibitive running times in practice. We present the algorithm and its analysis alongside with some theoretical lower bounds and an improved algorithm for the special problem of supervised learning of a distance metric.",
        "bibtex": "@inproceedings{NIPS2011_2f37d101,\n author = {Garber, Dan and Hazan, Elad},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Approximating Semidefinite Programs in Sublinear Time},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/2f37d10131f2a483a8dd005b3d14b0d9-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/2f37d10131f2a483a8dd005b3d14b0d9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/2f37d10131f2a483a8dd005b3d14b0d9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 224464,
        "gs_citation": 47,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17281692850043498049&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Technion - Israel Institute of Technology; Technion - Israel Institute of Technology",
        "aff_domain": "cs.technion.ac.il;ie.technion.ac.il",
        "email": "cs.technion.ac.il;ie.technion.ac.il",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Technion - Israel Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.technion.ac.il/en/",
        "aff_unique_abbr": "Technion",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "cb6df59392",
        "title": "Automated Refinement of Bayes Networks' Parameters based on Test Ordering Constraints",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/28fc2782ea7ef51c1104ccf7b9bea13d-Abstract.html",
        "author": "Omar Z. Khan; Pascal Poupart; John-mark M. Agosta",
        "abstract": "In this paper, we derive a method to refine a Bayes network diagnostic model by exploiting constraints implied by expert decisions on test ordering. At each step, the expert executes an evidence gathering test, which suggests the test's relative diagnostic value. We demonstrate that consistency with an expert's test selection leads to non-convex constraints on the model parameters.  We incorporate these constraints by augmenting the network with nodes that represent the constraint likelihoods. Gibbs sampling, stochastic hill climbing and greedy search algorithms are proposed to find a MAP estimate that takes into account test ordering constraints and any data available. We demonstrate our approach on diagnostic sessions from a manufacturing scenario.",
        "bibtex": "@inproceedings{NIPS2011_28fc2782,\n author = {Khan, Omar and Poupart, Pascal and Agosta, John-mark},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Automated Refinement of Bayes Networks\\textquotesingle  Parameters based on Test Ordering Constraints},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/28fc2782ea7ef51c1104ccf7b9bea13d-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/28fc2782ea7ef51c1104ccf7b9bea13d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/28fc2782ea7ef51c1104ccf7b9bea13d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 313584,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7122980844807406784&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, ON Canada; David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, ON Canada; Intel Labs, Santa Clara, CA, USA",
        "aff_domain": "cs.uwaterloo.ca;cs.uwaterloo.ca;gmail.com",
        "email": "cs.uwaterloo.ca;cs.uwaterloo.ca;gmail.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, ON Canada;Intel Labs",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://www.intel.com/research",
        "aff_unique_abbr": ";Intel",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Santa Clara",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "fcc6d2a590",
        "title": "Autonomous Learning of Action Models for Planning",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/4671aeaf49c792689533b00664a5c3ef-Abstract.html",
        "author": "Neville Mehta; Prasad Tadepalli; Alan Fern",
        "abstract": "This paper introduces two new frameworks for learning action models for planning.  In the mistake-bounded planning framework, the learner has access to a planner for the given model representation, a simulator, and a planning problem generator, and aims to learn a model with at most a polynomial number of faulty plans.  In the planned exploration framework, the learner does not have access to a problem generator and must instead design its own problems, plan for them, and converge with at most a polynomial number of planning attempts.  The paper reduces learning in these frameworks to concept learning with one-sided error and provides algorithms for successful learning in both frameworks.  A specific family of hypothesis spaces is shown to be efficiently learnable in both the frameworks.",
        "bibtex": "@inproceedings{NIPS2011_4671aeaf,\n author = {Mehta, Neville and Tadepalli, Prasad and Fern, Alan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Autonomous Learning of Action Models for Planning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/4671aeaf49c792689533b00664a5c3ef-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/4671aeaf49c792689533b00664a5c3ef-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/4671aeaf49c792689533b00664a5c3ef-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 194754,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6939592785060089554&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, OR 97331, USA; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, OR 97331, USA; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, OR 97331, USA",
        "aff_domain": "eecs.oregonstate.edu;eecs.oregonstate.edu;eecs.oregonstate.edu",
        "email": "eecs.oregonstate.edu;eecs.oregonstate.edu;eecs.oregonstate.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Oregon State University",
        "aff_unique_dep": "School of Electrical Engineering and Computer Science",
        "aff_unique_url": "https://osu.edu",
        "aff_unique_abbr": "OSU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Corvallis",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3f34fe3bcd",
        "title": "Bayesian Bias Mitigation for Crowdsourcing",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/0768281a05da9f27df178b5c39a51263-Abstract.html",
        "author": "Fabian L. Wauthier; Michael I. Jordan",
        "abstract": "Biased labelers are a systemic problem in crowdsourcing, and a  comprehensive toolbox for handling their responses is still being  developed. A typical crowdsourcing application can be divided into  three steps: data collection, data curation, and learning. At present  these steps are often treated separately. We present Bayesian Bias  Mitigation for Crowdsourcing (BBMC), a Bayesian model to unify all  three. Most data curation methods account for the {\\it effects} of  labeler bias by modeling all labels as coming from a single latent  truth. Our model captures the {\\it sources} of bias by describing  labelers as influenced by shared random effects. This approach can  account for more complex bias patterns that arise in ambiguous or hard  labeling tasks and allows us to merge data curation and learning into  a single computation. Active learning integrates data collection with  learning, but is commonly considered infeasible with Gibbs sampling  inference. We propose a general approximation strategy for Markov  chains to efficiently quantify the effect of a perturbation on the  stationary distribution and specialize this approach to active  learning. Experiments show BBMC to outperform many common heuristics.",
        "bibtex": "@inproceedings{NIPS2011_0768281a,\n author = {Wauthier, Fabian L and Jordan, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Bayesian Bias Mitigation for Crowdsourcing},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/0768281a05da9f27df178b5c39a51263-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/0768281a05da9f27df178b5c39a51263-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/0768281a05da9f27df178b5c39a51263-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 286437,
        "gs_citation": 158,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16463063397966923758&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 11,
        "aff": "University of California, Berkeley; University of California, Berkeley",
        "aff_domain": "cs.berkeley.edu;cs.berkeley.edu",
        "email": "cs.berkeley.edu;cs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "1cd5aedae6",
        "title": "Bayesian Partitioning of Large-Scale Distance Data",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/3621f1454cacf995530ea53652ddf8fb-Abstract.html",
        "author": "David Adametz; Volker Roth",
        "abstract": "A Bayesian approach to partitioning distance matrices is presented. It is inspired by the 'Translation-Invariant Wishart-Dirichlet' process (TIWD) in (Vogt et al., 2010) and shares a number of advantageous properties like the fully probabilistic nature of the inference model, automatic selection of the number of clusters and applicability in semi-supervised settings. In addition, our method (which we call 'fastTIWD') overcomes the main shortcoming of the original TIWD, namely its high computational costs. The fastTIWD reduces the workload in each iteration of a Gibbs sampler from O(n^3) in the TIWD to O(n^2). Our experiments show that this cost reduction does not compromise the quality of the inferred partitions. With this new method it is now possible to 'mine' large relational datasets with a probabilistic model, thereby automatically detecting new and potentially interesting clusters.",
        "bibtex": "@inproceedings{NIPS2011_3621f145,\n author = {Adametz, David and Roth, Volker},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Bayesian Partitioning of Large-Scale Distance Data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/3621f1454cacf995530ea53652ddf8fb-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/3621f1454cacf995530ea53652ddf8fb-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/3621f1454cacf995530ea53652ddf8fb-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 426542,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=800862983841869353&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science & Mathematics, University of Basel, Basel, Switzerland; Department of Computer Science & Mathematics, University of Basel, Basel, Switzerland",
        "aff_domain": "unibas.ch;unibas.ch",
        "email": "unibas.ch;unibas.ch",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Department of Computer Science & Mathematics, University of Basel, Basel, Switzerland",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "042c12a0bc",
        "title": "Bayesian Spike-Triggered Covariance Analysis",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/6395ebd0f4b478145ecfbaf939454fa4-Abstract.html",
        "author": "Ill Memming Park; Jonathan W. Pillow",
        "abstract": "Neurons typically respond to a restricted number of stimulus features within the high-dimensional space of natural stimuli. Here we describe an explicit model-based interpretation of traditional estimators for a neuron's multi-dimensional feature space, which allows for several important generalizations and extensions. First, we show that traditional estimators based on the spike-triggered average (STA) and spike-triggered covariance (STC) can be formalized in terms of the \"expected log-likelihood\" of a Linear-Nonlinear-Poisson (LNP) model with Gaussian stimuli. This model-based formulation allows us to define maximum-likelihood and Bayesian estimators that are statistically consistent and efficient in a wider variety of settings, such as with naturalistic (non-Gaussian) stimuli. It also allows us to employ Bayesian methods for regularization, smoothing, sparsification, and model comparison, and provides Bayesian confidence intervals on model parameters. We describe an empirical Bayes method for selecting the number of features, and extend the model to accommodate an arbitrary elliptical nonlinear response function, which results in a more powerful and more flexible model for feature space inference. We validate these methods using neural data recorded extracellularly from macaque primary visual cortex.",
        "bibtex": "@inproceedings{NIPS2011_6395ebd0,\n author = {Park, Il Memming and Pillow, Jonathan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Bayesian Spike-Triggered Covariance Analysis},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/6395ebd0f4b478145ecfbaf939454fa4-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/6395ebd0f4b478145ecfbaf939454fa4-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/6395ebd0f4b478145ecfbaf939454fa4-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/6395ebd0f4b478145ecfbaf939454fa4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 594409,
        "gs_citation": 87,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4252133862962393330&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Center for Perceptual Systems, University of Texas at Austin; Center for Perceptual Systems, University of Texas at Austin",
        "aff_domain": "austin.utexas.edu;mail.utexas.edu",
        "email": "austin.utexas.edu;mail.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Center for Perceptual Systems, University of Texas at Austin",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "cd09f643d0",
        "title": "Beating SGD: Learning SVMs in Sublinear Time",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/5f2c22cb4a5380af7ca75622a6426917-Abstract.html",
        "author": "Elad Hazan; Tomer Koren; Nati Srebro",
        "abstract": "We present an optimization approach for linear SVMs based on a stochastic primal-dual approach, where the primal step is akin to an importance-weighted SGD, and the dual step is a stochastic update on the importance weights.  This yields an optimization method with a sublinear dependence on the training set size, and the first method for learning linear SVMs with runtime less then the size of the training set required for learning!",
        "bibtex": "@inproceedings{NIPS2011_5f2c22cb,\n author = {Hazan, Elad and Koren, Tomer and Srebro, Nati},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Beating SGD: Learning SVMs in Sublinear Time},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/5f2c22cb4a5380af7ca75622a6426917-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/5f2c22cb4a5380af7ca75622a6426917-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/5f2c22cb4a5380af7ca75622a6426917-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 266991,
        "gs_citation": 76,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=524185531086462310&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Technion, Israel Institute of Technology; Technion, Israel Institute of Technology; Toyota Technological Institute",
        "aff_domain": "ie.technion.ac.il;cs.technion.ac.il;ttic.edu",
        "email": "ie.technion.ac.il;cs.technion.ac.il;ttic.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Israel Institute of Technology;Toyota Technological Institute",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.technion.ac.il/en/;https://www.tti.ac.jp",
        "aff_unique_abbr": "Technion;TTI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "Israel;Japan"
    },
    {
        "id": "e0187ce842",
        "title": "Better Mini-Batch Algorithms via Accelerated Gradient Methods",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/b55ec28c52d5f6205684a473a2193564-Abstract.html",
        "author": "Andrew Cotter; Ohad Shamir; Nati Srebro; Karthik Sridharan",
        "abstract": "Mini-batch algorithms have recently received significant attention as a way to speed-up stochastic convex optimization problems. In this paper, we study how such algorithms can be improved using accelerated gradient methods. We provide a novel analysis, which shows how standard gradient methods may sometimes be insufficient to obtain a significant speed-up. We propose a novel accelerated gradient algorithm, which deals with this deficiency, and enjoys a uniformly superior guarantee. We conclude our paper with experiments  on real-world datasets, which validates our algorithm and  substantiates our theoretical insights.",
        "bibtex": "@inproceedings{NIPS2011_b55ec28c,\n author = {Cotter, Andrew and Shamir, Ohad and Srebro, Nati and Sridharan, Karthik},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Better Mini-Batch Algorithms via Accelerated Gradient Methods},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/b55ec28c52d5f6205684a473a2193564-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/b55ec28c52d5f6205684a473a2193564-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/b55ec28c52d5f6205684a473a2193564-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/b55ec28c52d5f6205684a473a2193564-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 436030,
        "gs_citation": 409,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11848732921276895739&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Toyota Technological Institute at Chicago; Microsoft Research, NE; Toyota Technological Institute at Chicago; Toyota Technological Institute at Chicago",
        "aff_domain": "ttic.edu;microsoft.com;ttic.edu;ttic.edu",
        "email": "ttic.edu;microsoft.com;ttic.edu;ttic.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Toyota Technological Institute at Chicago;Microsoft Research, NE",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tti-chicago.org;",
        "aff_unique_abbr": "TTI Chicago;",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Chicago;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "1668f952c7",
        "title": "Beyond Spectral Clustering - Tight Relaxations of Balanced Graph Cuts",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/193002e668758ea9762904da1a22337c-Abstract.html",
        "author": "Matthias Hein; Simon Setzer",
        "abstract": "Spectral clustering is based on the spectral relaxation of the normalized/ratio graph cut criterion. While the  spectral relaxation is known to be loose, it has been shown recently that a non-linear  eigenproblem yields a tight relaxation of the Cheeger cut.   In this paper, we extend this result considerably by providing a   characterization of all balanced graph cuts which allow for a tight relaxation. Although the resulting optimization problems are non-convex and non-smooth, we provide   an efficient first-order scheme which scales to large graphs. Moreover, our approach  comes with the quality guarantee that given any partition as initialization the   algorithm either outputs a better partition or it stops immediately.",
        "bibtex": "@inproceedings{NIPS2011_193002e6,\n author = {Hein, Matthias and Setzer, Simon},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Beyond Spectral Clustering - Tight Relaxations of Balanced Graph Cuts},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/193002e668758ea9762904da1a22337c-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/193002e668758ea9762904da1a22337c-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/193002e668758ea9762904da1a22337c-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/193002e668758ea9762904da1a22337c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 470992,
        "gs_citation": 109,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9822047893958482462&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 9,
        "aff": "Saarland University, Saarbr\u00fccken, Germany; Saarland University, Saarbr\u00fccken, Germany",
        "aff_domain": "cs.uni-saarland.de;mia.uni-saarland.de",
        "email": "cs.uni-saarland.de;mia.uni-saarland.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Saarland University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uni-saarland.de",
        "aff_unique_abbr": "UdS",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Saarbr\u00fccken",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "54e0074baa",
        "title": "Blending Autonomous Exploration and Apprenticeship Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/e034fb6b66aacc1d48f445ddfb08da98-Abstract.html",
        "author": "Thomas J. Walsh; Daniel K. Hewlett; Clayton T. Morrison",
        "abstract": "We present theoretical and empirical results for a framework  that combines the benefits of apprenticeship and autonomous reinforcement learning.  Our approach modifies an existing apprenticeship learning framework that relies on teacher demonstrations and does not necessarily explore the environment.  The first change is replacing previously used Mistake Bound model learners with a recently proposed framework that melds the  KWIK and Mistake Bound supervised learning protocols.  The second change is introducing a communication of expected utility from the student to the teacher.   The resulting system only uses teacher traces when the agent needs to learn concepts it cannot efficiently learn on its own.",
        "bibtex": "@inproceedings{NIPS2011_e034fb6b,\n author = {Walsh, Thomas and Hewlett, Daniel and Morrison, Clayton},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Blending Autonomous Exploration and Apprenticeship Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/e034fb6b66aacc1d48f445ddfb08da98-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/e034fb6b66aacc1d48f445ddfb08da98-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/e034fb6b66aacc1d48f445ddfb08da98-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 417644,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2519001817267713179&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Center for Educational Testing and Evaluation, University of Kansas; School of Information: Science, Technology and Arts, University of Arizona; School of Information: Science, Technology and Arts, University of Arizona",
        "aff_domain": "ku.edu;cs.arizona.edu;sista.arizona.edu",
        "email": "ku.edu;cs.arizona.edu;sista.arizona.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Center for Educational Testing and Evaluation, University of Kansas;School of Information: Science, Technology and Arts, University of Arizona",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "218c144811",
        "title": "Boosting with Maximum Adaptive Sampling",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/8c6744c9d42ec2cb9e8885b54ff744d0-Abstract.html",
        "author": "Charles Dubout; Francois Fleuret",
        "abstract": "Classical Boosting algorithms, such as AdaBoost, build a strong classifier without concern about the computational cost.  Some applications, in particular in computer vision, may involve up to millions of training examples and features.  In such contexts, the training time may become prohibitive.  Several methods exist to accelerate training, typically either by sampling the features, or the examples, used to train the weak learners.  Even if those methods can precisely quantify the speed improvement they deliver, they offer no guarantee of being more efficient than any other, given the same amount of time.    This paper aims at shading some light on this problem, i.e. given a fixed amount of time, for a particular problem, which strategy is optimal in order to reduce the training loss the most.  We apply this analysis to the design of new algorithms which estimate on the fly at every iteration the optimal trade-off between the number of samples and the number of features to look at in order to maximize the expected loss reduction.  Experiments in object recognition with two standard computer vision data-sets show that the adaptive methods we propose outperform basic sampling and state-of-the-art bandit methods.",
        "bibtex": "@inproceedings{NIPS2011_8c6744c9,\n author = {Dubout, Charles and Fleuret, Francois},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Boosting with Maximum Adaptive Sampling},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/8c6744c9d42ec2cb9e8885b54ff744d0-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/8c6744c9d42ec2cb9e8885b54ff744d0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/8c6744c9d42ec2cb9e8885b54ff744d0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 251556,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17089494411123876444&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Idiap Research Institute; Idiap Research Institute",
        "aff_domain": "idiap.ch;idiap.ch",
        "email": "idiap.ch;idiap.ch",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Idiap Research Institute",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.idiap.ch",
        "aff_unique_abbr": "Idiap",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "af57802ca8",
        "title": "Budgeted Optimization with Concurrent Stochastic-Duration Experiments",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/3a066bda8c96b9478bb0512f0a43028c-Abstract.html",
        "author": "Javad Azimi; Alan Fern; Xiaoli Z. Fern",
        "abstract": "Budgeted optimization involves optimizing an unknown function that is costly to evaluate by requesting a limited number of function evaluations at intelligently selected inputs. Typical problem formulations assume that experiments are selected one at a time with a limited total number of experiments, which fail to capture important aspects of many real-world problems. This paper defines a novel problem formulation with the following important extensions: 1) allowing for concurrent experiments; 2) allowing for stochastic experiment durations; and 3) placing constraints on both the total number of experiments and the total experimental time. We develop both offline and online algorithms for selecting concurrent experiments in this new setting and provide experimental results on a number of optimization benchmarks. The results show that our algorithms produce highly effective schedules compared to natural baselines.",
        "bibtex": "@inproceedings{NIPS2011_3a066bda,\n author = {Azimi, Javad and Fern, Alan and Fern, Xiaoli},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Budgeted Optimization with Concurrent Stochastic-Duration Experiments},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/3a066bda8c96b9478bb0512f0a43028c-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/3a066bda8c96b9478bb0512f0a43028c-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/3a066bda8c96b9478bb0512f0a43028c-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/3a066bda8c96b9478bb0512f0a43028c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 296317,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17861738975435631876&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "School of EECS, Oregon State University; School of EECS, Oregon State University; School of EECS, Oregon State University",
        "aff_domain": "eecs.oregonstate.edu;eecs.oregonstate.edu;eecs.oregonstate.edu",
        "email": "eecs.oregonstate.edu;eecs.oregonstate.edu;eecs.oregonstate.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Oregon State University",
        "aff_unique_dep": "School of Electrical Engineering and Computer Science",
        "aff_unique_url": "https://eecs.oregonstate.edu",
        "aff_unique_abbr": "OSU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Corvallis",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "178b05d09f",
        "title": "Clustered Multi-Task Learning Via Alternating Structure Optimization",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/a516a87cfcaef229b342c437fe2b95f7-Abstract.html",
        "author": "Jiayu Zhou; Jianhui Chen; Jieping Ye",
        "abstract": "Multi-task learning (MTL) learns multiple related tasks simultaneously to improve generalization performance. Alternating structure optimization (ASO) is a popular MTL method that learns a shared low-dimensional predictive structure on hypothesis spaces from multiple related tasks. It has been applied successfully in many real world applications. As an alternative MTL approach, clustered multi-task learning (CMTL) assumes that multiple tasks follow a clustered structure, i.e., tasks are partitioned into a set of groups where tasks in the same group are similar to each other, and that such a clustered structure is unknown a priori. The objectives in ASO and CMTL differ in how multiple tasks are related. Interestingly, we show in this paper the equivalence relationship between ASO and CMTL, providing significant new insights into ASO and CMTL as well as their inherent relationship. The CMTL formulation is non-convex, and we adopt a convex relaxation to the CMTL formulation. We further establish the equivalence relationship between the proposed convex relaxation of CMTL and an existing convex relaxation of ASO, and show that the proposed convex CMTL formulation is significantly more efficient especially for high-dimensional data. In addition, we present three algorithms for solving the convex CMTL formulation. We report experimental results on benchmark datasets to demonstrate the efficiency of the proposed algorithms.",
        "bibtex": "@inproceedings{NIPS2011_a516a87c,\n author = {Zhou, Jiayu and Chen, Jianhui and Ye, Jieping},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Clustered Multi-Task Learning Via Alternating Structure Optimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/a516a87cfcaef229b342c437fe2b95f7-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/a516a87cfcaef229b342c437fe2b95f7-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/a516a87cfcaef229b342c437fe2b95f7-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/a516a87cfcaef229b342c437fe2b95f7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 125172,
        "gs_citation": 334,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10975581096828748170&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Computer Science and Engineering, Arizona State University; Computer Science and Engineering, Arizona State University; Computer Science and Engineering, Arizona State University",
        "aff_domain": "asu.edu;asu.edu;asu.edu",
        "email": "asu.edu;asu.edu;asu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Arizona State University",
        "aff_unique_dep": "Computer Science and Engineering",
        "aff_unique_url": "https://www.asu.edu",
        "aff_unique_abbr": "ASU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d29598cfee",
        "title": "Clustering via Dirichlet Process Mixture Models for Portable Skill Discovery",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/e17184bcb70dcf3942c54e0b537ffc6d-Abstract.html",
        "author": "Scott Niekum; Andrew G. Barto",
        "abstract": "Skill discovery algorithms in reinforcement learning typically identify single states or regions in state space that correspond to task-specific subgoals.  However, such methods do not directly address the question of how many distinct skills are appropriate for solving the tasks that the agent faces.  This can be highly inefficient when many identified subgoals correspond to the same underlying skill, but are all used individually as skill goals.  Furthermore, skills created in this manner are often only transferable to tasks that share identical state spaces, since corresponding subgoals across tasks are not merged into a single skill goal.  We show that these problems can be overcome by clustering subgoal data defined in an agent-space and using the resulting clusters as templates for skill termination conditions.  Clustering via a Dirichlet process mixture model is used to discover a minimal, sufficient collection of portable skills.",
        "bibtex": "@inproceedings{NIPS2011_e17184bc,\n author = {Niekum, Scott and Barto, Andrew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Clustering via Dirichlet Process Mixture Models for Portable Skill Discovery},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/e17184bcb70dcf3942c54e0b537ffc6d-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/e17184bcb70dcf3942c54e0b537ffc6d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/e17184bcb70dcf3942c54e0b537ffc6d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 373408,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17531437766018894495&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Department of Computer Science, University of Massachusetts Amherst; Department of Computer Science, University of Massachusetts Amherst",
        "aff_domain": "cs.umass.edu;cs.umass.edu",
        "email": "cs.umass.edu;cs.umass.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Department of Computer Science, University of Massachusetts Amherst",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "ecfd1d7966",
        "title": "Co-Training for Domain Adaptation",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/93fb9d4b16aa750c7475b6d601c35c2c-Abstract.html",
        "author": "Minmin Chen; Kilian Q. Weinberger; John Blitzer",
        "abstract": "Domain adaptation algorithms seek to generalize a model trained in a source domain to a new target domain.  In many  practical cases, the source and target distributions can differ substantially, and in some cases crucial target features may not have support in the source domain.  In this paper we introduce an algorithm that bridges the gap between source and target domains by slowly adding both the target features and instances in which the current  algorithm is the most confident.  Our algorithm is a variant of co-training, and we name it CODA (Co-training for domain adaptation).  Unlike the original co-training work, we do not assume a particular feature split.  Instead, for each iteration of co-training, we add target features and formulate a single optimization problem which simultaneously learns a target predictor, a split of the feature space into views, and a shared subset of source  and target features to include in the predictor.  CODA significantly out-performs the state-of-the-art on the 12-domain benchmark data set of Blitzer et al.. Indeed, over a wide range (65 of 84 comparisons) of target supervision, ranging from no labeled target data to a relatively large number of target labels, CODA achieves the best performance.",
        "bibtex": "@inproceedings{NIPS2011_93fb9d4b,\n author = {Chen, Minmin and Weinberger, Kilian Q and Blitzer, John},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Co-Training for Domain Adaptation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/93fb9d4b16aa750c7475b6d601c35c2c-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/93fb9d4b16aa750c7475b6d601c35c2c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/93fb9d4b16aa750c7475b6d601c35c2c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1340276,
        "gs_citation": 530,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6437352312869659389&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Science and Engineering, Washington University in St. Louis; Department of Computer Science and Engineering, Washington University in St. Louis; Google Research",
        "aff_domain": "wustl.edu;wustl.edu;google.com",
        "email": "wustl.edu;wustl.edu;google.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Washington University in St. Louis;Google",
        "aff_unique_dep": "Department of Computer Science and Engineering;Google Research",
        "aff_unique_url": "https://wustl.edu;https://research.google",
        "aff_unique_abbr": "WashU;Google Research",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "St. Louis;Mountain View",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "85d0349792",
        "title": "Co-regularized Multi-view Spectral Clustering",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/31839b036f63806cba3f47b93af8ccb5-Abstract.html",
        "author": "Abhishek Kumar; Piyush Rai; Hal Daume",
        "abstract": "In many clustering problems, we have access to multiple views of the data each  of which could be individually used for clustering. Exploiting information from  multiple views, one can hope to find a clustering that is more accurate than the  ones obtained using the individual views. Since the true clustering would assign  a point to the same cluster irrespective of the view, we can approach this problem  by looking for clusterings that are consistent across the views, i.e., corresponding  data points in each view should have same cluster membership. We propose a  spectral clustering framework that achieves this goal by co-regularizing the clustering  hypotheses, and propose two co-regularization schemes to accomplish this.  Experimental comparisons with a number of baselines on two synthetic and three  real-world datasets establish the efficacy of our proposed approaches.",
        "bibtex": "@inproceedings{NIPS2011_31839b03,\n author = {Kumar, Abhishek and Rai, Piyush and Daume, Hal},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Co-regularized Multi-view Spectral Clustering},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/31839b036f63806cba3f47b93af8ccb5-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/31839b036f63806cba3f47b93af8ccb5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/31839b036f63806cba3f47b93af8ccb5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 117654,
        "gs_citation": 1504,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14863094102692736570&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Dept. of Computer Science, University of Maryland, College Park, MD; Dept. of Computer Science, University of Utah, Salt Lake City, UT; Dept. of Computer Science, University of Maryland, College Park, MD",
        "aff_domain": "cs.umd.edu;cs.utah.edu;umiacs.umd.edu",
        "email": "cs.umd.edu;cs.utah.edu;umiacs.umd.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Dept. of Computer Science, University of Maryland, College Park, MD;Dept. of Computer Science, University of Utah, Salt Lake City, UT",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "4eaa1b1fcd",
        "title": "Collective Graphical Models",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/fccb3cdc9acc14a6e70a12f74560c026-Abstract.html",
        "author": "Daniel R. Sheldon; Thomas G. Dietterich",
        "abstract": "There are many settings in which we wish to fit a model of the behavior of individuals but where our data consist only of aggregate information (counts or low-dimensional contingency tables).  This paper introduces Collective Graphical Models---a framework for modeling and probabilistic inference that operates directly on the sufficient statistics of the individual model.  We derive a highly-efficient Gibbs sampling algorithm for sampling from the posterior distribution of the sufficient statistics conditioned on noisy aggregate observations, prove its correctness, and demonstrate its effectiveness experimentally.",
        "bibtex": "@inproceedings{NIPS2011_fccb3cdc,\n author = {Sheldon, Daniel R and Dietterich, Thomas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Collective Graphical Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/fccb3cdc9acc14a6e70a12f74560c026-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/fccb3cdc9acc14a6e70a12f74560c026-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/fccb3cdc9acc14a6e70a12f74560c026-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/fccb3cdc9acc14a6e70a12f74560c026-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 270775,
        "gs_citation": 94,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11323304307256939892&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Oregon State University; Oregon State University",
        "aff_domain": "eecs.oregonstate.edu;eecs.oregonstate.edu",
        "email": "eecs.oregonstate.edu;eecs.oregonstate.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Oregon State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://oregonstate.edu",
        "aff_unique_abbr": "OSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "5d5f78c88f",
        "title": "Committing Bandits",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/d56b9fc4b0f1be8871f5e1c40c0067e7-Abstract.html",
        "author": "Loc X. Bui; Ramesh Johari; Shie Mannor",
        "abstract": "We consider a multi-armed bandit problem where there are two phases. The first phase is an experimentation phase where the decision maker is free to explore multiple options. In the second phase the decision maker has to commit to one of the arms and stick with it. Cost is incurred during both phases with a higher cost during the experimentation phase. We analyze the regret in this setup, and both propose algorithms and provide upper and lower bounds that depend on the ratio of the duration of the experimentation phase to the duration of the commitment phase. Our analysis reveals that if given the choice, it is optimal to experiment $\\Theta(\\ln T)$ steps and then commit, where $T$ is the time horizon.",
        "bibtex": "@inproceedings{NIPS2011_d56b9fc4,\n author = {Bui, Loc and Johari, Ramesh and Mannor, Shie},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Committing Bandits},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/d56b9fc4b0f1be8871f5e1c40c0067e7-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/d56b9fc4b0f1be8871f5e1c40c0067e7-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/d56b9fc4b0f1be8871f5e1c40c0067e7-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/d56b9fc4b0f1be8871f5e1c40c0067e7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 258880,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2562860917676049170&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "MS&E Department, Stanford University; MS&E Department, Stanford University; EE Department, Technion",
        "aff_domain": "stanford.edu;stanford.edu;ee.technion.ac.il",
        "email": "stanford.edu;stanford.edu;ee.technion.ac.il",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "MS&E Department, Stanford University;Technion",
        "aff_unique_dep": ";EE Department",
        "aff_unique_url": ";https://www.technion.ac.il",
        "aff_unique_abbr": ";Technion",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";Israel"
    },
    {
        "id": "d3c55599e8",
        "title": "Comparative Analysis of Viterbi Training and Maximum Likelihood Estimation for HMMs",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/c8c41c4a18675a74e01c8a20e8a0f662-Abstract.html",
        "author": "Armen Allahverdyan; Aram Galstyan",
        "abstract": "We present an asymptotic analysis of Viterbi Training (VT) and contrast it with a more conventional Maximum Likelihood (ML) approach to parameter estimation in Hidden Markov Models. While ML estimator works by (locally) maximizing the likelihood of the observed data, VT seeks to maximize the probability of the most likely hidden state sequence. We develop an analytical framework based on a generating function formalism and illustrate it on an exactly solvable model of HMM with one unambiguous symbol. For this particular model the ML objective function is continuously degenerate. VT objective, in contrast, is shown to have only finite degeneracy. Furthermore, VT converges faster and results in sparser (simpler) models, thus realizing an automatic Occam's razor for HMM learning. For more general scenario VT can be worse compared to ML but still capable of correctly recovering most of the parameters.",
        "bibtex": "@inproceedings{NIPS2011_c8c41c4a,\n author = {Allahverdyan, Armen and Galstyan, Aram},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Comparative Analysis of Viterbi Training and Maximum Likelihood Estimation for HMMs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/c8c41c4a18675a74e01c8a20e8a0f662-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/c8c41c4a18675a74e01c8a20e8a0f662-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/c8c41c4a18675a74e01c8a20e8a0f662-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/c8c41c4a18675a74e01c8a20e8a0f662-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 397254,
        "gs_citation": 55,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2317569453105669485&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Yerevan Physics Institute, Yerevan, Armenia+Laboratoire de Physique Statistique et Systemes Complexes, ISMANS, Le Mans, France; USC Information Sciences Institute, Marina del Rey, CA, USA",
        "aff_domain": "yerphi.am;isi.edu",
        "email": "yerphi.am;isi.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2",
        "aff_unique_norm": "Yerevan Physics Institute, Yerevan, Armenia;Laboratoire de Physique Statistique et Systemes Complexes, ISMANS, Le Mans, France;University of Southern California",
        "aff_unique_dep": ";;Information Sciences Institute",
        "aff_unique_url": ";;https://isi.usc.edu",
        "aff_unique_abbr": ";;USC ISI",
        "aff_campus_unique_index": ";1",
        "aff_campus_unique": ";Marina del Rey",
        "aff_country_unique_index": ";1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "0202a049d3",
        "title": "Complexity of Inference in Latent Dirichlet Allocation",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/3871bd64012152bfb53fdf04b401193f-Abstract.html",
        "author": "David Sontag; Dan Roy",
        "abstract": "We consider the computational complexity of probabilistic inference in Latent Dirichlet Allocation (LDA).  First, we study the problem of finding the maximum a posteriori (MAP) assignment of topics to words, where the document's topic distribution is integrated out.  We show that, when the effective number of topics per document is small, exact inference takes polynomial time. In contrast, we show that, when a document has a large number of topics, finding the MAP assignment of topics to words in LDA is NP-hard.  Next, we consider the problem of finding the MAP topic distribution for a document, where the topic-word assignments are integrated out.  We show that this problem is also NP-hard.  Finally, we briefly discuss the problem of sampling from the posterior, showing that this is NP-hard in one restricted setting, but leaving open the general question.",
        "bibtex": "@inproceedings{NIPS2011_3871bd64,\n author = {Sontag, David and Roy, Dan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Complexity of Inference in Latent Dirichlet Allocation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/3871bd64012152bfb53fdf04b401193f-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/3871bd64012152bfb53fdf04b401193f-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/3871bd64012152bfb53fdf04b401193f-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/3871bd64012152bfb53fdf04b401193f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 617182,
        "gs_citation": 141,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6286882403861549937&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "New York University; University of Cambridge",
        "aff_domain": "; ",
        "email": "; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "New York University;University of Cambridge",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.nyu.edu;https://www.cam.ac.uk",
        "aff_unique_abbr": "NYU;Cambridge",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "id": "029af92fb4",
        "title": "Composite Multiclass Losses",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/2afe4567e1bf64d32a5527244d104cea-Abstract.html",
        "author": "Elodie Vernet; Mark D. Reid; Robert C. Williamson",
        "abstract": "We consider loss functions for multiclass prediction problems. We   show when a  multiclass loss can be expressed as a",
        "bibtex": "@inproceedings{NIPS2011_2afe4567,\n author = {Vernet, Elodie and Reid, Mark D and Williamson, Robert C},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Composite Multiclass Losses},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/2afe4567e1bf64d32a5527244d104cea-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/2afe4567e1bf64d32a5527244d104cea-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/2afe4567e1bf64d32a5527244d104cea-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 419027,
        "gs_citation": 68,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15709832998865290104&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "ENS Cachan; ANU and NICTA; ANU and NICTA",
        "aff_domain": "ens-cachan.fr;anu.edu.au;anu.edu.au",
        "email": "ens-cachan.fr;anu.edu.au;anu.edu.au",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Ecole Normale Superieure de Cachan;ANU and NICTA",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ens-cachan.fr;",
        "aff_unique_abbr": "ENS Cachan;",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Cachan;",
        "aff_country_unique_index": "0",
        "aff_country_unique": "France;"
    },
    {
        "id": "c145409ced",
        "title": "Confidence Sets for Network Structure",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/a7d8ae4569120b5bec12e7b6e9648b86-Abstract.html",
        "author": "David S. Choi; Patrick J. Wolfe; Edoardo M. Airoldi",
        "abstract": "Latent variable models are frequently used to identify structure in dichotomous network data, in part because they give rise to a Bernoulli product likelihood that is both well understood and consistent with the notion of exchangeable random graphs.  In this article we propose conservative confidence sets that hold with respect to these underlying Bernoulli parameters as a function of any given partition of network nodes, enabling us to assess estimates of \\emph{residual} network structure, that is, structure that cannot be explained by known covariates and thus cannot be easily verified by manual inspection. We demonstrate the proposed methodology by analyzing student friendship networks from the National Longitudinal Survey of Adolescent Health that include race, gender, and school year as covariates.  We employ a stochastic expectation-maximization algorithm to fit a logistic regression model that includes these explanatory variables as well as a latent stochastic blockmodel component and additional node-specific effects.  Although maximum-likelihood estimates do not appear consistent in this context, we are able to evaluate confidence sets as a function of different blockmodel partitions, which enables us to qualitatively assess the significance of estimated residual network structure relative to a baseline, which models covariates but lacks block structure.",
        "bibtex": "@inproceedings{NIPS2011_a7d8ae45,\n author = {Choi, David and Wolfe, Patrick and Airoldi, Edo M},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Confidence Sets for Network Structure},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/a7d8ae4569120b5bec12e7b6e9648b86-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/a7d8ae4569120b5bec12e7b6e9648b86-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/a7d8ae4569120b5bec12e7b6e9648b86-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 460726,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6802148983783483837&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "School of Engineering and Applied Sciences, Harvard University; School of Engineering and Applied Sciences, Harvard University; Department of Statistics, Harvard University",
        "aff_domain": "seas.harvard.edu;seas.harvard.edu;fas.harvard.edu",
        "email": "seas.harvard.edu;seas.harvard.edu;fas.harvard.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Harvard University",
        "aff_unique_dep": "School of Engineering and Applied Sciences",
        "aff_unique_url": "https://www.harvard.edu",
        "aff_unique_abbr": "Harvard",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "4adc234df0",
        "title": "Contextual Gaussian Process Bandit Optimization",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/f3f1b7fc5a8779a9e618e1f23a7b7860-Abstract.html",
        "author": "Andreas Krause; Cheng S. Ong",
        "abstract": "How should we design experiments to maximize performance of a complex system, taking into account uncontrollable environmental conditions? How should we select relevant documents (ads) to display, given information about the user? These tasks can be formalized as contextual bandit problems, where at each round, we receive context (about the experimental conditions, the query), and have to choose an action (parameters, documents). The key challenge is to trade off exploration by gathering data for estimating the mean payoff function over the context-action space, and to exploit by choosing an action deemed optimal based on the gathered data. We model the payoff function as a sample from a Gaussian process defined over the joint context-action space, and develop CGP-UCB, an intuitive upper-confidence style algorithm. We show that by mixing and matching kernels for contexts and actions, CGP-UCB can handle a variety of practical applications. We further provide generic tools for deriving regret bounds when using such composite kernel functions. Lastly, we evaluate our algorithm on two case studies, in the context of automated vaccine design and sensor management. We show that context-sensitive optimization outperforms no or naive use of context.",
        "bibtex": "@inproceedings{NIPS2011_f3f1b7fc,\n author = {Krause, Andreas and Ong, Cheng},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Contextual Gaussian Process Bandit Optimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/f3f1b7fc5a8779a9e618e1f23a7b7860-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/f3f1b7fc5a8779a9e618e1f23a7b7860-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/f3f1b7fc5a8779a9e618e1f23a7b7860-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/f3f1b7fc5a8779a9e618e1f23a7b7860-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 508934,
        "gs_citation": 537,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6938634596907247599&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff": "Department of Computer Science, ETH Zurich; Department of Computer Science, ETH Zurich",
        "aff_domain": "ethz.ch;inf.ethz.ch",
        "email": "ethz.ch;inf.ethz.ch",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "ETH Zurich",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ethz.ch",
        "aff_unique_abbr": "ETHZ",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "f621570306",
        "title": "Continuous-Time Regression Models for Longitudinal Networks",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/674bfc5f6b72706fb769f5e93667bd23-Abstract.html",
        "author": "Duy Q. Vu; David Hunter; Padhraic Smyth; Arthur U. Asuncion",
        "abstract": "The development of statistical models for continuous-time longitudinal network data is of increasing interest in machine learning and social science. Leveraging ideas from survival and event history analysis, we introduce a continuous-time regression modeling framework for network event data that can incorporate both time-dependent network statistics and time-varying regression coefficients. We also develop an efficient inference scheme that allows our approach to scale to large networks. On synthetic and real-world data, empirical results demonstrate that the proposed inference approach can accurately estimate the coefficients of the regression model, which is useful for interpreting the evolution of the network; furthermore, the learned model has systematically better predictive performance compared to standard baseline methods.",
        "bibtex": "@inproceedings{NIPS2011_674bfc5f,\n author = {Vu, Duy and Hunter, David and Smyth, Padhraic and Asuncion, Arthur},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Continuous-Time Regression Models for Longitudinal Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/674bfc5f6b72706fb769f5e93667bd23-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/674bfc5f6b72706fb769f5e93667bd23-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/674bfc5f6b72706fb769f5e93667bd23-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 135986,
        "gs_citation": 124,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10935825313690375321&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Department of Statistics, Pennsylvania State University; Department of Computer Science, University of California, Irvine + Google Inc.; Department of Statistics, Pennsylvania State University; Department of Computer Science, University of California, Irvine",
        "aff_domain": "stat.psu.edu;ics.uci.edu;stat.psu.edu;ics.uci.edu",
        "email": "stat.psu.edu;ics.uci.edu;stat.psu.edu;ics.uci.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+2;0;1",
        "aff_unique_norm": "Pennsylvania State University;University of California, Irvine;Google",
        "aff_unique_dep": "Department of Statistics;Department of Computer Science;",
        "aff_unique_url": "https://www.psu.edu;https://www.uci.edu;https://www.google.com",
        "aff_unique_abbr": "PSU;UCI;Google",
        "aff_campus_unique_index": "1+2;1",
        "aff_campus_unique": ";Irvine;Mountain View",
        "aff_country_unique_index": "0;0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "212bda8a95",
        "title": "Convergence Rates of Inexact Proximal-Gradient Methods for Convex Optimization",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/8f7d807e1f53eff5f9efbe5cb81090fb-Abstract.html",
        "author": "Mark Schmidt; Nicolas L. Roux; Francis R. Bach",
        "abstract": "We consider the problem of optimizing the sum of a smooth convex function and a non-smooth convex function using proximal-gradient methods, where an error is present in the calculation of the gradient of the smooth term or in the proximity operator with respect to the second term. We show that the basic proximal-gradient method, the basic proximal-gradient method with a strong convexity assumption, and the accelerated proximal-gradient method achieve the same convergence rates as in the error-free case, provided the errors decrease at an appropriate rate.  Our experimental results on a structured sparsity problem indicate that sequences of errors with these appealing theoretical properties can lead to practical performance improvements.",
        "bibtex": "@inproceedings{NIPS2011_8f7d807e,\n author = {Schmidt, Mark and Roux, Nicolas and Bach, Francis},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Convergence Rates of Inexact Proximal-Gradient Methods for Convex Optimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/8f7d807e1f53eff5f9efbe5cb81090fb-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/8f7d807e1f53eff5f9efbe5cb81090fb-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/8f7d807e1f53eff5f9efbe5cb81090fb-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 358837,
        "gs_citation": 718,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8936781430634437833&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff": "\u00b4Ecole Normale Sup\u00e9rieure, Paris; INRIA - SIERRA Project Team; INRIA - SIERRA Project Team",
        "aff_domain": "inria.fr;le-roux.name;ens.fr",
        "email": "inria.fr;le-roux.name;ens.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "\u00b4Ecole Normale Sup\u00e9rieure, Paris;INRIA - SIERRA Project Team",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "798c7e6739",
        "title": "Convergent Bounds on the Euclidean Distance",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/16a5cdae362b8d27a1d8f8c7b78b4330-Abstract.html",
        "author": "Yoonho Hwang; Hee-kap Ahn",
        "abstract": "Given a set V of n vectors in d-dimensional space, we provide an efficient method  for computing quality upper and lower bounds of the Euclidean distances between  a pair of the vectors in V . For this purpose, we define a distance measure, called  the MS-distance, by using the mean and the standard deviation values of vectors in  V . Once we compute the mean and the standard deviation values of vectors in V in  O(dn) time, the MS-distance between them provides upper and lower bounds of  Euclidean distance between a pair of vectors in V in constant time. Furthermore,  these bounds can be refined further such that they converge monotonically to the  exact Euclidean distance within d refinement steps. We also provide an analysis on  a random sequence of refinement steps which can justify why MS-distance should  be refined to provide very tight bounds in a few steps of a typical sequence. The  MS-distance can be used to various problems where the Euclidean distance is used  to measure the proximity or similarity between objects. We provide experimental  results on the nearest and the farthest neighbor searches.",
        "bibtex": "@inproceedings{NIPS2011_16a5cdae,\n author = {Hwang, Yoonho and Ahn, Hee-kap},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Convergent Bounds on the Euclidean Distance},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/16a5cdae362b8d27a1d8f8c7b78b4330-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/16a5cdae362b8d27a1d8f8c7b78b4330-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/16a5cdae362b8d27a1d8f8c7b78b4330-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/16a5cdae362b8d27a1d8f8c7b78b4330-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 884384,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7266686995933910073&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science and Engineering, Pohang University of Science and Technology; Department of Computer Science and Engineering, Pohang University of Science and Technology",
        "aff_domain": "postech.ac.kr;postech.ac.kr",
        "email": "postech.ac.kr;postech.ac.kr",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Pohang University of Science and Technology",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.postech.ac.kr",
        "aff_unique_abbr": "POSTECH",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pohang",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "3ec67f88c0",
        "title": "Convergent Fitted Value Iteration with Linear Function Approximation",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/79a49b3e3762632813f9e35f4ba53d6c-Abstract.html",
        "author": "Daniel J. Lizotte",
        "abstract": "Fitted value iteration (FVI) with ordinary least squares regression is known to diverge. We present a new method, \"Expansion-Constrained Ordinary Least Squares\" (ECOLS), that produces a linear approximation but also guarantees convergence when used with FVI. To ensure convergence, we constrain the least squares regression operator to be a non-expansion in the infinity-norm. We show that the space of function approximators that satisfy this constraint is more rich than the space of \"averagers,\" we prove a minimax property of the ECOLS residual error, and we give an efficient algorithm for computing the coefficients of ECOLS based on constraint generation. We illustrate the algorithmic convergence of FVI with ECOLS in a suite of experiments, and discuss its properties.",
        "bibtex": "@inproceedings{NIPS2011_79a49b3e,\n author = {Lizotte, Daniel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Convergent Fitted Value Iteration with Linear Function Approximation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/79a49b3e3762632813f9e35f4ba53d6c-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/79a49b3e3762632813f9e35f4ba53d6c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/79a49b3e3762632813f9e35f4ba53d6c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 289033,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12944034728867535336&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, ON N2L 3G1 Canada",
        "aff_domain": "uwaterloo.ca",
        "email": "uwaterloo.ca",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, ON N2L 3G1 Canada",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "3de1649403",
        "title": "Crowdclustering",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/c86a7ee3d8ef0b551ed58e354a836f2b-Abstract.html",
        "author": "Ryan G. Gomes; Peter Welinder; Andreas Krause; Pietro Perona",
        "abstract": "Is it possible to crowdsource categorization? Amongst the challenges: (a) each annotator has only a partial view of the data, (b) different annotators may have different clustering criteria and may produce different numbers of categories, (c) the underlying category structure may be hierarchical. We propose a Bayesian model of how annotators may approach clustering and show how one may infer clusters/categories, as well as annotator parameters, using this model. Our experiments, carried out on large collections of images, suggest that Bayesian crowdclustering works well and may be superior to single-expert annotations.",
        "bibtex": "@inproceedings{NIPS2011_c86a7ee3,\n author = {Gomes, Ryan and Welinder, Peter and Krause, Andreas and Perona, Pietro},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Crowdclustering},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/c86a7ee3d8ef0b551ed58e354a836f2b-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/c86a7ee3d8ef0b551ed58e354a836f2b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/c86a7ee3d8ef0b551ed58e354a836f2b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1321254,
        "gs_citation": 232,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2019093996947890465&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Caltech; Caltech; ETH Zurich & Caltech; Caltech",
        "aff_domain": "vision.caltech.edu; ; ; ",
        "email": "vision.caltech.edu; ; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "California Institute of Technology;ETH Zurich & Caltech",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.caltech.edu;",
        "aff_unique_abbr": "Caltech;",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Pasadena;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "c7afdefd27",
        "title": "Data Skeletonization via Reeb Graphs",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/3a0772443a0739141292a5429b952fe6-Abstract.html",
        "author": "Xiaoyin Ge; Issam I. Safa; Mikhail Belkin; Yusu Wang",
        "abstract": "Recovering hidden structure from complex and noisy non-linear data is one of the most fundamental problems in machine learning and statistical inference. While such data is often high-dimensional, it is of interest to approximate it with a low-dimensional or even one-dimensional space, since  many important aspects of  data are often intrinsically low-dimensional.   Furthermore, there are many scenarios where the underlying structure is graph-like, e.g, river/road networks or various trajectories.   In this paper, we develop a framework to extract, as well as to simplify, a one-dimensional \"skeleton\" from unorganized data using the Reeb graph.   Our algorithm is very simple, does not require complex optimizations and can be easily applied to unorganized high-dimensional data such as point clouds or  proximity graphs.   It can also represent arbitrary graph structures in the data.  We also give  theoretical results to justify our method.  We  provide a number of experiments to demonstrate the effectiveness and generality of our algorithm, including comparisons to existing methods, such as principal curves.  We believe that the simplicity and practicality of our algorithm will help to promote skeleton graphs as a data analysis tool for a broad range of applications.",
        "bibtex": "@inproceedings{NIPS2011_3a077244,\n author = {Ge, Xiaoyin and Safa, Issam and Belkin, Mikhail and Wang, Yusu},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Data Skeletonization via Reeb Graphs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/3a0772443a0739141292a5429b952fe6-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/3a0772443a0739141292a5429b952fe6-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/3a0772443a0739141292a5429b952fe6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1406108,
        "gs_citation": 136,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7406744586470794605&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Computer Science and Engineering Department, The Ohio State University; Computer Science and Engineering Department, The Ohio State University; Computer Science and Engineering Department, The Ohio State University; Computer Science and Engineering Department, The Ohio State University",
        "aff_domain": "cse.ohio-state.edu;cse.ohio-state.edu;cse.ohio-state.edu;cse.ohio-state.edu",
        "email": "cse.ohio-state.edu;cse.ohio-state.edu;cse.ohio-state.edu;cse.ohio-state.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Computer Science and Engineering Department, The Ohio State University",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "af67a28f84",
        "title": "Demixed Principal Component Analysis",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/f4a331b7a22d1b237565d8813a34d8ac-Abstract.html",
        "author": "Wieland Brendel; Ranulfo Romo; Christian K. Machens",
        "abstract": "In many experiments, the data points collected live in high-dimensional observation spaces, yet can be assigned a set of labels or parameters. In electrophysiological recordings, for instance, the responses of populations of neurons generally depend on mixtures of experimentally controlled parameters. The heterogeneity and diversity of these parameter dependencies can make visualization and interpretation of such data extremely difficult. Standard dimensionality reduction techniques such as principal component analysis (PCA) can provide a succinct and complete description of the data, but the description is constructed independent of the relevant task variables and is often hard to interpret. Here, we start with the assumption that a particularly informative description is one that reveals the dependency of the high-dimensional data on the individual parameters. We show how to modify the loss function of PCA so that the principal components seek to capture both the maximum amount of variance about the data, while also depending on a minimum number of parameters. We call this method demixed principal component analysis (dPCA) as the principal components here segregate the parameter dependencies. We phrase the problem as a probabilistic graphical model, and present a fast Expectation-Maximization (EM) algorithm. We demonstrate the use of this algorithm for electrophysiological data and show that it serves to demix the parameter-dependence of a neural population response.",
        "bibtex": "@inproceedings{NIPS2011_f4a331b7,\n author = {Brendel, Wieland and Romo, Ranulfo and Machens, Christian K},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Demixed Principal Component Analysis},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/f4a331b7a22d1b237565d8813a34d8ac-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/f4a331b7a22d1b237565d8813a34d8ac-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/f4a331b7a22d1b237565d8813a34d8ac-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1115508,
        "gs_citation": 77,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12268464642064495747&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "d31eba19bf",
        "title": "Differentially Private M-Estimators",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/f718499c1c8cef6730f9fd03c8125cab-Abstract.html",
        "author": "Jing Lei",
        "abstract": "This paper studies privacy preserving M-estimators using perturbed histograms. The proposed approach allows the release of a  wide class of M-estimators with both differential privacy and statistical utility without knowing a priori the particular inference procedure. The performance of the proposed method is demonstrated through a careful study of the convergence rates. A practical algorithm is given and applied on a real world data set containing both continuous and categorical  variables.",
        "bibtex": "@inproceedings{NIPS2011_f718499c,\n author = {Lei, Jing},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Differentially Private M-Estimators},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/f718499c1c8cef6730f9fd03c8125cab-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/f718499c1c8cef6730f9fd03c8125cab-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/f718499c1c8cef6730f9fd03c8125cab-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/f718499c1c8cef6730f9fd03c8125cab-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 293541,
        "gs_citation": 162,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11667436434459214313&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Statistics, Carnegie Mellon University",
        "aff_domain": "andrew.cmu.edu",
        "email": "andrew.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a77cdedffc",
        "title": "Dimensionality Reduction Using the Sparse Linear Model",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/3644a684f98ea8fe223c713b77189a77-Abstract.html",
        "author": "Ioannis A. Gkioulekas; Todd Zickler",
        "abstract": "We propose an approach for linear unsupervised dimensionality reduction, based on the sparse linear model that has been used to probabilistically interpret sparse coding. We formulate an optimization problem for learning a linear projection from the original signal domain to a lower-dimensional one in a way that approximately preserves, in expectation, pairwise inner products in the sparse domain. We derive solutions to the problem, present nonlinear extensions, and discuss relations to compressed sensing. Our experiments using facial images, texture patches, and images of object categories suggest that the approach can improve our ability to recover meaningful structure in many classes of signals.",
        "bibtex": "@inproceedings{NIPS2011_3644a684,\n author = {Gkioulekas, Ioannis and Zickler, Todd},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Dimensionality Reduction Using the Sparse Linear Model},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/3644a684f98ea8fe223c713b77189a77-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/3644a684f98ea8fe223c713b77189a77-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/3644a684f98ea8fe223c713b77189a77-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/3644a684f98ea8fe223c713b77189a77-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1782559,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13652098726036890349&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Harvard SEAS; Harvard SEAS",
        "aff_domain": "seas.harvard.edu;seas.harvard.edu",
        "email": "seas.harvard.edu;seas.harvard.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Harvard University",
        "aff_unique_dep": "School of Engineering and Applied Sciences",
        "aff_unique_url": "https://seas.harvard.edu",
        "aff_unique_abbr": "SEAS",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "38a8048aa6",
        "title": "Directed Graph Embedding: an Algorithm based on Continuous Limits of Laplacian-type Operators",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/8ebda540cbcc4d7336496819a46a1b68-Abstract.html",
        "author": "Dominique C. Perrault-joncas; Marina Meila",
        "abstract": "This paper considers the problem of embedding directed graphs in Euclidean space while retaining directional information. We model the observed graph as a sample from a manifold endowed with a vector field, and we design an algo- rithm that separates and recovers the features of this process: the geometry of the manifold, the data density and the vector field. The algorithm is motivated by our analysis of Laplacian-type operators and their continuous limit as generators of diffusions on a manifold. We illustrate the recovery algorithm on both artificially constructed and real data.",
        "bibtex": "@inproceedings{NIPS2011_8ebda540,\n author = {Perrault-joncas, Dominique and Meila, Marina},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Directed Graph Embedding: an Algorithm based on Continuous Limits of Laplacian-type Operators},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/8ebda540cbcc4d7336496819a46a1b68-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/8ebda540cbcc4d7336496819a46a1b68-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/8ebda540cbcc4d7336496819a46a1b68-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 995418,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4096055973478623606&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Department of Statistics, University of Washington; Department of Statistics, University of Washington",
        "aff_domain": "stat.washington.edu;stat.washington.edu",
        "email": "stat.washington.edu;stat.washington.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Washington",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "https://www.washington.edu",
        "aff_unique_abbr": "UW",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Seattle",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "64a1186aa1",
        "title": "Distributed Delayed Stochastic Optimization",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/f0e52b27a7a5d6a1a87373dffa53dbe5-Abstract.html",
        "author": "Alekh Agarwal; John C. Duchi",
        "abstract": "We analyze the convergence of gradient-based optimization algorithms whose updates depend on delayed stochastic gradient information. The  main application of our results is to the development of distributed  minimization algorithms where a master node performs parameter updates while worker nodes compute stochastic gradients based on local information in parallel, which may give rise to delays due to  asynchrony. Our main contribution is to show that for smooth stochastic problems, the delays are asymptotically negligible. In  application to distributed optimization, we show $n$-node architectures whose optimization error in stochastic problems---in spite of asynchronous delays---scales asymptotically as $\\order(1 / \\sqrt{nT})$, which is known to be optimal even in the absence of delays.",
        "bibtex": "@inproceedings{NIPS2011_f0e52b27,\n author = {Agarwal, Alekh and Duchi, John C},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Distributed Delayed Stochastic Optimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/f0e52b27a7a5d6a1a87373dffa53dbe5-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/f0e52b27a7a5d6a1a87373dffa53dbe5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/f0e52b27a7a5d6a1a87373dffa53dbe5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 250066,
        "gs_citation": 798,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8950182686724532637&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Department of Electrical Engineering and Computer Science, University of California, Berkeley; Department of Electrical Engineering and Computer Science, University of California, Berkeley",
        "aff_domain": "eecs.berkeley.edu;eecs.berkeley.edu",
        "email": "eecs.berkeley.edu;eecs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "Department of Electrical Engineering and Computer Science",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "602e5e5e17",
        "title": "Divide-and-Conquer Matrix Factorization",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/5c04925674920eb58467fb52ce4ef728-Abstract.html",
        "author": "Lester W. Mackey; Michael I. Jordan; Ameet Talwalkar",
        "abstract": "This work introduces Divide-Factor-Combine (DFC), a parallel divide-and-conquer framework for noisy matrix factorization.  DFC divides a large-scale matrix factorization task into smaller subproblems, solves each subproblem in parallel using an arbitrary base matrix factorization algorithm, and combines the subproblem solutions using techniques from randomized matrix approximation.  Our experiments with collaborative filtering, video background modeling, and simulated data demonstrate the near-linear to super-linear speed-ups attainable with this approach.  Moreover, our analysis shows that DFC enjoys high-probability recovery guarantees comparable to those of its base algorithm.",
        "bibtex": "@inproceedings{NIPS2011_5c049256,\n author = {Mackey, Lester and Jordan, Michael and Talwalkar, Ameet},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Divide-and-Conquer Matrix Factorization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/5c04925674920eb58467fb52ce4ef728-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/5c04925674920eb58467fb52ce4ef728-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/5c04925674920eb58467fb52ce4ef728-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/5c04925674920eb58467fb52ce4ef728-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 713120,
        "gs_citation": 252,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=568473405679241332&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "37ef61766f",
        "title": "Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/3335881e06d4d23091389226225e17c7-Abstract.html",
        "author": "Richard Socher; Eric H. Huang; Jeffrey Pennin; Christopher D. Manning; Andrew Y. Ng",
        "abstract": "Paraphrase detection is the task of examining two sentences and determining whether they have the same meaning. In order to obtain high accuracy on this task, thorough syntactic and semantic analysis of the two statements is needed. We introduce a method for paraphrase detection based on recursive autoencoders (RAE). Our unsupervised RAEs are based on a novel unfolding objective and learn feature vectors for phrases in syntactic trees. These features are used to measure the word- and phrase-wise similarity between two sentences. Since sentences may be of arbitrary length, the resulting matrix of similarity measures is of variable size. We introduce a novel dynamic pooling layer which computes a fixed-sized representation from the variable-sized matrices. The pooled representation is then used as input to a classifier. Our method outperforms other state-of-the-art approaches on the challenging MSRP paraphrase corpus.",
        "bibtex": "@inproceedings{NIPS2011_3335881e,\n author = {Socher, Richard and Huang, Eric and Pennin, Jeffrey and Manning, Christopher D and Ng, Andrew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/3335881e06d4d23091389226225e17c7-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/3335881e06d4d23091389226225e17c7-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/3335881e06d4d23091389226225e17c7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 929237,
        "gs_citation": 1188,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16769505746668914526&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 20,
        "aff": "Computer Science Department, Stanford University; Computer Science Department, Stanford University + SLAC National Accelerator Laboratory, Stanford University; Computer Science Department, Stanford University; Computer Science Department, Stanford University; Computer Science Department, Stanford University",
        "aff_domain": "socher.org;stanford.edu;stanford.edu;stanford.edu;stanford.edu",
        "email": "socher.org;stanford.edu;stanford.edu;stanford.edu;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1;0;0;0",
        "aff_unique_norm": "Stanford University;SLAC National Accelerator Laboratory, Stanford University",
        "aff_unique_dep": "Computer Science Department;",
        "aff_unique_url": "https://www.stanford.edu;",
        "aff_unique_abbr": "Stanford;",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "17416771d8",
        "title": "Dynamical segmentation of single trials from population neural data",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/2d6cc4b2d139a53512fb8cbb3086ae2e-Abstract.html",
        "author": "Biljana Petreska; Byron M. Yu; John P. Cunningham; Gopal Santhanam; Stephen I. Ryu; Krishna V. Shenoy; Maneesh Sahani",
        "abstract": "Simultaneous recordings of many neurons embedded within a  recurrently-connected cortical network may provide concurrent views into the dynamical processes of that network, and thus its computational function.  In principle, these dynamics might be identified by purely unsupervised, statistical means.  Here, we show that a Hidden Switching Linear Dynamical Systems (HSLDS) model---in which multiple linear dynamical laws approximate a nonlinear and potentially non-stationary dynamical process---is able to distinguish different dynamical regimes within single-trial motor cortical activity associated with the preparation and initiation of hand movements.  The regimes are identified without reference to behavioural or experimental epochs, but nonetheless transitions between them correlate strongly with external events whose timing may vary from trial to trial.  The HSLDS model also performs better than recent comparable models in predicting the firing rate of an isolated neuron based on the firing rates of others, suggesting that it  captures more of the \"shared variance\" of the data.  Thus, the method is able to trace the dynamical processes underlying the coordinated evolution of network activity in a way that appears to reflect its computational role.",
        "bibtex": "@inproceedings{NIPS2011_2d6cc4b2,\n author = {Petreska, Biljana and Yu, Byron M and Cunningham, John P and Santhanam, Gopal and Ryu, Stephen and Shenoy, Krishna V and Sahani, Maneesh},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Dynamical segmentation of single trials from population neural data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/2d6cc4b2d139a53512fb8cbb3086ae2e-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/2d6cc4b2d139a53512fb8cbb3086ae2e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/2d6cc4b2d139a53512fb8cbb3086ae2e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 4121762,
        "gs_citation": 95,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5493567510880707052&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "b941f7f830",
        "title": "Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/beda24c1e1b46055dff2c39c98fd6fc1-Abstract.html",
        "author": "Philipp Kr\u00e4henb\u00fchl; Vladlen Koltun",
        "abstract": "Most state-of-the-art techniques for multi-class image segmentation and labeling use conditional random \ufb01elds de\ufb01ned over pixels or image regions. While region- level models often feature dense pairwise connectivity, pixel-level models are con- siderably larger and have only permitted sparse graph structures. In this paper, we consider fully connected CRF models de\ufb01ned on the complete set of pixels in an image. The resulting graphs have billions of edges, making traditional inference algorithms impractical. Our main contribution is a highly ef\ufb01cient approximate inference algorithm for fully connected CRF models in which the pairwise edge potentials are de\ufb01ned by a linear combination of Gaussian kernels. Our experi- ments demonstrate that dense connectivity at the pixel level substantially improves segmentation and labeling accuracy.",
        "bibtex": "@inproceedings{NIPS2011_beda24c1,\n author = {Kr\\\"{a}henb\\\"{u}hl, Philipp and Koltun, Vladlen},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/beda24c1e1b46055dff2c39c98fd6fc1-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/beda24c1e1b46055dff2c39c98fd6fc1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/beda24c1e1b46055dff2c39c98fd6fc1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 4047104,
        "gs_citation": 4311,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15966298905145781857&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 24,
        "aff": "Computer Science Department, Stanford University; Computer Science Department, Stanford University",
        "aff_domain": "cs.stanford.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "4473984bb7",
        "title": "Efficient Learning of Generalized Linear and Single Index Models with Isotonic Regression",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/30bb3825e8f631cc6075c0f87bb4978c-Abstract.html",
        "author": "Sham M. Kakade; Varun Kanade; Ohad Shamir; Adam Kalai",
        "abstract": "Generalized Linear Models (GLMs) and Single Index Models (SIMs) provide powerful generalizations of linear regression, where the target variable is assumed to be a (possibly unknown) 1-dimensional function of a linear predictor. In general, these problems entail non-convex estimation procedures, and, in practice, iterative local search heuristics are often used.  Kalai and Sastry (2009) provided the first provably efficient method, the \\emph{Isotron} algorithm, for learning SIMs and GLMs, under the assumption that the data is in fact generated under a GLM and under certain monotonicity and Lipschitz (bounded slope) constraints. The Isotron algorithm interleaves steps of perceptron-like updates with isotonic regression (fitting a one-dimensional non-decreasing function). However, to obtain provable performance, the method requires a fresh sample every iteration. In this paper, we provide algorithms for learning GLMs and SIMs, which are both computationally and statistically efficient. We modify the isotonic regression step in Isotron to fit a Lipschitz monotonic function, and also provide an efficient $O(n \\log(n))$ algorithm for this step, improving upon the previous $O(n^2)$ algorithm. We provide a brief empirical study, demonstrating the feasibility of our algorithms in practice.",
        "bibtex": "@inproceedings{NIPS2011_30bb3825,\n author = {Kakade, Sham M and Kanade, Varun and Shamir, Ohad and Kalai, Adam},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Efficient Learning of Generalized Linear and Single Index Models with Isotonic Regression},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/30bb3825e8f631cc6075c0f87bb4978c-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/30bb3825e8f631cc6075c0f87bb4978c-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/30bb3825e8f631cc6075c0f87bb4978c-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/30bb3825e8f631cc6075c0f87bb4978c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 277260,
        "gs_citation": 221,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18395952679148605471&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 24,
        "aff": "Microsoft Research and Wharton, U Penn; Microsoft Research; SEAS, Harvard University; Microsoft Research",
        "aff_domain": "microsoft.com;microsoft.com;fas.harvard.edu;microsoft.com",
        "email": "microsoft.com;microsoft.com;fas.harvard.edu;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;1",
        "aff_unique_norm": "Microsoft Research and Wharton, U Penn;Microsoft Corporation;Harvard University",
        "aff_unique_dep": ";Microsoft Research;School of Engineering and Applied Sciences",
        "aff_unique_url": ";https://www.microsoft.com/en-us/research;https://www.seas.harvard.edu",
        "aff_unique_abbr": ";MSR;SEAS",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "1;1;1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "198ebafb40",
        "title": "Efficient Methods for Overlapping Group Lasso",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/03c6b06952c750899bb03d998e631860-Abstract.html",
        "author": "Lei Yuan; Jun Liu; Jieping Ye",
        "abstract": "The group Lasso is an extension of the Lasso for feature selection on (predefined) non-overlapping groups of features. The non-overlapping group structure limits its applicability in practice. There have been several recent attempts to study a more general formulation, where groups of features are given, potentially with overlaps between the groups. The resulting optimization is, however, much more challenging to solve due to the group overlaps. In this paper, we consider the efficient optimization of the overlapping group Lasso penalized problem. We reveal several key properties of the proximal operator associated with the overlapping group Lasso, and compute the proximal operator by solving the smooth and convex dual problem, which allows the use of the gradient descent type of algorithms for the optimization. We have performed empirical evaluations using both synthetic and the breast cancer gene expression data set, which consists of 8,141 genes organized into (overlapping) gene sets. Experimental results show that the proposed algorithm is more efficient than existing state-of-the-art algorithms.",
        "bibtex": "@inproceedings{NIPS2011_03c6b069,\n author = {Yuan, Lei and Liu, Jun and Ye, Jieping},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Efficient Methods for Overlapping Group Lasso},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/03c6b06952c750899bb03d998e631860-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/03c6b06952c750899bb03d998e631860-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/03c6b06952c750899bb03d998e631860-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/03c6b06952c750899bb03d998e631860-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 173312,
        "gs_citation": 255,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13364877165119951738&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 23,
        "aff": "Arizona State University; Arizona State University; Arizona State University",
        "aff_domain": "asu.edu;asu.edu;asu.edu",
        "email": "asu.edu;asu.edu;asu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Arizona State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.asu.edu",
        "aff_unique_abbr": "ASU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "8c325ee8ce",
        "title": "Efficient Offline Communication Policies for Factored Multiagent POMDPs",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/e5e63da79fcd2bebbd7cb8bf1c1d0274-Abstract.html",
        "author": "Jo\u00e3o V. Messias; Matthijs Spaan; Pedro U. Lima",
        "abstract": "Factored Decentralized Partially Observable Markov Decision Processes (Dec-POMDPs) form a powerful framework for multiagent planning under uncertainty, but optimal solutions require a rigid history-based policy representation. In this paper we allow inter-agent communication which turns the problem in a centralized Multiagent POMDP (MPOMDP). We map belief distributions over state factors to an agent's local actions by exploiting structure in the joint MPOMDP policy.  The key point is that when sparse dependencies between the agents' decisions exist, often the belief over its local state factors is sufficient for an agent to unequivocally identify the optimal action, and communication can be avoided. We formalize these notions by casting the problem into convex optimization form, and present experimental results illustrating the savings in communication that we can obtain.",
        "bibtex": "@inproceedings{NIPS2011_e5e63da7,\n author = {Messias, Jo\\~{a}o and Spaan, Matthijs and Lima, Pedro},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Efficient Offline Communication Policies for Factored Multiagent POMDPs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/e5e63da79fcd2bebbd7cb8bf1c1d0274-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/e5e63da79fcd2bebbd7cb8bf1c1d0274-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/e5e63da79fcd2bebbd7cb8bf1c1d0274-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/e5e63da79fcd2bebbd7cb8bf1c1d0274-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 161434,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17827660658582377628&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "InstituteforSystemsandRobotics+InstitutoSuperiorT\u00b4ecnico; DelftUniversityofTechnology; InstituteforSystemsandRobotics+InstitutoSuperiorT\u00b4ecnico",
        "aff_domain": "isr.ist.utl.pt;tudelft.nl;isr.ist.utl.pt",
        "email": "isr.ist.utl.pt;tudelft.nl;isr.ist.utl.pt",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2;0+1",
        "aff_unique_norm": "InstituteforSystemsandRobotics;InstitutoSuperiorT\u00b4ecnico;DelftUniversityofTechnology",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": ";",
        "aff_country_unique": ""
    },
    {
        "id": "62bf9511a9",
        "title": "Efficient Online Learning via Randomized Rounding",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/3cec07e9ba5f5bb252d13f5f431e4bbb-Abstract.html",
        "author": "Nicol\u00f2 Cesa-bianchi; Ohad Shamir",
        "abstract": "Most online algorithms used in machine learning today are based on variants of mirror descent or follow-the-leader. In this paper, we present an online algorithm based on a completely different approach, which combines ``random playout'' and randomized rounding of loss subgradients. As an application of our approach, we provide the first computationally efficient online algorithm for collaborative filtering with trace-norm constrained matrices. As a second application, we solve an open question linking batch learning and transductive online learning.",
        "bibtex": "@inproceedings{NIPS2011_3cec07e9,\n author = {Cesa-bianchi, Nicol\\`{o} and Shamir, Ohad},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Efficient Online Learning via Randomized Rounding},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/3cec07e9ba5f5bb252d13f5f431e4bbb-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/3cec07e9ba5f5bb252d13f5f431e4bbb-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/3cec07e9ba5f5bb252d13f5f431e4bbb-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/3cec07e9ba5f5bb252d13f5f431e4bbb-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 331275,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18334570568687243121&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "DSI, Universit\u00e0 degli Studi di Milano, Italy; Microsoft Research New England, USA",
        "aff_domain": "unimi.it;microsoft.com",
        "email": "unimi.it;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "DSI, Universit\u00e0 degli Studi di Milano, Italy;Microsoft Research",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": ";https://www.microsoft.com/en-us/research/group/microsoft-research-new-england",
        "aff_unique_abbr": ";MSR NE",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";New England",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "8d84acd582",
        "title": "Efficient anomaly detection using bipartite k-NN graphs",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/8dd48d6a2e2cad213179a3992c0be53c-Abstract.html",
        "author": "Kumar Sricharan; Alfred O. Hero",
        "abstract": "Learning minimum volume sets of an underlying nominal distribution is a very effective approach to anomaly detection. Several approaches to learning minimum volume sets have been proposed in the literature, including the K-point nearest neighbor graph (K-kNNG) algorithm based on the geometric entropy minimization (GEM) principle [4]. The K-kNNG detector, while possessing several desirable characteristics, suffers from high computation complexity, and in [4] a simpler heuristic approximation, the leave-one-out kNNG (L1O-kNNG) was proposed. In this paper, we propose a novel bipartite k-nearest neighbor graph (BP-kNNG) anomaly detection scheme for estimating minimum volume sets. Our bipartite estimator retains all the desirable theoretical properties of the K-kNNG, while being computationally simpler than the K-kNNG and the surrogate L1O-kNNG detectors. We show that BP-kNNG is asymptotically consistent in recovering the p-value of each test point. Experimental results are given that illustrate the superior performance of BP-kNNG as compared to the L1O-kNNG and other state of the art anomaly detection schemes.",
        "bibtex": "@inproceedings{NIPS2011_8dd48d6a,\n author = {Sricharan, Kumar and Hero, Alfred},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Efficient anomaly detection using bipartite k-NN graphs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/8dd48d6a2e2cad213179a3992c0be53c-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/8dd48d6a2e2cad213179a3992c0be53c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/8dd48d6a2e2cad213179a3992c0be53c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 101480,
        "gs_citation": 64,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10332956460519526357&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of EECS, University of Michigan, Ann Arbor, MI 48104; Department of EECS, University of Michigan, Ann Arbor, MI 48104",
        "aff_domain": "umich.edu;umich.edu",
        "email": "umich.edu;umich.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Department of EECS, University of Michigan, Ann Arbor, MI 48104",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "e283798d4d",
        "title": "Efficient coding of natural images with a population of noisy Linear-Nonlinear neurons",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/12e59a33dea1bf0630f46edfe13d6ea2-Abstract.html",
        "author": "Yan Karklin; Eero P. Simoncelli",
        "abstract": "Efficient coding provides a powerful principle for explaining early sensory coding. Most attempts to test this principle have been limited to linear, noiseless models, and when applied to natural images, have yielded oriented filters consistent with responses in primary visual cortex. Here we show that an efficient coding model that incorporates biologically realistic ingredients  input and output noise, nonlinear response functions, and a metabolic cost on the firing rate  predicts receptive fields and response nonlinearities similar to those observed in the retina. Specifically, we develop numerical methods for simultaneously learning the linear filters and response nonlinearities of a population of model neurons, so as to maximize information transmission subject to metabolic costs. When applied to an ensemble of natural images, the method yields filters that are center-surround and nonlinearities that are rectifying. The filters are organized into two populations, with On- and Off-centers, which independently tile the visual space. As observed in the primate retina, the Off-center neurons are more numerous and have filters with smaller spatial extent. In the absence of noise, our method reduces to a generalized version of independent components analysis, with an adapted nonlinear \"contrast\" function; in this case, the optimal filters are localized and oriented.",
        "bibtex": "@inproceedings{NIPS2011_12e59a33,\n author = {Karklin, Yan and Simoncelli, Eero},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Efficient coding of natural images with a population of noisy Linear-Nonlinear neurons},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/12e59a33dea1bf0630f46edfe13d6ea2-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/12e59a33dea1bf0630f46edfe13d6ea2-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/12e59a33dea1bf0630f46edfe13d6ea2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 251947,
        "gs_citation": 127,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18133494484953003738&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "Howard Hughes Medical Institute and Center for Neural Science, New York University; Howard Hughes Medical Institute and Center for Neural Science, New York University",
        "aff_domain": "nyu.edu;nyu.edu",
        "email": "nyu.edu;nyu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Howard Hughes Medical Institute and Center for Neural Science, New York University",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "848703b6f4",
        "title": "Efficient inference in matrix-variate Gaussian models with \\iid observation noise",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/a732804c8566fc8f498947ea59a841f8-Abstract.html",
        "author": "Oliver Stegle; Christoph Lippert; Joris M. Mooij; Neil D. Lawrence; Karsten Borgwardt",
        "abstract": "Inference in matrix-variate Gaussian models has major applications for multi- output prediction and joint learning of row and column covariances from matrix- variate data. Here, we discuss an approach for ef\ufb01cient inference in such models that explicitly account for iid observation noise. Computational tractability can be retained by exploiting the Kronecker product between row and column covariance matrices. Using this framework, we show how to generalize the Graphical Lasso in order to learn a sparse inverse covariance between features while accounting for a low-rank confounding covariance between samples. We show practical utility on applications to biology, where we model covariances with more than 100,000 di- mensions. We \ufb01nd greater accuracy in recovering biological network structures and are able to better reconstruct the confounders.",
        "bibtex": "@inproceedings{NIPS2011_a732804c,\n author = {Stegle, Oliver and Lippert, Christoph and Mooij, Joris M and Lawrence, Neil and Borgwardt, Karsten},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Efficient inference in matrix-variate Gaussian models with \\textbackslash iid observation noise},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/a732804c8566fc8f498947ea59a841f8-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/a732804c8566fc8f498947ea59a841f8-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/a732804c8566fc8f498947ea59a841f8-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/a732804c8566fc8f498947ea59a841f8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1497453,
        "gs_citation": 122,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1710136635353562695&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "Max Planck Institutes, T\u00a8ubingen, Germany; Max Planck Institutes, T\u00a8ubingen, Germany; Institute for Computing and Information Sciences, Radboud University, Nijmegen, The Netherlands; Department of Computer Science, University of Shef\ufb01eld, Shef\ufb01eld, UK; Max Planck Institutes + Eberhard Karls Universit \u00a8at, T\u00a8ubingen, Germany",
        "aff_domain": "tuebingen.mpg.de;tuebingen.mpg.de;cs.ru.nl;sheffield.ac.uk;tuebingen.mpg.de",
        "email": "tuebingen.mpg.de;tuebingen.mpg.de;cs.ru.nl;sheffield.ac.uk;tuebingen.mpg.de",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2;3+4",
        "aff_unique_norm": "Max Planck Institutes, T\u00a8ubingen, Germany;Institute for Computing and Information Sciences, Radboud University, Nijmegen, The Netherlands;Department of Computer Science, University of Shef\ufb01eld, Shef\ufb01eld, UK;Max Planck Institutes;Eberhard Karls Universit \u00a8at, T\u00a8ubingen, Germany",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": ";;;;",
        "aff_unique_abbr": ";;;;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "9a8272ead4",
        "title": "EigenNet: A Bayesian hybrid of generative and conditional models for sparse learning",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/c5cc17e395d3049b03e0f1ccebb02b4d-Abstract.html",
        "author": "Feng Yan; Yuan Qi",
        "abstract": "For many real-world applications, we often need to select correlated variables---such as genetic variations and imaging features associated with Alzheimer's disease---in a high dimensional space. The correlation between variables presents a challenge to classical variable selection methods. To address this challenge, the elastic net has been developed and successfully applied to many applications. Despite its great success, the elastic net does not exploit the correlation information embedded in the data to select correlated variables. To overcome this limitation, we present a novel hybrid model, EigenNet, that uses the eigenstructures of data to guide variable selection. Specifically, it integrates a sparse conditional classification model with a generative model capturing variable correlations in a principled Bayesian framework. We develop an efficient active-set algorithm to estimate the model via evidence maximization. Experiments on synthetic data and imaging genetics data demonstrated the superior predictive performance of the EigenNet over the lasso, the elastic net, and the automatic relevance determination.",
        "bibtex": "@inproceedings{NIPS2011_c5cc17e3,\n author = {Yan, Feng and Qi, Yuan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {EigenNet: A Bayesian hybrid of generative and conditional models for sparse learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/c5cc17e395d3049b03e0f1ccebb02b4d-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/c5cc17e395d3049b03e0f1ccebb02b4d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/c5cc17e395d3049b03e0f1ccebb02b4d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 334791,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12281066558155268202&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Computer Science and Statistics Depts., Purdue University; Computer Science Dept., Purdue University",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Computer Science and Statistics Depts., Purdue University;Computer Science Dept., Purdue University",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "9483d48259",
        "title": "Emergence of Multiplication in a Biophysical Model of a Wide-Field Visual Neuron for Computing Object Approaches: Dynamics, Peaks, & Fits",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/01386bd6d8e091c2ab4c7c7de644d37b-Abstract.html",
        "author": "Matthias S. Keil",
        "abstract": "Many species show avoidance reactions in response to looming object approaches.  In locusts, the corresponding escape behavior correlates with the activity  of the lobula giant movement detector (LGMD) neuron.  During an object approach,  its firing rate was reported to gradually increase until a peak is reached,  and then it declines quickly.  The $\\eta$-function predicts that the LGMD activity  is a product between an exponential function of angular size $\\exp(-\\Theta)$ and  angular velocity $\\dot{\\Theta}$, and that peak activity is reached before time-to-contact  (ttc).  The $\\eta$-function has become the prevailing LGMD model because it  reproduces many experimental observations, and even experimental evidence for  the multiplicative operation was reported.  Several inconsistencies remain  unresolved, though.  Here we address these issues with a new model ($\\psi$-model),  which explicitly connects $\\Theta$ and $\\dot{\\Theta}$ to biophysical quantities.  The $\\psi$-model avoids biophysical problems associated with implementing  $\\exp(\\cdot)$, implements the multiplicative operation of $\\eta$ via divisive  inhibition, and explains why activity peaks could occur after ttc.  It consistently  predicts response features of the LGMD, and provides excellent fits to published  experimental data, with goodness of fit measures comparable to corresponding  fits with the $\\eta$-function.",
        "bibtex": "@inproceedings{NIPS2011_01386bd6,\n author = {Keil, Matthias},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Emergence of Multiplication in a Biophysical Model of a Wide-Field Visual Neuron for Computing Object Approaches: Dynamics, Peaks, \\&amp; Fits},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/01386bd6d8e091c2ab4c7c7de644d37b-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/01386bd6d8e091c2ab4c7c7de644d37b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/01386bd6d8e091c2ab4c7c7de644d37b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 507165,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14026884704051306934&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "f23d51a3be",
        "title": "Empirical models of spiking in neural populations",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/7143d7fbadfa4693b9eec507d9d37443-Abstract.html",
        "author": "Jakob H. Macke; Lars Buesing; John P. Cunningham; Byron M. Yu; Krishna V. Shenoy; Maneesh Sahani",
        "abstract": "Neurons in the neocortex code and compute as part of a locally interconnected population. Large-scale multi-electrode recording makes it possible to access these population processes empirically by fitting statistical models to unaveraged data. What statistical structure best describes the concurrent spiking of cells within  a local network? We argue that in the cortex, where firing exhibits extensive correlations in both time and space and where a typical sample of neurons still reflects only a very small fraction of the local population, the most appropriate model captures shared variability by a low-dimensional latent process evolving with smooth dynamics, rather than by putative direct coupling. We test this claim by comparing  a latent dynamical model with realistic spiking observations to coupled generalised  linear spike-response models (GLMs) using cortical recordings. We find that the latent dynamical approach outperforms the GLM in terms of goodness-of-fit, and reproduces the temporal correlations in the data more accurately. We also compare models whose observations models are either derived from a Gaussian or point-process models, finding that the non-Gaussian model provides slightly  better goodness-of-fit and more realistic population spike counts.",
        "bibtex": "@inproceedings{NIPS2011_7143d7fb,\n author = {Macke, Jakob H and Buesing, Lars and Cunningham, John P and Yu, Byron M and Shenoy, Krishna V and Sahani, Maneesh},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Empirical models of spiking in neural populations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/7143d7fbadfa4693b9eec507d9d37443-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/7143d7fbadfa4693b9eec507d9d37443-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/7143d7fbadfa4693b9eec507d9d37443-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 293697,
        "gs_citation": 304,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11789815298086937420&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff": "Gatsby Computational Neuroscience Unit, University College London, UK; Gatsby Computational Neuroscience Unit, University College London, UK; Department of Engineering, University of Cambridge, UK; ECE and BME, Carnegie Mellon University; Department of Electrical Engineering, Stanford University; Gatsby Computational Neuroscience Unit, University College London, UK",
        "aff_domain": "gatsby.ucl.ac.uk;gatsby.ucl.ac.uk;cam.ac.uk;cmu.edu;stanford.edu;gatsby.ucl.ac.uk",
        "email": "gatsby.ucl.ac.uk;gatsby.ucl.ac.uk;cam.ac.uk;cmu.edu;stanford.edu;gatsby.ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2;3;0",
        "aff_unique_norm": "University College London;University of Cambridge;ECE and BME, Carnegie Mellon University;Stanford University",
        "aff_unique_dep": "Gatsby Computational Neuroscience Unit;Department of Engineering;;Department of Electrical Engineering",
        "aff_unique_url": "https://www.ucl.ac.uk;https://www.cam.ac.uk;;https://www.stanford.edu",
        "aff_unique_abbr": "UCL;Cambridge;;Stanford",
        "aff_campus_unique_index": "0;0;1;3;0",
        "aff_campus_unique": "London;Cambridge;;Stanford",
        "aff_country_unique_index": "0;0;0;2;0",
        "aff_country_unique": "United Kingdom;;United States"
    },
    {
        "id": "28f51cb1b8",
        "title": "Energetically Optimal Action Potentials",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/b6a1085a27ab7bff7550f8a3bd017df8-Abstract.html",
        "author": "Martin B. Stemmler; Biswa Sengupta; Simon Laughlin; Jeremy Niven",
        "abstract": "Most action potentials in the nervous system take on the form of strong, rapid, and brief voltage deflections known as spikes, in stark contrast to other action potentials, such as in the heart, that are characterized by broad voltage plateaus. We derive the shape of the neuronal action potential from first principles, by postulating that action potential generation is strongly constrained by the brain's need to minimize energy expenditure. For a given height of an action potential, the least energy is consumed when the underlying currents obey the bang-bang principle: the currents giving rise to the spike should be intense, yet short-lived, yielding  spikes with sharp onsets and offsets. Energy optimality predicts features in the biophysics that are not per se required for producing the characteristic neuronal action potential:  sodium currents should be extraordinarily powerful and inactivate with voltage; both potassium and sodium currents should have kinetics that have a bell-shaped voltage-dependence; and the cooperative action of multiple `gates'  should start the flow of current.",
        "bibtex": "@inproceedings{NIPS2011_b6a1085a,\n author = {Stemmler, Martin and Sengupta, Biswa and Laughlin, Simon and Niven, Jeremy},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Energetically Optimal Action Potentials},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/b6a1085a27ab7bff7550f8a3bd017df8-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/b6a1085a27ab7bff7550f8a3bd017df8-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/b6a1085a27ab7bff7550f8a3bd017df8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1168453,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16531266887073429624&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "354e2b790a",
        "title": "Environmental statistics and the trade-off between model-based and TD learning in humans",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/c9e1074f5b3f9fc8ea15d152add07294-Abstract.html",
        "author": "Dylan A. Simon; Nathaniel D. Daw",
        "abstract": "There is much evidence that humans and other animals utilize a combination of model-based and model-free RL methods.  Although it has been proposed that these systems may dominate according to their relative statistical efficiency in different circumstances, there is little specific evidence -- especially in humans -- as to the details of this trade-off.  Accordingly, we examine the relative performance of different RL approaches under situations in which the statistics of reward are differentially noisy and volatile.  Using theory and simulation, we show that model-free TD learning is relatively most disadvantaged in cases of high volatility and low noise.  We present data from a decision-making experiment manipulating these parameters, showing that humans shift learning strategies in accord with these predictions.  The statistical circumstances favoring model-based RL are also those that promote a high learning rate, which helps explain why, in psychology, the distinction between these strategies is traditionally conceived in terms of rule-based vs. incremental learning.",
        "bibtex": "@inproceedings{NIPS2011_c9e1074f,\n author = {Simon, Dylan and Daw, Nathaniel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Environmental statistics and the trade-off between model-based and TD learning in humans},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/c9e1074f5b3f9fc8ea15d152add07294-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/c9e1074f5b3f9fc8ea15d152add07294-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/c9e1074f5b3f9fc8ea15d152add07294-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/c9e1074f5b3f9fc8ea15d152add07294-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 386104,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2342197730103196023&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Psychology, New York University; Center for Neural Science and Department of Psychology, New York University",
        "aff_domain": "nyu.edu;nyu.edu",
        "email": "nyu.edu;nyu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "New York University;Center for Neural Science and Department of Psychology, New York University",
        "aff_unique_dep": "Department of Psychology;",
        "aff_unique_url": "https://www.nyu.edu;",
        "aff_unique_abbr": "NYU;",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "New York;",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "6252512409",
        "title": "Estimating time-varying input signals and ion channel states from a single voltage trace of a neuron",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/fc221309746013ac554571fbd180e1c8-Abstract.html",
        "author": "Ryota Kobayashi; Yasuhiro Tsubo; Petr Lansky; Shigeru Shinomoto",
        "abstract": "State-of-the-art statistical methods in neuroscience have enabled us to fit mathematical models to experimental data and subsequently to infer the dynamics of hidden parameters underlying the observable phenomena. Here, we develop a Bayesian method for inferring the time-varying mean and variance of the synaptic input, along with the dynamics of each ion channel from a single voltage trace of a neuron. An estimation problem may be formulated on the basis of the state-space model with prior distributions that penalize large fluctuations in these parameters. After optimizing the hyperparameters by maximizing the marginal likelihood, the state-space model provides the time-varying parameters of the input signals and the ion channel states. The proposed method is tested not only on the simulated data from the Hodgkin-Huxley type models but also on experimental data obtained from a cortical slice in vitro.",
        "bibtex": "@inproceedings{NIPS2011_fc221309,\n author = {Kobayashi, Ryota and Tsubo, Yasuhiro and Lansky, Petr and Shinomoto, Shigeru},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Estimating time-varying input signals and ion channel states from a single voltage trace of a neuron},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/fc221309746013ac554571fbd180e1c8-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/fc221309746013ac554571fbd180e1c8-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/fc221309746013ac554571fbd180e1c8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1131385,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1342302651035861&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Human and Computer Intelligence, Ritsumeikan University; Laboratory for Neural Circuit Theory, Brain Science Institute, RIKEN; Institute of Physiology, Academy of Sciences of the Czech Republic; Department of Physics, Kyoto University",
        "aff_domain": "cns.ci.ritsumei.ac.jp;riken.jp;biomed.cas.cz;scphys.kyoto-u.ac.jp",
        "email": "cns.ci.ritsumei.ac.jp;riken.jp;biomed.cas.cz;scphys.kyoto-u.ac.jp",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "Department of Human and Computer Intelligence, Ritsumeikan University;Laboratory for Neural Circuit Theory, Brain Science Institute, RIKEN;Institute of Physiology, Academy of Sciences of the Czech Republic;Department of Physics, Kyoto University",
        "aff_unique_dep": ";;;",
        "aff_unique_url": ";;;",
        "aff_unique_abbr": ";;;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "654ff8a246",
        "title": "Evaluating the inverse decision-making approach to preference learning",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/1e51e0f3b6b60070219ccb91bb619a6b-Abstract.html",
        "author": "Alan Jern; Christopher G. Lucas; Charles Kemp",
        "abstract": "Psychologists have recently begun to develop computational accounts of how people infer others' preferences from their behavior. The inverse decision-making approach proposes that people infer preferences by inverting a generative model of decision-making. Existing data sets, however, do not provide sufficient resolution to thoroughly evaluate this approach. We introduce a new preference learning task that provides a benchmark for evaluating computational accounts and use it to compare the inverse decision-making approach to a feature-based approach, which relies on a discriminative combination of decision features. Our data support the inverse decision-making approach to preference learning. A basic principle of decision-making is that knowing people's preferences allows us to predict how they will behave: if you know your friend likes comedies and hates horror films, you can probably guess which of these options she will choose when she goes to the theater. Often, however, we do not know what other people like and we can only infer their preferences from their behavior. If you know that a different friend saw a comedy today, does that mean that he likes comedies in general? The conclusion you draw will likely depend on what else was playing and what movie choices he has made in the past. A goal for social cognition research is to develop a computational account of people's ability to infer others' preferences. One computational approach is based on inverse decision-making. This approach begins with a model of how someone's preferences lead to a decision. Then, this model is inverted to determine the most likely preferences that motivated an observed decision. An alternative approach might simply learn a functional mapping between features of an observed decision and the preferences that motivated it. For instance, in your friend's decision to see a comedy, perhaps the more movie options he turned down, the more likely it is that he has a true preference for comedies. The difference between the inverse decision-making approach and the feature-based approach maps onto the standard dichotomy between generative and discriminative models. Economists have developed an instance of the inverse decision-making approach known as the multinomial logit model [1] that has been widely used to infer consumer's preferences from their choices. This model has recently been explored as a psychological model [2, 3, 4], but there are few behavioral data sets for evaluating it as a model of how people learn others' preferences. Additionally, the data sets that do exist tend to be drawn from the developmental literature, which focuses on simple tasks that collect only one or two judgments from children [5, 6, 7]. The limitations of these data sets make it difficult to evaluate the multinomial logit model with respect to alternative accounts of preference learning like the feature-based approach. In this paper, we use data from a new experimental task that elicits a detailed set of preference judgments from a single participant in order to evaluate the predictions of several preference learning models from both the inverse decision-making and feature-based classes. Our task requires each participant to sort a large number of observed decisions on the basis of how strongly they indicate 1",
        "bibtex": "@inproceedings{NIPS2011_1e51e0f3,\n author = {Jern, Alan and Lucas, Christopher and Kemp, Charles},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Evaluating the inverse decision-making approach to preference learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/1e51e0f3b6b60070219ccb91bb619a6b-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/1e51e0f3b6b60070219ccb91bb619a6b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/1e51e0f3b6b60070219ccb91bb619a6b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 319975,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13450987979539257465&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Department of Psychology, Carnegie Mellon University; Department of Psychology, Carnegie Mellon University; Department of Psychology, Carnegie Mellon University",
        "aff_domain": "cmu.edu;andrew.cmu.edu;cmu.edu",
        "email": "cmu.edu;andrew.cmu.edu;cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Department of Psychology, Carnegie Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "39b035185a",
        "title": "Exploiting spatial overlap to efficiently compute appearance distances between image windows",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/729c68884bd359ade15d5f163166738a-Abstract.html",
        "author": "Bogdan Alexe; Viviana Petrescu; Vittorio Ferrari",
        "abstract": "We present a computationally efficient technique to compute the distance of high-dimensional appearance descriptor vectors between image windows.  The method exploits the relation between appearance distance and spatial overlap.  We derive an upper bound on appearance distance given the spatial overlap of two windows in an image,  and use it to bound the distances of many pairs between two images.  We propose algorithms that build on these basic operations to efficiently solve tasks relevant to many computer vision applications, such as finding all pairs of windows between two images with distance smaller than a threshold,  or finding the single pair with the smallest distance. In experiments on the PASCAL VOC 07 dataset, our algorithms accurately solve these problems while greatly reducing the number of appearance distances computed, and achieve larger speedups than approximate nearest neighbour algorithms based on trees [18]and on hashing [21].    For example, our algorithm finds the most similar pair of windows between two images while computing only 1% of all distances on average.",
        "bibtex": "@inproceedings{NIPS2011_729c6888,\n author = {Alexe, Bogdan and Petrescu, Viviana and Ferrari, Vittorio},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Exploiting spatial overlap to efficiently compute appearance distances between image windows},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/729c68884bd359ade15d5f163166738a-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/729c68884bd359ade15d5f163166738a-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/729c68884bd359ade15d5f163166738a-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/729c68884bd359ade15d5f163166738a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 363654,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9138361788530619236&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "028356b893",
        "title": "Expressive Power and Approximation Errors of Restricted Boltzmann Machines",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/8e98d81f8217304975ccb23337bb5761-Abstract.html",
        "author": "Guido F. Montufar; Johannes Rauh; Nihat Ay",
        "abstract": "We present explicit classes of probability distributions that can be learned by Restricted Boltzmann Machines (RBMs) depending on the number of units that they contain, and which are representative for the expressive power of the model. We use this to show that the maximal Kullback-Leibler divergence to the RBM model with n visible and m hidden units is bounded from above by (n-1)-log(m+1). In this way we can specify the number of hidden units that guarantees a sufficiently rich model containing different classes of distributions and respecting a given error tolerance.",
        "bibtex": "@inproceedings{NIPS2011_8e98d81f,\n author = {Montufar, Guido F and Rauh, Johannes and Ay, Nihat},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Expressive Power and Approximation Errors of Restricted Boltzmann Machines},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/8e98d81f8217304975ccb23337bb5761-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/8e98d81f8217304975ccb23337bb5761-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/8e98d81f8217304975ccb23337bb5761-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 693131,
        "gs_citation": 62,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15978604348924923635&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Max Planck Institute for Mathematics in the Sciences; Max Planck Institute for Mathematics in the Sciences; Max Planck Institute for Mathematics in the Sciences + Santa Fe Institute",
        "aff_domain": "mis.mpg.de;mis.mpg.de;mis.mpg.de",
        "email": "mis.mpg.de;mis.mpg.de;mis.mpg.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0+1",
        "aff_unique_norm": "Max Planck Institute for Mathematics in the Sciences;Santa Fe Institute",
        "aff_unique_dep": "Mathematics;",
        "aff_unique_url": "https://www.mis.mpg.de;https://www.santafe.edu",
        "aff_unique_abbr": "MPI MIS;SFI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+1",
        "aff_country_unique": "Germany;United States"
    },
    {
        "id": "cd5db207f5",
        "title": "Extracting Speaker-Specific Information with a Regularized Siamese Deep Network",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/45fbc6d3e05ebd93369ce542e8f2322d-Abstract.html",
        "author": "Ke Chen; Ahmad Salman",
        "abstract": "Speech conveys different yet mixed information ranging from linguistic to speaker-specific components, and each of them should be exclusively used in a specific task. However, it is extremely difficult to extract a specific information component given the fact that nearly all existing acoustic representations carry all types of speech information. Thus, the use of the same representation in both speech and speaker recognition hinders a system from producing better performance due to interference of irrelevant information. In this paper, we present a deep neural architecture to extract speaker-specific information from MFCCs. As a result, a multi-objective loss function is proposed for learning speaker-specific characteristics and regularization via normalizing interference of non-speaker related information and avoiding information loss. With LDC benchmark corpora and a Chinese speech corpus, we demonstrate that a resultant speaker-specific representation is insensitive to text/languages spoken and environmental mismatches and hence outperforms MFCCs and other state-of-the-art techniques in speaker recognition. We discuss relevant issues and relate our approach to previous work.",
        "bibtex": "@inproceedings{NIPS2011_45fbc6d3,\n author = {Chen, Ke and Salman, Ahmad},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Extracting Speaker-Specific Information with a Regularized Siamese Deep Network},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/45fbc6d3e05ebd93369ce542e8f2322d-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/45fbc6d3e05ebd93369ce542e8f2322d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/45fbc6d3e05ebd93369ce542e8f2322d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/45fbc6d3e05ebd93369ce542e8f2322d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 725612,
        "gs_citation": 118,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18282015806294566026&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "9f49525a2b",
        "title": "Facial Expression Transfer with Input-Output Temporal Restricted Boltzmann Machines",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/d2ed45a52bc0edfa11c2064e9edee8bf-Abstract.html",
        "author": "Matthew D. Zeiler; Graham W. Taylor; Leonid Sigal; Iain Matthews; Rob Fergus",
        "abstract": "We present a type of Temporal Restricted Boltzmann Machine that defines a probability distribution over an output sequence conditional on an input sequence. It shares the desirable properties of RBMs: efficient exact inference, an exponentially more expressive latent state than HMMs, and the ability to model nonlinear structure and dynamics. We apply our model to a challenging real-world graphics problem: facial expression transfer. Our results demonstrate improved performance over several baselines modeling high-dimensional 2D and 3D data.",
        "bibtex": "@inproceedings{NIPS2011_d2ed45a5,\n author = {Zeiler, Matthew and Taylor, Graham W and Sigal, Leonid and Matthews, Iain and Fergus, Rob},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Facial Expression Transfer with Input-Output Temporal Restricted Boltzmann Machines},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/d2ed45a52bc0edfa11c2064e9edee8bf-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/d2ed45a52bc0edfa11c2064e9edee8bf-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/d2ed45a52bc0edfa11c2064e9edee8bf-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2439008,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12413422640212739150&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Department of Computer Science, New York University; Department of Computer Science, New York University; Disney Research; Disney Research; Department of Computer Science, New York University",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;1;0",
        "aff_unique_norm": "New York University;Disney Research",
        "aff_unique_dep": "Department of Computer Science;",
        "aff_unique_url": "https://www.nyu.edu;https://research.disney.com",
        "aff_unique_abbr": "NYU;Disney Research",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "New York;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "dc61dfd8a9",
        "title": "Fast and Accurate k-means For Large Datasets",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/52c670999cdef4b09eb656850da777c4-Abstract.html",
        "author": "Michael Shindler; Alex Wong; Adam W. Meyerson",
        "abstract": "Clustering is a popular problem with many applications. We consider the k-means problem in the situation where the data is too large to be stored in main memory and must be accessed sequentially, such as from a disk, and where we must use as little memory as possible. Our algorithm is based on recent theoretical results, with significant improvements to make it practical. Our approach greatly simpli(cid:173) fies a recently developed algorithm, both in design and in analysis, and eliminates large constant factors in the approximation guarantee, the memory requirements, and the running time. We then incorporate approximate nearest neighbor search to compute k-means in o(nk) (where n is the number of data points; note that com(cid:173) puting the cost, given a solution, takes 8(nk) time). We show that our algorithm compares favorably to existing algorithms - both theoretically and experimentally, thus providing state-of-the-art performance in both theory and practice.",
        "bibtex": "@inproceedings{NIPS2011_52c67099,\n author = {Shindler, Michael and Wong, Alex and Meyerson, Adam},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Fast and Accurate k-means For Large Datasets},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/52c670999cdef4b09eb656850da777c4-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/52c670999cdef4b09eb656850da777c4-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/52c670999cdef4b09eb656850da777c4-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/52c670999cdef4b09eb656850da777c4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1212595,
        "gs_citation": 230,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5530185829552099710&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "School of EECS, Oregon State University; Department of Computer Science, UCLA; Google, Inc.",
        "aff_domain": "eecs.oregonstate.edu;seas.ucla.edu;google.com",
        "email": "eecs.oregonstate.edu;seas.ucla.edu;google.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Oregon State University;University of California, Los Angeles;Google",
        "aff_unique_dep": "School of Electrical Engineering and Computer Science;Department of Computer Science;",
        "aff_unique_url": "https://eecs.oregonstate.edu;https://www.ucla.edu;https://www.google.com",
        "aff_unique_abbr": "OSU;UCLA;Google",
        "aff_campus_unique_index": "0;1;2",
        "aff_campus_unique": "Corvallis;Los Angeles;Mountain View",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "4b55b210b5",
        "title": "Fast and Balanced: Efficient Label Tree Learning for Large Scale Object Recognition",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/5a4b25aaed25c2ee1b74de72dc03c14e-Abstract.html",
        "author": "Jia Deng; Sanjeev Satheesh; Alexander C. Berg; Fei Li",
        "abstract": "We present a novel approach to efficiently learn a label tree for large scale classification with many classes. The key contribution of the approach is a technique to simultaneously determine the structure of the tree and learn the classifiers for each node in the tree. This approach also allows fine grained control over the efficiency vs accuracy trade-off in designing a label tree, leading to more balanced trees. Experiments are performed on large scale image classification with 10184 classes and 9 million images. We demonstrate significant improvements in test accuracy and efficiency with less training time and more balanced trees compared to the previous state of the art by Bengio et al.",
        "bibtex": "@inproceedings{NIPS2011_5a4b25aa,\n author = {Deng, Jia and Satheesh, Sanjeev and Berg, Alexander and Li, Fei},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Fast and Balanced: Efficient Label Tree Learning for Large Scale Object Recognition},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/5a4b25aaed25c2ee1b74de72dc03c14e-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/5a4b25aaed25c2ee1b74de72dc03c14e-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/5a4b25aaed25c2ee1b74de72dc03c14e-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/5a4b25aaed25c2ee1b74de72dc03c14e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 536604,
        "gs_citation": 260,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6058053951519985222&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 20,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "1858c7af79",
        "title": "Finite Time Analysis of Stratified Sampling for Monte Carlo",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/872488f88d1b2db54d55bc8bba2fad1b-Abstract.html",
        "author": "Alexandra Carpentier; R\u00e9mi Munos",
        "abstract": "We consider the problem of stratified sampling for Monte-Carlo integration. We model this problem in a multi-armed bandit setting, where the arms represent the strata, and the goal is to estimate a weighted average of the mean values of the arms. We propose a strategy that samples the arms according to an upper bound on their standard deviations and compare its estimation quality to an ideal allocation that would know the standard deviations of the arms. We provide two regret analyses: a distribution-dependent bound O(n^{-3/2}) that depends on a measure of the disparity of the arms, and a distribution-free bound O(n^{-4/3}) that does not. To the best of our knowledge, such a finite-time analysis is new for this problem.",
        "bibtex": "@inproceedings{NIPS2011_872488f8,\n author = {Carpentier, Alexandra and Munos, R\\'{e}mi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Finite Time Analysis of Stratified Sampling for Monte Carlo},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/872488f88d1b2db54d55bc8bba2fad1b-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/872488f88d1b2db54d55bc8bba2fad1b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/872488f88d1b2db54d55bc8bba2fad1b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 309349,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2668210851616553858&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "INRIA Lille-Nord Europe; INRIA Lille-Nord Europe",
        "aff_domain": "inria.fr;inria.fr",
        "email": "inria.fr;inria.fr",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "INRIA Lille-Nord Europe",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "fb8a09a163",
        "title": "From Bandits to Experts: On the Value of Side-Observations",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/e1e32e235eee1f970470a3a6658dfdd5-Abstract.html",
        "author": "Shie Mannor; Ohad Shamir",
        "abstract": "We consider an adversarial online learning setting where a decision maker can choose an action in every stage of the game. In addition to observing the reward of the chosen action, the decision maker gets side observations on the reward he would have obtained had he chosen some of the other actions. The observation structure is encoded as a graph, where node i is linked to  node j if sampling i provides information on the reward of j. This setting naturally interpolates between the well-known ``experts'' setting, where the decision maker can view all rewards, and the multi-armed bandits setting, where the decision maker can only view the reward of the chosen action. We develop practical algorithms with provable regret guarantees, which depend on non-trivial graph-theoretic properties of the information feedback structure. We also provide partially-matching lower bounds.",
        "bibtex": "@inproceedings{NIPS2011_e1e32e23,\n author = {Mannor, Shie and Shamir, Ohad},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {From Bandits to Experts: On the Value of Side-Observations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/e1e32e235eee1f970470a3a6658dfdd5-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/e1e32e235eee1f970470a3a6658dfdd5-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/e1e32e235eee1f970470a3a6658dfdd5-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/e1e32e235eee1f970470a3a6658dfdd5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 345100,
        "gs_citation": 266,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14555180025900109072&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Electrical Engineering, Technion, Israel; Microsoft Research New England, USA",
        "aff_domain": "ee.technion.ac.il;microsoft.com",
        "email": "ee.technion.ac.il;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Technion;Microsoft Research",
        "aff_unique_dep": "Department of Electrical Engineering;Microsoft Research",
        "aff_unique_url": "https://www.technion.ac.il;https://www.microsoft.com/en-us/research/group/microsoft-research-new-england",
        "aff_unique_abbr": "Technion;MSR NE",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";New England",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Israel;United States"
    },
    {
        "id": "051ec4b5d0",
        "title": "From Stochastic Nonlinear Integrate-and-Fire to Generalized Linear Models",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/82489c9737cc245530c7a6ebef3753ec-Abstract.html",
        "author": "Skander Mensi; Richard Naud; Wulfram Gerstner",
        "abstract": "Variability in single neuron models is typically implemented either by a stochastic Leaky-Integrate-and-Fire model or by a model of the Generalized Linear Model (GLM) family. We use analytical and numerical methods to relate state-of-the-art models from both schools of thought. First we find the analytical expressions relating the subthreshold voltage from the Adaptive Exponential Integrate-and-Fire model (AdEx) to the Spike-Response Model with escape noise (SRM as an example of a GLM). Then we calculate numerically the link-function that provides the firing probability given a deterministic membrane potential. We find a mathematical expression for this link-function and test the ability of the GLM to predict the firing probability of a neuron receiving complex stimulation. Comparing the prediction performance of various link-functions, we find that a GLM with an exponential link-function provides an excellent approximation to the Adaptive Exponential Integrate-and-Fire with colored-noise input. These results help to understand the relationship between the different approaches to stochastic neuron models.",
        "bibtex": "@inproceedings{NIPS2011_82489c97,\n author = {Mensi, Skander and Naud, Richard and Gerstner, Wulfram},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {From Stochastic Nonlinear Integrate-and-Fire to Generalized Linear Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/82489c9737cc245530c7a6ebef3753ec-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/82489c9737cc245530c7a6ebef3753ec-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/82489c9737cc245530c7a6ebef3753ec-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 633891,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4799537149898888836&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "School of Computer and Communication Sciences and Brain-Mind Institute, Ecole Polytechnique Federale de Lausanne, 1015 Lausanne EPFL, SWITZERLAND; School of Computer and Communication Sciences and Brain-Mind Institute, Ecole Polytechnique Federale de Lausanne, 1015 Lausanne EPFL, SWITZERLAND; School of Computer and Communication Sciences and Brain-Mind Institute, Ecole Polytechnique Federale de Lausanne, 1015 Lausanne EPFL, SWITZERLAND",
        "aff_domain": "epfl.ch;epfl.ch;epfl.ch",
        "email": "epfl.ch;epfl.ch;epfl.ch",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "School of Computer and Communication Sciences and Brain-Mind Institute, Ecole Polytechnique Federale de Lausanne, 1015 Lausanne EPFL, SWITZERLAND",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "a00ff309e1",
        "title": "Gaussian Process Training with Input Noise",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/a8e864d04c95572d1aece099af852d0a-Abstract.html",
        "author": "Andrew Mchutchon; Carl E. Rasmussen",
        "abstract": "In standard Gaussian Process regression input locations are assumed to be noise free. We present a simple yet effective GP model for training on input points corrupted by i.i.d. Gaussian noise. To make computations tractable we use a local linear expansion about each input point. This allows the input noise to be recast as output noise proportional to the squared gradient of the GP posterior mean. The input noise variances are inferred from the data as extra hyperparameters. They are trained alongside other hyperparameters by the usual method of maximisation of the marginal likelihood. Training uses an iterative scheme, which alternates between optimising the hyperparameters and calculating the posterior gradient. Analytic predictive moments can then be found for Gaussian distributed test points. We compare our model to others over a range of different regression problems and show that it improves over current methods.",
        "bibtex": "@inproceedings{NIPS2011_a8e864d0,\n author = {Mchutchon, Andrew and Rasmussen, Carl},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Gaussian Process Training with Input Noise},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/a8e864d04c95572d1aece099af852d0a-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/a8e864d04c95572d1aece099af852d0a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/a8e864d04c95572d1aece099af852d0a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 479105,
        "gs_citation": 357,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14611802419472243692&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Engineering, Cambridge University; Department of Engineering, Cambridge University",
        "aff_domain": "cam.ac.uk;cam.ac.uk",
        "email": "cam.ac.uk;cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Department of Engineering, Cambridge University",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "37a4816311",
        "title": "Gaussian process modulated renewal processes",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/ff49cc40a8890e6a60f40ff3026d2730-Abstract.html",
        "author": "Yee W. Teh; Vinayak Rao",
        "abstract": "Renewal processes are generalizations of the Poisson process on the real line, whose intervals are drawn i.i.d. from some distribution. Modulated renewal processes allow these distributions to vary with time, allowing the introduction nonstationarity. In this work, we take a nonparametric Bayesian approach, modeling this nonstationarity with a Gaussian process. Our approach is based on the idea of uniformization, allowing us to draw exact samples from an otherwise intractable distribution. We develop a novel and efficient MCMC sampler for posterior inference. In our experiments, we test these on a number of synthetic and real datasets.",
        "bibtex": "@inproceedings{NIPS2011_ff49cc40,\n author = {Teh, Yee and Rao, Vinayak},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Gaussian process modulated renewal processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/ff49cc40a8890e6a60f40ff3026d2730-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/ff49cc40a8890e6a60f40ff3026d2730-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/ff49cc40a8890e6a60f40ff3026d2730-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/ff49cc40a8890e6a60f40ff3026d2730-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 337987,
        "gs_citation": 64,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8593513479822897309&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Gatsby Computational Neuroscience Unit, University College London; Gatsby Computational Neuroscience Unit, University College London",
        "aff_domain": "gatsby.ucl.ac.uk;gatsby.ucl.ac.uk",
        "email": "gatsby.ucl.ac.uk;gatsby.ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University College London",
        "aff_unique_dep": "Gatsby Computational Neuroscience Unit",
        "aff_unique_url": "https://www.ucl.ac.uk",
        "aff_unique_abbr": "UCL",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "London",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "4818898011",
        "title": "Generalised Coupled Tensor Factorisation",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/82c2559140b95ccda9c6ca4a8b981f1e-Abstract.html",
        "author": "Kenan Y. Y\u0131lmaz; Ali T. Cemgil; Umut Simsekli",
        "abstract": "We derive algorithms for generalised tensor factorisation (GTF) by building upon the well-established theory of Generalised Linear Models. Our algorithms are general in the sense that we can compute arbitrary factorisations in a message passing framework, derived for a broad class of exponential family distributions including special cases such as Tweedie's distributions corresponding to $\\beta$-divergences. By bounding the step size of the Fisher Scoring iteration of the GLM, we obtain general updates for real data and multiplicative updates for non-negative data. The GTF framework is, then extended easily to address the problems when multiple observed tensors are factorised simultaneously. We illustrate our coupled factorisation approach on synthetic data as well as on a musical audio restoration problem.",
        "bibtex": "@inproceedings{NIPS2011_82c25591,\n author = {Y\\i lmaz, Kenan and Cemgil, Ali and Simsekli, Umut},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Generalised Coupled Tensor Factorisation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/82c2559140b95ccda9c6ca4a8b981f1e-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/82c2559140b95ccda9c6ca4a8b981f1e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/82c2559140b95ccda9c6ca4a8b981f1e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 285581,
        "gs_citation": 128,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10845619156589267945&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Engineering, Bo\u02d8gazic\u00b8i University, Istanbul, Turkey; Department of Computer Engineering, Bo\u02d8gazic\u00b8i University, Istanbul, Turkey; Department of Computer Engineering, Bo\u02d8gazic\u00b8i University, Istanbul, Turkey",
        "aff_domain": "sibnet.com.tr;boun.edu.tr;boun.edu.tr",
        "email": "sibnet.com.tr;boun.edu.tr;boun.edu.tr",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Department of Computer Engineering, Bo\u02d8gazic\u00b8i University, Istanbul, Turkey",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "614276e6dc",
        "title": "Generalization Bounds and Consistency for Latent Structural Probit and Ramp Loss",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/3a029f04d76d32e79367c4b3255dda4d-Abstract.html",
        "author": "Joseph Keshet; David A. McAllester",
        "abstract": "We consider latent structural versions of probit loss and ramp loss. We show that  these surrogate loss functions are consistent in the strong sense that for any feature map  (finite or infinite dimensional) they yield predictors approaching the infimum  task loss achievable by any linear predictor over the given features.  We also give  finite sample generalization bounds (convergence rates) for these loss functions.  These bounds suggest that probit loss converges more rapidly.  However, ramp loss is more easily optimized and may ultimately  be more practical.",
        "bibtex": "@inproceedings{NIPS2011_3a029f04,\n author = {Keshet, Joseph and McAllester, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Generalization Bounds and Consistency for Latent Structural Probit and Ramp Loss},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/3a029f04d76d32e79367c4b3255dda4d-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/3a029f04d76d32e79367c4b3255dda4d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/3a029f04d76d32e79367c4b3255dda4d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 188148,
        "gs_citation": 75,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11900861072691765490&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "TTI-Chicago; TTI-Chicago",
        "aff_domain": "ttic.edu;ttic.edu",
        "email": "ttic.edu;ttic.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Toyota Technological Institute at Chicago",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.tti-chicago.org",
        "aff_unique_abbr": "TTI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Chicago",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "54bb90ed85",
        "title": "Generalized Beta Mixtures of Gaussians",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/ad972f10e0800b49d76fed33a21f6698-Abstract.html",
        "author": "Artin Armagan; Merlise Clyde; David B. Dunson",
        "abstract": "In recent years, a rich variety of shrinkage priors have been proposed that have great promise in addressing massive regression problems.  In general, these new priors can be expressed as scale mixtures of normals, but have more complex forms and better properties than traditional Cauchy and double exponential priors. We first propose a new class of normal scale mixtures through a novel generalized beta distribution that encompasses many interesting priors as special cases.  This encompassing framework should prove useful in comparing competing priors, considering properties and revealing close connections. We then develop a class of variational Bayes approximations through the new hierarchy presented that will scale more efficiently to the types of truly massive data sets that are now encountered routinely.",
        "bibtex": "@inproceedings{NIPS2011_ad972f10,\n author = {Armagan, Artin and Clyde, Merlise and Dunson, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Generalized Beta Mixtures of Gaussians},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/ad972f10e0800b49d76fed33a21f6698-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/ad972f10e0800b49d76fed33a21f6698-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/ad972f10e0800b49d76fed33a21f6698-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1558994,
        "gs_citation": 216,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11738280281914780124&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Dept. of Statistical Science, Duke University; Dept. of Statistical Science, Duke University; Dept. of Statistical Science, Duke University",
        "aff_domain": "stat.duke.edu;stat.duke.edu;stat.duke.edu",
        "email": "stat.duke.edu;stat.duke.edu;stat.duke.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Duke University",
        "aff_unique_dep": "Department of Statistical Science",
        "aff_unique_url": "https://www.duke.edu",
        "aff_unique_abbr": "Duke",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b2b129adc0",
        "title": "Generalized Lasso based Approximation of Sparse Coding for Visual Recognition",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/0f28b5d49b3020afeecd95b4009adf4c-Abstract.html",
        "author": "Nobuyuki Morioka; Shin'ichi Satoh",
        "abstract": "Sparse coding, a method of explaining sensory data with as few dictionary bases as possible, has attracted much attention in computer vision. For visual object category recognition, L1 regularized sparse coding is combined with spatial pyramid representation to obtain state-of-the-art performance. However, because of its iterative optimization, applying sparse coding onto every local feature descriptor extracted from an image database can become a major bottleneck. To overcome this computational challenge, this paper presents \"Generalized Lasso based Approximation of Sparse coding\" (GLAS). By representing the distribution of sparse coefficients with slice transform, we fit a piece-wise linear mapping function with generalized lasso. We also propose an efficient post-refinement procedure to perform mutual inhibition between bases which is essential for an overcomplete setting. The experiments show that GLAS obtains comparable performance to L1 regularized sparse coding, yet achieves significant speed up demonstrating its effectiveness for large-scale visual recognition problems.",
        "bibtex": "@inproceedings{NIPS2011_0f28b5d4,\n author = {Morioka, Nobuyuki and Satoh, Shin\\textquotesingle ichi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Generalized Lasso based Approximation of Sparse Coding for Visual Recognition},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/0f28b5d49b3020afeecd95b4009adf4c-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/0f28b5d49b3020afeecd95b4009adf4c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/0f28b5d49b3020afeecd95b4009adf4c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 923515,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7047031500333500424&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "The University of New South Wales & NICTA; National Institute of Informatics",
        "aff_domain": "cse.unsw.edu.au;nii.ac.jp",
        "email": "cse.unsw.edu.au;nii.ac.jp",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "The University of New South Wales & NICTA;National Institute of Informatics",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://www.nii.ac.jp/",
        "aff_unique_abbr": ";NII",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";Japan"
    },
    {
        "id": "66edbdcdde",
        "title": "Generalizing from Several Related Classification Tasks to a New Unlabeled Sample",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/b571ecea16a9824023ee1af16897a582-Abstract.html",
        "author": "Gilles Blanchard; Gyemin Lee; Clayton Scott",
        "abstract": "We consider the problem of assigning class labels to an unlabeled test  data set, given several labeled training data sets drawn from similar  distributions. This problem arises in several applications where data  distributions fluctuate because of biological, technical, or other sources  of variation. We develop a distribution-free, kernel-based approach to the  problem. This approach involves identifying an appropriate reproducing     kernel Hilbert space and optimizing a regularized empirical risk over the  space.  We present generalization error analysis, describe universal  kernels, and establish universal consistency of the proposed methodology.  Experimental results on flow cytometry data are presented.",
        "bibtex": "@inproceedings{NIPS2011_b571ecea,\n author = {Blanchard, Gilles and Lee, Gyemin and Scott, Clayton},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Generalizing from Several Related Classification Tasks to a New Unlabeled Sample},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/b571ecea16a9824023ee1af16897a582-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/b571ecea16a9824023ee1af16897a582-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/b571ecea16a9824023ee1af16897a582-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/b571ecea16a9824023ee1af16897a582-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 229296,
        "gs_citation": 588,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7537988530832609467&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Universit \u00a8at Potsdam; University of Michigan; University of Michigan",
        "aff_domain": "math.uni-potsdam.de;umich.edu;umich.edu",
        "email": "math.uni-potsdam.de;umich.edu;umich.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Universit \u00a8at Potsdam;University of Michigan",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://www.umich.edu",
        "aff_unique_abbr": ";UM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "0528163c03",
        "title": "Global Solution of Fully-Observed Variational Bayesian Matrix Factorization is Column-Wise Independent",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/b73ce398c39f506af761d2277d853a92-Abstract.html",
        "author": "Shinichi Nakajima; Masashi Sugiyama; S. D. Babacan",
        "abstract": "Variational Bayesian matrix factorization (VBMF) efficiently   approximates the posterior distribution of factorized matrices   by assuming matrix-wise independence of the two factors.  A recent study on fully-observed VBMF  showed that, under a stronger assumption that the two factorized matrices are  column-wise independent,  the global optimal solution can be analytically computed.  However, it was not clear how restrictive  the column-wise independence assumption is.  In this paper, we prove that the global solution under matrix-wise  independence is actually column-wise independent,  implying that the column-wise independence assumption is harmless.  A practical consequence of our theoretical finding is that  the global solution under matrix-wise independence (which is a standard setup)  can be obtained analytically in a computationally very efficient way  without any iterative algorithms.  We experimentally illustrate advantages of using our analytic solution  in probabilistic principal component analysis.",
        "bibtex": "@inproceedings{NIPS2011_b73ce398,\n author = {Nakajima, Shinichi and Sugiyama, Masashi and Babacan, S.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Global Solution of Fully-Observed Variational Bayesian Matrix Factorization is Column-Wise Independent},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/b73ce398c39f506af761d2277d853a92-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/b73ce398c39f506af761d2277d853a92-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/b73ce398c39f506af761d2277d853a92-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 208436,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2101782588515537551&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Nikon Corporation, Tokyo, 140-8601, Japan; Tokyo Institute of Technology, Tokyo 152-8552, Japan; University of Illinois at Urbana-Champaign, Urbana, IL 61801, USA",
        "aff_domain": "nikon.co.jp;cs.titech.ac.jp;illinois.edu",
        "email": "nikon.co.jp;cs.titech.ac.jp;illinois.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Nikon Corporation;Tokyo Institute of Technology;University of Illinois at Urbana-Champaign",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.nikon.com;https://www.titech.ac.jp;https://illinois.edu",
        "aff_unique_abbr": "Nikon;Titech;UIUC",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Tokyo;Urbana",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "Japan;United States"
    },
    {
        "id": "c8c69297bf",
        "title": "Greedy Algorithms for Structurally Constrained High Dimensional Problems",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/ffeabd223de0d4eacb9a3e6e53e5448d-Abstract.html",
        "author": "Ambuj Tewari; Pradeep K. Ravikumar; Inderjit S. Dhillon",
        "abstract": "A hallmark of modern machine learning is its ability to deal with high dimensional problems by exploiting structural assumptions that limit the degrees of freedom in the underlying model.  A deep understanding of the capabilities and limits of high dimensional learning methods under specific assumptions such as sparsity, group sparsity, and low rank has been attained.  Efforts (Negahban et al., 2010, Chandrasekaran et al., 2010} are now underway to distill this valuable experience by proposing general unified frameworks that can achieve the twin goals of summarizing previous analyses and enabling their application to notions of structure hitherto unexplored. Inspired by these developments, we propose and analyze a general computational scheme based on a greedy strategy to solve convex optimization problems that arise when dealing with structurally constrained high-dimensional problems. Our framework not only unifies existing greedy algorithms by recovering them as special cases but also yields novel ones. Finally, we extend our results to infinite dimensional problems by using interesting connections between smoothness of norms and behavior of martingales in Banach spaces.",
        "bibtex": "@inproceedings{NIPS2011_ffeabd22,\n author = {Tewari, Ambuj and Ravikumar, Pradeep and Dhillon, Inderjit},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Greedy Algorithms for Structurally Constrained High Dimensional Problems},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/ffeabd223de0d4eacb9a3e6e53e5448d-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/ffeabd223de0d4eacb9a3e6e53e5448d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/ffeabd223de0d4eacb9a3e6e53e5448d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/ffeabd223de0d4eacb9a3e6e53e5448d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 381149,
        "gs_citation": 119,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9776328970324913251&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Computer Science, University of Texas at Austin; Department of Computer Science, University of Texas at Austin; Department of Computer Science, University of Texas at Austin",
        "aff_domain": "cs.utexas.edu;cs.utexas.edu;cs.utexas.edu",
        "email": "cs.utexas.edu;cs.utexas.edu;cs.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "28db8b312e",
        "title": "Greedy Model Averaging",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/aba3b6fd5d186d28e06ff97135cade7f-Abstract.html",
        "author": "Dong Dai; Tong Zhang",
        "abstract": "This paper considers the problem of combining multiple models to achieve a prediction accuracy not much worse than that of the best single model for least squares regression.  It is known that if the models are mis-specified, model averaging is superior to model selection.  Specifically, let $n$ be the sample size, then the worst case regret of the former decays at the rate of $O(1/n)$   while the worst case regret of the latter decays at the rate of $O(1/\\sqrt{n})$.  In the literature, the most important and widely studied model averaging method that achieves the optimal $O(1/n)$ average regret   is the exponential weighted model averaging (EWMA) algorithm. However this method suffers from several limitations.   The purpose of this paper is to present a new greedy model averaging procedure that improves EWMA.  We prove strong theoretical guarantees for the new procedure and illustrate our theoretical results with empirical examples.",
        "bibtex": "@inproceedings{NIPS2011_aba3b6fd,\n author = {Dai, Dong and Zhang, Tong},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Greedy Model Averaging},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/aba3b6fd5d186d28e06ff97135cade7f-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/aba3b6fd5d186d28e06ff97135cade7f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/aba3b6fd5d186d28e06ff97135cade7f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 239522,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5572553290230307407&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Statistics Rutgers University, New Jersey, 08816; Department of Statistics, Rutgers University, New Jersey, 08816",
        "aff_domain": "gmail.com;stat.rutgers.edu",
        "email": "gmail.com;stat.rutgers.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Department of Statistics Rutgers University, New Jersey, 08816;Department of Statistics, Rutgers University, New Jersey, 08816",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "e96f218df6",
        "title": "Group Anomaly Detection using Flexible Genre Models",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/eaae339c4d89fc102edd9dbdb6a28915-Abstract.html",
        "author": "Liang Xiong; Barnab\u00e1s P\u00f3czos; Jeff G. Schneider",
        "abstract": "An important task in exploring and analyzing real-world data sets is to detect unusual and interesting phenomena. In this paper, we study the group anomaly detection problem. Unlike traditional anomaly detection research that focuses on data points, our goal is to discover anomalous aggregated behaviors of groups of points. For this purpose, we propose the Flexible Genre Model (FGM). FGM is designed to characterize data groups at both the point level and the group level so as to detect various types of group anomalies. We evaluate the effectiveness of FGM on both synthetic and real data sets including images and turbulence data, and show that it is superior to existing approaches in detecting group anomalies.",
        "bibtex": "@inproceedings{NIPS2011_eaae339c,\n author = {Xiong, Liang and P\\'{o}czos, Barnab\\'{a}s and Schneider, Jeff},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Group Anomaly Detection using Flexible Genre Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/eaae339c4d89fc102edd9dbdb6a28915-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/eaae339c4d89fc102edd9dbdb6a28915-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/eaae339c4d89fc102edd9dbdb6a28915-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1449148,
        "gs_citation": 156,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4819012877403488290&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Machine Learning Department, Carnegie Mellon University; Robotics Institute, Carnegie Mellon University; Robotics Institute, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Machine Learning Department",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Pittsburgh",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "98a0c12bce",
        "title": "Hashing Algorithms for Large-Scale Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/0a1bf96b7165e962e90cb14648c9462d-Abstract.html",
        "author": "Ping Li; Anshumali Shrivastava; Joshua L. Moore; Arnd C. K\u00f6nig",
        "abstract": "Minwise hashing is a standard technique in the context of search for efficiently computing set similarities. The recent development of b-bit minwise hashing provides a  substantial improvement by storing only the lowest b bits of each hashed value. In this paper, we demonstrate that  b-bit minwise hashing can be naturally integrated with linear learning algorithms such as linear SVM and logistic regression, to solve large-scale and high-dimensional statistical learning tasks, especially when the data do not fit in memory.   We  compare $b$-bit minwise hashing with  the Count-Min (CM)  and  Vowpal Wabbit (VW) algorithms, which have essentially the same variances as random projections. Our theoretical and empirical comparisons illustrate that b-bit minwise hashing is significantly more accurate (at the same storage cost) than VW (and random projections) for binary data.",
        "bibtex": "@inproceedings{NIPS2011_0a1bf96b,\n author = {Li, Ping and Shrivastava, Anshumali and Moore, Joshua and K\\\"{o}nig, Arnd},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Hashing Algorithms for Large-Scale Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/0a1bf96b7165e962e90cb14648c9462d-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/0a1bf96b7165e962e90cb14648c9462d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/0a1bf96b7165e962e90cb14648c9462d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 337013,
        "gs_citation": 203,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13502661394190787973&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Cornell University; Cornell University; Cornell University; Microsoft Research",
        "aff_domain": "cornell.edu;cs.cornell.edu;cs.cornell.edu;microsoft.com",
        "email": "cornell.edu;cs.cornell.edu;cs.cornell.edu;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Cornell University;Microsoft Corporation",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.cornell.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "Cornell;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "dfb11b0c5b",
        "title": "Heavy-tailed Distances for Gradient Based Image Descriptors",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/577bcc914f9e55d5e4e4f82f9f00e7d4-Abstract.html",
        "author": "Yangqing Jia; Trevor Darrell",
        "abstract": "Many applications in computer vision measure the similarity between images or image patches based on some statistics such as oriented gradients. These are often modeled implicitly or explicitly with a Gaussian noise assumption, leading to the use of the Euclidean distance when comparing image descriptors. In this paper, we show that the statistics of gradient based image descriptors often follow a heavy-tailed distribution, which undermines any principled motivation for the use of Euclidean distances. We advocate for the use of a distance measure based on the likelihood ratio test with appropriate probabilistic models that fit the empirical data distribution. We instantiate this similarity measure with the Gamma-compound-Laplace distribution, and show significant improvement over existing distance measures in the application of SIFT feature matching, at relatively low computational cost.",
        "bibtex": "@inproceedings{NIPS2011_577bcc91,\n author = {Jia, Yangqing and Darrell, Trevor},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Heavy-tailed Distances for Gradient Based Image Descriptors},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/577bcc914f9e55d5e4e4f82f9f00e7d4-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/577bcc914f9e55d5e4e4f82f9f00e7d4-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/577bcc914f9e55d5e4e4f82f9f00e7d4-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/577bcc914f9e55d5e4e4f82f9f00e7d4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 658154,
        "gs_citation": 55,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15451345931483454314&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "UC Berkeley EECS + ICSI; UC Berkeley EECS + ICSI",
        "aff_domain": "eecs.berkeley.edu;eecs.berkeley.edu",
        "email": "eecs.berkeley.edu;eecs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "University of California, Berkeley;International Computer Science Institute",
        "aff_unique_dep": "Electrical Engineering and Computer Sciences;",
        "aff_unique_url": "https://www.berkeley.edu;https://www.icsi.berkeley.edu/",
        "aff_unique_abbr": "UC Berkeley;ICSI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f67c19bf5d",
        "title": "Hierarchical Matching Pursuit for Image Classification: Architecture and Fast Algorithms",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/f8151fdd6026f82036ab63052b97505b-Abstract.html",
        "author": "Liefeng Bo; Xiaofeng Ren; Dieter Fox",
        "abstract": "Extracting good representations from images is essential for many computer vision tasks. In this paper, we propose hierarchical matching pursuit (HMP), which builds a feature hierarchy layer-by-layer using an efficient matching pursuit encoder. It includes three modules: batch (tree) orthogonal matching pursuit, spatial pyramid max pooling, and contrast normalization. We investigate the architecture of HMP, and show that all three components are critical for good performance. To speed up the orthogonal matching pursuit, we propose a batch tree orthogonal matching pursuit that is particularly suitable to encode a large number of observations that share the same large dictionary. HMP is scalable and can efficiently handle full-size images. In addition, HMP enables linear support vector machines (SVM) to match the performance of nonlinear SVM while being scalable to large datasets. We compare HMP with many state-of-the-art algorithms including convolutional deep belief networks, SIFT based single layer sparse coding, and kernel based feature learning. HMP consistently yields superior accuracy on three types of image classification problems: object recognition (Caltech-101), scene recognition (MIT-Scene), and static event recognition (UIUC-Sports).",
        "bibtex": "@inproceedings{NIPS2011_f8151fdd,\n author = {Bo, Liefeng and Ren, Xiaofeng and Fox, Dieter},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Hierarchical Matching Pursuit for Image Classification: Architecture and Fast Algorithms},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/f8151fdd6026f82036ab63052b97505b-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/f8151fdd6026f82036ab63052b97505b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/f8151fdd6026f82036ab63052b97505b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1058915,
        "gs_citation": 290,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1400571964076514521&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "f46369e15a",
        "title": "Hierarchical Multitask Structured Output Learning for Large-scale Sequence Segmentation",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/ac796a52db3f16bbdb6557d3d89d1c5a-Abstract.html",
        "author": "Nico Goernitz; Christian K. Widmer; Georg Zeller; Andre Kahles; Gunnar R\u00e4tsch; S\u00f6ren Sonnenburg",
        "abstract": "We present a novel regularization-based Multitask Learning (MTL) formulation  for Structured Output (SO) prediction for the case of hierarchical task relations.  Structured output learning often results in dif\ufb01cult inference problems and   requires large amounts of training data to obtain accurate models. We propose to  use MTL to exploit information available for related structured output learning  tasks by means of hierarchical regularization. Due to the combination of example   sets, the cost of training models for structured output prediction can easily  become infeasible for real world applications. We thus propose an ef\ufb01cient   algorithm based on bundle methods to solve the optimization problems resulting from  MTL structured output learning. We demonstrate the performance of our approach  on gene \ufb01nding problems from the application domain of computational biology.  We show that 1) our proposed solver achieves much faster convergence than previous   methods and 2) that the Hierarchical SO-MTL approach clearly outperforms  considered non-MTL methods.",
        "bibtex": "@inproceedings{NIPS2011_ac796a52,\n author = {Goernitz, Nico and Widmer, Christian and Zeller, Georg and Kahles, Andre and R\\\"{a}tsch, Gunnar and Sonnenburg, S\\\"{o}ren},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Hierarchical Multitask Structured Output Learning for Large-scale Sequence Segmentation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/ac796a52db3f16bbdb6557d3d89d1c5a-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/ac796a52db3f16bbdb6557d3d89d1c5a-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/ac796a52db3f16bbdb6557d3d89d1c5a-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/ac796a52db3f16bbdb6557d3d89d1c5a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 665740,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4420540162468759823&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Technical University Berlin; FML of the Max Planck Society; European Molecular Biology Laboratory; FML of the Max Planck Society; TomTom + Technical University Berlin; FML of the Max Planck Society",
        "aff_domain": "tu-berlin.de;tue.mpg.de;gmail.com;tue.mpg.de;tomtom.com;tue.mpg.de",
        "email": "tu-berlin.de;tue.mpg.de;gmail.com;tue.mpg.de;tomtom.com;tue.mpg.de",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;1;3+0;1",
        "aff_unique_norm": "Technical University Berlin;FML of the Max Planck Society;European Molecular Biology Laboratory;TomTom",
        "aff_unique_dep": ";;;",
        "aff_unique_url": ";;https://www.embl.org;https://www.tomtom.com",
        "aff_unique_abbr": ";;EMBL;TomTom",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;2",
        "aff_country_unique": ";Unknown;Netherlands"
    },
    {
        "id": "6fff0a16e5",
        "title": "Hierarchical Topic Modeling for Analysis of Time-Evolving Personal Choices",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/96b9bff013acedfb1d140579e2fbeb63-Abstract.html",
        "author": "Xianxing Zhang; Lawrence Carin; David B. Dunson",
        "abstract": "The nested Chinese restaurant process is extended to design a nonparametric  topic-model tree for representation of human choices. Each tree branch corresponds  to a type of person, and each node (topic) has a corresponding probability  vector over items that may be selected. The observed data are assumed to have  associated temporal covariates (corresponding to the time at which choices are  made), and we wish to impose that with increasing time it is more probable that  topics deeper in the tree are utilized. This structure is imposed by developing  a new \u201cchange point\" stick-breaking model that is coupled with a Poisson and  product-of-gammas construction. To share topics across the tree nodes, topic distributions  are drawn from a Dirichlet process. As a demonstration of this concept,  we analyze real data on course selections of undergraduate students at Duke University,  with the goal of uncovering and concisely representing structure in the  curriculum and in the characteristics of the student body.",
        "bibtex": "@inproceedings{NIPS2011_96b9bff0,\n author = {Zhang, Xianxing and Carin, Lawrence and Dunson, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Hierarchical Topic Modeling for Analysis of Time-Evolving Personal Choices},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/96b9bff013acedfb1d140579e2fbeb63-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/96b9bff013acedfb1d140579e2fbeb63-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/96b9bff013acedfb1d140579e2fbeb63-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/96b9bff013acedfb1d140579e2fbeb63-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 449326,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5541506806423262355&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Duke University; Duke University; Duke University",
        "aff_domain": "duke.edu;stat.duke.edu;ee.duke.edu",
        "email": "duke.edu;stat.duke.edu;ee.duke.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Duke University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.duke.edu",
        "aff_unique_abbr": "Duke",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "4b5f4db93b",
        "title": "Hierarchically Supervised Latent Dirichlet Allocation",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/489d0396e6826eb0c1e611d82ca8b215-Abstract.html",
        "author": "Adler J. Perotte; Frank Wood; Noemie Elhadad; Nicholas Bartlett",
        "abstract": "We introduce hierarchically supervised latent Dirichlet allocation (HSLDA), a model for hierarchically and multiply labeled bag-of-word data. Examples of such data include web pages and their placement in directories, product descriptions and associated categories from product hierarchies, and free-text clinical records and their assigned diagnosis codes. Out-of-sample label prediction is the primary goal of this work, but improved lower-dimensional representations of the bag-of-word data are also of interest. We demonstrate HSLDA on large-scale data from clinical document labeling and retail product categorization tasks. We show that leveraging the structure from hierarchical labels improves out-of-sample label prediction substantially when compared to models that do not.",
        "bibtex": "@inproceedings{NIPS2011_489d0396,\n author = {Perotte, Adler and Wood, Frank and Elhadad, Noemie and Bartlett, Nicholas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Hierarchically Supervised Latent Dirichlet Allocation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/489d0396e6826eb0c1e611d82ca8b215-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/489d0396e6826eb0c1e611d82ca8b215-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/489d0396e6826eb0c1e611d82ca8b215-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 437193,
        "gs_citation": 185,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3361475085763822919&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Columbia University; Columbia University; Columbia University; Columbia University",
        "aff_domain": "dbmi.columbia.edu;stat.columbia.edu;dbmi.columbia.edu;stat.columbia.edu",
        "email": "dbmi.columbia.edu;stat.columbia.edu;dbmi.columbia.edu;stat.columbia.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Columbia University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.columbia.edu",
        "aff_unique_abbr": "Columbia",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3923b0ef3f",
        "title": "High-Dimensional Graphical Model Selection: Tractable Graph Families and Necessary Conditions",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/5055cbf43fac3f7e2336b27310f0b9ef-Abstract.html",
        "author": "Animashree Anandkumar; Vincent Tan; Alan S. Willsky",
        "abstract": "We consider the problem of Ising and Gaussian graphical model selection given n i.i.d. samples from the model. We propose an efficient threshold-based algorithm   for structure estimation based known as  conditional mutual information test. This simple local algorithm    requires only low-order statistics of the data and decides    whether  two nodes   are neighbors in the unknown graph. Under some transparent assumptions, we establish that the proposed algorithm is structurally consistent (or sparsistent)  when the number of samples scales as n= Omega(J",
        "bibtex": "@inproceedings{NIPS2011_5055cbf4,\n author = {Anandkumar, Animashree and Tan, Vincent and Willsky, Alan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {High-Dimensional Graphical Model Selection: Tractable Graph Families and Necessary Conditions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/5055cbf43fac3f7e2336b27310f0b9ef-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/5055cbf43fac3f7e2336b27310f0b9ef-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/5055cbf43fac3f7e2336b27310f0b9ef-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 130391,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6258027816498143346&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Dept. of EECS, Univ. of California Irvine, CA, 92697; Dept. of ECE, Univ. of Wisconsin Madison, WI, 53706; Dept. of EECS, Massachusetts Inst. of Technology, Cambridge, MA, 02139",
        "aff_domain": "uci.edu;wisc.edu;mit.edu",
        "email": "uci.edu;wisc.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Dept. of EECS, Univ. of California Irvine, CA, 92697;Dept. of ECE, Univ. of Wisconsin Madison, WI, 53706;Dept. of EECS, Massachusetts Inst. of Technology, Cambridge, MA, 02139",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "cfa2c50f01",
        "title": "High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/ab541d874c7bc19ab77642849e02b89f-Abstract.html",
        "author": "Po-ling Loh; Martin J. Wainwright",
        "abstract": "Although the standard formulations of prediction problems involve fully-observed and noiseless data drawn in an i.i.d. manner, many  applications involve noisy and/or missing data, possibly involving dependencies. We study these issues in the context of high-dimensional  sparse linear regression, and propose novel estimators for the cases of noisy, missing, and/or dependent data. Many standard approaches to noisy or missing data, such as those using the EM algorithm, lead to optimization problems that are inherently non-convex, and it is difficult to establish theoretical guarantees on practical algorithms. While our approach also involves optimizing non-convex programs, we are able to both analyze the statistical error associated with any global optimum, and prove that a simple projected gradient descent algorithm will converge in polynomial time to a small neighborhood of the set of global minimizers. On the statistical side, we provide non-asymptotic bounds that hold with high probability for the cases of noisy, missing, and/or dependent data. On the computational side, we prove that under the same types of conditions required for statistical consistency, the projected gradient descent algorithm will converge at geometric rates to a near-global minimizer. We illustrate these theoretical predictions with simulations, showing agreement with the predicted scalings.",
        "bibtex": "@inproceedings{NIPS2011_ab541d87,\n author = {Loh, Po-ling and Wainwright, Martin J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/ab541d874c7bc19ab77642849e02b89f-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/ab541d874c7bc19ab77642849e02b89f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/ab541d874c7bc19ab77642849e02b89f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 379287,
        "gs_citation": 716,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5068824928723556243&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 21,
        "aff": "Department of Statistics, University of California, Berkeley; Departments of Statistics and EECS, University of California, Berkeley",
        "aff_domain": "berkeley.edu;stat.berkeley.edu",
        "email": "berkeley.edu;stat.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of California, Berkeley;Departments of Statistics and EECS, University of California, Berkeley",
        "aff_unique_dep": "Department of Statistics;",
        "aff_unique_url": "https://www.berkeley.edu;",
        "aff_unique_abbr": "UC Berkeley;",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "43cedadb36",
        "title": "Higher-Order Correlation Clustering for Image Segmentation",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/98d6f58ab0dafbb86b083a001561bb34-Abstract.html",
        "author": "Sungwoong Kim; Sebastian Nowozin; Pushmeet Kohli; Chang D. Yoo",
        "abstract": "For many of the state-of-the-art computer vision algorithms, image segmentation is an important preprocessing step. As such, several image segmentation algorithms have been proposed, however, with certain reservation due to high computational load and many hand-tuning parameters. Correlation clustering, a graph-partitioning algorithm often used in natural language processing and document clustering, has the potential to perform better than previously proposed image segmentation algorithms. We improve the basic correlation clustering formulation by taking into account higher-order cluster relationships. This improves clustering in the presence of local boundary ambiguities. We first apply the pairwise correlation clustering to image segmentation over a pairwise superpixel graph and then develop higher-order correlation clustering over a hypergraph that considers higher-order relations among superpixels. Fast inference is possible by linear programming relaxation, and also effective parameter learning framework by structured support vector machine is possible. Experimental results on various datasets show that the proposed higher-order correlation clustering outperforms other state-of-the-art image segmentation algorithms.",
        "bibtex": "@inproceedings{NIPS2011_98d6f58a,\n author = {Kim, Sungwoong and Nowozin, Sebastian and Kohli, Pushmeet and Yoo, Chang},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Higher-Order Correlation Clustering for Image Segmentation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/98d6f58ab0dafbb86b083a001561bb34-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/98d6f58ab0dafbb86b083a001561bb34-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/98d6f58ab0dafbb86b083a001561bb34-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1343985,
        "gs_citation": 183,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=134835500474365277&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of EE, KAIST; Microsoft Research Cambridge; Microsoft Research Cambridge; Department of EE, KAIST",
        "aff_domain": "gmail.com;microsoft.com;microsoft.com;ee.kaist.ac.kr",
        "email": "gmail.com;microsoft.com;microsoft.com;ee.kaist.ac.kr",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "Department of EE, KAIST;Microsoft Research",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": ";https://www.microsoft.com/en-us/research/group/cambridge",
        "aff_unique_abbr": ";MSR Cambridge",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "1;1",
        "aff_country_unique": ";United Kingdom"
    },
    {
        "id": "1f0091dfb3",
        "title": "History distribution matching method for predicting effectiveness of HIV combination therapies",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/3fe94a002317b5f9259f82690aeea4cd-Abstract.html",
        "author": "Jasmina Bogojeska",
        "abstract": "This paper presents an approach that predicts the effectiveness of HIV combination therapies by simultaneously addressing several problems affecting the available HIV clinical data sets: the different treatment backgrounds of the samples, the uneven representation of the levels of therapy experience, the missing treatment history information, the uneven therapy representation and the unbalanced therapy outcome representation. The computational validation on clinical data shows that, compared to the most commonly used approach that does not account for the issues mentioned above, our model has significantly higher predictive power. This is especially true for samples stemming from patients with longer treatment history and samples associated with rare therapies. Furthermore, our approach is at least as powerful for the remaining samples.",
        "bibtex": "@inproceedings{NIPS2011_3fe94a00,\n author = {Bogojeska, Jasmina},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {History distribution matching method for predicting effectiveness of HIV combination therapies},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/3fe94a002317b5f9259f82690aeea4cd-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/3fe94a002317b5f9259f82690aeea4cd-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/3fe94a002317b5f9259f82690aeea4cd-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/3fe94a002317b5f9259f82690aeea4cd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 245097,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14739095071059109847&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Max-Planck Institute for Computer Science",
        "aff_domain": "mpi-inf.mpg.de",
        "email": "mpi-inf.mpg.de",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Max-Planck Institute for Computer Science",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "3099d7612e",
        "title": "Hogwild!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/218a0aefd1d1a4be65601cc6ddc1520e-Abstract.html",
        "author": "Benjamin Recht; Christopher Re; Stephen Wright; Feng Niu",
        "abstract": "Stochastic Gradient Descent (SGD) is a popular algorithm that can achieve state-of-the-art performance on a variety of machine  learning tasks.  Several researchers have recently proposed schemes  to parallelize SGD, but all require  performance-destroying memory locking and synchronization. This work aims to show using novel theoretical analysis, algorithms,  and implementation that SGD can be implemented",
        "bibtex": "@inproceedings{NIPS2011_218a0aef,\n author = {Recht, Benjamin and Re, Christopher and Wright, Stephen and Niu, Feng},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Hogwild!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/218a0aefd1d1a4be65601cc6ddc1520e-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/218a0aefd1d1a4be65601cc6ddc1520e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/218a0aefd1d1a4be65601cc6ddc1520e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 318797,
        "gs_citation": 2860,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15767443633562170558&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 33,
        "aff": "Computer Sciences Department, University of Wisconsin-Madison; Computer Sciences Department, University of Wisconsin-Madison; Computer Sciences Department, University of Wisconsin-Madison; Computer Sciences Department, University of Wisconsin-Madison",
        "aff_domain": "cs.wisc.edu;cs.wisc.edu;cs.wisc.edu;cs.wisc.edu",
        "email": "cs.wisc.edu;cs.wisc.edu;cs.wisc.edu;cs.wisc.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Computer Sciences Department, University of Wisconsin-Madison",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "6859c8733c",
        "title": "How Do Humans Teach: On Curriculum Learning and Teaching Dimension",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/f9028faec74be6ec9b852b0a542e2f39-Abstract.html",
        "author": "Faisal Khan; Bilge Mutlu; Xiaojin Zhu",
        "abstract": "We study the empirical strategies that humans follow as they teach a target concept with a simple 1D threshold to a robot.  Previous studies of computational teaching, particularly the teaching dimension model and the curriculum learning principle, offer contradictory predictions on what optimal strategy the teacher should follow in this teaching task. We show through behavioral studies that humans employ three distinct teaching strategies, one of which is consistent with the curriculum learning principle, and propose a novel theoretical framework as a potential explanation for this strategy. This framework, which assumes a teaching goal of minimizing the learner's expected generalization error at each iteration, extends the standard teaching dimension model and offers a theoretical justification for curriculum learning.",
        "bibtex": "@inproceedings{NIPS2011_f9028fae,\n author = {Khan, Faisal and Mutlu, Bilge and Zhu, Jerry},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {How Do Humans Teach: On Curriculum Learning and Teaching Dimension},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/f9028faec74be6ec9b852b0a542e2f39-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/f9028faec74be6ec9b852b0a542e2f39-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/f9028faec74be6ec9b852b0a542e2f39-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1898864,
        "gs_citation": 210,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2134087117450860493&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "http://pages.cs.wisc.edu/~jerryzhu/pub/humanteaching.tgz",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "e56f2efb1d",
        "title": "How biased are maximum entropy models?",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/feab05aa91085b7a8012516bc3533958-Abstract.html",
        "author": "Jakob H. Macke; Iain Murray; Peter E. Latham",
        "abstract": "Maximum entropy models have become popular statistical models in neuroscience and other areas in biology, and can be useful tools for obtaining estimates of mu- tual information in biological systems. However, maximum entropy models fit to small data sets can be subject to sampling bias; i.e. the true entropy of the data can be severely underestimated. Here we study the sampling properties of estimates of the entropy obtained from maximum entropy models. We show that if the data is generated by a distribution that lies in the model class, the bias is equal to the number of parameters divided by twice the number of observations. However, in practice, the true distribution is usually outside the model class, and we show here that this misspecification can lead to much larger bias. We provide a perturba- tive approximation of the maximally expected bias when the true model is out of model class, and we illustrate our results using numerical simulations of an Ising model; i.e. the second-order maximum entropy distribution on binary data.",
        "bibtex": "@inproceedings{NIPS2011_feab05aa,\n author = {Macke, Jakob H and Murray, Iain and Latham, Peter},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {How biased are maximum entropy models?},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/feab05aa91085b7a8012516bc3533958-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/feab05aa91085b7a8012516bc3533958-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/feab05aa91085b7a8012516bc3533958-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/feab05aa91085b7a8012516bc3533958-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 519704,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12158651051837905714&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "Gatsby Computational Neuroscience Unit, University College London, UK; School of Informatics, University of Edinburgh, UK; Gatsby Computational Neuroscience Unit, University College London, UK",
        "aff_domain": "gatsby.ucl.ac.uk;ed.ac.uk;gatsby.ucl.ac.uk",
        "email": "gatsby.ucl.ac.uk;ed.ac.uk;gatsby.ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University College London;University of Edinburgh",
        "aff_unique_dep": "Gatsby Computational Neuroscience Unit;School of Informatics",
        "aff_unique_url": "https://www.ucl.ac.uk;https://www.ed.ac.uk",
        "aff_unique_abbr": "UCL;Edinburgh",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "London;Edinburgh",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "18a18ecd10",
        "title": "ICA with Reconstruction Cost for Efficient Overcomplete Feature Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/233509073ed3432027d48b1a83f5fbd2-Abstract.html",
        "author": "Quoc V. Le; Alexandre Karpenko; Jiquan Ngiam; Andrew Y. Ng",
        "abstract": "Independent Components Analysis (ICA) and its variants have been successfully used for unsupervised feature learning. However, standard ICA requires an orthonoramlity constraint to be enforced, which makes it dif\ufb01cult to learn overcomplete features. In addition, ICA is sensitive to whitening. These properties make it challenging to scale ICA to high dimensional data. In this paper, we propose a robust soft reconstruction cost for ICA that allows us to learn highly overcomplete sparse features even on unwhitened data. Our formulation reveals formal connections between ICA and sparse autoencoders, which have previously been observed only empirically. Our algorithm can be used in conjunction with off-the-shelf fast unconstrained optimizers. We show that the soft reconstruction cost can also be used to prevent replicated features in tiled convolutional neural networks. Using our method to learn highly overcomplete sparse features and tiled convolutional neural networks, we obtain competitive performances on a wide variety of object recognition tasks. We achieve state-of-the-art test accuracies on the STL-10 and Hollywood2 datasets.",
        "bibtex": "@inproceedings{NIPS2011_23350907,\n author = {Le, Quoc and Karpenko, Alexandre and Ngiam, Jiquan and Ng, Andrew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {ICA with Reconstruction Cost for Efficient Overcomplete Feature Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/233509073ed3432027d48b1a83f5fbd2-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/233509073ed3432027d48b1a83f5fbd2-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/233509073ed3432027d48b1a83f5fbd2-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/233509073ed3432027d48b1a83f5fbd2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 424903,
        "gs_citation": 467,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10353415925673942557&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Computer Science Department, Stanford University; Computer Science Department, Stanford University; Computer Science Department, Stanford University; Computer Science Department, Stanford University",
        "aff_domain": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2d08068563",
        "title": "Identifying Alzheimer's Disease-Related Brain Regions from Multi-Modality Neuroimaging Data using Sparse Composite Linear Discrimination Analysis",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/fa3a3c407f82377f55c19c5d403335c7-Abstract.html",
        "author": "Shuai Huang; Jing Li; Jieping Ye; Teresa Wu; Kewei Chen; Adam Fleisher; Eric Reiman",
        "abstract": "Diagnosis of Alzheimer's disease (AD) at the early stage of the disease development is of great clinical importance. Current clinical assessment that relies primarily on cognitive measures proves low sensitivity and specificity. The fast growing neuroimaging techniques hold great promise. Research so far has focused on single neuroimaging modalities. However, as different modalities provide complementary measures for the same disease pathology, fusion of multi-modality data may increase the statistical power in identification of disease-related brain regions. This is especially true for early AD, at which stage the disease-related regions are most likely to be weak-effect regions that are difficult to be detected from a single modality alone. We propose a sparse composite linear discriminant analysis model (SCLDA) for identification of disease-related brain regions of early AD from multi-modality data. SCLDA uses a novel formulation that decomposes each LDA parameter into a product of a common parameter shared by all the modalities and a parameter specific to each modality, which enables joint analysis of all the modalities and borrowing strength from one another. We prove that this formulation is equivalent to a penalized likelihood with non-convex regularization, which can be solved by the DC ((difference of convex functions) programming. We show that in using the DC programming, the property of the non-convex regularization in terms of preserving weak-effect features can be nicely revealed. We perform extensive simulations to show that SCLDA outperforms existing competing algorithms on feature selection, especially on the ability for identifying weak-effect features. We apply SCLDA to the Magnetic Resonance Imaging (MRI) and Positron Emission Tomography (PET) images of 49 AD patients and 67 normal controls (NC). Our study identifies disease-related brain regions consistent with findings in the AD literature.",
        "bibtex": "@inproceedings{NIPS2011_fa3a3c40,\n author = {Huang, Shuai and Li, Jing and Ye, Jieping and Wu, Teresa and Chen, Kewei and Fleisher, Adam and Reiman, Eric},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Identifying Alzheimer\\textquotesingle s Disease-Related Brain Regions from Multi-Modality Neuroimaging Data using Sparse Composite Linear Discrimination Analysis},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/fa3a3c407f82377f55c19c5d403335c7-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/fa3a3c407f82377f55c19c5d403335c7-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/fa3a3c407f82377f55c19c5d403335c7-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/fa3a3c407f82377f55c19c5d403335c7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 6037806,
        "gs_citation": 57,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14456756689485270393&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Industrial Engineering; Industrial Engineering; Computer Science and Engineering + Center for Evolutionary Medicine and Informatics, The Biodesign Institute, Arizona State University, Tempe, USA; Banner Alzheimer\u2019s Institute and Banner PET Center, Banner Good Samaritan Medical Center, Phoenix, USA; Industrial Engineering; Banner Alzheimer\u2019s Institute and Banner PET Center, Banner Good Samaritan Medical Center, Phoenix, USA; Banner Alzheimer\u2019s Institute and Banner PET Center, Banner Good Samaritan Medical Center, Phoenix, USA",
        "aff_domain": "asu.edu;asu.edu;asu.edu;bannerhealth.com;asu.edu;bannerhealth.com;bannerhealth.com",
        "email": "asu.edu;asu.edu;asu.edu;bannerhealth.com;asu.edu;bannerhealth.com;bannerhealth.com",
        "github": "",
        "project": "",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1+2;3;0;3;3",
        "aff_unique_norm": "Industrial Engineering;Computer Science and Engineering;Center for Evolutionary Medicine and Informatics, The Biodesign Institute, Arizona State University, Tempe, USA;Banner Alzheimer\u2019s Institute and Banner PET Center, Banner Good Samaritan Medical Center, Phoenix, USA",
        "aff_unique_dep": ";Computer Science and Engineering;;",
        "aff_unique_url": ";;;",
        "aff_unique_abbr": ";;;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "ea81145a23",
        "title": "Im2Text: Describing Images Using 1 Million Captioned Photographs",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html",
        "author": "Vicente Ordonez; Girish Kulkarni; Tamara L. Berg",
        "abstract": "We develop and demonstrate automatic image description methods using a large captioned photo collection.  One contribution is our technique for the automatic collection of this new dataset -- performing a huge number of Flickr queries and then filtering the noisy results down to 1 million images with associated visually relevant captions.  Such a collection allows us to approach the extremely challenging problem of description generation using relatively simple non-parametric methods and produces surprisingly effective results. We also develop methods incorporating many state of the art, but fairly noisy, estimates of image content to produce even more pleasing results. Finally we introduce a new objective performance measure for image captioning.",
        "bibtex": "@inproceedings{NIPS2011_5dd9db5e,\n author = {Ordonez, Vicente and Kulkarni, Girish and Berg, Tamara},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Im2Text: Describing Images Using 1 Million Captioned Photographs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1529075,
        "gs_citation": 1736,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3580841766769044071&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 22,
        "aff": "Stony Brook University; Stony Brook University; Stony Brook University",
        "aff_domain": "cs.stonybrook.edu;cs.stonybrook.edu;cs.stonybrook.edu",
        "email": "cs.stonybrook.edu;cs.stonybrook.edu;cs.stonybrook.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Stony Brook University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stonybrook.edu",
        "aff_unique_abbr": "SBU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e57329cbf8",
        "title": "Image Parsing with Stochastic Scene Grammar",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/32bb90e8976aab5298d5da10fe66f21d-Abstract.html",
        "author": "Yibiao Zhao; Song-chun Zhu",
        "abstract": "This paper proposes a parsing algorithm for scene understanding which includes four aspects: computing 3D scene layout, detecting 3D objects (e.g. furniture), detecting 2D faces (windows, doors etc.), and segmenting background. In contrast to previous scene labeling work that applied discriminative classifiers to pixels (or super-pixels), we use a generative Stochastic Scene Grammar (SSG). This grammar represents the compositional structures of visual entities from scene categories, 3D foreground/background, 2D faces, to 1D lines. The grammar includes three types of production rules and two types of contextual relations. Production rules: (i) AND rules represent the decomposition of an entity into sub-parts; (ii) OR rules represent the switching among sub-types of an entity; (iii) SET rules rep- resent an ensemble of visual entities. Contextual relations: (i) Cooperative \u201c+\u201d relations represent positive links between binding entities, such as hinged faces of a object or aligned boxes; (ii) Competitive \u201c-\u201d relations represents negative links between competing entities, such as mutually exclusive boxes. We design an efficient MCMC inference algorithm, namely Hierarchical cluster sampling, to search in the large solution space of scene configurations. The algorithm has two stages: (i) Clustering: It forms all possible higher-level structures (clusters) from lower-level entities by production rules and contextual relations. (ii) Sampling: It jumps between alternative structures (clusters) in each layer of the hierarchy to find the most probable configuration (represented by a parse tree). In our experiment, we demonstrate the superiority of our algorithm over existing methods on public dataset. In addition, our approach achieves richer structures in the parse tree.",
        "bibtex": "@inproceedings{NIPS2011_32bb90e8,\n author = {Zhao, Yibiao and Zhu, Song-chun},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Image Parsing with Stochastic Scene Grammar},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/32bb90e8976aab5298d5da10fe66f21d-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/32bb90e8976aab5298d5da10fe66f21d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/32bb90e8976aab5298d5da10fe66f21d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 5414980,
        "gs_citation": 128,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17069982979827392691&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Statistics, University of California, Los Angeles; Department of Statistics and Computer Science, University of California, Los Angeles",
        "aff_domain": "ucla.edu;stat.ucla.edu",
        "email": "ucla.edu;stat.ucla.edu",
        "github": "",
        "project": "http://www.stat.ucla.edu/~ybzhao/research/sceneparsing",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Los Angeles",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "https://www.ucla.edu",
        "aff_unique_abbr": "UCLA",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "1cc2871dd2",
        "title": "Improved Algorithms for Linear Stochastic Bandits",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/e1d5be1c7f2f456670de3d53c7b54f4a-Abstract.html",
        "author": "Yasin Abbasi-yadkori; D\u00e1vid P\u00e1l; Csaba Szepesv\u00e1ri",
        "abstract": "We improve the theoretical analysis and empirical performance of algorithms for the stochastic multi-armed bandit problem and the linear stochastic multi-armed bandit problem. In particular, we show that a simple modification of Auer\u2019s UCB algorithm (Auer, 2002) achieves with high probability constant regret. More importantly, we modify and, consequently, improve the analysis of the algorithm for the for linear stochastic bandit problem studied by Auer (2002), Dani et al. (2008), Rusmevichientong and Tsitsiklis (2010), Li et al. (2010). Our modification improves the regret bound by a logarithmic factor, though experiments show a vast improvement. In both cases, the improvement stems from the construction of smaller confidence sets. For their construction we use a novel tail inequality for vector-valued martingales.",
        "bibtex": "@inproceedings{NIPS2011_e1d5be1c,\n author = {Abbasi-yadkori, Yasin and P\\'{a}l, D\\'{a}vid and Szepesv\\'{a}ri, Csaba},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Improved Algorithms for Linear Stochastic Bandits},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/e1d5be1c7f2f456670de3d53c7b54f4a-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/e1d5be1c7f2f456670de3d53c7b54f4a-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/e1d5be1c7f2f456670de3d53c7b54f4a-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/e1d5be1c7f2f456670de3d53c7b54f4a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 374644,
        "gs_citation": 2303,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16143143244279906247&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "Dept. of Computing Science, University of Alberta; Dept. of Computing Science, University of Alberta; Dept. of Computing Science, University of Alberta",
        "aff_domain": "ualberta.ca;google.com;ualberta.ca",
        "email": "ualberta.ca;google.com;ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Alberta",
        "aff_unique_dep": "Dept. of Computing Science",
        "aff_unique_url": "https://www.ualberta.ca",
        "aff_unique_abbr": "UAlberta",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "69aed56b7e",
        "title": "Improving Topic Coherence with Regularized Topic Models",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/5ef698cd9fe650923ea331c15af3b160-Abstract.html",
        "author": "David Newman; Edwin V. Bonilla; Wray Buntine",
        "abstract": "Topic models have the potential to improve search and browsing by extracting useful semantic themes from web pages and other text documents. When learned topics are coherent and interpretable, they can be valuable for faceted browsing, results set diversity analysis, and document retrieval. However, when dealing with small collections or noisy text (e.g. web search result snippets or blog posts), learned topics can be less coherent, less interpretable, and less useful. To overcome this, we propose two methods to regularize the learning of topic models. Our regularizers work by creating a structured prior over words that reflect broad patterns in the external data. Using thirteen datasets we show that both regularizers improve topic coherence and interpretability while learning a faithful representation of the collection of interest. Overall, this work makes topic models more useful across a broader range of text data.",
        "bibtex": "@inproceedings{NIPS2011_5ef698cd,\n author = {Newman, David and Bonilla, Edwin V and Buntine, Wray},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Improving Topic Coherence with Regularized Topic Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/5ef698cd9fe650923ea331c15af3b160-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/5ef698cd9fe650923ea331c15af3b160-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/5ef698cd9fe650923ea331c15af3b160-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 257236,
        "gs_citation": 278,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8486929774089999144&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "University of California, Irvine; NICTA & Australian National University; NICTA & Australian National University",
        "aff_domain": "uci.edu;nicta.com.au;nicta.com.au",
        "email": "uci.edu;nicta.com.au;nicta.com.au",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of California, Irvine;Australian National University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uci.edu;https://www.anu.edu.au",
        "aff_unique_abbr": "UCI;ANU",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Irvine;",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "United States;Australia"
    },
    {
        "id": "9df0d76f7b",
        "title": "Inductive reasoning about chimeric creatures",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/705f2172834666788607efbfca35afb3-Abstract.html",
        "author": "Charles Kemp",
        "abstract": "Given one feature of a novel animal, humans readily make inferences about other features of the animal. For example, winged creatures often fly, and creatures that eat fish often live in the water. We explore the knowledge that supports these inferences and compare two approaches. The first approach proposes that humans rely on abstract representations of dependency relationships between features, and is formalized here as a graphical model.  The second approach proposes that humans rely on specific knowledge of previously encountered animals, and is formalized here as a family of exemplar models. We evaluate these models using a task where participants reason about chimeras, or animals with pairs of features that have not previously been observed to co-occur. The results support the hypothesis that humans rely on explicit representations of relationships between features.",
        "bibtex": "@inproceedings{NIPS2011_705f2172,\n author = {Kemp, Charles},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Inductive reasoning about chimeric creatures},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/705f2172834666788607efbfca35afb3-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/705f2172834666788607efbfca35afb3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/705f2172834666788607efbfca35afb3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1294467,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3667985971299882396&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "d71fec9738",
        "title": "Inference in continuous-time change-point models",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/e816c635cad85a60fabd6b97b03cbcc9-Abstract.html",
        "author": "Florian Stimberg; Manfred Opper; Guido Sanguinetti; Andreas Ruttor",
        "abstract": "We consider the problem of Bayesian inference for continuous time multi-stable stochastic systems which can change both their diffusion and drift parameters at discrete times. We propose exact inference and sampling methodologies for two specific cases where the discontinuous dynamics is given by a Poisson process and a two-state Markovian switch. We test the methodology on simulated data, and apply it to two real data sets in finance and systems biology. Our experimental results show that the approach leads to valid inferences and non-trivial insights.",
        "bibtex": "@inproceedings{NIPS2011_e816c635,\n author = {Stimberg, Florian and Opper, Manfred and Sanguinetti, Guido and Ruttor, Andreas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Inference in continuous-time change-point models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/e816c635cad85a60fabd6b97b03cbcc9-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/e816c635cad85a60fabd6b97b03cbcc9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/e816c635cad85a60fabd6b97b03cbcc9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 403733,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13138634990995460394&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Computer Science, TU Berlin; Computer Science, TU Berlin; Computer Science, TU Berlin; School of Informatics, University of Edinburgh",
        "aff_domain": "cs.tu-berlin.de;cs.tu-berlin.de;cs.tu-berlin.de;inf.ed.ac.uk",
        "email": "cs.tu-berlin.de;cs.tu-berlin.de;cs.tu-berlin.de;inf.ed.ac.uk",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Computer Science, TU Berlin;University of Edinburgh",
        "aff_unique_dep": ";School of Informatics",
        "aff_unique_url": ";https://www.ed.ac.uk",
        "aff_unique_abbr": ";Edinburgh",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Edinburgh",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United Kingdom"
    },
    {
        "id": "c16fb51155",
        "title": "Inferring Interaction Networks using the IBP applied to microRNA Target Prediction",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/cedebb6e872f539bef8c3f919874e9d7-Abstract.html",
        "author": "Hai-son P. Le; Ziv Bar-joseph",
        "abstract": "Determining interactions between entities and the overall organization and clustering of nodes in networks is a major challenge when analyzing biological and social network data. Here we extend the Indian Buffet Process (IBP), a nonparametric Bayesian model, to integrate noisy interaction scores with properties of individual entities for inferring interaction networks and clustering nodes within these networks. We present an application of this method to study how microRNAs regulate mRNAs in cells. Analysis of synthetic and real data indicates that the method improves upon prior methods, correctly recovers interactions and clusters, and provides accurate biological predictions.",
        "bibtex": "@inproceedings{NIPS2011_cedebb6e,\n author = {Le, Hai-son and Bar-joseph, Ziv},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Inferring Interaction Networks using the IBP applied to microRNA Target Prediction},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/cedebb6e872f539bef8c3f919874e9d7-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/cedebb6e872f539bef8c3f919874e9d7-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/cedebb6e872f539bef8c3f919874e9d7-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/cedebb6e872f539bef8c3f919874e9d7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 908658,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6684372004343695605&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA, USA; Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA, USA",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Machine Learning Department",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ab3c4583cc",
        "title": "Inferring spike-timing-dependent plasticity from spike train data",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/602d1305678a8d5fdb372271e980da6a-Abstract.html",
        "author": "Ian Stevenson; Konrad Koerding",
        "abstract": "Synaptic plasticity underlies learning and is thus central for development, memory, and recovery from injury. However, it is often difficult to detect changes in synaptic strength in vivo, since intracellular recordings are experimentally challenging. Here we present two methods aimed at inferring changes in the coupling between pairs of neurons from extracellularly recorded spike trains. First, using a generalized bilinear model with Poisson output we estimate time-varying coupling assuming that all changes are spike-timing-dependent. This approach allows model-based estimation of STDP modification functions from pairs of spike trains. Then, using recursive point-process adaptive filtering methods we estimate more general variation in coupling strength over time. Using simulations of neurons undergoing spike-timing dependent modification, we show that the true modification function can be recovered. Using multi-electrode data from motor cortex we then illustrate the use of this technique on in vivo data.",
        "bibtex": "@inproceedings{NIPS2011_602d1305,\n author = {Stevenson, Ian and Koerding, Konrad},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Inferring spike-timing-dependent plasticity from spike train data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/602d1305678a8d5fdb372271e980da6a-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/602d1305678a8d5fdb372271e980da6a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/602d1305678a8d5fdb372271e980da6a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1489968,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13237728701414028666&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Physical Medicine and Rehabilitation, Northwestern University; Department of Physical Medicine and Rehabilitation, Northwestern University",
        "aff_domain": "northwestern.edu;northwestern.edu",
        "email": "northwestern.edu;northwestern.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Department of Physical Medicine and Rehabilitation, Northwestern University",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "3d41a2a00f",
        "title": "Infinite Latent SVM for Classification and Multi-task Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/9f53d83ec0691550f7d2507d57f4f5a2-Abstract.html",
        "author": "Jun Zhu; Ning Chen; Eric P. Xing",
        "abstract": "Unlike existing nonparametric Bayesian models, which rely solely on specially conceived priors to incorporate domain knowledge for discovering improved latent representations, we study nonparametric Bayesian inference with regularization on the desired posterior distributions. While priors can indirectly affect posterior distributions through Bayes' theorem, imposing posterior regularization is arguably more direct and in some cases can be much easier. We particularly focus on developing infinite latent support vector machines (iLSVM) and multi-task infinite latent support vector machines (MT-iLSVM), which explore the large-margin idea in combination with a nonparametric Bayesian model for discovering predictive latent features for classification and multi-task learning, respectively. We present efficient inference methods and report empirical studies on several benchmark datasets. Our results appear to demonstrate the merits inherited from both large-margin learning and Bayesian nonparametrics.",
        "bibtex": "@inproceedings{NIPS2011_9f53d83e,\n author = {Zhu, Jun and Chen, Ning and Xing, Eric},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Infinite Latent SVM for Classification and Multi-task Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/9f53d83ec0691550f7d2507d57f4f5a2-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/9f53d83ec0691550f7d2507d57f4f5a2-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/9f53d83ec0691550f7d2507d57f4f5a2-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/9f53d83ec0691550f7d2507d57f4f5a2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1109158,
        "gs_citation": 77,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9058921311512887254&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 11,
        "aff": "Dept. of Computer Science & Tech., TNList Lab, Tsinghua University, Beijing 100084, China; Dept. of Computer Science & Tech., TNList Lab, Tsinghua University, Beijing 100084, China; Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA 15213, USA",
        "aff_domain": "tsinghua.edu.cn;mails.thu.edu.cn;cs.cmu.edu",
        "email": "tsinghua.edu.cn;mails.thu.edu.cn;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Dept. of Computer Science & Tech., TNList Lab, Tsinghua University, Beijing 100084, China;Carnegie Mellon University",
        "aff_unique_dep": ";Machine Learning Department",
        "aff_unique_url": ";https://www.cmu.edu",
        "aff_unique_abbr": ";CMU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Pittsburgh",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "d4d0c74165",
        "title": "Information Rates and Optimal Decoding in Large Neural Populations",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/8eefcfdf5990e441f0fb6f3fad709e21-Abstract.html",
        "author": "Kamiar R. Rad; Liam Paninski",
        "abstract": "Many fundamental questions in theoretical neuroscience involve optimal  decoding and the computation of Shannon information rates in  populations of spiking neurons.  In this paper, we apply methods from  the asymptotic theory of statistical inference to obtain a clearer  analytical understanding of these quantities.  We find that for large  neural populations carrying a finite total amount of information, the  full spiking population response is asymptotically as informative as a  single observation from a Gaussian process whose mean and covariance  can be characterized explicitly in terms of network and single neuron  properties.  The Gaussian form of this asymptotic sufficient  statistic allows us in certain cases to perform optimal Bayesian  decoding by simple linear transformations, and to obtain closed-form  expressions of the Shannon information carried by the network.  One  technical advantage of the theory is that it may be applied easily  even to non-Poisson point process network models; for example, we find  that under some conditions, neural populations with strong  history-dependent (non-Poisson) effects carry exactly the same  information as do simpler equivalent populations of non-interacting  Poisson neurons with matched firing rates.  We argue that our findings  help to clarify some results from the recent literature on neural  decoding and neuroprosthetic design.",
        "bibtex": "@inproceedings{NIPS2011_8eefcfdf,\n author = {Rad, Kamiar and Paninski, Liam},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Information Rates and Optimal Decoding in Large Neural Populations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/8eefcfdf5990e441f0fb6f3fad709e21-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/8eefcfdf5990e441f0fb6f3fad709e21-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/8eefcfdf5990e441f0fb6f3fad709e21-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/8eefcfdf5990e441f0fb6f3fad709e21-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 344639,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2662995656829200022&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Statistics, Columbia University; Department of Statistics, Columbia University",
        "aff_domain": "stat.columbia.edu;stat.columbia.edu",
        "email": "stat.columbia.edu;stat.columbia.edu",
        "github": "",
        "project": "http://www.stat.columbia.edu/~liam/research/pubs/kamiar-ss-info.pdf",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Columbia University",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "https://www.columbia.edu",
        "aff_unique_abbr": "Columbia",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "201a56b57b",
        "title": "Inverting Grice's Maxims to Learn Rules from Natural Language Extractions",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/8c7bbbba95c1025975e548cee86dfadc-Abstract.html",
        "author": "Mohammad S. Sorower; Janardhan R. Doppa; Walker Orr; Prasad Tadepalli; Thomas G. Dietterich; Xiaoli Z. Fern",
        "abstract": "We consider the problem of learning rules from natural language text  sources. These sources, such as news articles and web texts, are  created by a writer to communicate information to a reader, where the  writer and reader share substantial domain knowledge.  Consequently,  the texts tend to be concise and mention the minimum information  necessary for the reader to draw the correct conclusions.  We study  the problem of learning domain knowledge from such concise texts,  which is an instance of the general problem of learning in the  presence of missing data.  However, unlike standard approaches to  missing data, in this setting we know that facts are more likely to be  missing from the text in cases where the reader can infer them from  the facts that are mentioned combined with the domain knowledge.  Hence, we can explicitly model this \"missingness\" process and invert  it via probabilistic inference to learn the underlying domain  knowledge.  This paper introduces a mention model that models  the probability of facts being mentioned in the text based on what  other facts have already been mentioned and domain knowledge in the  form of Horn clause rules.  Learning must simultaneously search the  space of rules and learn the parameters of the mention model.  We  accomplish this via an application of Expectation Maximization within  a Markov Logic framework.  An experimental evaluation on synthetic and  natural text data shows that the method can learn accurate rules and  apply them to new texts to make correct inferences.  Experiments also  show that the method out-performs the standard EM approach that  assumes mentions are missing at random.",
        "bibtex": "@inproceedings{NIPS2011_8c7bbbba,\n author = {Sorower, Mohammad and Doppa, Janardhan and Orr, Walker and Tadepalli, Prasad and Dietterich, Thomas and Fern, Xiaoli},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Inverting Grice\\textquotesingle s Maxims to Learn Rules from Natural Language Extractions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/8c7bbbba95c1025975e548cee86dfadc-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/8c7bbbba95c1025975e548cee86dfadc-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/8c7bbbba95c1025975e548cee86dfadc-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 190573,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1865131738122478196&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "School of Electrical Engineering and Computer Science, Oregon State University; School of Electrical Engineering and Computer Science, Oregon State University; School of Electrical Engineering and Computer Science, Oregon State University; School of Electrical Engineering and Computer Science, Oregon State University; School of Electrical Engineering and Computer Science, Oregon State University; School of Electrical Engineering and Computer Science, Oregon State University",
        "aff_domain": "eecs.oregonstate.edu;eecs.oregonstate.edu;eecs.oregonstate.edu;eecs.oregonstate.edu;eecs.oregonstate.edu;eecs.oregonstate.edu",
        "email": "eecs.oregonstate.edu;eecs.oregonstate.edu;eecs.oregonstate.edu;eecs.oregonstate.edu;eecs.oregonstate.edu;eecs.oregonstate.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Oregon State University",
        "aff_unique_dep": "School of Electrical Engineering and Computer Science",
        "aff_unique_url": "https://osu.edu",
        "aff_unique_abbr": "OSU",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Corvallis",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "301d29eaa0",
        "title": "Iterative Learning for Reliable Crowdsourcing Systems",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/c667d53acd899a97a85de0c201ba99be-Abstract.html",
        "author": "David R. Karger; Sewoong Oh; Devavrat Shah",
        "abstract": "Crowdsourcing systems, in which tasks are electronically distributed to numerous ``information piece-workers'', have emerged as an effective paradigm for human-powered solving of large scale problems in domains such as image classification, data entry, optical character recognition, recommendation, and proofreading. Because these low-paid workers can be unreliable, nearly all crowdsourcers must devise schemes to increase confidence in their answers, typically by assigning each task multiple times and combining the answers in some way such as majority voting. In this paper, we consider a general model of such  rowdsourcing tasks, and pose the problem of minimizing the total price (i.e., number of task assignments) that must be paid to achieve a target overall reliability. We give new algorithms for deciding which tasks to assign to which workers and for inferring correct answers from the workers\u2019 answers. We show that our algorithm significantly outperforms majority voting and, in fact, are asymptotically optimal through comparison to an oracle that knows the reliability of every worker.",
        "bibtex": "@inproceedings{NIPS2011_c667d53a,\n author = {Karger, David and Oh, Sewoong and Shah, Devavrat},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Iterative Learning for Reliable Crowdsourcing Systems},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/c667d53acd899a97a85de0c201ba99be-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/c667d53acd899a97a85de0c201ba99be-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/c667d53acd899a97a85de0c201ba99be-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 299866,
        "gs_citation": 746,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14797602227708818853&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Electrical Engineering and Computer Science; Department of Electrical Engineering and Computer Science; Department of Electrical Engineering and Computer Science",
        "aff_domain": "mit.edu;mit.edu;mit.edu",
        "email": "mit.edu;mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Department of Electrical Engineering and Computer Science",
        "aff_unique_url": "https://web.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e2ec51ea7f",
        "title": "Joint 3D Estimation of Objects and Scene Layout",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/fc3cf452d3da8402bebb765225ce8c0e-Abstract.html",
        "author": "Andreas Geiger; Christian Wojek; Raquel Urtasun",
        "abstract": "We propose a novel generative model that is able to reason jointly about the 3D scene layout as well as the 3D location and orientation of objects in the scene. In particular, we infer the scene topology, geometry as well as traffic activities from a short video sequence acquired with a single camera mounted on a moving car. Our generative model takes advantage of dynamic information in the form of vehicle tracklets as well as static information coming from semantic labels and geometry  (i.e., vanishing points). Experiments show that our approach outperforms a discriminative baseline based on multiple kernel learning (MKL) which has access to the same image information. Furthermore, as we reason about objects in 3D, we are able to significantly increase the performance of state-of-the-art object detectors in their ability to estimate  object orientation.",
        "bibtex": "@inproceedings{NIPS2011_fc3cf452,\n author = {Geiger, Andreas and Wojek, Christian and Urtasun, Raquel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Joint 3D Estimation of Objects and Scene Layout},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/fc3cf452d3da8402bebb765225ce8c0e-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/fc3cf452d3da8402bebb765225ce8c0e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/fc3cf452d3da8402bebb765225ce8c0e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 3067057,
        "gs_citation": 160,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3024667158513521236&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Karlsruhe Institute of Technology; MPI Saarbr \u00a8ucken; TTI Chicago",
        "aff_domain": "kit.edu;mpi-inf.mpg.de;ttic.edu",
        "email": "kit.edu;mpi-inf.mpg.de;ttic.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Karlsruhe Institute of Technology;MPI Saarbr \u00a8ucken;Toyota Technological Institute at Chicago",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.kit.edu;;https://www.tti-chicago.org",
        "aff_unique_abbr": "KIT;;TTI",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Chicago",
        "aff_country_unique_index": "0;2",
        "aff_country_unique": "Germany;;United States"
    },
    {
        "id": "5cb58881b0",
        "title": "Kernel Bayes' Rule",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/54a367d629152b720749e187b3eaa11b-Abstract.html",
        "author": "Kenji Fukumizu; Le Song; Arthur Gretton",
        "abstract": "A nonparametric kernel-based method for realizing Bayes' rule is proposed, based on kernel representations of probabilities in reproducing kernel Hilbert spaces. The prior and conditional probabilities are expressed as empirical kernel mean and covariance operators, respectively, and the kernel mean of the posterior distribution is computed in the form of a weighted sample.  The kernel Bayes' rule can be applied to a wide variety of Bayesian inference problems: we demonstrate Bayesian computation without likelihood, and filtering with a nonparametric state-space model. A consistency rate for the posterior estimate is established.",
        "bibtex": "@inproceedings{NIPS2011_54a367d6,\n author = {Fukumizu, Kenji and Song, Le and Gretton, Arthur},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Kernel Bayes\\textquotesingle  Rule},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/54a367d629152b720749e187b3eaa11b-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/54a367d629152b720749e187b3eaa11b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/54a367d629152b720749e187b3eaa11b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/54a367d629152b720749e187b3eaa11b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 171632,
        "gs_citation": 84,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12991416769313208892&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "The Institute of Statistical Mathematics, Tokyo; College of Computing, Georgia Institute of Technology; Gatsby Unit, UCL + MPI for Intelligent Systems",
        "aff_domain": "ism.ac.jp;cc.gatech.edu;gmail.com",
        "email": "ism.ac.jp;cc.gatech.edu;gmail.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2+3",
        "aff_unique_norm": "The Institute of Statistical Mathematics, Tokyo;Georgia Institute of Technology;University College London;Max Planck Institute for Intelligent Systems",
        "aff_unique_dep": ";College of Computing;Gatsby Unit;",
        "aff_unique_url": ";https://www.gatech.edu;https://www.ucl.ac.uk;https://www.mpi-is.mpg.de",
        "aff_unique_abbr": ";Georgia Tech;UCL;MPI-IS",
        "aff_campus_unique_index": "1;",
        "aff_campus_unique": ";Atlanta",
        "aff_country_unique_index": "1;2+3",
        "aff_country_unique": ";United States;United Kingdom;Germany"
    },
    {
        "id": "e791324e66",
        "title": "Kernel Embeddings of Latent Tree Graphical Models",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/dc09c97fd73d7a324bdbfe7c79525f64-Abstract.html",
        "author": "Le Song; Eric P. Xing; Ankur P. Parikh",
        "abstract": "Latent tree graphical models are natural tools for expressing long range and hierarchical dependencies among many variables which are common in computer vision, bioinformatics and natural language processing problems. However, existing models are largely restricted to discrete and Gaussian variables due to computational constraints; furthermore, algorithms for estimating the latent tree structure and learning the model parameters are largely restricted to heuristic local search. We present a method based on kernel embeddings of distributions for latent tree graphical models with continuous and non-Gaussian variables. Our method can recover the latent tree structures with provable guarantees and perform local-minimum free parameter learning and efficient inference. Experiments on simulated and real data show the advantage of our proposed approach.",
        "bibtex": "@inproceedings{NIPS2011_dc09c97f,\n author = {Song, Le and Xing, Eric and Parikh, Ankur},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Kernel Embeddings of Latent Tree Graphical Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/dc09c97fd73d7a324bdbfe7c79525f64-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/dc09c97fd73d7a324bdbfe7c79525f64-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/dc09c97fd73d7a324bdbfe7c79525f64-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/dc09c97fd73d7a324bdbfe7c79525f64-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 485199,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12953059309231563546&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "College of Computing, Georgia Institute of Technology; School of Computer Science, Carnegie Mellon University; School of Computer Science, Carnegie Mellon University",
        "aff_domain": "cc.gatech.edu;cs.cmu.edu;cs.cmu.edu",
        "email": "cc.gatech.edu;cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Georgia Institute of Technology;Carnegie Mellon University",
        "aff_unique_dep": "College of Computing;School of Computer Science",
        "aff_unique_url": "https://www.gatech.edu;https://www.cmu.edu",
        "aff_unique_abbr": "Georgia Tech;CMU",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Atlanta;Pittsburgh",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "6df61cb398",
        "title": "Large-Scale Category Structure Aware Image Categorization",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/d5cfead94f5350c12c322b5b664544c1-Abstract.html",
        "author": "Bin Zhao; Fei Li; Eric P. Xing",
        "abstract": "Most previous research on image categorization has focused on medium-scale data sets, while large-scale image categorization with millions of images from thousands of categories remains a challenge. With the emergence of structured large-scale dataset such as the ImageNet, rich information about the conceptual relationships between images, such as a tree hierarchy among various image categories, become available. As human cognition of complex visual world benefits from underlying semantic relationships between object classes, we believe a machine learning system can and should leverage such information as well for better performance. In this paper, we employ such semantic relatedness among image categories for large-scale image categorization. Specifically, a category hierarchy is utilized to properly define loss function and select common set of features for  related categories. An efficient optimization method based on proximal approximation and accelerated parallel gradient method is introduced. Experimental results on a subset of ImageNet containing 1.2 million images from 1000 categories demonstrate the effectiveness and promise of our proposed approach.",
        "bibtex": "@inproceedings{NIPS2011_d5cfead9,\n author = {Zhao, Bin and Li, Fei and Xing, Eric},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Large-Scale Category Structure Aware Image Categorization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/d5cfead94f5350c12c322b5b664544c1-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/d5cfead94f5350c12c322b5b664544c1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/d5cfead94f5350c12c322b5b664544c1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 849218,
        "gs_citation": 118,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4134075272031579264&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff": "School of Computer Science, Carnegie Mellon University; Computer Science Department, Stanford University; School of Computer Science, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.stanford.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.stanford.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Carnegie Mellon University;Stanford University",
        "aff_unique_dep": "School of Computer Science;Computer Science Department",
        "aff_unique_url": "https://www.cmu.edu;https://www.stanford.edu",
        "aff_unique_abbr": "CMU;Stanford",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Pittsburgh;Stanford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ef536b2ab9",
        "title": "Large-Scale Sparse Principal Component Analysis with Application to Text Data",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/142949df56ea8ae0be8b5306971900a4-Abstract.html",
        "author": "Youwei Zhang; Laurent E. Ghaoui",
        "abstract": "Sparse PCA provides a linear combination of small number of features that maximizes variance across data. Although Sparse PCA has apparent advantages compared to PCA, such as better interpretability, it is generally thought to be computationally much more expensive. In this paper, we demonstrate the surprising fact that sparse PCA can be easier than PCA in practice, and that it can be reliably applied to very large data sets. This comes from a rigorous feature elimination  pre-processing result, coupled with the favorable fact that features in real-life data typically have exponentially decreasing variances, which allows for many features to be eliminated. We introduce a fast block coordinate ascent algorithm with much better computational complexity than the existing first-order ones. We provide  experimental results obtained on text corpora involving millions of documents and hundreds of thousands of features. These results illustrate how Sparse PCA can help organize a large corpus of text data in a user-interpretable way, providing an attractive alternative approach to topic models.",
        "bibtex": "@inproceedings{NIPS2011_142949df,\n author = {Zhang, Youwei and Ghaoui, Laurent},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Large-Scale Sparse Principal Component Analysis with Application to Text Data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/142949df56ea8ae0be8b5306971900a4-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/142949df56ea8ae0be8b5306971900a4-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/142949df56ea8ae0be8b5306971900a4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 220935,
        "gs_citation": 93,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6970535521001957000&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Electrical Engineering and Computer Sciences, University of California, Berkeley; Department of Electrical Engineering and Computer Sciences, University of California, Berkeley",
        "aff_domain": "eecs.berkeley.edu;eecs.berkeley.edu",
        "email": "eecs.berkeley.edu;eecs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "Department of Electrical Engineering and Computer Sciences",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "fabb3da430",
        "title": "Learning Anchor Planes for Classification",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/23ce1851341ec1fa9e0c259de10bf87c-Abstract.html",
        "author": "Ziming Zhang; Lubor Ladicky; Philip Torr; Amir Saffari",
        "abstract": "Local Coordinate Coding (LCC) [18] is a method for modeling functions of data lying on non-linear manifolds. It provides a set of anchor points which form a local coordinate system, such that each data point on the manifold can be approximated by a linear combination of its anchor points, and the linear weights become the local coordinate coding. In this paper we propose encoding data using orthogonal  anchor planes, rather than anchor points. Our method needs only a few orthogonal anchor planes for coding, and it can linearize any (\\alpha,\\beta,p)-Lipschitz smooth nonlinear  function with a fixed expected value of the upper-bound approximation error on any high dimensional data. In practice, the orthogonal coordinate system can be easily learned by minimizing this upper bound using singular value decomposition (SVD). We apply our method to model the coordinates locally in linear SVMs for classification tasks, and our experiment on MNIST shows that using only 50 anchor planes our method achieves 1.72% error rate, while LCC achieves 1.90% error rate using 4096 anchor points.",
        "bibtex": "@inproceedings{NIPS2011_23ce1851,\n author = {Zhang, Ziming and Ladicky, Lubor and Torr, Philip and Saffari, Amir},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Anchor Planes for Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/23ce1851341ec1fa9e0c259de10bf87c-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/23ce1851341ec1fa9e0c259de10bf87c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/23ce1851341ec1fa9e0c259de10bf87c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 831744,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6296820829785184585&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computing, Oxford Brookes University, Wheatley, Oxford, OX33 1HX, U.K.+Sony Computer Entertainment Europe, London, UK; Department of Engineering Science, University of Oxford, Parks Road, Oxford, OX1 3PJ, U.K.; Department of Computing, Oxford Brookes University, Wheatley, Oxford, OX33 1HX, U.K.+Sony Computer Entertainment Europe, London, UK; Sony Computer Entertainment Europe, London, UK",
        "aff_domain": "brookes.ac.uk;brookes.ac.uk;robots.ox.ac.uk;ymer.org",
        "email": "brookes.ac.uk;brookes.ac.uk;robots.ox.ac.uk;ymer.org",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2;0+1;1",
        "aff_unique_norm": "Department of Computing, Oxford Brookes University, Wheatley, Oxford, OX33 1HX, U.K.;Sony Computer Entertainment Europe, London, UK;Department of Engineering Science, University of Oxford, Parks Road, Oxford, OX1 3PJ, U.K.",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": ";",
        "aff_country_unique": ""
    },
    {
        "id": "6fbef2074e",
        "title": "Learning Auto-regressive Models from Sequence and Non-sequence Data",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/6c3cf77d52820cd0fe646d38bc2145ca-Abstract.html",
        "author": "Tzu-kuo Huang; Jeff G. Schneider",
        "abstract": "Vector Auto-regressive models (VAR) are useful tools for analyzing time series  data. In quite a few modern time series modelling tasks, the collection of reliable  time series turns out to be a major challenge, either due to the slow progression of  the dynamic process of interest, or inaccessibility of repetitive measurements of  the same dynamic process over time. In those situations, however, we observe that  it is often easier to collect a large amount of non-sequence samples, or snapshots  of the dynamic process of interest. In this work, we assume a small amount of time  series data are available, and propose methods to incorporate non-sequence data  into penalized least-square estimation of VAR models. We consider non-sequence  data as samples drawn from the stationary distribution of the underlying VAR  model, and devise a novel penalization scheme based on the discrete-time Lyapunov  equation concerning the covariance of the stationary distribution. Experiments  on synthetic and video data demonstrate the effectiveness of the proposed  methods.",
        "bibtex": "@inproceedings{NIPS2011_6c3cf77d,\n author = {Huang, Tzu-kuo and Schneider, Jeff},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Auto-regressive Models from Sequence and Non-sequence Data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/6c3cf77d52820cd0fe646d38bc2145ca-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/6c3cf77d52820cd0fe646d38bc2145ca-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/6c3cf77d52820cd0fe646d38bc2145ca-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 269731,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1129437110414331866&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Machine Learning Department, Carnegie Mellon University; Robotics Institute, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Machine Learning Department",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Pittsburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "46da6e8145",
        "title": "Learning Eigenvectors for Free",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/3493894fa4ea036cfc6433c3e2ee63b0-Abstract.html",
        "author": "Wouter M. Koolen; Wojciech Kotlowski; Manfred K. Warmuth",
        "abstract": "We extend the classical problem of predicting a sequence  of outcomes from a finite alphabet to the matrix domain.   In this extension, the alphabet of $n$ outcomes is replaced by the set of all dyads, i.e.   outer products $\\u\\u^\\top$ where $\\u$ is a vector in $\\R^n$ of unit length.  Whereas in the classical case the goal is to learn   (i.e. sequentially predict as well as) the best multinomial distribution,  in the matrix case we desire to learn the density matrix  that best explains the observed sequence of dyads. We show  how popular online algorithms for learning a multinomial  distribution can be extended to learn density matrices.  Intuitively, learning the $n^2$ parameters of a density matrix is much harder than learning the $n$ parameters of a multinomial distribution. Completely surprisingly, we prove that the worst-case regrets of   certain classical algorithms and   their matrix generalizations are identical.   The reason is that the worst-case sequence of dyads share a common   eigensystem, i.e. the worst case regret is achieved in the  classical case. So these matrix algorithms learn the eigenvectors  without any regret.",
        "bibtex": "@inproceedings{NIPS2011_3493894f,\n author = {Koolen, Wouter M and Kotlowski, Wojciech and Warmuth, Manfred K. K},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Eigenvectors for Free},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/3493894fa4ea036cfc6433c3e2ee63b0-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/3493894fa4ea036cfc6433c3e2ee63b0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/3493894fa4ea036cfc6433c3e2ee63b0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 377631,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1631143100403873283&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Royal Holloway and CWI; Centrum Wiskunde & Informatica; UC Santa Cruz",
        "aff_domain": "cs.rhul.ac.uk;cwi.nl;cse.ucsc.edu",
        "email": "cs.rhul.ac.uk;cwi.nl;cse.ucsc.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Royal Holloway and CWI;Centrum Wiskunde & Informatica;University of California, Santa Cruz",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";https://www.cwi.nl/;https://www.ucsc.edu",
        "aff_unique_abbr": ";CWI;UCSC",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Santa Cruz",
        "aff_country_unique_index": "1;2",
        "aff_country_unique": ";Netherlands;United States"
    },
    {
        "id": "d2a18b5947",
        "title": "Learning Higher-Order Graph Structure with Features by Structure Penalty",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/0336dcbab05b9d5ad24f4333c7658a0e-Abstract.html",
        "author": "Shilin Ding; Grace Wahba; Xiaojin Zhu",
        "abstract": "In discrete undirected graphical models, the conditional independence of node labels Y is specified by the graph structure. We study the case where there is another input random vector X (e.g. observed features) such that the distribution P (Y | X) is determined by functions of X that characterize the (higher-order) interactions among the Y \u2019s. The main contribution of this paper is to learn the graph structure and the functions conditioned on X at the same time. We prove that discrete undirected graphical models with feature X are equivalent to mul- tivariate discrete models. The reparameterization of the potential functions in graphical models by conditional log odds ratios of the latter offers advantages in representation of the conditional independence structure. The functional spaces can be flexibly determined by kernels. Additionally, we impose a Structure Lasso (SLasso) penalty on groups of functions to learn the graph structure. These groups with overlaps are designed to enforce hierarchical function selection. In this way, we are able to shrink higher order interactions to obtain a sparse graph structure.",
        "bibtex": "@inproceedings{NIPS2011_0336dcba,\n author = {Ding, Shilin and Wahba, Grace and Zhu, Jerry},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Higher-Order Graph Structure with Features by Structure Penalty},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/0336dcbab05b9d5ad24f4333c7658a0e-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/0336dcbab05b9d5ad24f4333c7658a0e-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/0336dcbab05b9d5ad24f4333c7658a0e-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/0336dcbab05b9d5ad24f4333c7658a0e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 173077,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13498865142806244004&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Statistics; Department of Statistics + Department of Computer Sciences + Department of Biostatistics and Medical Informatics; Department of Computer Sciences",
        "aff_domain": "stat.wisc.edu;stat.wisc.edu;cs.wisc.edu",
        "email": "stat.wisc.edu;stat.wisc.edu;cs.wisc.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1+1;1",
        "aff_unique_norm": "University Affiliation Not Specified;University of Wisconsin-Madison",
        "aff_unique_dep": "Department of Statistics;Department of Computer Sciences",
        "aff_unique_url": ";https://www.cs.wisc.edu",
        "aff_unique_abbr": ";UW-Madison",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1+1;1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "e60180b791",
        "title": "Learning Patient-Specific Cancer Survival Distributions as a Sequence of Dependent Regressors",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/1019c8091693ef5c5f55970346633f92-Abstract.html",
        "author": "Chun-Nam Yu; Russell Greiner; Hsiu-Chin Lin; Vickie Baracos",
        "abstract": "An accurate model of patient survival time can help in the treatment and care of cancer patients. The common practice of providing survival time estimates based only on population averages for the site and stage of cancer ignores many important individual differences among patients. In this paper, we propose a local regression method for learning patient-specific survival time distribution based on patient attributes such as blood tests and clinical assessments. When tested on a cohort of more than 2000 cancer patients, our method gives survival time predictions that are much more accurate than popular survival analysis models such as the Cox and Aalen regression models. Our results also show that using patient-specific attributes can reduce the prediction error on survival time by as much as 20% when compared to using cancer site and stage only.",
        "bibtex": "@inproceedings{NIPS2011_1019c809,\n author = {Yu, Chun-Nam and Greiner, Russell and Lin, Hsiu-Chin and Baracos, Vickie},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Patient-Specific Cancer Survival Distributions as a Sequence of Dependent Regressors},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/1019c8091693ef5c5f55970346633f92-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/1019c8091693ef5c5f55970346633f92-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/1019c8091693ef5c5f55970346633f92-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 483338,
        "gs_citation": 265,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4838302450157372348&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computing Science, University of Alberta; Department of Computing Science, University of Alberta; Department of Computing Science, University of Alberta; Department of Oncology, University of Alberta",
        "aff_domain": "ualberta.ca;ualberta.ca;ualberta.ca;ualberta.ca",
        "email": "ualberta.ca;ualberta.ca;ualberta.ca;ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "University of Alberta;Department of Oncology, University of Alberta",
        "aff_unique_dep": "Department of Computing Science;",
        "aff_unique_url": "https://www.ualberta.ca;",
        "aff_unique_abbr": "UAlberta;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada;"
    },
    {
        "id": "3074e1fa17",
        "title": "Learning Probabilistic Non-Linear Latent Variable Models for Tracking Complex Activities",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/fc8001f834f6a5f0561080d134d53d29-Abstract.html",
        "author": "Angela Yao; Juergen Gall; Luc V. Gool; Raquel Urtasun",
        "abstract": "A common approach for handling the complexity and inherent ambiguities of 3D human pose estimation is to use pose priors learned from  training data. Existing approaches however, are either too simplistic (linear), too complex to learn, or  can only learn latent spaces from \"simple data\", i.e., single activities such as walking or running. In this paper, we present an efficient stochastic gradient descent algorithm that is able to learn probabilistic non-linear latent  spaces composed of multiple  activities. Furthermore, we derive an incremental algorithm for the online setting which can update the latent space without extensive relearning. We demonstrate the  effectiveness of our approach on the  task of monocular and multi-view tracking and show that our approach  outperforms the state-of-the-art.",
        "bibtex": "@inproceedings{NIPS2011_fc8001f8,\n author = {Yao, Angela and Gall, Juergen and Gool, Luc V and Urtasun, Raquel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Probabilistic Non-Linear Latent Variable Models for Tracking Complex Activities},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/fc8001f834f6a5f0561080d134d53d29-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/fc8001f834f6a5f0561080d134d53d29-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/fc8001f834f6a5f0561080d134d53d29-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/fc8001f834f6a5f0561080d134d53d29-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1551842,
        "gs_citation": 70,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14568290616760807398&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "ETH Zurich; ETH Zurich; ETH Zurich; TTI Chicago",
        "aff_domain": "vision.ee.ethz.ch;vision.ee.ethz.ch;vision.ee.ethz.ch;ttic.edu",
        "email": "vision.ee.ethz.ch;vision.ee.ethz.ch;vision.ee.ethz.ch;ttic.edu",
        "github": "",
        "project": "www.vision.ee.ethz.ch/yaoa",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "ETH Zurich;Toyota Technological Institute at Chicago",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ethz.ch;https://www.tti-chicago.org",
        "aff_unique_abbr": "ETHZ;TTI",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Chicago",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "Switzerland;United States"
    },
    {
        "id": "40d7ce4dd4",
        "title": "Learning Sparse Representations of High Dimensional Data on Large Scale Dictionaries",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/3ce3bd7d63a2c9c81983cc8e9bd02ae5-Abstract.html",
        "author": "Zhen J. Xiang; Hao Xu; Peter J. Ramadge",
        "abstract": "Learning sparse representations on data adaptive dictionaries is a state-of-the-art method for modeling data. But when the dictionary is large and the data dimension is high, it is a computationally challenging problem. We explore three aspects of the problem. First, we derive new, greatly improved screening tests that quickly identify codewords that are guaranteed to have zero weights. Second, we study the properties of random projections in the context of learning sparse representations. Finally, we develop a hierarchical framework that uses incremental random projections and screening to learn, in small stages, a hierarchically structured dictionary for sparse representations. Empirical results show that our framework can learn informative hierarchical sparse representations more efficiently.",
        "bibtex": "@inproceedings{NIPS2011_3ce3bd7d,\n author = {Xiang, Zhen and Xu, Hao and Ramadge, Peter J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Sparse Representations of High Dimensional Data on Large Scale Dictionaries},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/3ce3bd7d63a2c9c81983cc8e9bd02ae5-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/3ce3bd7d63a2c9c81983cc8e9bd02ae5-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/3ce3bd7d63a2c9c81983cc8e9bd02ae5-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/3ce3bd7d63a2c9c81983cc8e9bd02ae5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1113045,
        "gs_citation": 175,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2578129083627415615&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "5bd51e7aee",
        "title": "Learning a Distance Metric from a Network",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/dc58e3a306451c9d670adcd37004f48f-Abstract.html",
        "author": "Blake Shaw; Bert Huang; Tony Jebara",
        "abstract": "Many real-world networks are described by both connectivity information and features for every node.  To better model and understand these networks, we present structure preserving metric learning (SPML), an algorithm for learning a Mahalanobis distance metric from a network such that the learned distances are tied to the inherent connectivity structure of the network.  Like the graph embedding algorithm structure preserving embedding, SPML learns a metric which is structure preserving, meaning a connectivity algorithm such as k-nearest neighbors will yield the correct connectivity when applied using the distances from the learned metric.  We show a variety of synthetic and real-world experiments where SPML predicts link patterns from node features more accurately than standard techniques.  We further demonstrate a method for optimizing SPML based on stochastic gradient descent which removes the running-time dependency on the size of the network and allows the method to easily scale to networks of thousands of nodes and millions of edges.",
        "bibtex": "@inproceedings{NIPS2011_dc58e3a3,\n author = {Shaw, Blake and Huang, Bert and Jebara, Tony},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning a Distance Metric from a Network},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/dc58e3a306451c9d670adcd37004f48f-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/dc58e3a306451c9d670adcd37004f48f-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/dc58e3a306451c9d670adcd37004f48f-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/dc58e3a306451c9d670adcd37004f48f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 370060,
        "gs_citation": 109,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6046719763473271637&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Computer Science Dept., Columbia University + Foursquare; Computer Science Dept., Columbia University + University of Maryland; Computer Science Dept., Columbia University",
        "aff_domain": "cs.columbia.edu;cs.columbia.edu;cs.columbia.edu",
        "email": "cs.columbia.edu;cs.columbia.edu;cs.columbia.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+2;0",
        "aff_unique_norm": "Columbia University;Foursquare;University of Maryland",
        "aff_unique_dep": "Computer Science Dept.;;",
        "aff_unique_url": "https://www.columbia.edu;;https://www/umd.edu",
        "aff_unique_abbr": "Columbia;;UMD",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "c6c1d14707",
        "title": "Learning a Tree of Metrics with Disjoint Visual Features",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/eed5af6add95a9a6f1252739b1ad8c24-Abstract.html",
        "author": "Kristen Grauman; Fei Sha; Sung Ju Hwang",
        "abstract": "We introduce an approach to learn discriminative visual representations while exploiting external semantic knowledge about object category relationships.  Given a hierarchical taxonomy that captures semantic similarity between the objects, we learn a corresponding tree of metrics (ToM).  In this tree, we have one metric for each non-leaf node of the object hierarchy, and each metric is responsible for discriminating among its immediate subcategory children.  Specifically, a Mahalanobis metric learned for a given node must satisfy the appropriate (dis)similarity constraints generated only among its subtree members' training instances.  To further exploit the semantics, we introduce a novel regularizer coupling the metrics that prefers a sparse disjoint set of features to be selected for each metric relative to its ancestor supercategory nodes' metrics.  Intuitively, this reflects that visual cues most useful to distinguish the generic classes (e.g., feline vs. canine) should be different than those cues most useful to distinguish their component fine-grained classes (e.g., Persian cat vs. Siamese cat).  We validate our approach with multiple image datasets using the WordNet taxonomy, show its advantages over alternative metric learning approaches, and analyze the meaning of attribute features selected by our algorithm.",
        "bibtex": "@inproceedings{NIPS2011_eed5af6a,\n author = {Grauman, Kristen and Sha, Fei and Hwang, Sung},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning a Tree of Metrics with Disjoint Visual Features},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/eed5af6add95a9a6f1252739b1ad8c24-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/eed5af6add95a9a6f1252739b1ad8c24-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/eed5af6add95a9a6f1252739b1ad8c24-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 371136,
        "gs_citation": 84,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3319335417395584554&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "University of Texas; University of Texas; University of Southern California",
        "aff_domain": "cs.utexas.edu;cs.utexas.edu;usc.edu",
        "email": "cs.utexas.edu;cs.utexas.edu;usc.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of Texas;University of Southern California",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.utexas.edu;https://www.usc.edu",
        "aff_unique_abbr": "UT;USC",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Los Angeles",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "230a29da2a",
        "title": "Learning in Hilbert vs. Banach Spaces: A Measure Embedding Viewpoint",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/7b13b2203029ed80337f27127a9f1d28-Abstract.html",
        "author": "Kenji Fukumizu; Gert R. Lanckriet; Bharath K. Sriperumbudur",
        "abstract": "The goal of this paper is to investigate the advantages and disadvantages of learning in Banach spaces over Hilbert spaces. While many works have been carried out in generalizing Hilbert methods to Banach spaces, in this paper, we consider the simple problem of learning a Parzen window classifier in a reproducing kernel Banach space (RKBS)---which is closely related to the notion of embedding probability measures into an RKBS---in order to carefully understand its pros and cons over the Hilbert space classifier. We show that while this generalization yields richer distance measures on probabilities compared to its Hilbert space counterpart, it however suffers from serious computational drawback limiting its practical applicability, which therefore demonstrates the need for developing efficient learning algorithms in Banach spaces.",
        "bibtex": "@inproceedings{NIPS2011_7b13b220,\n author = {Fukumizu, Kenji and Lanckriet, Gert and Sriperumbudur, Bharath K.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning in Hilbert vs. Banach Spaces: A Measure Embedding Viewpoint},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/7b13b2203029ed80337f27127a9f1d28-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/7b13b2203029ed80337f27127a9f1d28-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/7b13b2203029ed80337f27127a9f1d28-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/7b13b2203029ed80337f27127a9f1d28-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 142108,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11989041163702299163&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Gatsby Unit, University College London; The Institute of Statistical Mathematics, Tokyo; Dept. of ECE, UCSanDiego",
        "aff_domain": "gatsby.ucl.ac.uk;ism.ac.jp;ece.ucsd.edu",
        "email": "gatsby.ucl.ac.uk;ism.ac.jp;ece.ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University College London;The Institute of Statistical Mathematics, Tokyo;Dept. of ECE, UCSanDiego",
        "aff_unique_dep": "Gatsby Unit;;",
        "aff_unique_url": "https://www.ucl.ac.uk;;",
        "aff_unique_abbr": "UCL;;",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "London;",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United Kingdom;"
    },
    {
        "id": "fe8220baeb",
        "title": "Learning large-margin halfspaces with more malicious noise",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/98dce83da57b0395e163467c9dae521b-Abstract.html",
        "author": "Phil Long; Rocco Servedio",
        "abstract": "We describe a simple algorithm that runs in time  poly(n,1/gamma,1/eps) and learns an unknown n-dimensional  gamma-margin halfspace to accuracy 1-eps in the presence of  malicious noise, when the noise rate is allowed to be as high as  Theta(eps gamma sqrt(log(1/gamma))). Previous efficient  algorithms could only learn to accuracy eps in the presence of  malicious noise of rate at most Theta(eps gamma).    Our algorithm does not work by optimizing a convex loss function.  We  show that no algorithm for learning gamma-margin halfspaces that  minimizes a convex proxy for misclassification error can tolerate  malicious noise at a rate greater than Theta(eps gamma); this may  partially explain why previous algorithms could not achieve the higher  noise tolerance of our new algorithm.",
        "bibtex": "@inproceedings{NIPS2011_98dce83d,\n author = {Long, Phil and Servedio, Rocco},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning large-margin halfspaces with more malicious noise},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/98dce83da57b0395e163467c9dae521b-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/98dce83da57b0395e163467c9dae521b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/98dce83da57b0395e163467c9dae521b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 193355,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5916355488301305802&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "Google; Columbia University",
        "aff_domain": "google.com;cs.columbia.edu",
        "email": "google.com;cs.columbia.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Google;Columbia University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.google.com;https://www.columbia.edu",
        "aff_unique_abbr": "Google;Columbia",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Mountain View;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "eb25dd112d",
        "title": "Learning person-object interactions for action recognition in still images",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/a67f096809415ca1c9f112d96d27689b-Abstract.html",
        "author": "Vincent Delaitre; Josef Sivic; Ivan Laptev",
        "abstract": "We investigate a discriminatively trained model of person-object interactions for recognizing common human actions in still images. We build on the locally order-less spatial pyramid bag-of-features model, which was shown to perform extremely well on a range of object, scene and human action recognition tasks. We introduce three principal contributions. First, we replace the standard quantized local HOG/SIFT features with stronger discriminatively trained body part and object detectors. Second, we introduce new person-object interaction features based on spatial co-occurrences of individual body parts and objects. Third, we address the combinatorial problem of a large number of possible interaction pairs and propose a discriminative selection procedure using a linear support vector machine (SVM) with a sparsity inducing regularizer. Learning of action-specific body part and object interactions bypasses the difficult problem of estimating the complete human body pose configuration. Benefits of the proposed model are shown on human action recognition in consumer photographs, outperforming the strong bag-of-features baseline.",
        "bibtex": "@inproceedings{NIPS2011_a67f0968,\n author = {Delaitre, Vincent and Sivic, Josef and Laptev, Ivan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning person-object interactions for action recognition in still images},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/a67f096809415ca1c9f112d96d27689b-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/a67f096809415ca1c9f112d96d27689b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/a67f096809415ca1c9f112d96d27689b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2838708,
        "gs_citation": 188,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10157541598039720371&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "\u00b4Ecole Normale Sup\u00e9rieure*; INRIA Paris - Rocquencourt*; INRIA Paris - Rocquencourt*",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "\u00b4Ecole Normale Sup\u00e9rieure*;INRIA Paris - Rocquencourt*",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "08a519ac10",
        "title": "Learning to Agglomerate Superpixel Hierarchies",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/941e1aaaba585b952b62c14a3a175a61-Abstract.html",
        "author": "Viren Jain; Srinivas C. Turaga; K Briggman; Moritz N. Helmstaedter; Winfried Denk; H. S. Seung",
        "abstract": "An agglomerative clustering algorithm merges the most similar pair of clusters at every iteration. The function that evaluates similarity is traditionally hand- designed, but there has been recent interest in supervised or semisupervised settings in which ground-truth clustered data is available for training. Here we show how to train a similarity function by regarding it as the action-value function of a reinforcement learning problem. We apply this general method to segment images by clustering superpixels, an application that we call Learning to Agglomerate Superpixel Hierarchies (LASH). When applied to a challenging dataset of brain images from serial electron microscopy, LASH dramatically improved segmentation accuracy when clustering supervoxels generated by state of the boundary detection algorithms. The naive strategy of directly training only supervoxel similarities and applying single linkage clustering produced less improvement.",
        "bibtex": "@inproceedings{NIPS2011_941e1aaa,\n author = {Jain, Viren and Turaga, Srinivas C and Briggman, K and Helmstaedter, Moritz and Denk, Winfried and Seung, H.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning to Agglomerate Superpixel Hierarchies},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/941e1aaaba585b952b62c14a3a175a61-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/941e1aaaba585b952b62c14a3a175a61-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/941e1aaaba585b952b62c14a3a175a61-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 734962,
        "gs_citation": 86,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11491773735240995758&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Janelia Farm Research Campus; Brain & Cognitive Sciences; Department of Biomedical Optics; Department of Biomedical Optics; Department of Biomedical Optics; Howard Hughes Medical Institute+Massachusetts Institute of Technology",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;2;2;3+4",
        "aff_unique_norm": "Janelia Farm Research Campus;Brain & Cognitive Sciences;Department of Biomedical Optics;Howard Hughes Medical Institute;Massachusetts Institute of Technology",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": ";;;https://www.hhmi.org;https://web.mit.edu",
        "aff_unique_abbr": ";;;HHMI;MIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1+1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "583258a4eb",
        "title": "Learning to Learn with Compound HD Models",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/6e7d2da6d3953058db75714ac400b584-Abstract.html",
        "author": "Antonio Torralba; Joshua B. Tenenbaum; Ruslan Salakhutdinov",
        "abstract": "We introduce HD (or ``Hierarchical-Deep'') models, a new compositional learning architecture that integrates deep learning models with structured hierarchical Bayesian models. Specifically we show how we can learn a hierarchical Dirichlet process (HDP) prior over the activities of the top-level features in a Deep Boltzmann Machine (DBM).  This compound HDP-DBM model learns to learn novel concepts from very few training examples, by learning low-level generic features, high-level features that capture correlations among low-level features, and a category hierarchy for sharing priors over the high-level features that are typical of different kinds of concepts.  We present efficient learning and inference algorithms for the HDP-DBM model and show that it is able to learn new concepts from very few examples on CIFAR-100 object recognition, handwritten character recognition, and human motion capture datasets.",
        "bibtex": "@inproceedings{NIPS2011_6e7d2da6,\n author = {Torralba, Antonio and Tenenbaum, Joshua and Salakhutdinov, Russ R},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning to Learn with Compound HD Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/6e7d2da6d3953058db75714ac400b584-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/6e7d2da6d3953058db75714ac400b584-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/6e7d2da6d3953058db75714ac400b584-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 692203,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5948501822398751785&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Statistics, University of Toronto; Brain and Cognitive Sciences, MIT; CSAIL, MIT",
        "aff_domain": "utstat.toronto.edu;mit.edu;mit.edu",
        "email": "utstat.toronto.edu;mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of Toronto;Massachusetts Institute of Technology",
        "aff_unique_dep": "Department of Statistics;Department of Brain and Cognitive Sciences",
        "aff_unique_url": "https://www.utoronto.ca;https://web.mit.edu",
        "aff_unique_abbr": "U of T;MIT",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Toronto;Cambridge",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "Canada;United States"
    },
    {
        "id": "9cb28b5d2e",
        "title": "Learning to Search Efficiently in High Dimensions",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/eeb69a3cb92300456b6a5f4162093851-Abstract.html",
        "author": "Zhen Li; Huazhong Ning; Liangliang Cao; Tong Zhang; Yihong Gong; Thomas S. Huang",
        "abstract": "High dimensional similarity search in large scale databases becomes an important challenge due to the advent of Internet. For such applications, specialized data structures are required to achieve computational efficiency. Traditional approaches relied on algorithmic constructions that are often data independent (such as Locality Sensitive Hashing) or weakly dependent (such as kd-trees, k-means trees). While supervised learning algorithms have been applied to related problems, those proposed in the literature mainly focused on learning hash codes optimized for compact embedding of the data rather than search efficiency. Consequently such an embedding has to be used with linear scan or another search algorithm. Hence learning to hash does not directly address the search efficiency issue. This paper considers a new framework that applies supervised learning to directly optimize a data structure that supports efficient large scale search. Our approach takes both search quality and computational cost into consideration. Specifically, we learn a boosted search forest that is optimized using pair-wise similarity labeled examples. The output of this search forest can be efficiently converted into an inverted indexing data structure, which can leverage modern text search infrastructure to achieve both scalability and efficiency. Experimental results show that our approach significantly outperforms the start-of-the-art learning to hash methods (such as spectral hashing), as well as state-of-the-art high dimensional search algorithms (such as LSH and k-means trees).",
        "bibtex": "@inproceedings{NIPS2011_eeb69a3c,\n author = {Li, Zhen and Ning, Huazhong and Cao, Liangliang and Zhang, Tong and Gong, Yihong and Huang, Thomas S},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning to Search Efficiently in High Dimensions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/eeb69a3cb92300456b6a5f4162093851-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/eeb69a3cb92300456b6a5f4162093851-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/eeb69a3cb92300456b6a5f4162093851-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 114684,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=776352751685826325&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "UIUC; Google Inc.; IBM T.J. Watson Research Center; Rutgers University; NEC China; UIUC",
        "aff_domain": "uiuc.edu;gooogle.com;us.ibm.com;stat.rutgers.edu;gmail.com;ifp.uiuc.edu",
        "email": "uiuc.edu;gooogle.com;us.ibm.com;stat.rutgers.edu;gmail.com;ifp.uiuc.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3;4;0",
        "aff_unique_norm": "University of Illinois at Urbana-Champaign;Google;IBM;Rutgers University;NEC China",
        "aff_unique_dep": ";;Research Center;;",
        "aff_unique_url": "https://www illinois.edu;https://www.google.com;https://www.ibm.com/research/watson;https://www.rutgers.edu;",
        "aff_unique_abbr": "UIUC;Google;IBM;Rutgers;",
        "aff_campus_unique_index": "0;1;2;0",
        "aff_campus_unique": "Urbana-Champaign;Mountain View;T.J. Watson;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "360273e235",
        "title": "Learning unbelievable probabilities",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/cee631121c2ec9232f3a2f028ad5c89b-Abstract.html",
        "author": "Zachary Pitkow; Yashar Ahmadian; Ken D. Miller",
        "abstract": "Loopy belief propagation performs approximate inference on graphical models with loops. One might hope to compensate for the approximation by adjusting model parameters. Learning algorithms for this purpose have been explored previously, and the claim has been made that every set of locally consistent marginals can arise from belief propagation run on a graphical model. On the contrary, here we show that many probability distributions have marginals that cannot be reached by belief propagation using any set of model parameters or any learning algorithm. We call such marginals `unbelievable.' This problem occurs whenever the Hessian of the Bethe free energy is not positive-definite at the target marginals. All learning algorithms for belief propagation necessarily fail in these cases, producing beliefs or sets of beliefs that may even be worse than the pre-learning approximation. We then show that averaging inaccurate beliefs, each obtained from belief propagation using model parameters perturbed about some learned mean values, can achieve the unbelievable marginals.",
        "bibtex": "@inproceedings{NIPS2011_cee63112,\n author = {Pitkow, Zachary and Ahmadian, Yashar and Miller, Ken},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning unbelievable probabilities},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/cee631121c2ec9232f3a2f028ad5c89b-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/cee631121c2ec9232f3a2f028ad5c89b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/cee631121c2ec9232f3a2f028ad5c89b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/cee631121c2ec9232f3a2f028ad5c89b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1726997,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14740698964727746319&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Brain and Cognitive Science, University of Rochester; Center for Theoretical Neuroscience, Columbia University; Center for Theoretical Neuroscience, Columbia University",
        "aff_domain": "neurotheory.columbia.edu;columbia.edu;neurotheory.columbia.edu",
        "email": "neurotheory.columbia.edu;columbia.edu;neurotheory.columbia.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Department of Brain and Cognitive Science, University of Rochester;Columbia University",
        "aff_unique_dep": ";Center for Theoretical Neuroscience",
        "aff_unique_url": ";https://www.columbia.edu",
        "aff_unique_abbr": ";Columbia",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "d735a8e2c3",
        "title": "Learning with the weighted trace-norm under arbitrary sampling distributions",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/f47330643ae134ca204bf6b2481fec47-Abstract.html",
        "author": "Rina Foygel; Ohad Shamir; Nati Srebro; Ruslan Salakhutdinov",
        "abstract": "We provide rigorous guarantees on learning with the weighted trace-norm under arbitrary sampling distributions.  We show that the standard weighted-trace norm might fail when the sampling distribution is not a product distribution (i.e. when row and column indexes are not selected independently), present a corrected variant for which we establish strong learning guarantees, and demonstrate that it works better in practice.  We provide guarantees when weighting by either the true or empirical sampling distribution, and suggest that even if the true distribution is known (or is uniform), weighting by the empirical distribution may be beneficial.",
        "bibtex": "@inproceedings{NIPS2011_f4733064,\n author = {Foygel, Rina and Shamir, Ohad and Srebro, Nati and Salakhutdinov, Russ R},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning with the weighted trace-norm under arbitrary sampling distributions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/f47330643ae134ca204bf6b2481fec47-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/f47330643ae134ca204bf6b2481fec47-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/f47330643ae134ca204bf6b2481fec47-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/f47330643ae134ca204bf6b2481fec47-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 259717,
        "gs_citation": 78,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6921112463973941925&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 21,
        "aff": "Department of Statistics, University of Chicago; Department of Statistics, University of Toronto; Microsoft Research New England; Toyota Technological Institute at Chicago",
        "aff_domain": "uchicago.edu;ustat.toronto.edu;microsoft.com;ttic.edu",
        "email": "uchicago.edu;ustat.toronto.edu;microsoft.com;ttic.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "University of Chicago;University of Toronto;Microsoft Research;Toyota Technological Institute at Chicago",
        "aff_unique_dep": "Department of Statistics;Department of Statistics;Microsoft Research;",
        "aff_unique_url": "https://www.uchicago.edu;https://www.utoronto.ca;https://www.microsoft.com/en-us/research/group/microsoft-research-new-england;https://www.tti-chicago.org",
        "aff_unique_abbr": "UChicago;U of T;MSR NE;TTI Chicago",
        "aff_campus_unique_index": "1;2;3",
        "aff_campus_unique": ";Toronto;New England;Chicago",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "United States;Canada"
    },
    {
        "id": "5c66c501f4",
        "title": "Linear Submodular Bandits and their Application to Diversified Retrieval",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/33ebd5b07dc7e407752fe773eed20635-Abstract.html",
        "author": "Yisong Yue; Carlos Guestrin",
        "abstract": "Diversified retrieval and online learning are two core research areas in the design of modern information retrieval systems.In this paper, we propose the linear submodular bandits problem, which is an online learning setting for optimizing a general class of feature-rich submodular utility models for diversified retrieval. We present an algorithm, called LSBGREEDY, and prove that it efficiently converges to a near-optimal model. As a case study, we applied our approach to the setting of personalized news recommendation, where the system must recommend small sets of news articles selected from tens of thousands of available articles each day. In a live user study, we found that LSBGREEDY significantly outperforms existing online learning approaches.",
        "bibtex": "@inproceedings{NIPS2011_33ebd5b0,\n author = {Yue, Yisong and Guestrin, Carlos},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Linear Submodular Bandits and their Application to Diversified Retrieval},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/33ebd5b07dc7e407752fe773eed20635-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/33ebd5b07dc7e407752fe773eed20635-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/33ebd5b07dc7e407752fe773eed20635-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/33ebd5b07dc7e407752fe773eed20635-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 529960,
        "gs_citation": 206,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8501902443565075752&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "iLab, Heinz College, Carnegie Mellon University; Machine Learning Department, Carnegie Mellon University",
        "aff_domain": "cmu.edu;cs.cmu.edu",
        "email": "cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "iLab, Heinz College, Carnegie Mellon University;Carnegie Mellon University",
        "aff_unique_dep": ";Machine Learning Department",
        "aff_unique_url": ";https://www.cmu.edu",
        "aff_unique_abbr": ";CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "45b5d9e947",
        "title": "Linearized Alternating Direction Method with Adaptive Penalty for Low-Rank Representation",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/18997733ec258a9fcaf239cc55d53363-Abstract.html",
        "author": "Zhouchen Lin; Risheng Liu; Zhixun Su",
        "abstract": "Many machine learning and signal processing problems can be formulated as linearly constrained convex programs, which could be efficiently solved by the alternating direction method (ADM). However, usually the subproblems in ADM are easily solvable only when the linear mappings in the constraints are identities. To address this issue, we propose a linearized ADM (LADM) method by linearizing the quadratic penalty term and adding a proximal term when solving the subproblems. For fast convergence, we also allow the penalty to change adaptively according a novel update rule. We prove the global convergence of LADM with adaptive penalty (LADMAP). As an example, we apply LADMAP to solve low-rank representation (LRR), which is an important subspace clustering technique yet suffers from high computation cost. By combining LADMAP with a skinny SVD representation technique, we are able to reduce the complexity $O(n^3)$  of the original ADM based method to $O(rn^2)$, where $r$ and $n$ are the rank and size of the representation matrix, respectively, hence making LRR possible for large scale applications. Numerical experiments verify that for LRR our LADMAP based methods are much faster than state-of-the-art algorithms.",
        "bibtex": "@inproceedings{NIPS2011_18997733,\n author = {Lin, Zhouchen and Liu, Risheng and Su, Zhixun},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Linearized Alternating Direction Method with Adaptive Penalty for Low-Rank Representation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/18997733ec258a9fcaf239cc55d53363-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/18997733ec258a9fcaf239cc55d53363-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/18997733ec258a9fcaf239cc55d53363-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/18997733ec258a9fcaf239cc55d53363-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 107133,
        "gs_citation": 1501,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10690149927183190868&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "a3809a8bef",
        "title": "Lower Bounds for Passive and Active Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/051e4e127b92f5d98d3c79b195f2b291-Abstract.html",
        "author": "Maxim Raginsky; Alexander Rakhlin",
        "abstract": "We develop unified information-theoretic machinery for deriving lower bounds for passive and active learning schemes. Our bounds involve the so-called Alexander's capacity function. The supremum of this function has been recently rediscovered by Hanneke in the context of active learning under the name of \"disagreement coefficient.\" For passive learning, our lower bounds match the upper bounds of Gine and Koltchinskii up to constants and generalize analogous results of Massart and Nedelec. For active learning, we provide first known lower bounds based on the capacity function rather than the disagreement coefficient.",
        "bibtex": "@inproceedings{NIPS2011_051e4e12,\n author = {Raginsky, Maxim and Rakhlin, Alexander},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Lower Bounds for Passive and Active Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/051e4e127b92f5d98d3c79b195f2b291-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/051e4e127b92f5d98d3c79b195f2b291-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/051e4e127b92f5d98d3c79b195f2b291-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/051e4e127b92f5d98d3c79b195f2b291-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 211988,
        "gs_citation": 57,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9027316738139130662&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Coordinated Science Laboratory, University of Illinois at Urbana-Champaign + Department of Electrical and Computer Engineering, Duke University; Department of Statistics, University of Pennsylvania",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2",
        "aff_unique_norm": "University of Illinois at Urbana-Champaign;Duke University;University of Pennsylvania",
        "aff_unique_dep": "Coordinated Science Laboratory;Department of Electrical and Computer Engineering;Department of Statistics",
        "aff_unique_url": "https://www illinois.edu;https://www.duke.edu;https://www.upenn.edu",
        "aff_unique_abbr": "UIUC;Duke;UPenn",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Urbana-Champaign;",
        "aff_country_unique_index": "0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e7247d7f25",
        "title": "MAP Inference for Bayesian Inverse Reinforcement Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/3a15c7d0bbe60300a39f76f8a5ba6896-Abstract.html",
        "author": "Jaedeug Choi; Kee-eung Kim",
        "abstract": "The difficulty in inverse reinforcement learning (IRL) arises in choosing the best reward function since there are typically an infinite number of reward functions that yield the given behaviour data as optimal. Using a Bayesian framework, we address this challenge by using the maximum a posteriori (MAP) estimation for the reward function, and show that most of the previous IRL algorithms can be modeled into our framework. We also present a gradient method for the MAP estimation based on the (sub)differentiability of the posterior distribution. We show the effectiveness of our approach by comparing the performance of the proposed method to those of the previous algorithms.",
        "bibtex": "@inproceedings{NIPS2011_3a15c7d0,\n author = {Choi, Jaedeug and Kim, Kee-eung},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {MAP Inference for Bayesian Inverse Reinforcement Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/3a15c7d0bbe60300a39f76f8a5ba6896-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/3a15c7d0bbe60300a39f76f8a5ba6896-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/3a15c7d0bbe60300a39f76f8a5ba6896-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/3a15c7d0bbe60300a39f76f8a5ba6896-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 232838,
        "gs_citation": 120,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17266193590210824944&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science, Korea Advanced Institute of Science and Technology; Department of Computer Science, Korea Advanced Institute of Science and Technology",
        "aff_domain": "ai.kaist.ac.kr;cs.kaist.ac.kr",
        "email": "ai.kaist.ac.kr;cs.kaist.ac.kr",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Department of Computer Science, Korea Advanced Institute of Science and Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "8ba6358e96",
        "title": "Manifold Precis: An Annealing Technique for Diverse Sampling of Manifolds",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/d1f491a404d6854880943e5c3cd9ca25-Abstract.html",
        "author": "Nitesh Shroff; Pavan Turaga; Rama Chellappa",
        "abstract": "In this paper, we consider the 'Precis' problem of sampling K representative yet diverse data points from a large dataset. This problem arises frequently in applications such as video and document summarization, exploratory data analysis, and pre-filtering. We formulate a general theory which encompasses not just traditional techniques devised for vector spaces, but also non-Euclidean manifolds, thereby enabling these techniques to shapes, human activities, textures and many other image and video based datasets. We propose intrinsic manifold measures for measuring the quality of a selection of points with respect to their representative power, and their diversity. We then propose efficient algorithms to optimize the cost function using a novel annealing-based iterative alternation algorithm. The proposed formulation is applicable to manifolds of known geometry as well as to manifolds whose geometry needs to be estimated from samples. Experimental results show the strength and generality of the proposed approach.",
        "bibtex": "@inproceedings{NIPS2011_d1f491a4,\n author = {Shroff, Nitesh and Turaga, Pavan and Chellappa, Rama},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Manifold Precis: An Annealing Technique for Diverse Sampling of Manifolds},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/d1f491a404d6854880943e5c3cd9ca25-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/d1f491a404d6854880943e5c3cd9ca25-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/d1f491a404d6854880943e5c3cd9ca25-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1053675,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15507565091368719047&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "DepartmentofElectricalandComputerEngineering,UniversityofMaryland,CollegePark; SchoolofArts,Media,EngineeringandECEE,ArizonaStateUniversity; DepartmentofElectricalandComputerEngineering,UniversityofMaryland,CollegePark",
        "aff_domain": "umiacs.umd.edu;asu.edu;umiacs.umd.edu",
        "email": "umiacs.umd.edu;asu.edu;umiacs.umd.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "DepartmentofElectricalandComputerEngineering,UniversityofMaryland,CollegePark;SchoolofArts,Media,EngineeringandECEE,ArizonaStateUniversity",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "277df1c7c7",
        "title": "Matrix Completion for Multi-label Image Classification",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/65a99bb7a3115fdede20da98b08a370f-Abstract.html",
        "author": "Ricardo S. Cabral; Fernando Torre; Joao P. Costeira; Alexandre Bernardino",
        "abstract": "Recently, image categorization has been an active research topic due to the urgent need to retrieve and browse digital images via semantic keywords. This paper formulates image categorization as a multi-label classification problem using recent advances in matrix completion. Under this setting, classification of testing data is posed as a problem of completing unknown label entries on a data matrix that concatenates training and testing features with training labels. We propose two convex algorithms for matrix completion based on a Rank Minimization criterion specifically tailored to visual data, and prove its convergence properties. A major advantage of our approach w.r.t. standard discriminative classification methods for image categorization is its robustness to outliers, background noise and partial occlusions both in the feature and label space. Experimental validation on several datasets shows how our method outperforms state-of-the-art algorithms, while effectively capturing semantic concepts of classes.",
        "bibtex": "@inproceedings{NIPS2011_65a99bb7,\n author = {Cabral, Ricardo and Torre, Fernando and Costeira, Joao P and Bernardino, Alexandre},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Matrix Completion for Multi-label Image Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/65a99bb7a3115fdede20da98b08a370f-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/65a99bb7a3115fdede20da98b08a370f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/65a99bb7a3115fdede20da98b08a370f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 554570,
        "gs_citation": 431,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2903419015847185905&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 20,
        "aff": "Carnegie Mellon University, Pittsburgh, PA+ISR - Instituto Superior T\u00e9cnico, Lisboa, Portugal; Carnegie Mellon University, Pittsburgh, PA; ISR - Instituto Superior T\u00e9cnico, Lisboa, Portugal; ISR - Instituto Superior T\u00e9cnico, Lisboa, Portugal",
        "aff_domain": "cmu.edu;cs.cmu.edu;isr.ist.utl.pt;isr.ist.utl.pt",
        "email": "cmu.edu;cs.cmu.edu;isr.ist.utl.pt;isr.ist.utl.pt",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;1;1",
        "aff_unique_norm": "Carnegie Mellon University;Instituto Superior T\u00e9cnico",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cmu.edu;https://www IST.pt",
        "aff_unique_abbr": "CMU;ISR",
        "aff_campus_unique_index": "0+1;0;1;1",
        "aff_campus_unique": "Pittsburgh;Lisboa",
        "aff_country_unique_index": "0+1;0;1;1",
        "aff_country_unique": "United States;Portugal"
    },
    {
        "id": "6f4e01a1ec",
        "title": "Maximal Cliques that Satisfy Hard Constraints with Application to Deformable Object Model Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/e6b4b2a746ed40e1af829d1fa82daa10-Abstract.html",
        "author": "Xinggang Wang; Xiang Bai; Xingwei Yang; Wenyu Liu; Longin J. Latecki",
        "abstract": "We propose a novel inference framework for finding maximal cliques in a weighted graph that satisfy hard constraints.  The constraints specify the graph nodes that must belong to the solution as well as mutual exclusions of graph nodes, i.e., sets of nodes that cannot belong to the same solution.  The proposed inference is based on a novel particle filter algorithm with state permeations. We apply the inference framework to a challenging problem of learning part-based, deformable object models. Two core problems in the learning framework, matching of image patches and finding salient parts, are formulated as two instances of the problem of finding maximal cliques with hard constraints. Our learning framework yields discriminative part based object models that achieve very good detection rate, and outperform other methods on object classes with large deformation.",
        "bibtex": "@inproceedings{NIPS2011_e6b4b2a7,\n author = {Wang, Xinggang and Bai, Xiang and Yang, Xingwei and Liu, Wenyu and Latecki, Longin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Maximal Cliques that Satisfy Hard Constraints with Application to Deformable Object Model Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/e6b4b2a746ed40e1af829d1fa82daa10-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/e6b4b2a746ed40e1af829d1fa82daa10-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/e6b4b2a746ed40e1af829d1fa82daa10-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 521001,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4276115627542940304&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 11,
        "aff": "Dept. of Electronics and Information Engineering, Huazhong Univ. of Science and Technology, China; Dept. of Electronics and Information Engineering, Huazhong Univ. of Science and Technology, China; Image Analytics Lab, GE Research, One Research Circle, Niskayuna, NY 12309, USA + Dept. of Computer and Information Sciences, Temple Univ., USA; Dept. of Electronics and Information Engineering, Huazhong Univ. of Science and Technology, China; Dept. of Computer and Information Sciences, Temple Univ., USA",
        "aff_domain": "gmail.com;gmail.com;ge.com;hust.edu.cn;temple.edu",
        "email": "gmail.com;gmail.com;ge.com;hust.edu.cn;temple.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1+2;0;2",
        "aff_unique_norm": "Dept. of Electronics and Information Engineering, Huazhong Univ. of Science and Technology, China;Image Analytics Lab, GE Research, One Research Circle, Niskayuna, NY 12309, USA;Dept. of Computer and Information Sciences, Temple Univ., USA",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "77706b6277",
        "title": "Maximum Covariance Unfolding : Manifold Learning for Bimodal Data",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/daca41214b39c5dc66674d09081940f0-Abstract.html",
        "author": "Vijay Mahadevan; Chi W. Wong; Jose C. Pereira; Tom Liu; Nuno Vasconcelos; Lawrence K. Saul",
        "abstract": "We propose maximum covariance unfolding (MCU), a manifold learning algorithm for simultaneous dimensionality reduction of data from different input modalities.   Given high dimensional inputs from two different but naturally aligned sources, MCU computes a common low dimensional embedding that maximizes the cross-modal (inter-source) correlations while preserving the local (intra-source) distances.  In this paper, we explore two applications of MCU.  First we use MCU to analyze EEG-fMRI data, where an important goal is to visualize the fMRI voxels that are most strongly correlated with changes in EEG traces.  To perform this visualization, we augment MCU with an additional step for metric learning in the high dimensional voxel space.  Second, we use MCU to perform cross-modal retrieval of matched image and text samples from Wikipedia.  To manage large applications of MCU, we develop a fast implementation based on ideas from spectral graph theory.  These ideas transform the original problem for MCU, one of semidefinite programming, into a simpler problem in semidefinite quadratic linear programming.",
        "bibtex": "@inproceedings{NIPS2011_daca4121,\n author = {Mahadevan, Vijay and Wong, Chi and Pereira, Jose and Liu, Tom and Vasconcelos, Nuno and Saul, Lawrence},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Maximum Covariance Unfolding : Manifold Learning for Bimodal Data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/daca41214b39c5dc66674d09081940f0-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/daca41214b39c5dc66674d09081940f0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/daca41214b39c5dc66674d09081940f0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 886614,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15336040816666979214&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of ECE, University of California, San Diego; Department of Radiology, University of California, San Diego; Department of ECE, University of California, San Diego; Department of Radiology, University of California, San Diego; Department of ECE, University of California, San Diego; Department of CSE, University of California, San Diego",
        "aff_domain": "ucsd.edu;ieee.org;ucsd.edu;ucsd.edu;ucsd.edu;cs.ucsd.edu",
        "email": "ucsd.edu;ieee.org;ucsd.edu;ucsd.edu;ucsd.edu;cs.ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;1;0;2",
        "aff_unique_norm": "Department of ECE, University of California, San Diego;Department of Radiology, University of California, San Diego;Department of CSE, University of California, San Diego",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "804d36db2a",
        "title": "Maximum Margin Multi-Instance Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/11d867796d85db8cad5280ac44cec7c1-Abstract.html",
        "author": "Hua Wang; Heng Huang; Farhad Kamangar; Feiping Nie; Chris H. Ding",
        "abstract": "Multi-instance learning (MIL) considers input as bags of instances, in which labels are assigned to the bags. MIL is useful in many real-world applications. For example, in image categorization semantic meanings (labels) of an image mostly arise from its regions (instances) instead of the entire image (bag). Existing MIL methods typically build their models using the Bag-to-Bag (B2B) distance, which are often computationally expensive and may not truly reflect the semantic similarities. To tackle this, in this paper we approach MIL problems from a new perspective using the Class-to-Bag (C2B) distance, which directly assesses the relationships between the classes and the bags. Taking into account the two major challenges in MIL, high heterogeneity on data and weak label association, we propose a novel Maximum Margin Multi-Instance Learning (M3 I) approach to parameterize the C2B distance by introducing the class specific distance metrics and the locally adaptive significance coefficients. We apply our new approach to the automatic image categorization tasks on three (one single-label and two multilabel) benchmark data sets. Extensive experiments have demonstrated promising results that validate the proposed method.",
        "bibtex": "@inproceedings{NIPS2011_11d86779,\n author = {Wang, Hua and Huang, Heng and Kamangar, Farhad and Nie, Feiping and Ding, Chris},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Maximum Margin Multi-Instance Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/11d867796d85db8cad5280ac44cec7c1-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/11d867796d85db8cad5280ac44cec7c1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/11d867796d85db8cad5280ac44cec7c1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 294507,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1277169301085223503&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Computer Science and Engineering, University of Texas at Arlington; Computer Science and Engineering, University of Texas at Arlington; Computer Science and Engineering, University of Texas at Arlington; Computer Science and Engineering, University of Texas at Arlington; Computer Science and Engineering, University of Texas at Arlington",
        "aff_domain": "gmail.com;uta.edu;uta.edu;gmail.com;uta.edu",
        "email": "gmail.com;uta.edu;uta.edu;gmail.com;uta.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Computer Science and Engineering, University of Texas at Arlington",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "31fbfb61be",
        "title": "Maximum Margin Multi-Label Structured Prediction",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/69adc1e107f7f7d035d7baf04342e1ca-Abstract.html",
        "author": "Christoph H. Lampert",
        "abstract": "We study multi-label prediction for structured output spaces, a problem that occurs, for example, in object detection in images, secondary structure prediction in computational biology, and graph matching with symmetries. Conventional multi-label classification techniques are typically not applicable in this situation, because they require explicit enumeration of the label space, which is infeasible in case of structured outputs. Relying on techniques originally designed for single- label structured prediction, in particular structured support vector machines, results in reduced prediction accuracy, or leads to infeasible optimization problems.  In this work we derive a maximum-margin training formulation for multi-label structured prediction that remains computationally tractable while achieving high prediction accuracy. It also shares most beneficial properties with single-label maximum-margin approaches, in particular a formulation as a convex optimization problem, efficient working set training, and PAC-Bayesian generalization bounds.",
        "bibtex": "@inproceedings{NIPS2011_69adc1e1,\n author = {Lampert, Christoph H},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Maximum Margin Multi-Label Structured Prediction},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/69adc1e107f7f7d035d7baf04342e1ca-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/69adc1e107f7f7d035d7baf04342e1ca-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/69adc1e107f7f7d035d7baf04342e1ca-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/69adc1e107f7f7d035d7baf04342e1ca-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 296266,
        "gs_citation": 47,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14745374415652159821&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "IST Austria (Institute of Science and Technology Austria)",
        "aff_domain": "ist.ac.at",
        "email": "ist.ac.at",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Institute of Science and Technology Austria",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ist.ac.at",
        "aff_unique_abbr": "IST Austria",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Austria"
    },
    {
        "id": "66f42bb49f",
        "title": "Message-Passing for Approximate MAP Inference with Latent Variables",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/53e3a7161e428b65688f14b84d61c610-Abstract.html",
        "author": "Jiarong Jiang; Piyush Rai; Hal Daume",
        "abstract": "We consider a general inference setting for discrete probabilistic graphical models where we seek maximum a posteriori (MAP) estimates for a subset of the random variables (max nodes), marginalizing over the rest (sum nodes). We present a hybrid message-passing algorithm to accomplish this.  The hybrid algorithm passes a mix of sum and max messages depending on the type of source node (sum or max). We derive our algorithm by showing that it falls out as the solution of a particular relaxation of a variational framework.  We further show that the Expectation Maximization algorithm can be seen as an approximation to our algorithm. Experimental results on synthetic and real-world datasets, against several baselines, demonstrate the efficacy of our proposed algorithm.",
        "bibtex": "@inproceedings{NIPS2011_53e3a716,\n author = {Jiang, Jiarong and Rai, Piyush and Daume, Hal},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Message-Passing for Approximate MAP Inference with Latent Variables},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/53e3a7161e428b65688f14b84d61c610-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/53e3a7161e428b65688f14b84d61c610-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/53e3a7161e428b65688f14b84d61c610-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/53e3a7161e428b65688f14b84d61c610-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 156635,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17694655275192837512&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Dept. of Computer Science, University of Maryland, CP; School of Computing, University of Utah; Dept. of Computer Science, University of Maryland, CP",
        "aff_domain": "umiacs.umd.edu;cs.utah.edu;umiacs.umd.edu",
        "email": "umiacs.umd.edu;cs.utah.edu;umiacs.umd.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Dept. of Computer Science, University of Maryland, CP;University of Utah",
        "aff_unique_dep": ";School of Computing",
        "aff_unique_url": ";https://www.utah.edu",
        "aff_unique_abbr": ";U of U",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Utah",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "cea6c549a0",
        "title": "Metric Learning with Multiple Kernels",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/24681928425f5a9133504de568f5f6df-Abstract.html",
        "author": "Jun Wang; Huyen T. Do; Adam Woznica; Alexandros Kalousis",
        "abstract": "Metric learning has become a very active research field. The most popular representative--Mahalanobis metric learning--can be seen as learning a linear transformation and then computing the Euclidean metric in the transformed space. Since a linear transformation might not always be appropriate for a given learning problem, kernelized versions of various metric learning algorithms exist. However, the problem then becomes finding the appropriate kernel function. Multiple kernel learning addresses this limitation by learning a linear combination of a number of predefined kernels; this approach can be also readily used in the context of multiple-source learning to fuse different data sources. Surprisingly, and despite the extensive work on multiple kernel learning for SVMs, there has been no work in   the area of metric learning with multiple kernel learning. In this paper we fill this gap and present a general approach for metric learning with multiple kernel learning. Our approach can be instantiated with different metric learning algorithms provided that they satisfy some constraints. Experimental evidence suggests that our approach outperforms metric learning with an unweighted kernel combination and metric learning with cross-validation based kernel selection.",
        "bibtex": "@inproceedings{NIPS2011_24681928,\n author = {Wang, Jun and T., Huyen and Woznica, Adam and Kalousis, Alexandros},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Metric Learning with Multiple Kernels},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/24681928425f5a9133504de568f5f6df-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/24681928425f5a9133504de568f5f6df-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/24681928425f5a9133504de568f5f6df-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 184240,
        "gs_citation": 102,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6943062632455722656&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "AI Lab, Department of Informatics, University of Geneva, Switzerland; AI Lab, Department of Informatics, University of Geneva, Switzerland; AI Lab, Department of Informatics, University of Geneva, Switzerland; AI Lab, Department of Informatics, University of Geneva, Switzerland",
        "aff_domain": "unige.ch;unige.ch;unige.ch;unige.ch",
        "email": "unige.ch;unige.ch;unige.ch;unige.ch",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "AI Lab, Department of Informatics, University of Geneva, Switzerland",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "f03757c8b6",
        "title": "Minimax Localization of Structural Information in Large Noisy Matrices",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/c6e19e830859f2cb9f7c8f8cacb8d2a6-Abstract.html",
        "author": "Mladen Kolar; Sivaraman Balakrishnan; Alessandro Rinaldo; Aarti Singh",
        "abstract": "We consider the problem of identifying a sparse set of relevant columns and rows in a large data matrix with highly corrupted entries. This problem of identifying groups from a collection of bipartite variables such as proteins and drugs, biological species and gene sequences, malware and signatures, etc is commonly referred to as biclustering or co-clustering. Despite its great practical relevance, and although several ad-hoc methods are available for biclustering, theoretical analysis of the problem is largely non-existent. The problem we consider is also closely related to structured multiple hypothesis testing, an area of statistics that has recently witnessed a flurry of activity. We make the following contributions: i) We prove lower bounds on the minimum signal strength needed for successful recovery of a bicluster as a function of the noise variance, size of the matrix and bicluster of interest. ii) We show that a combinatorial procedure based on the scan statistic achieves this optimal limit. iii) We characterize the SNR required by several computationally tractable procedures for biclustering including element-wise thresholding, column/row average thresholding and a convex relaxation approach to sparse singular vector decomposition.",
        "bibtex": "@inproceedings{NIPS2011_c6e19e83,\n author = {Kolar, Mladen and Balakrishnan, Sivaraman and Rinaldo, Alessandro and Singh, Aarti},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Minimax Localization of Structural Information in Large Noisy Matrices},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/c6e19e830859f2cb9f7c8f8cacb8d2a6-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/c6e19e830859f2cb9f7c8f8cacb8d2a6-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/c6e19e830859f2cb9f7c8f8cacb8d2a6-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/c6e19e830859f2cb9f7c8f8cacb8d2a6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 569685,
        "gs_citation": 80,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=607255959985782457&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "School of Computer Science; School of Computer Science + Department of Statistics; Department of Statistics; School of Computer Science",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;stat.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;stat.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1;1;0",
        "aff_unique_norm": "School of Computer Science;University Affiliation Not Specified",
        "aff_unique_dep": "Computer Science;Department of Statistics",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "44d41e9abe",
        "title": "Modelling Genetic Variations using Fragmentation-Coagulation Processes",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/5e388103a391daabe3de1d76a6739ccd-Abstract.html",
        "author": "Yee W. Teh; Charles Blundell; Lloyd Elliott",
        "abstract": "We propose a novel class of Bayesian nonparametric models for sequential data called fragmentation-coagulation processes (FCPs).  FCPs model a set of sequences using  a partition-valued Markov process which evolves by splitting and merging clusters.  An FCP is exchangeable, projective, stationary and reversible, and its equilibrium distributions are given by the Chinese restaurant process.  As opposed to hidden Markov models, FCPs allow for flexible modelling of the number of clusters, and they avoid label switching non-identifiability problems. We develop an efficient Gibbs sampler for FCPs which uses uniformization and the forward-backward algorithm.  Our development of FCPs is motivated by applications in population genetics, and we demonstrate the utility of FCPs on problems of genotype imputation with phased and unphased SNP data.",
        "bibtex": "@inproceedings{NIPS2011_5e388103,\n author = {Teh, Yee and Blundell, Charles and Elliott, Lloyd},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Modelling Genetic Variations using Fragmentation-Coagulation Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/5e388103a391daabe3de1d76a6739ccd-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/5e388103a391daabe3de1d76a6739ccd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/5e388103a391daabe3de1d76a6739ccd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 631182,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17734435355495786656&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "2cfa532d1c",
        "title": "Monte Carlo Value Iteration with Macro-Actions",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/eefc9e10ebdc4a2333b42b2dbb8f27b6-Abstract.html",
        "author": "Zhan Lim; Lee Sun; David Hsu",
        "abstract": "POMDP planning faces two major computational challenges: large state spaces and long planning horizons. The recently introduced Monte Carlo Value Iteration (MCVI) can tackle POMDPs with very large discrete state spaces or continuous state spaces, but its performance degrades when faced with long planning horizons. This paper presents Macro-MCVI, which extends MCVI by exploiting macro-actions for temporal abstraction. We provide sufficient conditions for Macro-MCVI to inherit the good theoretical properties of MCVI. Macro-MCVI does not require explicit construction of probabilistic models for macro-actions and is thus easy to apply in practice. Experiments show that Macro-MCVI substantially improves the performance of MCVI with suitable macro-actions.",
        "bibtex": "@inproceedings{NIPS2011_eefc9e10,\n author = {Lim, Zhan and Sun, Lee and Hsu, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Monte Carlo Value Iteration with Macro-Actions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/eefc9e10ebdc4a2333b42b2dbb8f27b6-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/eefc9e10ebdc4a2333b42b2dbb8f27b6-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/eefc9e10ebdc4a2333b42b2dbb8f27b6-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/eefc9e10ebdc4a2333b42b2dbb8f27b6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 414229,
        "gs_citation": 58,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3991132967619400328&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "a8dc9f498d",
        "title": "Multi-Bandit Best Arm Identification",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/c4851e8e264415c4094e4e85b0baa7cc-Abstract.html",
        "author": "Victor Gabillon; Mohammad Ghavamzadeh; Alessandro Lazaric; S\u00e9bastien Bubeck",
        "abstract": "We study the problem of identifying the best arm in each of the bandits in a multi-bandit multi-armed setting. We first propose an algorithm called Gap-based Exploration (GapE) that focuses on the arms whose mean is close to the mean of the best arm in the same bandit (i.e., small gap). We then introduce an algorithm, called GapE-V, which takes into account the variance of the arms in addition to their gap. We prove an upper-bound on the probability of error for both algorithms. Since GapE and GapE-V need to tune an exploration parameter that depends on the complexity of the problem, which is often unknown in advance, we also introduce variations of these algorithms that estimate this complexity online. Finally, we evaluate the performance of these algorithms and compare them to other allocation strategies on a number of synthetic problems.",
        "bibtex": "@inproceedings{NIPS2011_c4851e8e,\n author = {Gabillon, Victor and Ghavamzadeh, Mohammad and Lazaric, Alessandro and Bubeck, S\\'{e}bastien},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multi-Bandit Best Arm Identification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/c4851e8e264415c4094e4e85b0baa7cc-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/c4851e8e264415c4094e4e85b0baa7cc-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/c4851e8e264415c4094e4e85b0baa7cc-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 151244,
        "gs_citation": 136,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15531685211220760100&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 22,
        "aff": "INRIA Lille - Nord Europe, Team SequeL; INRIA Lille - Nord Europe, Team SequeL; INRIA Lille - Nord Europe, Team SequeL; Department of Operations Research and Financial Engineering, Princeton University",
        "aff_domain": "inria.fr;inria.fr;inria.fr;princeton.edu",
        "email": "inria.fr;inria.fr;inria.fr;princeton.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "INRIA Lille - Nord Europe, Team SequeL;Princeton University",
        "aff_unique_dep": ";Department of Operations Research and Financial Engineering",
        "aff_unique_url": ";https://www.princeton.edu",
        "aff_unique_abbr": ";Princeton",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "70a6df8e32",
        "title": "Multi-View Learning of Word Embeddings via CCA",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/6c4b761a28b734fe93831e3fb400ce87-Abstract.html",
        "author": "Paramveer Dhillon; Dean P. Foster; Lyle H. Ungar",
        "abstract": "Recently, there has been substantial interest in using large amounts of unlabeled data to learn word representations which can then be used as features in supervised classifiers for NLP tasks. However, most current approaches are slow to train, do not model context of the word, and lack theoretical grounding. In this paper, we present a new learning method, Low Rank Multi-View Learning (LR-MVL) which uses a fast spectral method to estimate  low dimensional context-specific word representations from unlabeled data. These representation features can then be used with any supervised learner. LR-MVL is extremely fast, gives guaranteed convergence to a global optimum, is theoretically elegant, and achieves state-of-the-art performance on named entity recognition (NER) and chunking problems.",
        "bibtex": "@inproceedings{NIPS2011_6c4b761a,\n author = {Dhillon, Paramveer and Foster, Dean P and Ungar, Lyle},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multi-View Learning of Word Embeddings via CCA},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/6c4b761a28b734fe93831e3fb400ce87-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/6c4b761a28b734fe93831e3fb400ce87-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/6c4b761a28b734fe93831e3fb400ce87-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/6c4b761a28b734fe93831e3fb400ce87-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 350119,
        "gs_citation": 338,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7017865829282942140&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Computer & Information Science; Statistics; Computer & Information Science",
        "aff_domain": "cis.upenn.edu;wharton.upenn.edu;cis.upenn.edu",
        "email": "cis.upenn.edu;wharton.upenn.edu;cis.upenn.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Computer & Information Science;Statistics",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "06162168f4",
        "title": "Multi-armed bandits on implicit metric spaces",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/7634ea65a4e6d9041cfd3f7de18e334a-Abstract.html",
        "author": "Aleksandrs Slivkins",
        "abstract": "The multi-armed bandit (MAB) setting is a useful abstraction of many online learning tasks which focuses on the trade-off between exploration and exploitation. In this setting, an online algorithm has a fixed set of alternatives (\"arms\"), and in each round it selects one arm and then observes the corresponding reward. While the case of small number of arms is by now well-understood, a lot of recent work has focused on multi-armed bandits with (infinitely) many arms, where one needs to assume extra structure in order to make the problem tractable. In particular, in the Lipschitz MAB problem there is an underlying similarity metric space, known to the algorithm, such that any two arms that are close in this metric space have similar payoffs.  In this paper we consider the more realistic scenario in which the metric space is",
        "bibtex": "@inproceedings{NIPS2011_7634ea65,\n author = {Slivkins, Aleksandrs},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multi-armed bandits on implicit metric spaces},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/7634ea65a4e6d9041cfd3f7de18e334a-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/7634ea65a4e6d9041cfd3f7de18e334a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/7634ea65a4e6d9041cfd3f7de18e334a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 319902,
        "gs_citation": 70,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14990781831941065519&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Microsoft Research Silicon Valley",
        "aff_domain": "microsoft.com",
        "email": "microsoft.com",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Microsoft Research",
        "aff_unique_dep": "Microsoft Research",
        "aff_unique_url": "https://www.microsoft.com/en-us/research/group/microsoft-research-silicon-valley",
        "aff_unique_abbr": "MSR SV",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Silicon Valley",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "73c8408c4a",
        "title": "Multiclass Boosting: Theory and Algorithms",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/2ac2406e835bd49c70469acae337d292-Abstract.html",
        "author": "Mohammad J. Saberian; Nuno Vasconcelos",
        "abstract": "The problem of  multiclass boosting is considered. A new framework,based on multi-dimensional codewords and predictors is introduced. The optimal set of codewords is derived, and a margin enforcing loss proposed. The resulting risk is minimized by gradient descent on a multidimensional functional space. Two algorithms are proposed: 1) CD-MCBoost, based on coordinate descent, updates one predictor component at a time, 2) GD-MCBoost, based on gradient descent, updates all components jointly. The algorithms differ in the weak learners that they support but are both shown to be 1) Bayes consistent, 2) margin enforcing, and 3) convergent to the global minimum of the risk. They also reduce to AdaBoost when there are only two classes. Experiments show that both methods outperform previous multiclass boosting approaches on a number of datasets.",
        "bibtex": "@inproceedings{NIPS2011_2ac2406e,\n author = {Saberian, Mohammad and Vasconcelos, Nuno},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multiclass Boosting: Theory and Algorithms},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/2ac2406e835bd49c70469acae337d292-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/2ac2406e835bd49c70469acae337d292-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/2ac2406e835bd49c70469acae337d292-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/2ac2406e835bd49c70469acae337d292-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 396082,
        "gs_citation": 127,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6605818250265666423&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Statistical Visual Computing Laboratory, University of California, San Diego; Statistical Visual Computing Laboratory, University of California, San Diego",
        "aff_domain": "ucsd.edu;ucsd.edu",
        "email": "ucsd.edu;ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Statistical Visual Computing Laboratory, University of California, San Diego",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "222b2d45c4",
        "title": "Multilinear Subspace Regression: An Orthogonal Tensor Decomposition Approach",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/1343777b8ead1cef5a79b78a1a48d805-Abstract.html",
        "author": "Qibin Zhao; Cesar F. Caiafa; Danilo P. Mandic; Liqing Zhang; Tonio Ball; Andreas Schulze-bonhage; Andrzej S. Cichocki",
        "abstract": "A multilinear subspace regression model based on so called latent variable decomposition is introduced. Unlike standard regression methods which typically employ matrix (2D) data representations followed by vector subspace transformations, the proposed approach uses tensor subspace transformations to model common latent variables across both the independent and dependent data. The proposed approach aims to maximize the correlation between the so derived latent variables and is shown to be suitable for the prediction of multidimensional dependent data from multidimensional independent data, where for the estimation of the latent variables we introduce an algorithm based on Multilinear Singular Value Decomposition (MSVD) on a specially defined cross-covariance tensor. It is next shown that in this way we are also able to unify the existing Partial Least Squares (PLS) and N-way PLS regression algorithms within the same framework. Simulations on benchmark synthetic data confirm the advantages of the proposed approach, in terms of its predictive ability and robustness, especially for small sample sizes. The potential of the proposed technique is further illustrated on a real world task of the decoding of human intracranial electrocorticogram (ECoG) from a simultaneously recorded scalp electroencephalograph (EEG).",
        "bibtex": "@inproceedings{NIPS2011_1343777b,\n author = {Zhao, Qibin and Caiafa, Cesar F and Mandic, Danilo and Zhang, Liqing and Ball, Tonio and Schulze-bonhage, Andreas and Cichocki, Andrzej},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multilinear Subspace Regression: An Orthogonal Tensor Decomposition Approach},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/1343777b8ead1cef5a79b78a1a48d805-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/1343777b8ead1cef5a79b78a1a48d805-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/1343777b8ead1cef5a79b78a1a48d805-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1602849,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8199823014930668746&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Brain Science Institute, RIKEN, Japan; Instituto Argentino de Radioastronom\u00eda (IAR), CONICET, Argentina; Dept. of Electrical & Electronic Engineering, Imperial College, UK; Dept. of Computer Science & Engineering, Shanghai Jiao Tong University, China; BCCN, Albert-Ludwigs-University, Germany; BCCN, Albert-Ludwigs-University, Germany; Brain Science Institute, RIKEN, Japan",
        "aff_domain": "brain.riken.jp; ; ; ; ; ; ",
        "email": "brain.riken.jp; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3;4;4;0",
        "aff_unique_norm": "Brain Science Institute, RIKEN, Japan;Instituto Argentino de Radioastronom\u00eda (IAR), CONICET, Argentina;Dept. of Electrical & Electronic Engineering, Imperial College, UK;Dept. of Computer Science & Engineering, Shanghai Jiao Tong University, China;BCCN, Albert-Ludwigs-University, Germany",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": ";;;;",
        "aff_unique_abbr": ";;;;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "60722b040b",
        "title": "Multiple Instance Filtering",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/d947bf06a885db0d477d707121934ff8-Abstract.html",
        "author": "Kamil A. Wnuk; Stefano Soatto",
        "abstract": "We propose a robust filtering approach based on semi-supervised and multiple instance learning (MIL). We assume that the posterior density would be unimodal if not for the effect of outliers that we do not wish to explicitly model. Therefore, we seek for a point estimate at the outset, rather than a generic approximation of the entire posterior. Our approach can be thought of as a combination of standard finite-dimensional filtering (Extended Kalman Filter, or Unscented Filter) with multiple instance learning, whereby the initial condition comes with a putative set of inlier measurements. We show how both the state (regression) and the inlier set (classification) can be estimated iteratively and causally by processing only the current measurement. We illustrate our approach on visual tracking problems whereby the object of interest (target) moves and evolves as a result of occlusions and deformations, and partial knowledge of the target is given in the form of a bounding box (training set).",
        "bibtex": "@inproceedings{NIPS2011_d947bf06,\n author = {Wnuk, Kamil and Soatto, Stefano},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multiple Instance Filtering},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/d947bf06a885db0d477d707121934ff8-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/d947bf06a885db0d477d707121934ff8-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/d947bf06a885db0d477d707121934ff8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 4604885,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8453221470049987677&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "University of California, Los Angeles; University of California, Los Angeles",
        "aff_domain": "cs.ucla.edu;cs.ucla.edu",
        "email": "cs.ucla.edu;cs.ucla.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Los Angeles",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ucla.edu",
        "aff_unique_abbr": "UCLA",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "36747a8d83",
        "title": "Multiple Instance Learning on Structured Data",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/ec5decca5ed3d6b8079e2e7e7bacc9f2-Abstract.html",
        "author": "Dan Zhang; Yan Liu; Luo Si; Jian Zhang; Richard D. Lawrence",
        "abstract": "Most existing Multiple-Instance Learning (MIL) algorithms  assume data instances and/or data bags are independently and  identically distributed. But there often exists rich additional  dependency/structure information between instances/bags within many  applications of MIL. Ignoring this structure information limits the  performance of existing MIL algorithms. This paper explores the  research problem as multiple instance learning on structured  data (MILSD) and formulates a novel framework that considers  additional structure information. In particular, an effective and  efficient optimization algorithm has been proposed to solve the  original non-convex optimization problem by using a combination of  Concave-Convex Constraint Programming (CCCP) method and an adapted  Cutting Plane method, which deals with two sets of constraints caused  by learning on  instances within individual bags and learning on  structured data. Our method has the nice convergence property,  with specified precision on each set of constraints. Experimental  results on three different applications, i.e., webpage  classification, market targeting, and protein fold identification,  clearly demonstrate the advantages of the proposed method over  state-of-the-art methods.",
        "bibtex": "@inproceedings{NIPS2011_ec5decca,\n author = {Zhang, Dan and Liu, Yan and Si, Luo and Zhang, Jian and Lawrence, Richard},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multiple Instance Learning on Structured Data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/ec5decca5ed3d6b8079e2e7e7bacc9f2-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/ec5decca5ed3d6b8079e2e7e7bacc9f2-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/ec5decca5ed3d6b8079e2e7e7bacc9f2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 410314,
        "gs_citation": 61,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14558673525316750695&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "1524e8f3dc",
        "title": "Nearest Neighbor based Greedy Coordinate Descent",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/160c88652d47d0be60bfbfed25111412-Abstract.html",
        "author": "Inderjit S. Dhillon; Pradeep K. Ravikumar; Ambuj Tewari",
        "abstract": "Increasingly, optimization problems in machine learning, especially those arising from high-dimensional statistical estimation, have a large number of variables. Modern statistical estimators developed over the past decade have statistical or sample complexity that depends only weakly on the number of parameters when there is some structure to the problem, such as sparsity. A central question is whether similar advances can be made in their computational complexity as well. In this paper, we propose strategies that indicate that such advances can indeed be made. In particular, we investigate the greedy coordinate descent algorithm, and note that performing the greedy step efficiently weakens the costly dependence on the problem size provided the solution is sparse. We then propose a suite of methods that perform these greedy steps efficiently by a reduction to nearest neighbor search.   We also devise a more amenable form of greedy descent for composite non-smooth objectives; as well as several approximate variants of such greedy descent. We develop a practical implementation of our algorithm that combines greedy coordinate descent with locality sensitive hashing. Without tuning the latter data structure, we are not only able to significantly speed up the vanilla greedy method, but also outperform cyclic descent when the problem size becomes large. Our results indicate the effectiveness of our nearest neighbor strategies, and also point to many open questions regarding the development of computational geometric techniques tailored towards first-order optimization methods.",
        "bibtex": "@inproceedings{NIPS2011_160c8865,\n author = {Dhillon, Inderjit and Ravikumar, Pradeep and Tewari, Ambuj},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Nearest Neighbor based Greedy Coordinate Descent},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/160c88652d47d0be60bfbfed25111412-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/160c88652d47d0be60bfbfed25111412-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/160c88652d47d0be60bfbfed25111412-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/160c88652d47d0be60bfbfed25111412-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 3591200,
        "gs_citation": 87,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6083812679520525412&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Department of Computer Science, University of Texas at Austin; Department of Computer Science, University of Texas at Austin; Department of Computer Science, University of Texas at Austin",
        "aff_domain": "cs.utexas.edu;cs.utexas.edu;cs.utexas.edu",
        "email": "cs.utexas.edu;cs.utexas.edu;cs.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "1f50dfe130",
        "title": "Neural Reconstruction with Approximate Message Passing (NeuRAMP)",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/359f38463d487e9e29bd20e24f0c050a-Abstract.html",
        "author": "Alyson K. Fletcher; Sundeep Rangan; Lav R. Varshney; Aniruddha Bhargava",
        "abstract": "Many functional descriptions of spiking neurons assume a cascade structure where inputs are passed through an initial linear filtering stage that produces a low-dimensional signal that drives subsequent nonlinear stages.  This paper presents a novel and systematic parameter estimation procedure for such models and applies the method to two neural estimation problems: (i) compressed-sensing based neural mapping from multi-neuron excitation, and (ii) estimation of neural receptive yields in sensory neurons.  The proposed estimation algorithm models the neurons via a graphical model and then estimates the parameters in the model using a recently-developed generalized approximate message passing (GAMP) method.  The GAMP method is based on Gaussian approximations of loopy belief propagation.  In the neural connectivity problem, the GAMP-based   method is shown to be computational efficient, provides a more exact modeling of the sparsity, can incorporate nonlinearities in the output and significantly outperforms previous compressed-sensing methods.  For the receptive field estimation, the GAMP method can also exploit inherent structured sparsity in the linear weights.  The method is validated on estimation of linear nonlinear Poisson (LNP) cascade models for receptive fields of salamander retinal ganglion cells.",
        "bibtex": "@inproceedings{NIPS2011_359f3846,\n author = {Fletcher, Alyson K and Rangan, Sundeep and Varshney, Lav R and Bhargava, Aniruddha},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Neural Reconstruction with Approximate Message Passing (NeuRAMP)},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/359f38463d487e9e29bd20e24f0c050a-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/359f38463d487e9e29bd20e24f0c050a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/359f38463d487e9e29bd20e24f0c050a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 325524,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5444082414554833019&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "University of California, Berkeley; Polytechnic Institute of New York University; IBM Thomas J. Watson Research Center; University of Wisconsin Madison",
        "aff_domain": "eecs.berkeley.edu;poly.edu;us.ibm.com;wisc.edu",
        "email": "eecs.berkeley.edu;poly.edu;us.ibm.com;wisc.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "University of California, Berkeley;Polytechnic Institute of New York University;IBM;University of Wisconsin-Madison",
        "aff_unique_dep": ";;Research;",
        "aff_unique_url": "https://www.berkeley.edu;;https://www.ibm.com/research;https://www.wisc.edu",
        "aff_unique_abbr": "UC Berkeley;;IBM;UW-Madison",
        "aff_campus_unique_index": "0;2;3",
        "aff_campus_unique": "Berkeley;;Yorktown Heights;Madison",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "116a5f5995",
        "title": "Neuronal Adaptation for Sampling-Based Probabilistic Inference in Perceptual Bistability",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/b51a15f382ac914391a58850ab343b00-Abstract.html",
        "author": "David P. Reichert; Peggy Series; Amos J. Storkey",
        "abstract": "It has been argued that perceptual multistability reflects probabilistic inference performed by the brain when sensory input is ambiguous. Alternatively, more traditional explanations of multistability refer to low-level mechanisms such as neuronal adaptation. We employ a Deep Boltzmann Machine (DBM) model of cortical processing to demonstrate that these two different approaches can be combined in the same framework. Based on recent developments in machine learning, we show how neuronal adaptation can be understood as a mechanism that improves probabilistic, sampling-based inference. Using the ambiguous Necker cube image, we analyze the perceptual switching exhibited by the model. We also examine the influence of spatial attention, and explore how binocular rivalry can be modeled with the same approach. Our work joins earlier studies in demonstrating how the principles underlying DBMs relate to cortical processing, and offers novel perspectives on the neural implementation of approximate probabilistic inference in the brain.",
        "bibtex": "@inproceedings{NIPS2011_b51a15f3,\n author = {Reichert, David and Series, Peggy and Storkey, Amos J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Neuronal Adaptation for Sampling-Based Probabilistic Inference in Perceptual Bistability},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/b51a15f382ac914391a58850ab343b00-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/b51a15f382ac914391a58850ab343b00-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/b51a15f382ac914391a58850ab343b00-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 298326,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18081336941887709677&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "School of Informatics, University of Edinburgh; School of Informatics, University of Edinburgh; School of Informatics, University of Edinburgh",
        "aff_domain": "sms.ed.ac.uk;inf.ed.ac.uk;ed.ac.uk",
        "email": "sms.ed.ac.uk;inf.ed.ac.uk;ed.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Edinburgh",
        "aff_unique_dep": "School of Informatics",
        "aff_unique_url": "https://www.ed.ac.uk",
        "aff_unique_abbr": "Edinburgh",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Edinburgh",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "c4f0e9ea35",
        "title": "Newtron: an Efficient Bandit algorithm for Online Multiclass Prediction",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/fde9264cf376fffe2ee4ddf4a988880d-Abstract.html",
        "author": "Elad Hazan; Satyen Kale",
        "abstract": "We present an efficient algorithm for the problem of online multiclass prediction with bandit feedback in the fully adversarial setting. We measure its regret with respect to the log-loss defined in \\cite{AbernethyR09}, which is parameterized by a scalar (\\alpha). We prove that the regret of \\newtron is (O(\\log T)) when (\\alpha) is a constant that does not vary with horizon (T), and at most (O(T^{2/3})) if (\\alpha) is allowed to increase to infinity with (T). For (\\alpha) = (O(\\log T)), the regret is bounded by (O(\\sqrt{T})), thus solving the open problem of \\cite{KST08, AbernethyR09}. Our algorithm is based on a novel application of the online Newton method \\cite{HAK07}. We test our algorithm and show it to perform well in experiments, even when (\\alpha) is a small constant.",
        "bibtex": "@inproceedings{NIPS2011_fde9264c,\n author = {Hazan, Elad and Kale, Satyen},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Newtron: an Efficient Bandit algorithm for Online Multiclass Prediction},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/fde9264cf376fffe2ee4ddf4a988880d-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/fde9264cf376fffe2ee4ddf4a988880d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/fde9264cf376fffe2ee4ddf4a988880d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 303452,
        "gs_citation": 56,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13067478802613817681&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Industrial Engineering, Technion - Israel Institute of Technology; Yahoo! Research",
        "aff_domain": "ie.technion.ac.il;yahoo-inc.com",
        "email": "ie.technion.ac.il;yahoo-inc.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Department of Industrial Engineering, Technion - Israel Institute of Technology;Yahoo!",
        "aff_unique_dep": ";Yahoo! Research",
        "aff_unique_url": ";https://research.yahoo.com",
        "aff_unique_abbr": ";Yahoo!",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "e08696995e",
        "title": "Noise Thresholds for Spectral Clustering",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/dbe272bab69f8e13f14b405e038deb64-Abstract.html",
        "author": "Sivaraman Balakrishnan; Min Xu; Akshay Krishnamurthy; Aarti Singh",
        "abstract": "Although spectral clustering has enjoyed considerable empirical success in machine learning, its theoretical properties are not yet fully developed. We analyze the performance of a spectral algorithm for hierarchical clustering and show that on a class of hierarchically structured similarity matrices, this algorithm can tolerate noise that grows with the number of data points while still perfectly recovering the hierarchical clusters with high probability. We  additionally improve upon previous results for k-way spectral clustering to derive conditions under which spectral clustering makes no mistakes. Further, using minimax analysis, we derive tight upper and lower bounds for the  clustering problem and compare the performance of spectral clustering to these  information theoretic limits. We also present experiments on simulated and real  world data illustrating our results.",
        "bibtex": "@inproceedings{NIPS2011_dbe272ba,\n author = {Balakrishnan, Sivaraman and Xu, Min and Krishnamurthy, Akshay and Singh, Aarti},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Noise Thresholds for Spectral Clustering},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/dbe272bab69f8e13f14b405e038deb64-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/dbe272bab69f8e13f14b405e038deb64-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/dbe272bab69f8e13f14b405e038deb64-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/dbe272bab69f8e13f14b405e038deb64-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 6433061,
        "gs_citation": 122,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7151016739196622434&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "School of Computer Science, Carnegie Mellon University; School of Computer Science, Carnegie Mellon University; School of Computer Science, Carnegie Mellon University; School of Computer Science, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b2b193ffe8",
        "title": "Non-Asymptotic Analysis of Stochastic Approximation Algorithms for Machine Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/40008b9a5380fcacce3976bf7c08af5b-Abstract.html",
        "author": "Eric Moulines; Francis R. Bach",
        "abstract": "We consider the minimization of a convex objective function defined on a Hilbert space, which is only available through unbiased estimates of  its gradients.  This problem includes  standard machine learning algorithms such as kernel logistic regression and least-squares regression, and is commonly referred to as a stochastic approximation problem in the operations research community. We provide a non-asymptotic analysis of the  convergence of two well-known algorithms, stochastic gradient descent (a.k.a.~Robbins-Monro algorithm) as well as a simple modification where iterates are averaged (a.k.a.~Polyak-Ruppert averaging). Our analysis suggests that a learning rate proportional to the inverse of the number of iterations, while leading to the optimal convergence rate in the strongly convex case, is not robust to the lack of strong convexity or the setting of the proportionality constant. This situation is remedied when using slower decays together with averaging, robustly leading to the optimal rate of convergence. We illustrate our theoretical results with simulations on synthetic and standard datasets.",
        "bibtex": "@inproceedings{NIPS2011_40008b9a,\n author = {Moulines, Eric and Bach, Francis},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Non-Asymptotic Analysis of Stochastic Approximation Algorithms for Machine Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/40008b9a5380fcacce3976bf7c08af5b-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/40008b9a5380fcacce3976bf7c08af5b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/40008b9a5380fcacce3976bf7c08af5b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 414862,
        "gs_citation": 1020,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16911582673657312113&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "INRIA-Sierra Project-team, Ecole Normale Sup\u00e9rieure, Paris, France; LTCI, Telecom ParisTech, Paris, France",
        "aff_domain": "ens.fr;enst.fr",
        "email": "ens.fr;enst.fr",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "INRIA-Sierra Project-team, Ecole Normale Sup\u00e9rieure, Paris, France;Telecom ParisTech",
        "aff_unique_dep": ";LTCI",
        "aff_unique_url": ";https://www.telecom-paristech.fr",
        "aff_unique_abbr": ";Telecom ParisTech",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Paris",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";France"
    },
    {
        "id": "ad2ecaf6ef",
        "title": "Non-conjugate Variational Message Passing for Multinomial and Binary Regression",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/5c936263f3428a40227908d5a3847c0b-Abstract.html",
        "author": "David A. Knowles; Tom Minka",
        "abstract": "Variational Message Passing (VMP) is an algorithmic implementation of the Variational Bayes (VB) method which applies only in the special case of conjugate exponential family models. We propose an extension to VMP, which we refer to as Non-conjugate Variational Message Passing (NCVMP) which aims to alleviate this restriction while maintaining modularity, allowing choice in how expectations are calculated, and integrating into an existing message-passing framework: Infer.NET. We demonstrate NCVMP on logistic binary and multinomial regression. In the multinomial case we introduce a novel variational bound for the softmax factor which is tighter than other commonly used bounds whilst maintaining computational tractability.",
        "bibtex": "@inproceedings{NIPS2011_5c936263,\n author = {Knowles, David and Minka, Tom},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Non-conjugate Variational Message Passing for Multinomial and Binary Regression},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/5c936263f3428a40227908d5a3847c0b-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/5c936263f3428a40227908d5a3847c0b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/5c936263f3428a40227908d5a3847c0b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 812967,
        "gs_citation": 166,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11503661987947090140&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Engineering, University of Cambridge; Microsoft Research, Cambridge, UK",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Cambridge;Microsoft Research",
        "aff_unique_dep": "Department of Engineering;",
        "aff_unique_url": "https://www.cam.ac.uk;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "Cambridge;MSR",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "7ababeff00",
        "title": "Non-parametric Group Orthogonal Matching Pursuit for Sparse Learning with Multiple Kernels",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/effc299a1addb07e7089f9b269c31f2f-Abstract.html",
        "author": "Vikas Sindhwani; Aurelie C. Lozano",
        "abstract": "We consider regularized risk minimization in a large dictionary of Reproducing  kernel Hilbert Spaces (RKHSs) over which the target function has a sparse representation.  This setting, commonly referred to as Sparse Multiple Kernel Learning  (MKL), may be viewed as the non-parametric extension of group sparsity in linear  models. While the two dominant algorithmic strands of sparse learning, namely  convex relaxations using l1 norm (e.g., Lasso) and greedy methods (e.g., OMP),  have both been rigorously extended for group sparsity, the sparse MKL literature  has so farmainly adopted the former withmild empirical success. In this paper, we  close this gap by proposing a Group-OMP based framework for sparse multiple  kernel learning. Unlike l1-MKL, our approach decouples the sparsity regularizer  (via a direct l0 constraint) from the smoothness regularizer (via RKHS norms)  which leads to better empirical performance as well as a simpler optimization  procedure that only requires a black-box single-kernel solver. The algorithmic  development and empirical studies are complemented by theoretical analyses in  terms of Rademacher generalization bounds and sparse recovery conditions analogous  to those for OMP [27] and Group-OMP [16].",
        "bibtex": "@inproceedings{NIPS2011_effc299a,\n author = {Sindhwani, Vikas and Lozano, Aurelie C},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Non-parametric Group Orthogonal Matching Pursuit for Sparse Learning with Multiple Kernels},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/effc299a1addb07e7089f9b269c31f2f-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/effc299a1addb07e7089f9b269c31f2f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/effc299a1addb07e7089f9b269c31f2f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 148507,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4678645745553532308&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "IBM T.J. Watson Research Center; IBM T.J. Watson Research Center",
        "aff_domain": "us.ibm.com;us.ibm.com",
        "email": "us.ibm.com;us.ibm.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "IBM",
        "aff_unique_dep": "Research Center",
        "aff_unique_url": "https://www.ibm.com/research/watson",
        "aff_unique_abbr": "IBM",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "T.J. Watson",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "914095f35c",
        "title": "Nonlinear Inverse Reinforcement Learning with Gaussian Processes",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/c51ce410c124a10e0db5e4b97fc2af39-Abstract.html",
        "author": "Sergey Levine; Zoran Popovic; Vladlen Koltun",
        "abstract": "We present a probabilistic algorithm for nonlinear inverse reinforcement learning. The goal of inverse reinforcement learning is to learn the reward function in a Markov decision process from expert demonstrations. While most prior inverse reinforcement learning algorithms represent the reward as a linear combination of a set of features, we use Gaussian processes to learn the reward as a nonlinear function, while also determining the relevance of each feature to the expert's policy. Our probabilistic algorithm allows complex behaviors to be captured from suboptimal stochastic demonstrations, while automatically balancing the simplicity of the learned reward structure against its consistency with the observed actions.",
        "bibtex": "@inproceedings{NIPS2011_c51ce410,\n author = {Levine, Sergey and Popovic, Zoran and Koltun, Vladlen},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Nonlinear Inverse Reinforcement Learning with Gaussian Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/c51ce410c124a10e0db5e4b97fc2af39-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/c51ce410c124a10e0db5e4b97fc2af39-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/c51ce410c124a10e0db5e4b97fc2af39-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/c51ce410c124a10e0db5e4b97fc2af39-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 965418,
        "gs_citation": 521,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9103766588020641011&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "Stanford University; University of Washington; Stanford University",
        "aff_domain": "cs.stanford.edu;cs.washington.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;cs.washington.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Stanford University;University of Washington",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.stanford.edu;https://www.washington.edu",
        "aff_unique_abbr": "Stanford;UW",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f386d1088c",
        "title": "Nonnegative dictionary learning in the exponential noise model for adaptive music signal representation",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/81dc9bdb52d04dc20036dbd8313ed055-Abstract.html",
        "author": "Onur Dikmen; C\u00e9dric F\u00e9votte",
        "abstract": "In this paper we describe a maximum likelihood likelihood approach for dictionary learning in the multiplicative exponential noise model. This model is prevalent in audio signal processing where it underlies a generative composite model of the power spectrogram. Maximum joint likelihood estimation of the dictionary and expansion coefficients leads to a nonnegative matrix factorization problem where the Itakura-Saito divergence is used. The optimality of this approach is in question because the number of parameters (which include the expansion coefficients) grows with the number of observations. In this paper we describe a variational procedure for optimization of the marginal likelihood, i.e., the likelihood of the dictionary where the activation coefficients have been integrated out (given a specific prior). We compare the output of both maximum joint likelihood estimation (i.e., standard Itakura-Saito NMF) and maximum marginal likelihood estimation (MMLE) on real and synthetical datasets. The MMLE approach is shown to embed automatic model order selection, akin to automatic relevance determination.",
        "bibtex": "@inproceedings{NIPS2011_81dc9bdb,\n author = {Dikmen, Onur and F\\'{e}votte, C\\'{e}dric},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Nonnegative dictionary learning in the exponential noise model for adaptive music signal representation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/81dc9bdb52d04dc20036dbd8313ed055-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/81dc9bdb52d04dc20036dbd8313ed055-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/81dc9bdb52d04dc20036dbd8313ed055-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 875004,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4513019756402307049&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "CNRS LTCI + T\u00e9l\u00e9com ParisTech; CNRS LTCI + T\u00e9l\u00e9com ParisTech",
        "aff_domain": "telecom-paristech.fr;telecom-paristech.fr",
        "email": "telecom-paristech.fr;telecom-paristech.fr",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "CNRS;T\u00e9l\u00e9com ParisTech",
        "aff_unique_dep": "Laboratoire Traitement du signal et des images;",
        "aff_unique_url": "https://www.ltci.cnrs.fr;https://www.telecom-paristech.fr",
        "aff_unique_abbr": "LTCI;TP",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "France"
    },
    {
        "id": "fb750e6151",
        "title": "Nonstandard Interpretations of Probabilistic Programs for Efficient Inference",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/0d7de1aca9299fe63f3e0041f02638a3-Abstract.html",
        "author": "David Wingate; Noah Goodman; Andreas Stuhlmueller; Jeffrey M. Siskind",
        "abstract": "Probabilistic programming languages allow modelers to specify a stochastic    process using syntax that resembles modern programming languages.  Because    the program is in machine-readable format, a variety of techniques from    compiler design and program analysis can be used to examine the structure of    the distribution represented by the probabilistic program.  We show how    nonstandard interpretations of probabilistic programs can be used to    craft efficient inference algorithms: information about the structure of a    distribution (such as gradients or dependencies) is generated as a monad-like side    computation while executing the program.  These interpretations can be easily    coded using special-purpose objects and operator overloading.  We implement    two examples of nonstandard interpretations in two different languages, and    use them as building blocks to construct inference algorithms: automatic    differentiation, which enables gradient based methods, and provenance    tracking, which enables efficient construction of global proposals.",
        "bibtex": "@inproceedings{NIPS2011_0d7de1ac,\n author = {Wingate, David and Goodman, Noah and Stuhlmueller, Andreas and Siskind, Jeffrey},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Nonstandard Interpretations of Probabilistic Programs for Efficient Inference},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/0d7de1aca9299fe63f3e0041f02638a3-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/0d7de1aca9299fe63f3e0041f02638a3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/0d7de1aca9299fe63f3e0041f02638a3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 478438,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8495042189462180427&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "BCS / LIDS, MIT; Psychology, Stanford; BCS, MIT; ECE, Purdue",
        "aff_domain": "mit.edu;stanford.edu;mit.edu;purdue.edu",
        "email": "mit.edu;stanford.edu;mit.edu;purdue.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "BCS / LIDS, MIT;Psychology, Stanford;BCS, MIT;ECE, Purdue",
        "aff_unique_dep": ";;;",
        "aff_unique_url": ";;;",
        "aff_unique_abbr": ";;;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "7daf5d7dec",
        "title": "Object Detection with Grammar Models",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/6faa8040da20ef399b63a72d0e4ab575-Abstract.html",
        "author": "Ross B. Girshick; Pedro F. Felzenszwalb; David A. McAllester",
        "abstract": "Compositional models provide an elegant formalism for representing the  visual appearance of highly variable objects.  While such models are  appealing from a theoretical point of view, it has been difficult to  demonstrate that they lead to performance advantages on challenging  datasets.  Here we develop a grammar model for person detection  and show that it outperforms previous high-performance systems on the  PASCAL benchmark.  Our model represents people using a hierarchy of  deformable parts, variable structure and an explicit model of  occlusion for partially visible objects.  To train the model, we  introduce a new discriminative framework for learning structured  prediction models from weakly-labeled data.",
        "bibtex": "@inproceedings{NIPS2011_6faa8040,\n author = {Girshick, Ross and Felzenszwalb, Pedro and McAllester, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Object Detection with Grammar Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/6faa8040da20ef399b63a72d0e4ab575-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/6faa8040da20ef399b63a72d0e4ab575-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/6faa8040da20ef399b63a72d0e4ab575-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1149206,
        "gs_citation": 382,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1124537225747769485&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Dept. of Computer Science, University of Chicago; School of Engineering and Dept. of Computer Science, Brown University; TTI-Chicago",
        "aff_domain": "cs.uchicago.edu;brown.edu;ttic.edu",
        "email": "cs.uchicago.edu;brown.edu;ttic.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Dept. of Computer Science, University of Chicago;School of Engineering and Dept. of Computer Science, Brown University;Toyota Technological Institute at Chicago",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;https://www.tti-chicago.org",
        "aff_unique_abbr": ";;TTI",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Chicago",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "f5c0f5a555",
        "title": "On Causal Discovery with Cyclic Additive Noise Models",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/d61e4bbd6393c9111e6526ea173a7c8b-Abstract.html",
        "author": "Joris M. Mooij; Dominik Janzing; Tom Heskes; Bernhard Sch\u00f6lkopf",
        "abstract": "We study a particular class of cyclic causal models, where each variable is a (possibly nonlinear) function of its parents and additive noise. We prove that the causal graph of such models is generically identifiable in the bivariate, Gaussian-noise case. We also propose a method to learn such models from observational data. In the acyclic case, the method reduces to ordinary regression, but in the more challenging cyclic case, an additional term arises in the loss function, which makes it a special case of nonlinear independent component analysis. We illustrate the proposed method on synthetic data.",
        "bibtex": "@inproceedings{NIPS2011_d61e4bbd,\n author = {Mooij, Joris M and Janzing, Dominik and Heskes, Tom and Sch\\\"{o}lkopf, Bernhard},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On Causal Discovery with Cyclic Additive Noise Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/d61e4bbd6393c9111e6526ea173a7c8b-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/d61e4bbd6393c9111e6526ea173a7c8b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/d61e4bbd6393c9111e6526ea173a7c8b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/d61e4bbd6393c9111e6526ea173a7c8b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 853143,
        "gs_citation": 112,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1967944948509723861&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "Radboud University, Nijmegen, The Netherlands; Max Planck Institute for Intelligent Systems, T\u00a8ubingen, Germany; Radboud University, Nijmegen, The Netherlands; Max Planck Institute for Intelligent Systems, T\u00a8ubingen, Germany",
        "aff_domain": "cs.ru.nl;tuebingen.mpg.de;cs.ru.nl;tuebingen.mpg.de",
        "email": "cs.ru.nl;tuebingen.mpg.de;cs.ru.nl;tuebingen.mpg.de",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "Radboud University;Max Planck Institute for Intelligent Systems",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ru.nl;https://www.mpi-is.mpg.de",
        "aff_unique_abbr": "RU;MPI-IS",
        "aff_campus_unique_index": "0;1;0;1",
        "aff_campus_unique": "Nijmegen;T\u00fcbingen",
        "aff_country_unique_index": "0;1;0;1",
        "aff_country_unique": "Netherlands;Germany"
    },
    {
        "id": "c597e0f8ae",
        "title": "On Learning Discrete Graphical Models using Greedy Methods",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/a5e0ff62be0b08456fc7f1e88812af3d-Abstract.html",
        "author": "Ali Jalali; Christopher C. Johnson; Pradeep K. Ravikumar",
        "abstract": "In this paper, we address the problem of learning the structure of a pairwise graphical model from samples in a high-dimensional setting. Our first main result studies the sparsistency, or consistency in sparsity pattern recovery, properties of a forward-backward greedy algorithm as applied to general statistical models. As a special case, we then apply this algorithm to learn the structure of a discrete graphical model via neighborhood estimation. As a corollary of our general result, we derive sufficient conditions on the number of samples n, the maximum node-degree d and the problem size p, as well as other conditions on the model parameters, so that the algorithm recovers all the edges with high probability. Our result guarantees graph selection for samples scaling as n = Omega(d log(p)), in contrast to existing convex-optimization based algorithms that require a sample complexity of Omega(d^2 log(p)). Further, the greedy algorithm only requires a restricted strong convexity condition which is typically milder than irrepresentability assumptions. We corroborate these results using numerical simulations at the end.",
        "bibtex": "@inproceedings{NIPS2011_a5e0ff62,\n author = {Jalali, Ali and Johnson, Christopher and Ravikumar, Pradeep},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On Learning Discrete Graphical Models using Greedy Methods},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/a5e0ff62be0b08456fc7f1e88812af3d-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/a5e0ff62be0b08456fc7f1e88812af3d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/a5e0ff62be0b08456fc7f1e88812af3d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 167194,
        "gs_citation": 124,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17231521539318971579&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "University of Texas at Austin; University of Texas at Asutin; University of Texas at Asutin",
        "aff_domain": "mail.utexas.edu;cs.utexas.edu;cs.utexas.edu",
        "email": "mail.utexas.edu;cs.utexas.edu;cs.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of Texas at Austin;University of Texas at Asutin",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.utexas.edu;",
        "aff_unique_abbr": "UT Austin;",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Austin;",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "5a7c3b4b98",
        "title": "On Strategy Stitching in Large Extensive Form Multiplayer Games",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/812b4ba287f5ee0bc9d43bbf5bbe87fb-Abstract.html",
        "author": "Richard G. Gibson; Duane Szafron",
        "abstract": "Computing a good strategy in a large extensive form game often demands an extraordinary amount of computer memory, necessitating the use of abstraction to reduce the game size.  Typically, strategies from abstract games perform better in the real game as the granularity of abstraction is increased.  This paper investigates two techniques for stitching a base strategy in a coarse abstraction of the full game tree, to expert strategies in fine abstractions of smaller subtrees.  We provide a general framework for creating static experts, an approach that generalizes some previous strategy stitching efforts.  In addition, we show that static experts can create strong agents for both 2-player and 3-player Leduc and Limit Texas Hold'em poker, and that a specific class of static experts can be preferred among a number of alternatives.  Furthermore, we describe a poker agent that used static experts and won the 3-player events of the 2010 Annual Computer Poker Competition.",
        "bibtex": "@inproceedings{NIPS2011_812b4ba2,\n author = {Gibson, Richard and Szafron, Duane},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On Strategy Stitching in Large Extensive Form Multiplayer Games},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 162575,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8148577367131617138&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 11,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "b7bac2cde1",
        "title": "On Tracking The Partition Function",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/861dc9bd7f4e7dd3cccd534d0ae2a2e9-Abstract.html",
        "author": "Guillaume Desjardins; Yoshua Bengio; Aaron C. Courville",
        "abstract": "Markov Random Fields (MRFs) have proven very powerful both as density estimators and feature extractors for classification. However, their use is often limited by an inability to estimate the partition function $Z$. In this paper, we exploit the gradient descent training procedure of restricted Boltzmann machines (a type of MRF) to {\\bf track} the log partition function during learning. Our method relies on two distinct sources of information: (1) estimating the change $\\Delta Z$ incurred by each gradient update, (2) estimating the difference in $Z$ over a small set of tempered distributions using bridge sampling. The two sources of information are then combined using an inference procedure similar to Kalman filtering.  Learning MRFs through Tempered Stochastic Maximum Likelihood, we can estimate $Z$ using no more temperatures than are required for learning. Comparing to both exact values and estimates using annealed importance sampling (AIS), we show on several datasets that our method is able to accurately track the log partition function. In contrast to AIS, our method provides this estimate at each time-step, at a computational cost similar to that required for training alone.",
        "bibtex": "@inproceedings{NIPS2011_861dc9bd,\n author = {Desjardins, Guillaume and Bengio, Yoshua and Courville, Aaron C},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On Tracking The Partition Function},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/861dc9bd7f4e7dd3cccd534d0ae2a2e9-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/861dc9bd7f4e7dd3cccd534d0ae2a2e9-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/861dc9bd7f4e7dd3cccd534d0ae2a2e9-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/861dc9bd7f4e7dd3cccd534d0ae2a2e9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 652687,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3034957806261579998&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "D\u00b4epartement d\u2019informatique et de recherche op \u00b4erationnelle; D\u00b4epartement d\u2019informatique et de recherche op \u00b4erationnelle; D\u00b4epartement d\u2019informatique et de recherche op \u00b4erationnelle",
        "aff_domain": "iro.umontreal.ca;iro.umontreal.ca;iro.umontreal.ca",
        "email": "iro.umontreal.ca;iro.umontreal.ca;iro.umontreal.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "D\u00b4epartement d\u2019informatique et de recherche op \u00b4erationnelle",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "6e0daa2193",
        "title": "On U-processes and clustering performance",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/a1d0c6e83f027327d8461063f4ac58a6-Abstract.html",
        "author": "St\u00e9phan J. Cl\u00e9men\u00e7con",
        "abstract": "Many clustering techniques aim at optimizing empirical criteria that are of the  form of a U-statistic of degree two. Given a measure of dissimilarity between  pairs of observations, the goal is to minimize the within cluster point scatter over  a class of partitions of the feature space. It is the purpose of this paper to define  a general statistical framework, relying on the theory of U-processes, for studying  the performance of such clustering methods. In this setup, under adequate  assumptions on the complexity of the subsets forming the partition candidates, the  excess of clustering risk is proved to be of the order O(1/\\sqrt{n}). Based on recent  results related to the tail behavior of degenerate U-processes, it is also shown how  to establish tighter rate bounds. Model selection issues, related to the number of  clusters forming the data partition in particular, are also considered.",
        "bibtex": "@inproceedings{NIPS2011_a1d0c6e8,\n author = {Cl\\'{e}men\\c{c}con, St\\'{e}phan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On U-processes and clustering performance},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/a1d0c6e83f027327d8461063f4ac58a6-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/a1d0c6e83f027327d8461063f4ac58a6-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/a1d0c6e83f027327d8461063f4ac58a6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 196117,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9882704738892081103&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "http://www.tsi.enst.fr/\u223cclemenco/",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "c6670f816d",
        "title": "On fast approximate submodular minimization",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/d81f9c1be2e08964bf9f24b15f0e4900-Abstract.html",
        "author": "Stefanie Jegelka; Hui Lin; Jeff A. Bilmes",
        "abstract": "We are motivated by an application to extract a representative subset of machine learning training data and by the poor empirical performance we observe of the popular minimum norm algorithm. In fact, for our application, minimum norm can have a running time of about O(n^7 ) (O(n^5 ) oracle calls). We therefore propose a fast approximate method to minimize arbitrary submodular functions. For a large sub-class of submodular functions, the algorithm is exact. Other submodular functions are iteratively approximated by tight submodular upper bounds, and then repeatedly optimized. We show theoretical properties, and empirical results suggest significant speedups over minimum norm while retaining higher accuracies.",
        "bibtex": "@inproceedings{NIPS2011_d81f9c1b,\n author = {Jegelka, Stefanie and Lin, Hui and Bilmes, Jeff A},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On fast approximate submodular minimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/d81f9c1be2e08964bf9f24b15f0e4900-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/d81f9c1be2e08964bf9f24b15f0e4900-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/d81f9c1be2e08964bf9f24b15f0e4900-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1558694,
        "gs_citation": 72,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11897399238080302262&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Max Planck Institute for Intelligent Systems, Tuebingen, Germany; University of Washington, Dept. of EE, Seattle, U.S.A.; University of Washington, Dept. of EE, Seattle, U.S.A.",
        "aff_domain": "tuebingen.mgp.de;ee.washington.edu;ee.washington.edu",
        "email": "tuebingen.mgp.de;ee.washington.edu;ee.washington.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Max Planck Institute for Intelligent Systems;University of Washington, Dept. of EE, Seattle, U.S.A.",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.mpituebingen.mpg.de;",
        "aff_unique_abbr": "MPI-IS;",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Tuebingen;",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Germany;"
    },
    {
        "id": "47bea11c14",
        "title": "On the Analysis of Multi-Channel Neural Spike Data",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/08b255a5d42b89b0585260b6f2360bdd-Abstract.html",
        "author": "Bo Chen; David E. Carlson; Lawrence Carin",
        "abstract": "Nonparametric Bayesian methods are developed for analysis of multi-channel  spike-train data, with the feature learning and spike sorting performed jointly.  The feature learning and sorting are performed simultaneously across all channels.  Dictionary learning is implemented via the beta-Bernoulli process, with  spike sorting performed via the dynamic hierarchical Dirichlet process (dHDP),  with these two models coupled. The dHDP is augmented to eliminate refractoryperiod  violations, it allows the \u201cappearance\u201d and \u201cdisappearance\u201d of neurons over  time, and it models smooth variation in the spike statistics.",
        "bibtex": "@inproceedings{NIPS2011_08b255a5,\n author = {Chen, Bo and Carlson, David and Carin, Lawrence},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On the Analysis of Multi-Channel Neural Spike Data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/08b255a5d42b89b0585260b6f2360bdd-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/08b255a5d42b89b0585260b6f2360bdd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/08b255a5d42b89b0585260b6f2360bdd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1153963,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4771926186835920086&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Electrical and Computer Engineering, Duke University, Durham, NC 27708; Department of Electrical and Computer Engineering, Duke University, Durham, NC 27708; Department of Electrical and Computer Engineering, Duke University, Durham, NC 27708",
        "aff_domain": "duke.edu;duke.edu;duke.edu",
        "email": "duke.edu;duke.edu;duke.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Department of Electrical and Computer Engineering, Duke University, Durham, NC 27708",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "c0ef7ec613",
        "title": "On the Completeness of First-Order Knowledge Compilation for Lifted Probabilistic Inference",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/846c260d715e5b854ffad5f70a516c88-Abstract.html",
        "author": "Guy Broeck",
        "abstract": "Probabilistic logics are receiving a lot of attention today because of their expressive power for knowledge representation and learning. However, this expressivity is detrimental to the tractability of inference, when done at the propositional level.  To solve this problem, various lifted inference algorithms have been proposed that reason at the first-order level, about groups of objects as a whole. Despite the existence of various lifted inference approaches, there are currently no completeness results about these algorithms.  The key contribution of this paper is that we introduce a formal definition of lifted inference that allows us to reason about the completeness of lifted inference algorithms relative to a particular class of probabilistic models.  We then show how to obtain a completeness result using a first-order knowledge compilation approach for theories of formulae containing up to two logical variables.",
        "bibtex": "@inproceedings{NIPS2011_846c260d,\n author = {Broeck, Guy},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On the Completeness of First-Order Knowledge Compilation for Lifted Probabilistic Inference},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/846c260d715e5b854ffad5f70a516c88-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/846c260d715e5b854ffad5f70a516c88-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/846c260d715e5b854ffad5f70a516c88-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 306148,
        "gs_citation": 127,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17066622353629936820&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Department of Computer Science, Katholieke Universiteit Leuven",
        "aff_domain": "cs.kuleuven.be",
        "email": "cs.kuleuven.be",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Department of Computer Science, Katholieke Universiteit Leuven",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "f7cb2dff5d",
        "title": "On the Universality of Online Mirror Descent",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/a8f8f60264024dca151f164729b76c0b-Abstract.html",
        "author": "Nati Srebro; Karthik Sridharan; Ambuj Tewari",
        "abstract": "We show that for a general class of convex online learning  problems, Mirror Descent can always achieve a (nearly) optimal regret guarantee.",
        "bibtex": "@inproceedings{NIPS2011_a8f8f602,\n author = {Srebro, Nati and Sridharan, Karthik and Tewari, Ambuj},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On the Universality of Online Mirror Descent},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/a8f8f60264024dca151f164729b76c0b-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/a8f8f60264024dca151f164729b76c0b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/a8f8f60264024dca151f164729b76c0b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/a8f8f60264024dca151f164729b76c0b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 400293,
        "gs_citation": 153,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5740607558888768309&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "TT?; TT?; University of Texas at Austin",
        "aff_domain": "ttic.edu;ttic.edu;cs.utexas.edu",
        "email": "ttic.edu;ttic.edu;cs.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "TT?;University of Texas at Austin",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://www.utexas.edu",
        "aff_unique_abbr": ";UT Austin",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Austin",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "be21f35b4a",
        "title": "On the accuracy of l1-filtering of signals with block-sparse structure",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/e995f98d56967d946471af29d7bf99f1-Abstract.html",
        "author": "Fatma K. Karzan; Arkadi S. Nemirovski; Boris T. Polyak; Anatoli Juditsky",
        "abstract": "We discuss new methods for the recovery of signals with block-sparse structure, based on l1-minimization. Our emphasis is on the efficiently computable error bounds for the recovery routines. We optimize these bounds with respect to the method parameters to construct the estimators with improved statistical properties. We justify the proposed approach with an oracle inequality which links the properties of the recovery algorithms and the best estimation performance.",
        "bibtex": "@inproceedings{NIPS2011_e995f98d,\n author = {Karzan, Fatma and Nemirovski, Arkadi S and Polyak, Boris and Juditsky, Anatoli},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On the accuracy of l1-filtering of signals with block-sparse structure},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/e995f98d56967d946471af29d7bf99f1-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/e995f98d56967d946471af29d7bf99f1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/e995f98d56967d946471af29d7bf99f1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 408391,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3150051962684305764&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "LJK, Universit \u00b4e J. Fourier, B.P. 53, 38041 Grenoble Cedex 9, France; Carnegie Mellon University, Pittsburgh, Pennsylvania 15213, USA; Georgia Institute of Technology, Atlanta, Georgia 30332, USA; Institute of Control Sciences of Russian Academy of Sciences, Moscow 117997, Russia",
        "aff_domain": "imag.fr;andrew.cmu.edu;isye.gatech.edu;ipu.rssi.ru",
        "email": "imag.fr;andrew.cmu.edu;isye.gatech.edu;ipu.rssi.ru",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "LJK, Universit \u00b4e J. Fourier, B.P. 53, 38041 Grenoble Cedex 9, France;Carnegie Mellon University, Pittsburgh, Pennsylvania 15213, USA;Georgia Institute of Technology, Atlanta, Georgia 30332, USA;Institute of Control Sciences of Russian Academy of Sciences, Moscow 117997, Russia",
        "aff_unique_dep": ";;;",
        "aff_unique_url": ";;;",
        "aff_unique_abbr": ";;;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "7a1b333acc",
        "title": "Online Learning: Stochastic, Constrained, and Smoothed Adversaries",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/692f93be8c7a41525c0baf2076aecfb4-Abstract.html",
        "author": "Alexander Rakhlin; Karthik Sridharan; Ambuj Tewari",
        "abstract": "Learning theory has largely focused on two main learning scenarios: the classical statistical setting where instances are drawn i.i.d. from a fixed distribution, and the adversarial scenario whereby at every time step the worst instance is revealed to the player. It can be argued that in the real world neither of these assumptions is reasonable. We define the minimax value of a game where the adversary is restricted in his moves, capturing stochastic and non-stochastic assumptions on data. Building on the sequential symmetrization approach, we define a notion of distribution-dependent Rademacher complexity for the spectrum of problems ranging from i.i.d. to worst-case. The bounds let us immediately deduce variation-type bounds. We study a smoothed online learning scenario and show that exponentially small amount of noise can make function classes with infinite Littlestone dimension learnable.",
        "bibtex": "@inproceedings{NIPS2011_692f93be,\n author = {Rakhlin, Alexander and Sridharan, Karthik and Tewari, Ambuj},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Online Learning: Stochastic, Constrained, and Smoothed Adversaries},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/692f93be8c7a41525c0baf2076aecfb4-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/692f93be8c7a41525c0baf2076aecfb4-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/692f93be8c7a41525c0baf2076aecfb4-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/692f93be8c7a41525c0baf2076aecfb4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 163021,
        "gs_citation": 111,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9546150525448635694&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff": "Department of Statistics, University of Pennsylvania; Toyota Technological Institute at Chicago; Computer Science Department, University of Texas at Austin",
        "aff_domain": "wharton.upenn.edu;ttic.edu;cs.utexas.edu",
        "email": "wharton.upenn.edu;ttic.edu;cs.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University of Pennsylvania;Toyota Technological Institute at Chicago;Computer Science Department, University of Texas at Austin",
        "aff_unique_dep": "Department of Statistics;;",
        "aff_unique_url": "https://www.upenn.edu;https://www.tti-chicago.org;",
        "aff_unique_abbr": "UPenn;TTI Chicago;",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Chicago",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "c4fc5da6d6",
        "title": "Online Submodular Set Cover, Ranking, and Repeated Active Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/8757150decbd89b0f5442ca3db4d0e0e-Abstract.html",
        "author": "Andrew Guillory; Jeff A. Bilmes",
        "abstract": "We propose an online prediction version of submodular set cover with connections to ranking and repeated active learning.  In each round, the learning algorithm chooses a sequence of items.  The algorithm then receives a monotone submodular function and suffers loss equal to the cover time of the function: the number of items needed, when items are selected in order of the chosen sequence, to achieve a coverage constraint.  We develop an online learning algorithm whose loss converges to approximately that of the best sequence in hindsight.  Our proposed algorithm is readily extended to a setting where multiple functions are revealed at each round and to bandit and contextual bandit settings.",
        "bibtex": "@inproceedings{NIPS2011_8757150d,\n author = {Guillory, Andrew and Bilmes, Jeff A},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Online Submodular Set Cover, Ranking, and Repeated Active Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/8757150decbd89b0f5442ca3db4d0e0e-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/8757150decbd89b0f5442ca3db4d0e0e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/8757150decbd89b0f5442ca3db4d0e0e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 356383,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7957061034699042711&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Computer Science, University of Washington; Department of Electrical Engineering, University of Washington",
        "aff_domain": "cs.washington.edu;ee.washington.edu",
        "email": "cs.washington.edu;ee.washington.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Washington",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.washington.edu",
        "aff_unique_abbr": "UW",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Seattle",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2e59a10d59",
        "title": "Optimal Reinforcement Learning for Gaussian Systems",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/e4a6222cdb5b34375400904f03d8e6a5-Abstract.html",
        "author": "Philipp Hennig",
        "abstract": "The exploration-exploitation trade-off is among the central challenges of reinforcement learning. The optimal Bayesian solution is intractable in general. This paper studies to what extent analytic statements about optimal learning are possible if all beliefs are Gaussian processes. A first order approximation of learning of both loss and dynamics, for nonlinear, time-varying systems in continuous time and space, subject to a relatively weak restriction on the dynamics, is described by an infinite-dimensional partial differential equation. An approximate finite-dimensional projection gives an impression for how this result may be helpful.",
        "bibtex": "@inproceedings{NIPS2011_e4a6222c,\n author = {Hennig, Philipp},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Optimal Reinforcement Learning for Gaussian Systems},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/e4a6222cdb5b34375400904f03d8e6a5-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/e4a6222cdb5b34375400904f03d8e6a5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/e4a6222cdb5b34375400904f03d8e6a5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 294169,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13515500196510656990&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "5b3f91634f",
        "title": "Optimal learning rates for least squares SVMs using Gaussian kernels",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/51ef186e18dc00c2d31982567235c559-Abstract.html",
        "author": "Mona Eberts; Ingo Steinwart",
        "abstract": "We prove a new oracle inequality for support vector machines with Gaussian RBF kernels solving the regularized least squares regression problem. To this end, we apply the modulus of smoothness. With the help of the new oracle inequality we then derive learning rates that can also be achieved by a simple data-dependent parameter selection method. Finally, it turns out that our learning rates are asymptotically optimal for regression functions satisfying certain standard smoothness conditions.",
        "bibtex": "@inproceedings{NIPS2011_51ef186e,\n author = {Eberts, Mona and Steinwart, Ingo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Optimal learning rates for least squares SVMs using Gaussian kernels},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/51ef186e18dc00c2d31982567235c559-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/51ef186e18dc00c2d31982567235c559-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/51ef186e18dc00c2d31982567235c559-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/51ef186e18dc00c2d31982567235c559-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 295041,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6001190615426637742&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Institute for Stochastics and Applications, University of Stuttgart; Institute for Stochastics and Applications, University of Stuttgart",
        "aff_domain": "mathematik.uni-stuttgart.de;mathematik.uni-stuttgart.de",
        "email": "mathematik.uni-stuttgart.de;mathematik.uni-stuttgart.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Institute for Stochastics and Applications, University of Stuttgart",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "cfff94f05a",
        "title": "Optimistic Optimization of a Deterministic Function without the Knowledge of its Smoothness",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/7e889fb76e0e07c11733550f2a6c7a5a-Abstract.html",
        "author": "R\u00e9mi Munos",
        "abstract": "We consider a global optimization problem of a deterministic function f in a semimetric space, given a finite budget of n evaluations. The function f is assumed to be locally smooth (around one of its global maxima) with respect to a semi-metric . We describe two algorithms based on optimistic exploration that use a hierarchical partitioning of the space at all scales. A first contribution is an algorithm, DOO, that requires the knowledge of . We report a finite-sample performance bound in terms of a measure of the quantity of near-optimal states. We then define a second algorithm, SOO, which does not require the knowledge of the semimetric  under which f is smooth, and whose performance is almost as good as DOO optimally-fitted.",
        "bibtex": "@inproceedings{NIPS2011_7e889fb7,\n author = {Munos, R\\'{e}mi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Optimistic Optimization of a Deterministic Function without the Knowledge of its Smoothness},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/7e889fb76e0e07c11733550f2a6c7a5a-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/7e889fb76e0e07c11733550f2a6c7a5a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/7e889fb76e0e07c11733550f2a6c7a5a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 160429,
        "gs_citation": 246,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11387206282426120531&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "SequeL project, INRIA Lille \u2013 Nord Europe, France",
        "aff_domain": "inria.fr",
        "email": "inria.fr",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "SequeL project, INRIA Lille \u2013 Nord Europe, France",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "4a0a9fe000",
        "title": "Orthogonal Matching Pursuit with Replacement",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/500e75a036dc2d7d2fec5da1b71d36cc-Abstract.html",
        "author": "Prateek Jain; Ambuj Tewari; Inderjit S. Dhillon",
        "abstract": "In this paper, we consider the problem of compressed sensing where the goal is to recover almost all the sparse vectors using a small number of fixed linear measurements. For this problem, we propose a novel partial hard-thresholding operator leading to a general family of iterative algorithms. While one extreme of the family yields well known hard thresholding algorithms like ITI and HTP, the other end of the spectrum leads to a novel algorithm that we call  Orthogonal Matching Pursuit with Replacement (OMPR). OMPR, like the classic greedy algorithm OMP, adds exactly one coordinate to the support at each iteration, based on the correlation with the current residual. However, unlike OMP, OMPR also removes one coordinate from the support. This simple change allows us to prove the best known guarantees for OMPR in terms of the Restricted Isometry Property (a condition on the measurement matrix). In contrast, OMP is known to have very weak performance guarantees under RIP.  We also extend OMPR using locality sensitive hashing to get OMPR-Hash, the first provably sub-linear (in dimensionality) algorithm for sparse recovery. Our  proof techniques are novel and flexible enough to also permit the tightest known analysis of popular iterative algorithms such as CoSaMP and Subspace Pursuit.  We provide experimental results on large problems providing  recovery for vectors of size up to million dimensions.  We demonstrate that for large-scale problems our proposed methods are more robust and faster than the existing  methods.",
        "bibtex": "@inproceedings{NIPS2011_500e75a0,\n author = {Jain, Prateek and Tewari, Ambuj and Dhillon, Inderjit},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Orthogonal Matching Pursuit with Replacement},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/500e75a036dc2d7d2fec5da1b71d36cc-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/500e75a036dc2d7d2fec5da1b71d36cc-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/500e75a036dc2d7d2fec5da1b71d36cc-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/500e75a036dc2d7d2fec5da1b71d36cc-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 4407688,
        "gs_citation": 97,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10656414016247669890&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Microsoft Research India, Bangalore, INDIA; The University of Texas at Austin, Austin, TX; The University of Texas at Austin, Austin, TX",
        "aff_domain": "microsoft.com;cs.utexas.edu;cs.utexas.edu",
        "email": "microsoft.com;cs.utexas.edu;cs.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Microsoft Research India, Bangalore, INDIA;The University of Texas at Austin",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://www.utexas.edu",
        "aff_unique_abbr": ";UT Austin",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Austin",
        "aff_country_unique_index": "1;1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "e627ca9e6a",
        "title": "PAC-Bayesian Analysis of Contextual Bandits",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/58e4d44e550d0f7ee0a23d6b02d9b0db-Abstract.html",
        "author": "Yevgeny Seldin; Peter Auer; John S. Shawe-taylor; Ronald Ortner; Fran\u00e7ois Laviolette",
        "abstract": "We derive an instantaneous (per-round) data-dependent regret bound for stochastic multiarmed bandits with side information (also known as contextual bandits). The scaling of our regret bound with the number of states (contexts) $N$ goes as $\\sqrt{N I_{\\rho_t}(S;A)}$, where $I_{\\rho_t}(S;A)$ is the mutual information between states and actions (the side information) used by the algorithm at round $t$. If the algorithm uses all the side information, the regret bound scales as $\\sqrt{N \\ln K}$, where $K$ is the number of actions (arms). However, if the side information $I_{\\rho_t}(S;A)$ is not fully used, the regret bound is significantly tighter. In the extreme case, when $I_{\\rho_t}(S;A) = 0$, the dependence on the number of states reduces from linear to logarithmic. Our analysis allows to provide the algorithm large amount of side information, let the algorithm to decide which side information is relevant for the task, and penalize the algorithm only for the side information that it is using de facto. We also present an algorithm for multiarmed bandits with side information with computational complexity that is a linear in the number of actions.",
        "bibtex": "@inproceedings{NIPS2011_58e4d44e,\n author = {Seldin, Yevgeny and Auer, Peter and Shawe-taylor, John and Ortner, Ronald and Laviolette, Fran\\c{c}ois},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {PAC-Bayesian Analysis of Contextual Bandits},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/58e4d44e550d0f7ee0a23d6b02d9b0db-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/58e4d44e550d0f7ee0a23d6b02d9b0db-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/58e4d44e550d0f7ee0a23d6b02d9b0db-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/58e4d44e550d0f7ee0a23d6b02d9b0db-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 630718,
        "gs_citation": 58,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8815148543508989857&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Max Planck Institute for Intelligent Systems, T\u00fcbingen, Germany; Chair for Information Technology, Montanuniversit\u00e4t Leoben, Austria; D\u00e9partement d\u2019informatique, Universit\u00e9 Laval, Qu\u00e9bec, Canada; Department of Computer Science, University College London, UK; Chair for Information Technology, Montanuniversit\u00e4t Leoben, Austria",
        "aff_domain": "tuebingen.mpg.de;unileoben.ac.at;unileoben.ac.at;ift.ulaval.ca;cs.ucl.ac.uk",
        "email": "tuebingen.mpg.de;unileoben.ac.at;unileoben.ac.at;ift.ulaval.ca;cs.ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3;1",
        "aff_unique_norm": "Max Planck Institute for Intelligent Systems;Chair for Information Technology, Montanuniversit\u00e4t Leoben, Austria;D\u00e9partement d\u2019informatique, Universit\u00e9 Laval, Qu\u00e9bec, Canada;University College London",
        "aff_unique_dep": ";;;Department of Computer Science",
        "aff_unique_url": "https://www.mpi-is.mpg.de;;;https://www.ucl.ac.uk",
        "aff_unique_abbr": "MPI-IS;;;UCL",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "T\u00fcbingen;",
        "aff_country_unique_index": "0;2",
        "aff_country_unique": "Germany;;United Kingdom"
    },
    {
        "id": "526f078829",
        "title": "Penalty Decomposition Methods for Rank Minimization",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/7f39f8317fbdb1988ef4c628eba02591-Abstract.html",
        "author": "Yong Zhang; Zhaosong Lu",
        "abstract": "In this paper we consider general rank minimization problems with rank appearing in either objective function or constraint. We first show that a class of matrix optimization problems can be solved as lower dimensional vector optimization problems. As a consequence, we establish that a class of rank minimization problems have closed form solutions. Using this result, we then propose penalty decomposition methods for general rank minimization problems. The convergence results of the PD methods have been shown in the longer version of the paper. Finally, we test the performance of our methods by applying them to matrix completion and nearest low-rank correlation matrix problems. The computational results demonstrate that our methods generally outperform the existing methods in terms of solution quality and/or speed.",
        "bibtex": "@inproceedings{NIPS2011_7f39f831,\n author = {Zhang, Yong and Lu, Zhaosong},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Penalty Decomposition Methods for Rank Minimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/7f39f8317fbdb1988ef4c628eba02591-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/7f39f8317fbdb1988ef4c628eba02591-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/7f39f8317fbdb1988ef4c628eba02591-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 775275,
        "gs_citation": 103,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10536182965664175636&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 24,
        "aff": "Department of Mathematics, Simon Fraser University, Burnaby, BC, V5A 1S6, Canada; Department of Mathematics, Simon Fraser University, Burnaby, BC, V5A 1S6, Canada",
        "aff_domain": "sfu.ca;sfu.ca",
        "email": "sfu.ca;sfu.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Simon Fraser University",
        "aff_unique_dep": "Department of Mathematics",
        "aff_unique_url": "https://www.sfu.ca",
        "aff_unique_abbr": "SFU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Burnaby",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "314b44d8be",
        "title": "Periodic Finite State Controllers for Efficient POMDP and DEC-POMDP Planning",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/1f3202d820180a39f736f20fce790de8-Abstract.html",
        "author": "Joni K. Pajarinen; Jaakko Peltonen",
        "abstract": "Applications such as robot control and wireless communication require planning under uncertainty. Partially observable Markov decision processes (POMDPs) plan policies for single agents under uncertainty and their decentralized versions (DEC-POMDPs) find a policy for multiple agents. The policy in infinite-horizon POMDP and DEC-POMDP problems has been represented as finite state controllers (FSCs). We introduce a novel class of periodic FSCs, composed of layers connected only to the previous and next layer. Our periodic FSC method finds a deterministic finite-horizon policy and converts it to an initial periodic infinite-horizon policy. This policy is optimized by a new infinite-horizon algorithm to yield deterministic periodic policies, and by a new expectation maximization algorithm to yield stochastic periodic policies. Our method yields better results than earlier planning methods and can compute larger solutions than with regular FSCs.",
        "bibtex": "@inproceedings{NIPS2011_1f3202d8,\n author = {Pajarinen, Joni and Peltonen, Jaakko},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Periodic Finite State Controllers for Efficient POMDP and DEC-POMDP Planning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/1f3202d820180a39f736f20fce790de8-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/1f3202d820180a39f736f20fce790de8-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/1f3202d820180a39f736f20fce790de8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 153757,
        "gs_citation": 76,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8912111488899823908&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Aalto University, Department of Information and Computer Science, P.O. Box 15400, FI-00076 Aalto, Finland; Aalto University, Department of Information and Computer Science, Helsinki Institute for Information Technology HIIT, P.O. Box 15400, FI-00076 Aalto, Finland",
        "aff_domain": "aalto.fi;aalto.fi",
        "email": "aalto.fi;aalto.fi",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Aalto University, Department of Information and Computer Science, P.O. Box 15400, FI-00076 Aalto, Finland;Aalto University, Department of Information and Computer Science, Helsinki Institute for Information Technology HIIT, P.O. Box 15400, FI-00076 Aalto, Finland",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "8227d59761",
        "title": "Phase transition in the family of p-resistances",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/07cdfd23373b17c6b337251c22b7ea57-Abstract.html",
        "author": "Morteza Alamgir; Ulrike V. Luxburg",
        "abstract": "We study the family of p-resistances on graphs for p \u2265 1. This family generalizes the standard resistance distance. We prove that for any fixed graph, for p=1, the p-resistance coincides with the shortest path distance, for p=2 it coincides with the standard resistance distance, and for p \u2192 \u221e it converges to the inverse of the minimal s-t-cut in the graph. Secondly, we consider the special case of random geometric graphs (such as k-nearest neighbor graphs) when the number n of vertices in the graph tends to infinity. We prove that an interesting phase-transition takes place. There exist two critical thresholds p^* and p^",
        "bibtex": "@inproceedings{NIPS2011_07cdfd23,\n author = {Alamgir, Morteza and Luxburg, Ulrike},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Phase transition in the family of p-resistances},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/07cdfd23373b17c6b337251c22b7ea57-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/07cdfd23373b17c6b337251c22b7ea57-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/07cdfd23373b17c6b337251c22b7ea57-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/07cdfd23373b17c6b337251c22b7ea57-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1133564,
        "gs_citation": 91,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14034163876459846099&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 10,
        "aff": "Max Planck Institute for Intelligent Systems, T\u00fcbingen, Germany; Max Planck Institute for Intelligent Systems, T\u00fcbingen, Germany",
        "aff_domain": "tuebingen.mpg.de;tuebingen.mpg.de",
        "email": "tuebingen.mpg.de;tuebingen.mpg.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Max Planck Institute for Intelligent Systems",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.mpi-is.mpg.de",
        "aff_unique_abbr": "MPI-IS",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "T\u00fcbingen",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "a701c4625d",
        "title": "PiCoDes: Learning a Compact Code for Novel-Category Recognition",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/1896a3bf730516dd643ba67b4c447d36-Abstract.html",
        "author": "Alessandro Bergamo; Lorenzo Torresani; Andrew W. Fitzgibbon",
        "abstract": "We introduce PiCoDes: a very compact image descriptor which nevertheless allows high performance on object category recognition. In particular, we address novel-category recognition: the task of defining indexing structures and image representations which enable a large collection of images to be searched for an object category that was not known when the index was built. Instead, the training images defining the category are supplied at query time. We explicitly learn descriptors of a given length (from as small as 16 bytes per image) which have good object-recognition performance. In contrast to previous work in the domain of object recognition, we do not choose an arbitrary intermediate representation, but explicitly learn short codes. In contrast to previous approaches to learn compact codes, we optimize explicitly for (an upper bound on) classification performance. Optimization directly for binary features is difficult and nonconvex, but we present an alternation scheme and convex upper bound which demonstrate excellent performance in practice. PiCoDes of 256 bytes match the accuracy of the current best known classifier for the Caltech256 benchmark, but they decrease the database storage size by a factor of 100 and speed-up the training and testing of novel classes by orders of magnitude.",
        "bibtex": "@inproceedings{NIPS2011_1896a3bf,\n author = {Bergamo, Alessandro and Torresani, Lorenzo and Fitzgibbon, Andrew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {PiCoDes: Learning a Compact Code for Novel-Category Recognition},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/1896a3bf730516dd643ba67b4c447d36-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/1896a3bf730516dd643ba67b4c447d36-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/1896a3bf730516dd643ba67b4c447d36-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 332448,
        "gs_citation": 156,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11525833707218204608&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Dartmouth College; Dartmouth College; Microsoft Research",
        "aff_domain": "cs.dartmouth.edu;cs.dartmouth.edu;microsoft.com",
        "email": "cs.dartmouth.edu;cs.dartmouth.edu;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Dartmouth College;Microsoft Corporation",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.dartmouth.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "Dartmouth;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "38bb83333d",
        "title": "Policy Gradient Coagent Networks",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/1e6e0a04d20f50967c64dac2d639a577-Abstract.html",
        "author": "Philip S. Thomas",
        "abstract": "We present a novel class of actor-critic algorithms for actors consisting of sets of interacting modules. We present, analyze theoretically, and empirically evaluate an update rule for each module, which requires only local information: the module's input, output, and the TD error broadcast by a critic. Such updates are necessary when computation of compatible features becomes prohibitively difficult and are also desirable to increase the biological plausibility of reinforcement learning methods.",
        "bibtex": "@inproceedings{NIPS2011_1e6e0a04,\n author = {Thomas, Philip S.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Policy Gradient Coagent Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/1e6e0a04d20f50967c64dac2d639a577-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/1e6e0a04d20f50967c64dac2d639a577-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/1e6e0a04d20f50967c64dac2d639a577-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 398494,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9893748731001747619&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science, University of Massachusetts Amherst",
        "aff_domain": "cs.umass.edu",
        "email": "cs.umass.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Department of Computer Science, University of Massachusetts Amherst",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "9725fab88c",
        "title": "Portmanteau Vocabularies for Multi-Cue Image Representation",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/86b122d4358357d834a87ce618a55de0-Abstract.html",
        "author": "Fahad S. Khan; Joost Weijer; Andrew D. Bagdanov; Maria Vanrell",
        "abstract": "We describe a novel technique for feature combination in the bag-of-words model of image classification. Our approach builds discriminative compound words from primitive cues learned independently from training images. Our main observation is that modeling joint-cue distributions independently is more statistically robust for typical classification problems than attempting to empirically estimate the dependent, joint-cue distribution directly. We use Information theoretic vocabulary compression to find discriminative combinations of cues and the resulting vocabulary of portmanteau words is compact, has the cue binding property, and supports individual weighting of cues in the final image representation. State-of-the-art results on both the Oxford Flower-102 and Caltech-UCSD Bird-200 datasets demonstrate the effectiveness of our technique compared to other, significantly more complex approaches to multi-cue image representation",
        "bibtex": "@inproceedings{NIPS2011_86b122d4,\n author = {Khan, Fahad and Weijer, Joost and Bagdanov, Andrew and Vanrell, Maria},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Portmanteau Vocabularies for Multi-Cue Image Representation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/86b122d4358357d834a87ce618a55de0-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/86b122d4358357d834a87ce618a55de0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/86b122d4358357d834a87ce618a55de0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 4690804,
        "gs_citation": 74,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11116353820838697566&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 24,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "d6da2fdd0b",
        "title": "Practical Variational Inference for Neural Networks",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/7eb3c8be3d411e8ebfab08eba5f49632-Abstract.html",
        "author": "Alex Graves",
        "abstract": "Variational methods have been previously explored as a tractable approximation to Bayesian inference for neural networks. However the approaches proposed so far have only been applicable to a few simple network architectures. This paper introduces an easy-to-implement stochastic variational method (or equivalently, minimum description length loss function) that can be applied to most neural networks. Along the way it revisits several common regularisers from a variational perspective. It also provides a simple pruning heuristic that can both drastically reduce the number of network weights and lead to improved generalisation. Experimental results are provided for a hierarchical multidimensional recurrent neural network applied to the TIMIT speech corpus.",
        "bibtex": "@inproceedings{NIPS2011_7eb3c8be,\n author = {Graves, Alex},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Practical Variational Inference for Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/7eb3c8be3d411e8ebfab08eba5f49632-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/7eb3c8be3d411e8ebfab08eba5f49632-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/7eb3c8be3d411e8ebfab08eba5f49632-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 466648,
        "gs_citation": 2300,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16673382953830986184&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Computer Science, University of Toronto, Canada",
        "aff_domain": "cs.toronto.edu",
        "email": "cs.toronto.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Toronto",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utoronto.ca",
        "aff_unique_abbr": "U of T",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "ec17ea4a26",
        "title": "Predicting Dynamic Difficulty",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/7c9d0b1f96aebd7b5eca8c3edaa19ebb-Abstract.html",
        "author": "Olana Missura; Thomas G\u00e4rtner",
        "abstract": "Motivated by applications in electronic games as well as teaching systems, we investigate the problem of dynamic difficulty adjustment. The task here is to repeatedly find a game difficulty setting that is neither `too easy' and bores the player, nor `too difficult' and overburdens the player.  The contributions of this paper are ($i$) formulation of difficulty adjustment as an online learning problem on partially ordered sets, ($ii$) an exponential update algorithm for dynamic difficulty adjustment, ($iii$) a bound on the number of wrong difficulty settings relative to the best static setting chosen in hindsight, and ($iv$) an empirical investigation of the algorithm when playing against adversaries.",
        "bibtex": "@inproceedings{NIPS2011_7c9d0b1f,\n author = {Missura, Olana and G\\\"{a}rtner, Thomas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Predicting Dynamic Difficulty},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/7c9d0b1f96aebd7b5eca8c3edaa19ebb-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/7c9d0b1f96aebd7b5eca8c3edaa19ebb-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/7c9d0b1f96aebd7b5eca8c3edaa19ebb-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 262270,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4463130098097528914&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "University of Bonn + Fraunhofer IAIS; University of Bonn + Fraunhofer IAIS",
        "aff_domain": "uni-bonn.de;uni-bonn.de",
        "email": "uni-bonn.de;uni-bonn.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "University of Bonn;Fraunhofer Institute for Applied Information Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uni-bonn.de/;https://www.iais.fraunhofer.de/",
        "aff_unique_abbr": "UBonn;Fraunhofer IAIS",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "eee7720c45",
        "title": "Predicting response time and error rates in visual search",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/78b9cab19959e4af8ff46156ee460c74-Abstract.html",
        "author": "Bo Chen; Vidhya Navalpakkam; Pietro Perona",
        "abstract": "A model of human visual search is proposed. It predicts both response time (RT) and error rates (RT) as a function of image parameters such as target contrast and clutter. The model is an ideal observer, in that it optimizes the Bayes ratio of tar- get present vs target absent. The ratio is computed on the firing pattern of V1/V2 neurons, modeled by Poisson distributions. The optimal mechanism for integrat- ing information over time is shown to be a \u2018soft max\u2019 of diffusions, computed over the visual field by \u2018hypercolumns\u2019 of neurons that share the same receptive field and have different response properties to image features. An approximation of the optimal Bayesian observer, based on integrating local decisions, rather than diffusions, is also derived; it is shown experimentally to produce very similar pre- dictions. A psychophyisics experiment is proposed that may discriminate between which mechanism is used in the human brain.",
        "bibtex": "@inproceedings{NIPS2011_78b9cab1,\n author = {Chen, Bo and Navalpakkam, Vidhya and Perona, Pietro},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Predicting response time and error rates in visual search},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/78b9cab19959e4af8ff46156ee460c74-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/78b9cab19959e4af8ff46156ee460c74-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/78b9cab19959e4af8ff46156ee460c74-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2566846,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1219570514333109723&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Caltech; Yahoo! Research; Caltech",
        "aff_domain": "caltech.edu;yahoo-inc.com;caltech.edu",
        "email": "caltech.edu;yahoo-inc.com;caltech.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "California Institute of Technology;Yahoo!",
        "aff_unique_dep": ";Yahoo! Research",
        "aff_unique_url": "https://www.caltech.edu;https://research.yahoo.com",
        "aff_unique_abbr": "Caltech;Yahoo!",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pasadena;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7cd4977f41",
        "title": "Prediction strategies without loss",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/11b921ef080f7736089c757404650e40-Abstract.html",
        "author": "Michael Kapralov; Rina Panigrahy",
        "abstract": "Consider a sequence of bits where we are trying to predict the next bit from the previous bits. Assume we are allowed to say `predict 0' or `predict 1', and our payoff is $+1$ if the prediction is correct and $-1$ otherwise. We will say that at each point in time the loss of an algorithm is the number of wrong predictions minus the number of right predictions so far.  In this paper we are interested in algorithms that have essentially zero (expected) loss over any string at any point in time and yet have small regret with respect to always predicting $0$ or always predicting $1$. For a sequence of length $T$ our algorithm has  regret $14\\epsilon T $ and loss   $2\\sqrt{T}e^{-\\epsilon^2 T} $ in expectation for all strings. We show that the tradeoff between loss and regret is optimal up to constant factors.    Our techniques extend to the general setting of $N$ experts, where the related problem of trading off regret to the best expert for regret to the 'special' expert has been studied by Even-Dar et al. (COLT'07). We obtain essentially zero loss with respect to the special expert and optimal loss/regret tradeoff,  improving upon the results of Even-Dar et al (COLT'07) and settling the main question left open in their paper.     The strong loss bounds of the algorithm have some surprising consequences. First, we obtain a parameter free algorithm for the experts problem that has optimal regret bounds with respect to $k$-shifting optima, i.e. bounds with respect to the optimum that is allowed to change arms multiple times. Moreover, for {\\em any window of size $n$} the regret of our algorithm to any expert never exceeds $O(\\sqrt{n(\\log N+\\log T)})$, where $N$ is the number of experts and $T$ is the time horizon, while maintaining the essentially zero loss property.",
        "bibtex": "@inproceedings{NIPS2011_11b921ef,\n author = {Kapralov, Michael and Panigrahy, Rina},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Prediction strategies without loss},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/11b921ef080f7736089c757404650e40-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/11b921ef080f7736089c757404650e40-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/11b921ef080f7736089c757404650e40-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 283896,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5657446814712110340&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Stanford University; Microsoft Research Silicon Valley",
        "aff_domain": "stanford.edu;microsoft.com",
        "email": "stanford.edu;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Stanford University;Microsoft Research",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.stanford.edu;https://www.microsoft.com/en-us/research/group/microsoft-research-silicon-valley",
        "aff_unique_abbr": "Stanford;MSR SV",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Stanford;Silicon Valley",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7d0bfae6e9",
        "title": "Priors over Recurrent Continuous Time Processes",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/043ab21fc5a1607b381ac3896176dac6-Abstract.html",
        "author": "Ardavan Saeedi; Alexandre Bouchard-c\u00f4t\u00e9",
        "abstract": "We introduce the Gamma-Exponential Process (GEP), a prior over a large family of continuous time stochastic processes. A hierarchical version of this prior (HGEP; the Hierarchical GEP) yields a useful model for analyzing complex time series. Models based on HGEPs display many attractive properties: conjugacy, exchangeability and closed-form predictive distribution for the waiting times, and exact Gibbs updates for the time scale parameters. After establishing these properties, we show how posterior inference can be carried efficiently using Particle MCMC methods [1]. This yields a MCMC algorithm that can resample entire sequences atomically while avoiding the complications of introducing slice and stick auxiliary variables of the beam sampler [2]. We applied our model to the problem of estimating the disease progression in multiple sclerosis [3], and to RNA evolutionary modeling [4]. In both domains, we found that our model outperformed the standard rate matrix estimation approach.",
        "bibtex": "@inproceedings{NIPS2011_043ab21f,\n author = {Saeedi, Ardavan and Bouchard-c\\^{o}t\\'{e}, Alexandre},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Priors over Recurrent Continuous Time Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/043ab21fc5a1607b381ac3896176dac6-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/043ab21fc5a1607b381ac3896176dac6-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/043ab21fc5a1607b381ac3896176dac6-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/043ab21fc5a1607b381ac3896176dac6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1174450,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1059953321701174087&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Statistics, University of British Columbia; Department of Statistics, University of British Columbia",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of British Columbia",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "https://www.ubc.ca",
        "aff_unique_abbr": "UBC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Vancouver",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "8316be23a7",
        "title": "Prismatic Algorithm for Discrete D.C. Programming Problem",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/83f97f4825290be4cb794ec6a234595f-Abstract.html",
        "author": "Yoshinobu Kawahara; Takashi Washio",
        "abstract": "In this paper, we propose the first exact algorithm for minimizing the difference of two submodular functions (D.S.), i.e., the discrete version of the D.C. programming problem. The developed algorithm is a branch-and-bound-based algorithm which responds to the structure of this problem through the relationship between submodularity and convexity. The D.S. programming problem covers a broad range of applications in machine learning because this generalizes the optimization of a wide class of set functions. We empirically investigate the performance of our algorithm, and illustrate the difference between exact and approximate solutions respectively obtained by the proposed and existing algorithms in feature selection and discriminative structure learning.",
        "bibtex": "@inproceedings{NIPS2011_83f97f48,\n author = {Kawahara, Yoshinobu and Washio, Takashi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Prismatic Algorithm for Discrete D.C. Programming Problem},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/83f97f4825290be4cb794ec6a234595f-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/83f97f4825290be4cb794ec6a234595f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/83f97f4825290be4cb794ec6a234595f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 152771,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2630697210372442491&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "The Institute of Scienti\ufb01c and Industrial Research (ISIR), Osaka University; The Institute of Scienti\ufb01c and Industrial Research (ISIR), Osaka University",
        "aff_domain": "ar.sanken.osaka-u.ac.jp;ar.sanken.osaka-u.ac.jp",
        "email": "ar.sanken.osaka-u.ac.jp;ar.sanken.osaka-u.ac.jp",
        "github": "",
        "project": "http://www.ar.sanken.osaka-u.ac.jp/\u0018kawahara/",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "The Institute of Scienti\ufb01c and Industrial Research (ISIR), Osaka University",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "152afcf204",
        "title": "Probabilistic Joint Image Segmentation and Labeling",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/eddb904a6db773755d2857aacadb1cb0-Abstract.html",
        "author": "Adrian Ion; Joao Carreira; Cristian Sminchisescu",
        "abstract": "We present a joint image segmentation and labeling model (JSL) which, given a bag of figure-ground segment hypotheses extracted at multiple image locations and scales, constructs a joint probability distribution over both the compatible image interpretations (tilings or image segmentations) composed from those segments, and over their labeling into categories.  The process of drawing samples from the joint distribution can be interpreted as first sampling tilings, modeled as maximal cliques, from a graph connecting spatially non-overlapping segments in the bag, followed by sampling labels for those segments, conditioned on the choice of a particular tiling. We learn the segmentation and labeling parameters jointly, based on Maximum Likelihood with a novel Incremental Saddle Point estimation procedure. The partition function over tilings and labelings is increasingly more accurately approximated by including incorrect configurations that a not-yet-competent model rates probable during learning.   We show that the proposed methodology matches the current state of the art in the Stanford dataset, as well as in VOC2010, where 41.7% accuracy on the test set is achieved.",
        "bibtex": "@inproceedings{NIPS2011_eddb904a,\n author = {Ion, Adrian and Carreira, Joao and Sminchisescu, Cristian},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Probabilistic Joint Image Segmentation and Labeling},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/eddb904a6db773755d2857aacadb1cb0-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/eddb904a6db773755d2857aacadb1cb0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/eddb904a6db773755d2857aacadb1cb0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 482831,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18408834959043640216&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Faculty of Mathematics and Natural Sciences, University of Bonn + PRIP, Vienna University of Technology & Institute of Science and Technology, Austria; Faculty of Mathematics and Natural Sciences, University of Bonn; Faculty of Mathematics and Natural Sciences, University of Bonn",
        "aff_domain": "ins.uni-bonn.de;ins.uni-bonn.de;ins.uni-bonn.de",
        "email": "ins.uni-bonn.de;ins.uni-bonn.de;ins.uni-bonn.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;0",
        "aff_unique_norm": "Faculty of Mathematics and Natural Sciences, University of Bonn;PRIP, Vienna University of Technology & Institute of Science and Technology, Austria",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "ab9aab3ac5",
        "title": "Probabilistic Modeling of Dependencies Among Visual Short-Term Memory Representations",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/07563a3fe3bbe7e3ba84431ad9d055af-Abstract.html",
        "author": "Emin Orhan; Robert A. Jacobs",
        "abstract": "Extensive evidence suggests that items are not encoded independently in visual short-term memory (VSTM). However, previous research has not quantitatively considered how the encoding of an item influences the encoding of other items. Here, we model the dependencies among VSTM representations using a multivariate Gaussian distribution with a stimulus-dependent mean and covariance matrix. We report the results of an experiment designed to determine the specific form of the stimulus-dependence of the mean and the covariance matrix. We find that the magnitude of the covariance between the representations of two items is a monotonically decreasing function of the difference between the items' feature values, similar to a Gaussian process with a distance-dependent, stationary kernel function. We further show that this type of covariance function can be explained as a natural consequence of encoding multiple stimuli in a population of neurons with correlated responses.",
        "bibtex": "@inproceedings{NIPS2011_07563a3f,\n author = {Orhan, Emin and Jacobs, Robert},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Probabilistic Modeling of Dependencies Among Visual Short-Term Memory Representations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/07563a3fe3bbe7e3ba84431ad9d055af-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/07563a3fe3bbe7e3ba84431ad9d055af-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/07563a3fe3bbe7e3ba84431ad9d055af-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/07563a3fe3bbe7e3ba84431ad9d055af-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 524425,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2165252024672689008&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Brain & Cognitive Sciences, University of Rochester; Department of Brain & Cognitive Sciences, University of Rochester",
        "aff_domain": "bcs.rochester.edu;bcs.rochester.edu",
        "email": "bcs.rochester.edu;bcs.rochester.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Department of Brain & Cognitive Sciences, University of Rochester",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "2e818b3b08",
        "title": "Probabilistic amplitude and frequency demodulation",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/c3992e9a68c5ae12bd18488bc579b30d-Abstract.html",
        "author": "Richard Turner; Maneesh Sahani",
        "abstract": "A number of recent scientific and engineering problems require signals to be decomposed into a product of a slowly varying positive envelope and a quickly varying carrier whose instantaneous frequency also varies slowly over time. Although signal processing provides algorithms for so-called amplitude- and frequency-demodulation (AFD), there are well known problems with all of the existing methods. Motivated by the fact that AFD is ill-posed, we approach the problem using probabilistic inference. The new approach, called probabilistic amplitude and frequency demodulation (PAFD), models instantaneous frequency using an auto-regressive generalization of the von Mises distribution, and the envelopes using Gaussian auto-regressive dynamics with a positivity constraint. A novel form of expectation propagation is used for inference. We demonstrate that although PAFD is computationally demanding, it outperforms previous approaches on synthetic and real signals in clean, noisy and missing data settings.",
        "bibtex": "@inproceedings{NIPS2011_c3992e9a,\n author = {Turner, Richard and Sahani, Maneesh},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Probabilistic amplitude and frequency demodulation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/c3992e9a68c5ae12bd18488bc579b30d-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/c3992e9a68c5ae12bd18488bc579b30d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/c3992e9a68c5ae12bd18488bc579b30d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/c3992e9a68c5ae12bd18488bc579b30d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 520835,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5163234283490336875&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Computational and Biological Learning Lab, Department of Engineering, University of Cambridge, Trumpington Street, Cambridge, CB2 1PZ, UK; Gatsby Computational Neuroscience Unit, University College London, Alexandra House, 17 Queen Square, London, WC1N 3AR, UK",
        "aff_domain": "cam.ac.uk;gatsby.ucl.ac.uk",
        "email": "cam.ac.uk;gatsby.ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Computational and Biological Learning Lab, Department of Engineering, University of Cambridge, Trumpington Street, Cambridge, CB2 1PZ, UK;Gatsby Computational Neuroscience Unit, University College London, Alexandra House, 17 Queen Square, London, WC1N 3AR, UK",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "19eecfb3ff",
        "title": "Projection onto A Nonnegative Max-Heap",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/6c524f9d5d7027454a783c841250ba71-Abstract.html",
        "author": "Jun Liu; Liang Sun; Jieping Ye",
        "abstract": "We consider the problem of computing the Euclidean projection of a vector of length $p$ onto a non-negative max-heap---an ordered tree where the values of the nodes are all nonnegative and the value of any parent node is no less than the value(s) of its child node(s). This Euclidean projection plays a building block role in the optimization problem with a non-negative max-heap constraint. Such a constraint is desirable when the features follow an ordered tree structure, that is, a given feature is selected for the given regression/classification task only if its parent node is selected. In this paper, we show that such Euclidean projection problem admits an analytical solution and we develop a top-down algorithm where the key operation is to find the so-called \\emph{maximal root-tree} of the subtree rooted at each node. A naive approach for finding the maximal root-tree is to enumerate all the possible root-trees, which, however, does not scale well. We reveal several important properties of the maximal root-tree, based on which we design a bottom-up algorithm with merge for efficiently finding the maximal root-tree. The proposed algorithm has a (worst-case) linear time complexity for a sequential list, and $O(p^2)$ for a general tree. We report simulation results showing the effectiveness of the max-heap for regression with an ordered tree structure. Empirical results show that the proposed algorithm has an expected linear time complexity for many special cases including a sequential list, a full binary tree, and a tree with depth 1.",
        "bibtex": "@inproceedings{NIPS2011_6c524f9d,\n author = {Liu, Jun and Sun, Liang and Ye, Jieping},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Projection onto A Nonnegative Max-Heap},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/6c524f9d5d7027454a783c841250ba71-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/6c524f9d5d7027454a783c841250ba71-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/6c524f9d5d7027454a783c841250ba71-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/6c524f9d5d7027454a783c841250ba71-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 222871,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11857752012859323755&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Arizona State University; Arizona State University; Arizona State University",
        "aff_domain": "asu.edu;asu.edu;asu.edu",
        "email": "asu.edu;asu.edu;asu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Arizona State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.asu.edu",
        "aff_unique_abbr": "ASU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a6051fc4ca",
        "title": "Pylon Model for Semantic Segmentation",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/fe8c15fed5f808006ce95eddb7366e35-Abstract.html",
        "author": "Victor Lempitsky; Andrea Vedaldi; Andrew Zisserman",
        "abstract": "Graph cut optimization is one of the standard workhorses of image segmentation since for binary random field representations of the  image, it gives globally optimal results and there are efficient  polynomial time implementations.  Often, the random field is applied  over a flat partitioning of the image into non-intersecting elements,  such as pixels or super-pixels.    In the paper we show that if, instead of a flat partitioning, the image is represented by a hierarchical segmentation tree, then the resulting energy combining unary and boundary terms can still be optimized using graph cut (with all the corresponding benefits of global optimality and efficiency). As a result of such inference, the image gets partitioned into a set of segments that may come from different layers of the tree.    We apply this formulation, which we call the pylon model, to the task of semantic segmentation where the goal is to separate an image into areas belonging to different semantic classes. The experiments highlight the advantage of inference on a segmentation tree (over a flat partitioning) and demonstrate that the optimization in the pylon model is able to flexibly choose the level of segmentation across the image. Overall, the proposed system has superior segmentation accuracy on several datasets (Graz-02, Stanford background) compared to previously suggested approaches.",
        "bibtex": "@inproceedings{NIPS2011_fe8c15fe,\n author = {Lempitsky, Victor and Vedaldi, Andrea and Zisserman, Andrew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Pylon Model for Semantic Segmentation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/fe8c15fed5f808006ce95eddb7366e35-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/fe8c15fed5f808006ce95eddb7366e35-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/fe8c15fed5f808006ce95eddb7366e35-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 3134832,
        "gs_citation": 197,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9845628989877200892&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Visual Geometry Group, University of Oxford+Yandex, Moscow; Visual Geometry Group, University of Oxford; Visual Geometry Group, University of Oxford",
        "aff_domain": "robots.ox.ac.uk;robots.ox.ac.uk;robots.ox.ac.uk",
        "email": "robots.ox.ac.uk;robots.ox.ac.uk;robots.ox.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;0",
        "aff_unique_norm": "University of Oxford;Yandex",
        "aff_unique_dep": "Visual Geometry Group;",
        "aff_unique_url": "https://www.ox.ac.uk;https://yandex.com",
        "aff_unique_abbr": "Oxford;Yandex",
        "aff_campus_unique_index": "0+1;0;0",
        "aff_campus_unique": "Oxford;Moscow",
        "aff_country_unique_index": "0+1;0;0",
        "aff_country_unique": "United Kingdom;Russia"
    },
    {
        "id": "99c5ed2b1f",
        "title": "Quasi-Newton Methods for Markov Chain Monte Carlo",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/e702e51da2c0f5be4dd354bb3e295d37-Abstract.html",
        "author": "Yichuan Zhang; Charles A. Sutton",
        "abstract": "The performance of Markov chain Monte Carlo methods is often sensitive to the scaling and correlations between the random variables of interest. An important source of information about the local correlation and scale is given by the Hessian matrix of the target distribution, but this is often either computationally expensive or infeasible. In this paper we propose MCMC samplers that make use of quasi-Newton approximations from the optimization literature, that approximate the Hessian of the target distribution from previous samples and gradients generated by the sampler. A key issue is that MCMC samplers that depend on the history of previous states are in general not valid. We address this problem by using limited memory quasi-Newton methods, which depend only on a fixed window of previous samples. On several real world datasets, we show that the quasi-Newton sampler is a more effective sampler than standard Hamiltonian Monte Carlo at a fraction of the cost of MCMC methods that require higher-order derivatives.",
        "bibtex": "@inproceedings{NIPS2011_e702e51d,\n author = {Zhang, Yichuan and Sutton, Charles},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Quasi-Newton Methods for Markov Chain Monte Carlo},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/e702e51da2c0f5be4dd354bb3e295d37-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/e702e51da2c0f5be4dd354bb3e295d37-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/e702e51da2c0f5be4dd354bb3e295d37-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 339770,
        "gs_citation": 91,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11420738842449459369&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "School of Informatics, University of Edinburgh; School of Informatics, University of Edinburgh",
        "aff_domain": "sms.ed.ac.uk;inf.ed.ac.uk",
        "email": "sms.ed.ac.uk;inf.ed.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Edinburgh",
        "aff_unique_dep": "School of Informatics",
        "aff_unique_url": "https://www.ed.ac.uk",
        "aff_unique_abbr": "Edinburgh",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Edinburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "114546de93",
        "title": "Query-Aware MCMC",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/215a71a12769b056c3c32e7299f1c5ed-Abstract.html",
        "author": "Michael L. Wick; Andrew McCallum",
        "abstract": "Traditional approaches to probabilistic inference such as loopy belief propagation and Gibbs sampling typically compute marginals for it all the unobserved variables in a graphical model. However, in many real-world applications the user's interests are focused on a subset of the variables, specified by a query. In this case it would be wasteful to uniformly sample, say, one million variables when the query concerns only ten. In this paper we propose a query-specific approach to MCMC that accounts for the query variables and their generalized mutual information with neighboring variables in order to achieve higher computational efficiency. Surprisingly there has been almost no previous work on query-aware MCMC. We demonstrate the success of our approach with positive experimental results on a wide range of graphical models.",
        "bibtex": "@inproceedings{NIPS2011_215a71a1,\n author = {Wick, Michael and McCallum, Andrew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Query-Aware MCMC},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/215a71a12769b056c3c32e7299f1c5ed-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/215a71a12769b056c3c32e7299f1c5ed-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/215a71a12769b056c3c32e7299f1c5ed-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1343097,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13500282313585557537&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, University of Massachusetts, Amherst, MA; Department of Computer Science, University of Massachusetts, Amherst, MA",
        "aff_domain": "cs.umass.edu;cs.umass.edu",
        "email": "cs.umass.edu;cs.umass.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Department of Computer Science, University of Massachusetts, Amherst, MA",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "ca80b5f82b",
        "title": "RTRMC: A Riemannian trust-region method for low-rank matrix completion",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/37bc2f75bf1bcfe8450a1a41c200364c-Abstract.html",
        "author": "Nicolas Boumal; Pierre-antoine Absil",
        "abstract": "We consider large matrices of low rank. We address the problem of recovering such matrices when most of the entries are unknown. Matrix completion finds applications in recommender systems. In this setting, the rows of the matrix may correspond to items and the columns may correspond to users. The known entries are the ratings given by users to some items. The aim is to predict the unobserved ratings. This problem is commonly stated in a constrained optimization framework. We follow an approach that exploits the geometry of the low-rank constraint to recast the problem as an unconstrained optimization problem on the Grassmann manifold. We then apply first- and second-order Riemannian trust-region methods to solve it. The cost of each iteration is linear in the number of known entries. Our methods, RTRMC 1 and 2, outperform state-of-the-art algorithms on a wide range of problem instances.",
        "bibtex": "@inproceedings{NIPS2011_37bc2f75,\n author = {Boumal, Nicolas and Absil, Pierre-antoine},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {RTRMC: A Riemannian trust-region method for low-rank matrix completion},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/37bc2f75bf1bcfe8450a1a41c200364c-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/37bc2f75bf1bcfe8450a1a41c200364c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/37bc2f75bf1bcfe8450a1a41c200364c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 512160,
        "gs_citation": 258,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5011658556315279816&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "ICTEAM Institute, Universit \u00b4e catholique de Louvain; ICTEAM Institute, Universit \u00b4e catholique de Louvain",
        "aff_domain": "uclouvain.be;inma.ucl.ac.be",
        "email": "uclouvain.be;inma.ucl.ac.be",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "ICTEAM Institute, Universit \u00b4e catholique de Louvain",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "841a9c95ef",
        "title": "Randomized Algorithms for Comparison-based Search",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/eb86d510361fc23b59f18c1bc9802cc6-Abstract.html",
        "author": "Dominique Tschopp; Suhas Diggavi; Payam Delgosha; Soheil Mohajer",
        "abstract": "This paper addresses the problem of finding the nearest neighbor (or    one of the $R$-nearest neighbors) of a query object $q$ in a    database of $n$ objects, when we can only use a comparison    oracle. The comparison oracle, given two reference objects and a    query object, returns the reference object most similar to the query    object.  The main problem we study is how to search the database for    the nearest neighbor (NN) of a query, while minimizing the    questions.  The difficulty of this problem depends on properties of    the underlying database. We show the importance of a    characterization: \\emph{combinatorial disorder} $D$ which defines    approximate triangle inequalities on ranks. We present a lower bound    of $\\Omega(D\\log \\frac{n}{D}+D^2)$ average number of questions in    the search phase for any randomized algorithm, which demonstrates    the fundamental role of $D$ for worst case behavior. We develop a    randomized scheme for NN retrieval in $O(D^3\\log^2 n+ D\\log^2 n    \\log\\log n^{D^3})$ questions.  The learning requires asking $O(n    D^3\\log^2 n+ D \\log^2 n \\log\\log n^{D^3})$ questions and    $O(n\\log^2n/\\log(2D))$ bits to store.",
        "bibtex": "@inproceedings{NIPS2011_eb86d510,\n author = {Tschopp, Dominique and Diggavi, Suhas and Delgosha, Payam and Mohajer, Soheil},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Randomized Algorithms for Comparison-based Search},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/eb86d510361fc23b59f18c1bc9802cc6-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/eb86d510361fc23b59f18c1bc9802cc6-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/eb86d510361fc23b59f18c1bc9802cc6-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/eb86d510361fc23b59f18c1bc9802cc6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 162180,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15584421499530725391&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "AWK Group, Bern, Switzerland; University of California Los Angeles (UCLA), Los Angeles, CA 90095; Sharif University of Technology, Tehran, Iran; Princeton University, Princeton, NJ 08544",
        "aff_domain": "gmail.com;ucla.edu;ee.sharif.ir;princeton.edu",
        "email": "gmail.com;ucla.edu;ee.sharif.ir;princeton.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "AWK Group, Bern, Switzerland;University of California Los Angeles (UCLA), Los Angeles, CA 90095;Sharif University of Technology;Princeton University, Princeton, NJ 08544",
        "aff_unique_dep": ";;;",
        "aff_unique_url": ";;https://www.sharif.edu;",
        "aff_unique_abbr": ";;SUT;",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Tehran",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";Iran"
    },
    {
        "id": "accd49669f",
        "title": "Ranking annotators for crowdsourced labeling tasks",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/24146db4eb48c718b84cae0a0799dcfc-Abstract.html",
        "author": "Vikas C. Raykar; Shipeng Yu",
        "abstract": "With the advent of crowdsourcing services it has become quite cheap and reasonably effective to get a dataset labeled by multiple annotators in a short amount of time. Various methods have been proposed to estimate the consensus labels by correcting for the bias of annotators with different kinds of expertise. Often we have low quality annotators or spammers--annotators who assign labels randomly (e.g., without actually looking at the instance). Spammers can make the cost of acquiring labels very expensive and can potentially degrade the quality of the consensus labels. In this paper we formalize the notion of a spammer and define a score which can be used to rank the annotators---with the spammers having a score close to zero and the good annotators having a high score close to one.",
        "bibtex": "@inproceedings{NIPS2011_24146db4,\n author = {Raykar, Vikas C and Yu, Shipeng},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Ranking annotators for crowdsourced labeling tasks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/24146db4eb48c718b84cae0a0799dcfc-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/24146db4eb48c718b84cae0a0799dcfc-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/24146db4eb48c718b84cae0a0799dcfc-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 194505,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "Siemens Healthcare, Malvern, PA, USA; Siemens Healthcare, Malvern, PA, USA",
        "aff_domain": "siemens.com;siemens.com",
        "email": "siemens.com;siemens.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Siemens Healthcare, Malvern, PA, USA",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "b9be23f3c4",
        "title": "Rapid Deformable Object Detection using Dual-Tree Branch-and-Bound",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/07042ac7d03d3b9911a00da43ce0079a-Abstract.html",
        "author": "Iasonas Kokkinos",
        "abstract": "In this work we use Branch-and-Bound (BB) to efficiently detect objects with deformable part models.  Instead of evaluating the classifier score exhaustively over image locations and scales, we use BB to focus on promising image  locations.  The core problem is to compute bounds that accommodate part deformations; for this we adapt the Dual Trees data structure  to our problem.    We evaluate our approach using  Mixture-of-Deformable Part Models.  We obtain exactly the same results but are 10-20 times faster on average.  We also develop a multiple-object detection  variation of the system, where  hypotheses for 20 categories are inserted in a common   priority queue. For the problem of finding the strongest category in an image this results in up to a 100-fold speedup.",
        "bibtex": "@inproceedings{NIPS2011_07042ac7,\n author = {Kokkinos, Iasonas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Rapid Deformable Object Detection using Dual-Tree Branch-and-Bound},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/07042ac7d03d3b9911a00da43ce0079a-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/07042ac7d03d3b9911a00da43ce0079a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/07042ac7d03d3b9911a00da43ce0079a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 199753,
        "gs_citation": 82,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13377219916780306923&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "0b2431fe5f",
        "title": "Reconstructing Patterns of Information Diffusion from Incomplete Observations",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/f4be00279ee2e0a53eafdaa94a151e2c-Abstract.html",
        "author": "Flavio Chierichetti; David Liben-nowell; Jon M. Kleinberg",
        "abstract": "Motivated by the spread of on-line information in general and   on-line petitions in particular, recent research has raised the following   combinatorial estimation problem.  There is a tree T that we cannot observe directly (representing  the structure along which the information has spread), and certain  nodes randomly decide to make their copy of the information public.  In the case of a petition, the list of names on each public copy   of the petition also reveals a path leading back to the root of the tree.  What can we conclude about the properties of the tree we observe  from these revealed paths,  and can we use the structure of the observed tree  to estimate the size of the full unobserved tree T?    Here we provide the first algorithm for this size estimation task,  together with provable guarantees on its performance.  We also establish structural properties of the observed tree, providing the  first rigorous explanation for some of the unusual structural  phenomena present in the spread of real chain-letter petitions  on the Internet.",
        "bibtex": "@inproceedings{NIPS2011_f4be0027,\n author = {Chierichetti, Flavio and Liben-nowell, David and Kleinberg, Jon},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Reconstructing Patterns of Information Diffusion from Incomplete Observations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/f4be00279ee2e0a53eafdaa94a151e2c-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/f4be00279ee2e0a53eafdaa94a151e2c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/f4be00279ee2e0a53eafdaa94a151e2c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 259931,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1629139199292796719&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "4c3fdb30fc",
        "title": "Recovering Intrinsic Images with a Global Sparsity Prior on Reflectance",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/2b8a61594b1f4c4db0902a8a395ced93-Abstract.html",
        "author": "Carsten Rother; Martin Kiefel; Lumin Zhang; Bernhard Sch\u00f6lkopf; Peter V. Gehler",
        "abstract": "We address the challenging task of decoupling material properties from lighting properties given a single image. In the last two decades virtually all works have concentrated on exploiting edge information to address this problem. We take a different route by introducing a new prior on reflectance, that models reflectance values as being drawn from a sparse set of basis colors. This results in a Random Field model with global, latent variables (basis colors) and pixel-accurate output reflectance values. We show that without edge information high-quality results can be achieved, that are on par with methods exploiting this source of information. Finally, we present competitive results by integrating an additional edge model. We believe that our approach is a solid starting point for future development in this domain.",
        "bibtex": "@inproceedings{NIPS2011_2b8a6159,\n author = {Rother, Carsten and Kiefel, Martin and Zhang, Lumin and Sch\\\"{o}lkopf, Bernhard and Gehler, Peter},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Recovering Intrinsic Images with a Global Sparsity Prior on Reflectance},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/2b8a61594b1f4c4db0902a8a395ced93-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/2b8a61594b1f4c4db0902a8a395ced93-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/2b8a61594b1f4c4db0902a8a395ced93-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 4959342,
        "gs_citation": 234,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5438593079896905661&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "ba506b4110",
        "title": "Regularized Laplacian Estimation and Fast Eigenvector Approximation",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/0e9fa1f3e9e66792401a6972d477dcc3-Abstract.html",
        "author": "Patrick O. Perry; Michael W. Mahoney",
        "abstract": "Recently, Mahoney and Orecchia demonstrated that popular diffusion-based procedures to compute a quick approximation to the first nontrivial eigenvector of a data graph Laplacian exactly solve certain regularized Semi-Definite Programs (SDPs). In this paper, we extend that result by providing a statistical interpretation of their approximation procedure. Our interpretation will be analogous to the manner in which l2-regularized or l1-regularized l2 regression (often called Ridge regression and Lasso regression, respectively) can be interpreted in terms of a Gaussian prior or a Laplace prior, respectively, on the coefficient vector of the regression problem. Our framework will imply that the solutions to the Mahoney-Orecchia regularized SDP can be interpreted as regularized estimates of the pseudoinverse of the graph Laplacian. Conversely, it will imply that the solution to this regularized estimation problem can be computed very quickly by running, e.g., the fast diffusion-based PageRank procedure for computing an approximation to the first nontrivial eigenvector of the graph Laplacian. Empirical results are also provided to illustrate the manner in which approximate eigenvector computation implicitly performs statistical regularization, relative to running the corresponding exact algorithm.",
        "bibtex": "@inproceedings{NIPS2011_0e9fa1f3,\n author = {Perry, Patrick and Mahoney, Michael W},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Regularized Laplacian Estimation and Fast Eigenvector Approximation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/0e9fa1f3e9e66792401a6972d477dcc3-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/0e9fa1f3e9e66792401a6972d477dcc3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/0e9fa1f3e9e66792401a6972d477dcc3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 410643,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7990197310607102525&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Information, Operations, and Management Sciences, NYU Stern School of Business; Department of Mathematics, Stanford University",
        "aff_domain": "stern.nyu.edu;cs.stanford.edu",
        "email": "stern.nyu.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Information, Operations, and Management Sciences, NYU Stern School of Business;Stanford University",
        "aff_unique_dep": ";Department of Mathematics",
        "aff_unique_url": ";https://www.stanford.edu",
        "aff_unique_abbr": ";Stanford",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Stanford",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "208d94e4cf",
        "title": "Reinforcement Learning using Kernel-Based Stochastic Factorization",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/b534ba68236ba543ae44b22bd110a1d6-Abstract.html",
        "author": "Andre Barreto; Doina Precup; Joelle Pineau",
        "abstract": "Kernel-based reinforcement-learning (KBRL) is a method for learning a decision policy from a set of sample transitions which stands out for its strong theoretical guarantees. However, the size of the approximator grows with the number of transitions, which makes the approach impractical for large problems.  In this paper we introduce a novel algorithm to improve the scalability of KBRL. We resort to a special decomposition of a transition matrix, called stochastic factorization, to fix the size of the approximator while at the same time incorporating all the information contained in the data. The resulting algorithm, kernel-based stochastic factorization (KBSF), is much faster but still converges to a unique solution. We derive a theoretical upper bound for the distance between the value functions computed by KBRL and KBSF. The effectiveness of our method is illustrated with computational experiments on four reinforcement-learning problems, including a difficult task in which the goal is to learn a neurostimulation policy to suppress the occurrence of seizures in epileptic rat brains. We empirically demonstrate that the proposed approach is able to compress the information contained in KBRL's model. Also, on the tasks studied, KBSF outperforms two of the most prominent reinforcement-learning algorithms, namely least-squares policy iteration and fitted Q-iteration.",
        "bibtex": "@inproceedings{NIPS2011_b534ba68,\n author = {Barreto, Andre and Precup, Doina and Pineau, Joelle},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Reinforcement Learning using Kernel-Based Stochastic Factorization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/b534ba68236ba543ae44b22bd110a1d6-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/b534ba68236ba543ae44b22bd110a1d6-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/b534ba68236ba543ae44b22bd110a1d6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 124155,
        "gs_citation": 51,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5634321236767053295&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 9,
        "aff": "School of Computer Science, McGill University, Montreal, Canada; School of Computer Science, McGill University, Montreal, Canada; School of Computer Science, McGill University, Montreal, Canada",
        "aff_domain": "cs.mcgill.ca;cs.mcgill.ca;cs.mcgill.ca",
        "email": "cs.mcgill.ca;cs.mcgill.ca;cs.mcgill.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "McGill University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.mcgill.ca",
        "aff_unique_abbr": "McGill",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Montreal",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "b53260dd6a",
        "title": "Relative Density-Ratio Estimation for Robust Distribution Comparison",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/d1f255a373a3cef72e03aa9d980c7eca-Abstract.html",
        "author": "Makoto Yamada; Taiji Suzuki; Takafumi Kanamori; Hirotaka Hachiya; Masashi Sugiyama",
        "abstract": "Divergence estimators based on direct approximation of density-ratios without going through separate approximation of numerator and denominator densities have been successfully applied to machine learning tasks that involve distribution comparison such as outlier detection, transfer learning, and two-sample homogeneity test. However, since density-ratio functions often possess high fluctuation, divergence estimation is still a challenging task in practice. In this paper, we propose to use relative divergences for distribution comparison, which involves approximation of relative density-ratios. Since relative density-ratios are always smoother than corresponding ordinary density-ratios, our proposed method is favorable in terms of the non-parametric convergence speed. Furthermore, we show that the proposed divergence estimator has asymptotic variance independent of the model complexity under a parametric setup, implying that the proposed estimator hardly overfits even with complex models. Through experiments, we demonstrate the usefulness of the proposed approach.",
        "bibtex": "@inproceedings{NIPS2011_d1f255a3,\n author = {Yamada, Makoto and Suzuki, Taiji and Kanamori, Takafumi and Hachiya, Hirotaka and Sugiyama, Masashi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Relative Density-Ratio Estimation for Robust Distribution Comparison},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/d1f255a373a3cef72e03aa9d980c7eca-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/d1f255a373a3cef72e03aa9d980c7eca-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/d1f255a373a3cef72e03aa9d980c7eca-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/d1f255a373a3cef72e03aa9d980c7eca-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 219684,
        "gs_citation": 172,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4328848016927232162&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Tokyo Institute of Technology; The University of Tokyo; Nagoya University; Tokyo Institute of Technology; Tokyo Institute of Technology",
        "aff_domain": "sg.cs.titech.ac.jp;stat.t.u-tokyo.ac.jp;is.nagoya-u.ac.jp;sg.cs.titech.ac.jp;sg.cs.titech.ac.jp",
        "email": "sg.cs.titech.ac.jp;stat.t.u-tokyo.ac.jp;is.nagoya-u.ac.jp;sg.cs.titech.ac.jp;sg.cs.titech.ac.jp",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0;0",
        "aff_unique_norm": "Tokyo Institute of Technology;University of Tokyo;Nagoya University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.titech.ac.jp;https://www.u-tokyo.ac.jp;https://www.nagoya-u.ac.jp",
        "aff_unique_abbr": "Titech;UTokyo;Nagoya U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "4b8380bf16",
        "title": "Robust Lasso with missing and grossly corrupted observations",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/a2137a2ae8e39b5002a3f8909ecb88fe-Abstract.html",
        "author": "Nasser M. Nasrabadi; Trac D. Tran; Nam Nguyen",
        "abstract": "This paper studies the problem of accurately recovering a sparse vector $\\beta^{\\star}$ from highly corrupted linear measurements $y = X \\beta^{\\star} + e^{\\star} + w$ where $e^{\\star}$ is a sparse error vector whose nonzero entries may be unbounded and $w$ is a bounded noise. We propose a so-called extended Lasso optimization which takes into consideration sparse prior information of both $\\beta^{\\star}$ and $e^{\\star}$. Our first result shows that the extended Lasso can faithfully recover both the regression and the corruption vectors. Our analysis is relied on a notion of extended restricted eigenvalue for the design matrix $X$. Our second set of results applies to a general class of Gaussian design matrix $X$ with i.i.d rows $\\oper N(0, \\Sigma)$, for which we provide a surprising phenomenon: the extended Lasso can recover exact signed supports of both $\\beta^{\\star}$ and $e^{\\star}$ from only $\\Omega(k \\log p \\log n)$ observations, even the fraction of corruption is arbitrarily close to one. Our analysis also shows that this amount of observations required to achieve exact signed support is optimal.",
        "bibtex": "@inproceedings{NIPS2011_a2137a2a,\n author = {Nasrabadi, Nasser and Tran, Trac and Nguyen, Nam},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Robust Lasso with missing and grossly corrupted observations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/a2137a2ae8e39b5002a3f8909ecb88fe-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/a2137a2ae8e39b5002a3f8909ecb88fe-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/a2137a2ae8e39b5002a3f8909ecb88fe-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 268002,
        "gs_citation": 151,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16915507597944306218&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Johns Hopkins University; U.S. Army Research Lab; Johns Hopkins University",
        "aff_domain": "jhu.edu;mail.mil;jhu.edu",
        "email": "jhu.edu;mail.mil;jhu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Johns Hopkins University;U.S. Army Research Laboratory",
        "aff_unique_dep": ";Research",
        "aff_unique_url": "https://www.jhu.edu;https://www.arl.army.mil",
        "aff_unique_abbr": "JHU;ARL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d6ac396b5a",
        "title": "Robust Multi-Class Gaussian Process Classification",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/7eabe3a1649ffa2b3ff8c02ebfd5659f-Abstract.html",
        "author": "Daniel Hern\u00e1ndez-lobato; Jose M. Hern\u00e1ndez-lobato; Pierre Dupont",
        "abstract": "Multi-class Gaussian Process Classifiers (MGPCs) are often affected by over-fitting problems when labeling errors occur far from the decision boundaries. To prevent this, we investigate a robust MGPC (RMGPC) which considers labeling errors independently of their distance to the decision boundaries. Expectation propagation is used for approximate inference. Experiments with several datasets in which noise is injected in the class labels illustrate the benefits of RMGPC. This method performs better than other Gaussian process alternatives based on considering latent Gaussian noise or heavy-tailed processes. When no noise is injected in the labels, RMGPC still performs equal or better than the other methods. Finally, we show how RMGPC can be used for successfully identifying data instances which are difficult to classify accurately in practice.",
        "bibtex": "@inproceedings{NIPS2011_7eabe3a1,\n author = {Hern\\'{a}ndez-lobato, Daniel and Hern\\'{a}ndez-lobato, Jose and Dupont, Pierre},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Robust Multi-Class Gaussian Process Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/7eabe3a1649ffa2b3ff8c02ebfd5659f-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/7eabe3a1649ffa2b3ff8c02ebfd5659f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/7eabe3a1649ffa2b3ff8c02ebfd5659f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 355331,
        "gs_citation": 99,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9756920707180547042&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "ICTEAM - Machine Learning Group, Universit\u00e9 catholique de Louvain; Department of Engineering, University of Cambridge; ICTEAM - Machine Learning Group, Universit\u00e9 catholique de Louvain",
        "aff_domain": "gmail.com;eng.cam.ac.uk;uclouvain.be",
        "email": "gmail.com;eng.cam.ac.uk;uclouvain.be",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "ICTEAM - Machine Learning Group, Universit\u00e9 catholique de Louvain;University of Cambridge",
        "aff_unique_dep": ";Department of Engineering",
        "aff_unique_url": ";https://www.cam.ac.uk",
        "aff_unique_abbr": ";Cambridge",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United Kingdom"
    },
    {
        "id": "5c9c993e69",
        "title": "Scalable Training of Mixture Models via Coresets",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/2b6d65b9a9445c4271ab9076ead5605a-Abstract.html",
        "author": "Dan Feldman; Matthew Faulkner; Andreas Krause",
        "abstract": "How can we train a statistical mixture model on a massive data set? In this paper, we show how to construct coresets for mixtures of Gaussians and natural generalizations. A coreset is a weighted subset of the data, which guarantees that models fitting the coreset will also provide a good fit for the original data set. We show that, perhaps surprisingly, Gaussian mixtures admit coresets of size independent of the size of the data set. More precisely, we prove that a weighted set of $O(dk^3/\\eps^2)$ data points suffices for computing a $(1+\\eps)$-approximation for the optimal model on the original $n$ data points. Moreover, such coresets can be efficiently constructed in a map-reduce style computation, as well as in a streaming setting. Our results rely on a novel reduction of statistical estimation to problems in computational geometry, as well as new complexity results about mixtures of Gaussians. We empirically evaluate our algorithms on several real data sets, including a density estimation problem in the context of earthquake detection using accelerometers in  mobile phones.",
        "bibtex": "@inproceedings{NIPS2011_2b6d65b9,\n author = {Feldman, Dan and Faulkner, Matthew and Krause, Andreas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Scalable Training of Mixture Models via Coresets},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/2b6d65b9a9445c4271ab9076ead5605a-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/2b6d65b9a9445c4271ab9076ead5605a-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/2b6d65b9a9445c4271ab9076ead5605a-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/2b6d65b9a9445c4271ab9076ead5605a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 845464,
        "gs_citation": 193,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12836702362196351085&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "4f3fc93be1",
        "title": "See the Tree Through the Lines: The Shazoo Algorithm",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/8b6dd7db9af49e67306feb59a8bdc52c-Abstract.html",
        "author": "Fabio Vitale; Nicol\u00f2 Cesa-bianchi; Claudio Gentile; Giovanni Zappella",
        "abstract": "Predicting the nodes of a given graph is a fascinating   theoretical problem with applications in several domains.   Since graph sparsification via spanning trees   retains enough information while making the task much easier,   trees are an important special case of this problem.   Although it is known how to predict the nodes of an unweighted tree   in a nearly optimal way, in the weighted case a fully satisfactory   algorithm is not available yet. We fill this hole and introduce an efficient node predictor,   Shazoo, which is nearly optimal on any weighted tree. Moreover, we show that Shazoo can   be viewed as a common nontrivial generalization of both previous approaches for   unweighted trees and weighted lines.   Experiments on real-world datasets confirm that Shazoo performs well in that   it fully exploits the structure of the input tree,   and gets very close to (and sometimes better than)   less scalable energy minimization methods.",
        "bibtex": "@inproceedings{NIPS2011_8b6dd7db,\n author = {Vitale, Fabio and Cesa-bianchi, Nicol\\`{o} and Gentile, Claudio and Zappella, Giovanni},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {See the Tree Through the Lines: The Shazoo Algorithm},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/8b6dd7db9af49e67306feb59a8bdc52c-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/8b6dd7db9af49e67306feb59a8bdc52c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/8b6dd7db9af49e67306feb59a8bdc52c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 639149,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15489487890694939365&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "DSI, University of Milan, Italy; DSI, University of Milan, Italy; DICOM, University of Insubria, Italy; Dept. of Mathematics, Univ. of Milan, Italy",
        "aff_domain": "unimi.it;unimi.it;uninsubria.it;unimi.it",
        "email": "unimi.it;unimi.it;uninsubria.it;unimi.it",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2",
        "aff_unique_norm": "DSI, University of Milan, Italy;DICOM, University of Insubria, Italy;Dept. of Mathematics, Univ. of Milan, Italy",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "21270656a2",
        "title": "Select and Sample - A Model of Efficient Neural Inference and Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/a424ed4bd3a7d6aea720b86d4a360f75-Abstract.html",
        "author": "Jacquelyn A. Shelton; Abdul S. Sheikh; Pietro Berkes; Joerg Bornschein; J\u00f6rg L\u00fccke",
        "abstract": "An increasing number of experimental studies indicate that perception encodes a posterior probability distribution over possible causes of sensory stimuli, which is used to act close to optimally in the environment. One outstanding difficulty with this hypothesis is that the exact posterior will in general be too complex to be represented directly, and thus neurons will have to represent an approximation of this distribution. Two influential proposals of efficient posterior representation by neural populations are: 1) neural activity represents samples of the underlying distribution, or 2) they represent a parametric representation of a variational approximation of the posterior. We show that these approaches can be combined for an inference scheme that retains the advantages of both: it is able to represent multiple modes and arbitrary correlations, a feature of sampling methods, and it reduces the represented space to regions of high probability mass, a strength of variational approximations. Neurally, the combined method can be interpreted as a feed-forward preselection of the relevant state space, followed by a neural dynamics implementation of Markov Chain Monte Carlo (MCMC) to approximate the posterior over the relevant states. We demonstrate the effectiveness and efficiency of this approach on a sparse coding model. In numerical experiments on artificial data and image patches, we compare the performance of the algorithms to that of exact EM, variational state space selection alone, MCMC alone, and the combined select and sample approach. The select and sample approach integrates the advantages of the sampling and variational approximations, and forms a robust, neurally plausible, and very efficient model of processing and learning in cortical networks. For sparse coding we show applications easily exceeding a thousand observed and a thousand hidden dimensions.",
        "bibtex": "@inproceedings{NIPS2011_a424ed4b,\n author = {Shelton, Jacquelyn and Sheikh, Abdul and Berkes, Pietro and Bornschein, Joerg and L\\\"{u}cke, J\\\"{o}rg},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Select and Sample - A Model of Efficient Neural Inference and Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/a424ed4bd3a7d6aea720b86d4a360f75-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/a424ed4bd3a7d6aea720b86d4a360f75-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/a424ed4bd3a7d6aea720b86d4a360f75-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/a424ed4bd3a7d6aea720b86d4a360f75-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1466624,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14573000048929829486&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Frankfurt Institute for Advanced Studies + Goethe-University Frankfurt, Germany; Frankfurt Institute for Advanced Studies + Goethe-University Frankfurt, Germany; Frankfurt Institute for Advanced Studies + Goethe-University Frankfurt, Germany; V olen Center for Complex Systems + Brandeis University, Boston, USA; Frankfurt Institute for Advanced Studies + Goethe-University Frankfurt, Germany",
        "aff_domain": "fias.uni-frankfurt.de;fias.uni-frankfurt.de;fias.uni-frankfurt.de;brandeis.edu;fias.uni-frankfurt.de",
        "email": "fias.uni-frankfurt.de;fias.uni-frankfurt.de;fias.uni-frankfurt.de;brandeis.edu;fias.uni-frankfurt.de",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;0+1;2+3;0+1",
        "aff_unique_norm": "Frankfurt Institute for Advanced Studies;Goethe-University Frankfurt, Germany;V olen Center for Complex Systems;Brandeis University, Boston, USA",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.fias.uni-frankfurt.de/;;;",
        "aff_unique_abbr": "FIAS;;;",
        "aff_campus_unique_index": ";;;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;;0",
        "aff_country_unique": "Germany;"
    },
    {
        "id": "ccdcc83c69",
        "title": "Selecting Receptive Fields in Deep Networks",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/6c1da886822c67822bcf3679d04369fa-Abstract.html",
        "author": "Adam Coates; Andrew Y. Ng",
        "abstract": "Recent deep learning and unsupervised feature learning systems that learn from unlabeled data have achieved high performance in benchmarks by using extremely large architectures with many features (hidden units) at each layer.  Unfortunately, for such large architectures the number of parameters usually grows quadratically in the width of the network, thus necessitating hand-coded \"local receptive fields\" that limit the number of connections from lower level features to higher ones (e.g., based on spatial locality).  In this paper we propose a fast method to choose these connections that may be incorporated into a wide variety of unsupervised training methods.  Specifically, we choose local receptive fields that group together those low-level features that are most similar to each other according to a pairwise similarity metric.  This approach  allows us to harness the advantages of local receptive fields (such  as improved scalability, and reduced data requirements) when we do  not know how to specify such receptive fields by hand or where our  unsupervised training algorithm has no obvious generalization to a  topographic setting.  We produce results showing how this method allows us to use even simple unsupervised training algorithms to train successful multi-layered etworks that achieve state-of-the-art results on CIFAR and STL datasets: 82.0% and 60.1% accuracy, respectively.",
        "bibtex": "@inproceedings{NIPS2011_6c1da886,\n author = {Coates, Adam and Ng, Andrew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Selecting Receptive Fields in Deep Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/6c1da886822c67822bcf3679d04369fa-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/6c1da886822c67822bcf3679d04369fa-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/6c1da886822c67822bcf3679d04369fa-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 3477976,
        "gs_citation": 283,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5862329444572369795&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science, Stanford University; Department of Computer Science, Stanford University",
        "aff_domain": "cs.stanford.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b375334266",
        "title": "Selecting the State-Representation in Reinforcement Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/970af30e481057c48f87e101b61e6994-Abstract.html",
        "author": "Odalric-ambrym Maillard; Daniil Ryabko; R\u00e9mi Munos",
        "abstract": "The problem of selecting the right state-representation in a reinforcement learning problem is considered.   Several models (functions mapping past observations to a finite set) of the observations are given, and it is known   that for at least one of these models the resulting state dynamics are indeed Markovian. Without  knowing neither which of the models is the correct one, nor what are the probabilistic characteristics of the resulting   MDP, it is required to obtain as much reward as the optimal policy for the correct model (or for the best of the correct models, if there are several). We propose an algorithm that achieves that,   with a regret of order T^{2/3} where T is the horizon time.",
        "bibtex": "@inproceedings{NIPS2011_970af30e,\n author = {Maillard, Odalric-ambrym and Ryabko, Daniil and Munos, R\\'{e}mi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Selecting the State-Representation in Reinforcement Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/970af30e481057c48f87e101b61e6994-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/970af30e481057c48f87e101b61e6994-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/970af30e481057c48f87e101b61e6994-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 317663,
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5232184984464094657&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff": "INRIA Lille - Nord Europe; INRIA Lille - Nord Europe; INRIA Lille - Nord Europe",
        "aff_domain": "gmail.com;inria.fr;ryabko.net",
        "email": "gmail.com;inria.fr;ryabko.net",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "INRIA",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.inria.fr",
        "aff_unique_abbr": "INRIA",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Lille - Nord Europe",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "444119d572",
        "title": "Selective Prediction of Financial Trends with Hidden Markov Models",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/dd458505749b2941217ddd59394240e8-Abstract.html",
        "author": "Dmitry Pidan; Ran El-Yaniv",
        "abstract": "Focusing on short term trend prediction in a financial context, we consider the problem of selective prediction whereby the predictor can abstain from prediction in order to improve performance. We examine two types of selective mechanisms for HMM predictors. The first is a rejection in the spirit of Chow\u2019s well-known ambiguity principle. The second is a specialized mechanism for HMMs that identifies low quality HMM states and abstain from prediction in those states. We call this model selective HMM (sHMM). In both approaches we can trade-off prediction coverage to gain better accuracy in a controlled manner. We compare performance of the ambiguity-based rejection technique with that of the sHMM approach. Our results indicate that both methods are effective, and that the sHMM  model is superior.",
        "bibtex": "@inproceedings{NIPS2011_dd458505,\n author = {Pidan, Dmitry and El-Yaniv, Ran},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Selective Prediction of Financial Trends with Hidden Markov Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/dd458505749b2941217ddd59394240e8-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/dd458505749b2941217ddd59394240e8-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/dd458505749b2941217ddd59394240e8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 149655,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2588648633904007250&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "38cceb7dd2",
        "title": "Semantic Labeling of 3D Point Clouds for Indoor Scenes",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/9872ed9fc22fc182d371c3e9ed316094-Abstract.html",
        "author": "Hema S. Koppula; Abhishek Anand; Thorsten Joachims; Ashutosh Saxena",
        "abstract": "Inexpensive RGB-D cameras that give an RGB image together with depth data have become widely available. In this paper, we use this data to build 3D point clouds of full indoor scenes such as an office and address the task of semantic labeling of these 3D point clouds. We propose a graphical model that captures various features and contextual relations, including the local visual appearance and shape cues, object co-occurence relationships and geometric relationships. With a large number of object classes and relations, the model\u2019s parsimony becomes important and we address that by using multiple types of edge potentials. The model admits efficient approximate inference, and we train it using a maximum-margin learning approach. In our experiments over a total of 52 3D scenes of homes and offices (composed from about 550 views, having 2495 segments labeled with 27 object classes), we get a performance of 84.06% in labeling 17 object classes for offices, and 73.38% in labeling 17 object classes for home scenes. Finally, we applied these algorithms successfully on a mobile robot for the task of finding objects in large cluttered rooms.",
        "bibtex": "@inproceedings{NIPS2011_9872ed9f,\n author = {Koppula, Hema and Anand, Abhishek and Joachims, Thorsten and Saxena, Ashutosh},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Semantic Labeling of 3D Point Clouds for Indoor Scenes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/9872ed9fc22fc182d371c3e9ed316094-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/9872ed9fc22fc182d371c3e9ed316094-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/9872ed9fc22fc182d371c3e9ed316094-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2110200,
        "gs_citation": 477,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2136653913187717323&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science, Cornell University; Department of Computer Science, Cornell University; Department of Computer Science, Cornell University; Department of Computer Science, Cornell University",
        "aff_domain": "cs.cornell.edu;cs.cornell.edu;cs.cornell.edu;cs.cornell.edu",
        "email": "cs.cornell.edu;cs.cornell.edu;cs.cornell.edu;cs.cornell.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Cornell University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.cornell.edu",
        "aff_unique_abbr": "Cornell",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "4108f313c9",
        "title": "Semi-supervised Regression via Parallel Field Regularization",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/bc6dc48b743dc5d013b1abaebd2faed2-Abstract.html",
        "author": "Binbin Lin; Chiyuan Zhang; Xiaofei He",
        "abstract": "This paper studies the problem of semi-supervised learning from the vector field perspective. Many of the existing work use the graph Laplacian to ensure the smoothness of the prediction function on the data manifold. However, beyond smoothness, it is suggested by recent theoretical work that we should ensure second order smoothness for achieving faster rates of convergence for semi-supervised regression problems. To achieve this goal, we show that the second order smoothness measures the linearity of the function, and the gradient field of a linear function has to be a parallel vector field. Consequently, we propose to find a function which minimizes the empirical error, and simultaneously requires its gradient field to be as parallel as possible. We give a continuous objective function on the manifold and discuss how to discretize it by using random points. The discretized optimization problem turns out to be a sparse linear system which can be solved very efficiently. The experimental results have demonstrated the effectiveness of our proposed approach.",
        "bibtex": "@inproceedings{NIPS2011_bc6dc48b,\n author = {Lin, Binbin and Zhang, Chiyuan and He, Xiaofei},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Semi-supervised Regression via Parallel Field Regularization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/bc6dc48b743dc5d013b1abaebd2faed2-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/bc6dc48b743dc5d013b1abaebd2faed2-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/bc6dc48b743dc5d013b1abaebd2faed2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 3955598,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=924728031119825393&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "4e6afe8206",
        "title": "Sequence learning with hidden units in spiking neural networks",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/795c7a7a5ec6b460ec00c5841019b9e9-Abstract.html",
        "author": "Johanni Brea; Walter Senn; Jean-pascal Pfister",
        "abstract": "We consider a statistical framework in which recurrent networks of spiking neurons learn to generate spatio-temporal spike patterns. Given biologically realistic stochastic neuronal dynamics we derive a tractable learning rule for the synaptic weights towards hidden and visible neurons that leads to optimal recall of the training sequences. We show that learning synaptic weights towards hidden neurons significantly improves the storing capacity of the network. Furthermore, we derive an approximate online learning rule and show that our learning rule is consistent with Spike-Timing Dependent Plasticity in that if a presynaptic spike shortly precedes a postynaptic spike, potentiation is induced and otherwise depression is elicited.",
        "bibtex": "@inproceedings{NIPS2011_795c7a7a,\n author = {Brea, Johanni and Senn, Walter and Pfister, Jean-pascal},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sequence learning with hidden units in spiking neural networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/795c7a7a5ec6b460ec00c5841019b9e9-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/795c7a7a5ec6b460ec00c5841019b9e9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/795c7a7a5ec6b460ec00c5841019b9e9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 151175,
        "gs_citation": 55,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15658724172734686288&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Physiology, University of Bern; Department of Physiology, University of Bern; Department of Physiology, University of Bern",
        "aff_domain": "pyl.unibe.ch;pyl.unibe.ch;pyl.unibe.ch",
        "email": "pyl.unibe.ch;pyl.unibe.ch;pyl.unibe.ch",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Bern",
        "aff_unique_dep": "Department of Physiology",
        "aff_unique_url": "https://www.unibe.ch",
        "aff_unique_abbr": "UniBE",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "c6b36a610f",
        "title": "Shallow vs. Deep Sum-Product Networks",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/8e6b42f1644ecb1327dc03ab345e618b-Abstract.html",
        "author": "Olivier Delalleau; Yoshua Bengio",
        "abstract": "We investigate the representational power of sum-product networks  (computation networks analogous to neural networks,  but whose individual units compute either products  or weighted sums), through a theoretical analysis that compares  deep (multiple hidden layers) vs. shallow (one hidden layer) architectures.  We prove there exist families of functions that can be represented  much more efficiently with a deep network than with a shallow one, i.e.  with substantially fewer hidden units.  Such results were not available until now, and  contribute to motivate recent research involving learning of deep  sum-product networks, and more generally motivate research in Deep  Learning.",
        "bibtex": "@inproceedings{NIPS2011_8e6b42f1,\n author = {Delalleau, Olivier and Bengio, Yoshua},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Shallow vs. Deep Sum-Product Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/8e6b42f1644ecb1327dc03ab345e618b-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/8e6b42f1644ecb1327dc03ab345e618b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/8e6b42f1644ecb1327dc03ab345e618b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 119238,
        "gs_citation": 505,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17126636560011661491&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Computer Science and Operation Research, Universit\u00e9 de Montr\u00e9al; Department of Computer Science and Operation Research, Universit\u00e9 de Montr\u00e9al",
        "aff_domain": "iro.umontreal.ca;umontreal.ca",
        "email": "iro.umontreal.ca;umontreal.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Department of Computer Science and Operation Research, Universit\u00e9 de Montr\u00e9al",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "3ea41a115b",
        "title": "Shaping Level Sets with Submodular Functions",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/c20ad4d76fe97759aa27a0c99bff6710-Abstract.html",
        "author": "Francis R. Bach",
        "abstract": "We consider a class of sparsity-inducing regularization terms based on submodular functions. While previous work has focused on  non-decreasing functions, we explore symmetric submodular functions and their \\lova extensions. We show that the Lovasz extension may be seen as the convex envelope of a function that depends on level sets  (i.e., the set of indices whose corresponding components of the underlying predictor are greater than a given constant): this leads to a class of convex structured regularization terms that impose prior knowledge on the level sets, and not only on the supports of the underlying predictors. We provide a unified set of optimization algorithms, such as proximal operators, and theoretical guarantees (allowed level sets and recovery conditions). By selecting specific submodular functions,  we   give a new interpretation to known norms, such as the total variation; we also define new norms, in particular ones that are based on order statistics with application to clustering and outlier detection, and on noisy cuts in graphs with application to change point detection in the presence of outliers.",
        "bibtex": "@inproceedings{NIPS2011_c20ad4d7,\n author = {Bach, Francis},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Shaping Level Sets with Submodular Functions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/c20ad4d76fe97759aa27a0c99bff6710-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/c20ad4d76fe97759aa27a0c99bff6710-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/c20ad4d76fe97759aa27a0c99bff6710-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 241487,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17099180391590688135&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "bf10bbbe14",
        "title": "ShareBoost: Efficient multiclass learning with feature sharing",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/556f391937dfd4398cbac35e050a2177-Abstract.html",
        "author": "Shai Shalev-shwartz; Yonatan Wexler; Amnon Shashua",
        "abstract": "Multiclass prediction is the problem of classifying an object into a    relevant target class.  We consider the problem of learning a    multiclass predictor that uses only few features, and in particular,    the number of used features should increase sub-linearly with the    number of possible classes. This implies that features should be    shared by several classes. We describe and analyze the ShareBoost    algorithm for learning a multiclass predictor that uses few shared    features. We prove that ShareBoost efficiently finds a predictor    that uses few shared features (if such a predictor exists) and that    it has a small generalization error. We also describe how to use    ShareBoost for learning a non-linear predictor that has a fast    evaluation time. In a series of experiments with natural data sets    we demonstrate the benefits of ShareBoost and evaluate its success    relatively to other state-of-the-art approaches.",
        "bibtex": "@inproceedings{NIPS2011_556f3919,\n author = {Shalev-shwartz, Shai and Wexler, Yonatan and Shashua, Amnon},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {ShareBoost: Efficient multiclass learning with feature sharing},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/556f391937dfd4398cbac35e050a2177-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/556f391937dfd4398cbac35e050a2177-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/556f391937dfd4398cbac35e050a2177-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/556f391937dfd4398cbac35e050a2177-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 304468,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12158406511371817756&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "619ae2c186",
        "title": "Signal Estimation Under Random Time-Warpings and Nonlinear Signal Alignment",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/74071a673307ca7459bcf75fbd024e09-Abstract.html",
        "author": "Sebastian A. Kurtek; Anuj Srivastava; Wei Wu",
        "abstract": "While signal estimation under random amplitudes, phase shifts, and additive noise is studied frequently, the problem of estimating a deterministic signal under random time-warpings has been relatively unexplored. We present a novel framework for estimating the unknown signal that utilizes the action of the warping group to form an equivalence relation between signals. First, we derive an estimator for the equivalence class of the unknown signal using the notion of Karcher mean on the quotient space of equivalence classes. This step requires the use of Fisher-Rao Riemannian metric  and a square-root representation of signals to enable computations of distances and means under this metric. Then, we define a notion of the center of a class and show that the center of the estimated class is a consistent estimator of the underlying unknown signal. This estimation algorithm has many applications: (1)registration/alignment of functional data, (2) separation of phase/amplitude components of functional data, (3) joint demodulation and carrier estimation, and (4) sparse modeling of functional data. Here we demonstrate only (1) and (2):  Given signals are temporally aligned using nonlinear warpings and, thus, separated into their phase and amplitude components. The proposed method for signal alignment is shown to have state of the art performance using Berkeley growth, handwritten signatures, and neuroscience spike train data.",
        "bibtex": "@inproceedings{NIPS2011_74071a67,\n author = {Kurtek, Sebastian and Srivastava, Anuj and Wu, Wei},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Signal Estimation Under Random Time-Warpings and Nonlinear Signal Alignment},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/74071a673307ca7459bcf75fbd024e09-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/74071a673307ca7459bcf75fbd024e09-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/74071a673307ca7459bcf75fbd024e09-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 540483,
        "gs_citation": 88,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3074609085487574880&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Statistics, Florida State University, Tallahassee, FL 32306; Department of Statistics, Florida State University, Tallahassee, FL 32306; Department of Statistics, Florida State University, Tallahassee, FL 32306",
        "aff_domain": "stat.fsu.edu;stat.fsu.edu;stat.fsu.edu",
        "email": "stat.fsu.edu;stat.fsu.edu;stat.fsu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Department of Statistics, Florida State University, Tallahassee, FL 32306",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "fb8a1072a4",
        "title": "Similarity-based Learning via Data Driven Embeddings",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/69a5b5995110b36a9a347898d97a610e-Abstract.html",
        "author": "Purushottam Kar; Prateek Jain",
        "abstract": "We consider the problem of classification using similarity/distance functions over data. Specifically, we propose a framework for defining the goodness of a (dis)similarity function with respect to a given learning task and propose algorithms that have guaranteed generalization properties when working with such good functions. Our framework unifies and generalizes the frameworks proposed by (Balcan-Blum 2006) and (Wang et al 2007). An attractive feature of our framework is its adaptability to data - we do not promote a fixed notion of goodness but rather let data dictate it. We show, by giving theoretical guarantees that the goodness criterion best suited to a problem can itself be learned which makes our approach applicable to a variety of domains and problems. We propose a landmarking-based approach to obtaining a classifier from such learned goodness criteria. We then provide a novel diversity based heuristic to perform task-driven selection of landmark points instead of random selection. We demonstrate the effectiveness of our goodness criteria learning method as well as the landmark selection heuristic on a variety of similarity-based learning datasets and benchmark UCI datasets on which our method consistently outperforms existing approaches by a significant margin.",
        "bibtex": "@inproceedings{NIPS2011_69a5b599,\n author = {Kar, Purushottam and Jain, Prateek},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Similarity-based Learning via Data Driven Embeddings},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/69a5b5995110b36a9a347898d97a610e-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/69a5b5995110b36a9a347898d97a610e-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/69a5b5995110b36a9a347898d97a610e-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/69a5b5995110b36a9a347898d97a610e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 327752,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5672856244261100771&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Indian Institute of Technology Kanpur, INDIA; Microsoft Research India Bangalore, INDIA",
        "aff_domain": "cse.iitk.ac.in;microsoft.com",
        "email": "cse.iitk.ac.in;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Indian Institute of Technology Kanpur;Microsoft Research India Bangalore, INDIA",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.iitk.ac.in;",
        "aff_unique_abbr": "IIT Kanpur;",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Kanpur;",
        "aff_country_unique_index": "0",
        "aff_country_unique": "India;"
    },
    {
        "id": "eb0381cbb1",
        "title": "Simultaneous Sampling and Multi-Structure Fitting with Adaptive Reversible Jump MCMC",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/beed13602b9b0e6ecb5b568ff5058f07-Abstract.html",
        "author": "Trung T. Pham; Tat-jun Chin; Jin Yu; David Suter",
        "abstract": "Multi-structure model fitting has traditionally taken a two-stage approach: First, sample a (large) number of model hypotheses, then select the subset of hypotheses that optimise a joint fitting and model selection criterion. This disjoint two-stage approach is arguably suboptimal and inefficient - if the random sampling did not retrieve a good set of hypotheses, the optimised outcome will not represent a good fit. To overcome this weakness we propose a new multi-structure fitting approach based on Reversible Jump MCMC. Instrumental in raising the effectiveness of our method is an adaptive hypothesis generator, whose proposal distribution is learned incrementally and online. We prove that this adaptive proposal satisfies the diminishing adaptation property crucial for ensuring ergodicity in MCMC. Our method effectively conducts hypothesis sampling and optimisation simultaneously, and gives superior computational efficiency over other methods.",
        "bibtex": "@inproceedings{NIPS2011_beed1360,\n author = {Pham, Trung and Chin, Tat-jun and Yu, Jin and Suter, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Simultaneous Sampling and Multi-Structure Fitting with Adaptive Reversible Jump MCMC},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/beed13602b9b0e6ecb5b568ff5058f07-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/beed13602b9b0e6ecb5b568ff5058f07-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/beed13602b9b0e6ecb5b568ff5058f07-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/beed13602b9b0e6ecb5b568ff5058f07-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 648435,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3082474030873834449&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "School of Computer Science, The University of Adelaide, South Australia; School of Computer Science, The University of Adelaide, South Australia; School of Computer Science, The University of Adelaide, South Australia; School of Computer Science, The University of Adelaide, South Australia",
        "aff_domain": "cs.adelaide.edu.au;cs.adelaide.edu.au;cs.adelaide.edu.au;cs.adelaide.edu.au",
        "email": "cs.adelaide.edu.au;cs.adelaide.edu.au;cs.adelaide.edu.au;cs.adelaide.edu.au",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "The University of Adelaide",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.adelaide.edu.au",
        "aff_unique_abbr": "Adelaide",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Adelaide",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "a5be84d5f1",
        "title": "Solving Decision Problems with Limited Information",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/f85454e8279be180185cac7d243c5eb3-Abstract.html",
        "author": "Denis D. Maua; Cassio Campos",
        "abstract": "We present a new algorithm for exactly solving decision-making problems represented as an influence diagram. We do not require the usual assumptions of no forgetting and regularity, which allows us to solve problems with limited information. The algorithm, which implements a sophisticated variable elimination procedure, is empirically shown to outperform a state-of-the-art algorithm in randomly generated problems of up to 150 variables and $10^{64}$ strategies.",
        "bibtex": "@inproceedings{NIPS2011_f85454e8,\n author = {Maua, Denis D and Campos, Cassio},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Solving Decision Problems with Limited Information},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/f85454e8279be180185cac7d243c5eb3-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/f85454e8279be180185cac7d243c5eb3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/f85454e8279be180185cac7d243c5eb3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 444261,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11216922905456464826&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "IDSIA; IDSIA",
        "aff_domain": "idsia.ch;idsia.ch",
        "email": "idsia.ch;idsia.ch",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Institute of Digital Technologies",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.idsia.ch",
        "aff_unique_abbr": "IDSIA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "1c0451ffe7",
        "title": "SpaRCS: Recovering low-rank and sparse matrices from compressive measurements",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/0ff8033cf9437c213ee13937b1c4c455-Abstract.html",
        "author": "Andrew E. Waters; Aswin C. Sankaranarayanan; Richard Baraniuk",
        "abstract": "We consider the problem of recovering a matrix $\\mathbf{M}$ that is the sum of a low-rank matrix $\\mathbf{L}$ and a sparse matrix $\\mathbf{S}$ from a small set of linear measurements of the form $\\mathbf{y} = \\mathcal{A}(\\mathbf{M}) = \\mathcal{A}({\\bf L}+{\\bf S})$.  This model subsumes three important classes of signal recovery problems:  compressive sensing, affine rank minimization, and robust principal component analysis.  We propose a natural optimization problem for signal recovery under this model and develop a new greedy algorithm called SpaRCS to solve it.  SpaRCS inherits a number of desirable properties from the state-of-the-art CoSaMP and ADMiRA algorithms, including exponential convergence and efficient implementation.  Simulation results with video compressive sensing, hyperspectral imaging, and robust matrix completion data sets demonstrate both the accuracy and efficacy of the algorithm.",
        "bibtex": "@inproceedings{NIPS2011_0ff8033c,\n author = {Waters, Andrew and Sankaranarayanan, Aswin and Baraniuk, Richard},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {SpaRCS: Recovering low-rank and sparse matrices from compressive measurements},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/0ff8033cf9437c213ee13937b1c4c455-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/0ff8033cf9437c213ee13937b1c4c455-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/0ff8033cf9437c213ee13937b1c4c455-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 821977,
        "gs_citation": 265,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17481756475025842606&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Rice University; Rice University; Rice University",
        "aff_domain": "rice.edu;rice.edu;rice.edu",
        "email": "rice.edu;rice.edu;rice.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Rice University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.rice.edu",
        "aff_unique_abbr": "Rice",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "8186be98fa",
        "title": "Sparse Bayesian Multi-Task Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/4fac9ba115140ac4f1c22da82aa0bc7f-Abstract.html",
        "author": "Shengbo Guo; Onno Zoeter; C\u00e9dric Archambeau",
        "abstract": "We propose a new sparse Bayesian model for multi-task regression and classification. The model is able to capture correlations between tasks, or more specifically a low-rank approximation of the covariance matrix, while being sparse in the features. We introduce a general family of group sparsity inducing priors based on matrix-variate Gaussian scale mixtures. We show the amount of sparsity can be learnt from the data by combining an approximate inference approach with type II maximum likelihood estimation of the hyperparameters. Empirical evaluations on data sets from biology and vision demonstrate the applicability of the model, where on both regression and classification tasks it achieves competitive predictive performance compared to previously proposed methods.",
        "bibtex": "@inproceedings{NIPS2011_4fac9ba1,\n author = {Guo, Shengbo and Zoeter, Onno and Archambeau, C\\'{e}dric},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sparse Bayesian Multi-Task Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/4fac9ba115140ac4f1c22da82aa0bc7f-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/4fac9ba115140ac4f1c22da82aa0bc7f-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/4fac9ba115140ac4f1c22da82aa0bc7f-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/4fac9ba115140ac4f1c22da82aa0bc7f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 402887,
        "gs_citation": 62,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9777819019671721240&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Xerox Research Centre Europe; Xerox Research Centre Europe; Xerox Research Centre Europe",
        "aff_domain": "xrce.xerox.com;xrce.xerox.com;xrce.xerox.com",
        "email": "xrce.xerox.com;xrce.xerox.com;xrce.xerox.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Xerox Research Centre Europe",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.xerox.com/en-us/innovation/research-centers/europe",
        "aff_unique_abbr": "XRCE",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Unknown"
    },
    {
        "id": "b9ebe281f4",
        "title": "Sparse Estimation with Structured Dictionaries",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/fd2c5e4680d9a01dba3aada5ece22270-Abstract.html",
        "author": "David P. Wipf",
        "abstract": "In the vast majority of recent work on sparse estimation algorithms, performance has been evaluated using ideal or quasi-ideal dictionaries (e.g., random Gaussian or Fourier) characterized by unit $\\ell_2$ norm, incoherent columns or features.  But in reality, these types of dictionaries represent only a subset of the dictionaries that are actually used in practice (largely restricted to idealized compressive sensing applications).  In contrast, herein sparse estimation is considered in the context of structured dictionaries possibly exhibiting high coherence between arbitrary groups of columns and/or rows.  Sparse penalized regression models are analyzed with the purpose of finding, to the extent possible, regimes of dictionary invariant performance.  In particular, a Type II Bayesian estimator with a dictionary-dependent sparsity penalty is shown to have a number of desirable invariance properties leading to provable advantages over more conventional penalties such as the $\\ell_1$ norm, especially in areas where existing theoretical recovery guarantees no longer hold.  This can translate into improved performance in applications such as model selection with correlated features, source localization, and compressive sensing with constrained measurement directions.",
        "bibtex": "@inproceedings{NIPS2011_fd2c5e46,\n author = {Wipf, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sparse Estimation with Structured Dictionaries},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/fd2c5e4680d9a01dba3aada5ece22270-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/fd2c5e4680d9a01dba3aada5ece22270-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/fd2c5e4680d9a01dba3aada5ece22270-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 128836,
        "gs_citation": 65,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13324902006897650714&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "c1cbd3350f",
        "title": "Sparse Features for PCA-Like Linear Regression",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/b3ba8f1bee1238a2f37603d90b58898d-Abstract.html",
        "author": "Christos Boutsidis; Petros Drineas; Malik Magdon-Ismail",
        "abstract": "Principal Components Analysis~(PCA) is often used as a feature extraction procedure. Given a matrix $X \\in \\mathbb{R}^{n \\times d}$, whose rows represent $n$ data points with respect to $d$ features, the top $k$ right singular vectors of $X$ (the so-called \\textit{eigenfeatures}), are arbitrary linear combinations of all available features. The eigenfeatures are very useful in data analysis, including the regularization of linear regression. Enforcing sparsity on the eigenfeatures, i.e., forcing them to be linear combinations of only a \\textit{small} number of actual features (as opposed to all available features), can promote better generalization error and improve the interpretability of the eigenfeatures. We present deterministic and randomized algorithms that construct such sparse eigenfeatures while \\emph{provably} achieving in-sample performance comparable to regularized linear regression. Our algorithms are relatively simple and practically efficient, and we demonstrate their performance on several data sets.",
        "bibtex": "@inproceedings{NIPS2011_b3ba8f1b,\n author = {Boutsidis, Christos and Drineas, Petros and Magdon-Ismail, Malik},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sparse Features for PCA-Like Linear Regression},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/b3ba8f1bee1238a2f37603d90b58898d-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/b3ba8f1bee1238a2f37603d90b58898d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/b3ba8f1bee1238a2f37603d90b58898d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 131221,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4348603221421378341&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Mathematical Sciences Department, IBM T.J. Watson Research Center, Yorktown Heights, New York; Computer Science Department, Rensselaer Polytechnic Institute, Troy, NY 12180; Computer Science Department, Rensselaer Polytechnic Institute, Troy, NY 12180",
        "aff_domain": "us.ibm.com;cs.rpi.edu;cs.rpi.edu",
        "email": "us.ibm.com;cs.rpi.edu;cs.rpi.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Mathematical Sciences Department, IBM T.J. Watson Research Center, Yorktown Heights, New York;Computer Science Department, Rensselaer Polytechnic Institute, Troy, NY 12180",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "c1118cf48e",
        "title": "Sparse Filtering",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/192fc044e74dffea144f9ac5dc9f3395-Abstract.html",
        "author": "Jiquan Ngiam; Zhenghao Chen; Sonia A. Bhaskar; Pang W. Koh; Andrew Y. Ng",
        "abstract": "Unsupervised feature learning has been shown to be effective at learning representations that perform well on image, video and audio classification. However, many existing feature learning algorithms are hard to use and require extensive hyperparameter tuning. In this work, we present sparse filtering, a simple new algorithm which is efficient and only has one hyperparameter, the number of features to learn.  In contrast to most other feature learning methods, sparse filtering does not explicitly attempt to construct a model of the data distribution. Instead, it optimizes a simple cost function -- the sparsity of L2-normalized features -- which can easily be implemented in a few lines of MATLAB code. Sparse filtering scales gracefully to handle high-dimensional inputs, and can also be used to learn meaningful features in additional layers with greedy layer-wise stacking. We evaluate sparse filtering on natural images, object classification (STL-10), and phone classification (TIMIT), and show that our method works well on a range of different modalities.",
        "bibtex": "@inproceedings{NIPS2011_192fc044,\n author = {Ngiam, Jiquan and Chen, Zhenghao and Bhaskar, Sonia and Koh, Pang and Ng, Andrew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sparse Filtering},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/192fc044e74dffea144f9ac5dc9f3395-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/192fc044e74dffea144f9ac5dc9f3395-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/192fc044e74dffea144f9ac5dc9f3395-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/192fc044e74dffea144f9ac5dc9f3395-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 422611,
        "gs_citation": 374,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15047733252691508486&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Computer Science Department, Stanford University; Computer Science Department, Stanford University; Computer Science Department, Stanford University; Computer Science Department, Stanford University; Computer Science Department, Stanford University",
        "aff_domain": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "28263fd829",
        "title": "Sparse Inverse Covariance Matrix Estimation Using Quadratic Approximation",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/2ba8698b79439589fdd2b0f7218d8b07-Abstract.html",
        "author": "Cho-jui Hsieh; Inderjit S. Dhillon; Pradeep K. Ravikumar; M\u00e1ty\u00e1s A. Sustik",
        "abstract": "The L_1 regularized Gaussian maximum likelihood estimator has been shown to have strong statistical guarantees in recovering a sparse inverse covariance matrix, or alternatively the underlying graph structure of a Gaussian Markov Random Field, from very limited samples. We propose a novel algorithm for solving the resulting optimization problem which is a regularized log-determinant program.  In contrast to other state-of-the-art methods that largely use first order gradient information, our algorithm is based on Newton's method and employs a quadratic approximation, but with some modifications that leverage the structure of the sparse Gaussian MLE problem. We show that our method is superlinearly convergent, and also present experimental results using synthetic and real application data that demonstrate the considerable improvements in performance of our method when  compared to other state-of-the-art methods.",
        "bibtex": "@inproceedings{NIPS2011_2ba8698b,\n author = {Hsieh, Cho-jui and Dhillon, Inderjit and Ravikumar, Pradeep and Sustik, M\\'{a}ty\\'{a}s},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sparse Inverse Covariance Matrix Estimation Using Quadratic Approximation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/2ba8698b79439589fdd2b0f7218d8b07-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/2ba8698b79439589fdd2b0f7218d8b07-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/2ba8698b79439589fdd2b0f7218d8b07-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/2ba8698b79439589fdd2b0f7218d8b07-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 363625,
        "gs_citation": 441,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4300469206080825092&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "4673e7dff6",
        "title": "Sparse Manifold Clustering and Embedding",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/fc490ca45c00b1249bbe3554a4fdf6fb-Abstract.html",
        "author": "Ehsan Elhamifar; Ren\u00e9 Vidal",
        "abstract": "We propose an algorithm called Sparse Manifold Clustering and Embedding (SMCE) for simultaneous clustering and dimensionality reduction of data lying in multiple nonlinear manifolds. Similar to most dimensionality reduction methods, SMCE finds a small neighborhood around each data point and connects each point to its neighbors with appropriate weights. The key difference is that SMCE finds both the neighbors and the weights automatically. This is done by solving a sparse optimization problem, which encourages selecting nearby points that lie in the same manifold and approximately span a low-dimensional affine subspace. The optimal solution encodes information that can be used for clustering and dimensionality reduction using spectral clustering and embedding. Moreover, the size of the optimal neighborhood of a data point, which can be different for different points, provides an estimate of the dimension of the manifold to which the point belongs. Experiments demonstrate that our method can effectively handle multiple manifolds that are very close to each other, manifolds with non-uniform sampling and holes, as well as estimate the intrinsic dimensions of the manifolds.",
        "bibtex": "@inproceedings{NIPS2011_fc490ca4,\n author = {Elhamifar, Ehsan and Vidal, Ren\\'{e}},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sparse Manifold Clustering and Embedding},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/fc490ca45c00b1249bbe3554a4fdf6fb-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/fc490ca45c00b1249bbe3554a4fdf6fb-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/fc490ca45c00b1249bbe3554a4fdf6fb-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2836421,
        "gs_citation": 358,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2674240658149978639&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Center for Imaging Science, Johns Hopkins University; Center for Imaging Science, Johns Hopkins University",
        "aff_domain": "cis.jhu.edu;cis.jhu.edu",
        "email": "cis.jhu.edu;cis.jhu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Johns Hopkins University",
        "aff_unique_dep": "Center for Imaging Science",
        "aff_unique_url": "https://www.jhu.edu",
        "aff_unique_abbr": "JHU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "822e997a65",
        "title": "Sparse Recovery with Brownian Sensing",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/2387337ba1e0b0249ba90f55b2ba2521-Abstract.html",
        "author": "Alexandra Carpentier; Odalric-ambrym Maillard; R\u00e9mi Munos",
        "abstract": "We consider the problem of recovering the parameter alpha in R^K of a sparse function f, i.e. the number of non-zero entries of alpha is small compared to the number K of features, given noisy evaluations of f at a set of well-chosen sampling points. We introduce an additional randomisation process, called Brownian sensing, based on the computation of stochastic integrals, which produces a Gaussian sensing matrix, for which good recovery properties are proven independently on the number of sampling points N, even when the features are arbitrarily non-orthogonal. Under the assumption that f is H\u00f6lder continuous with exponent at least 1/2, we provide an estimate a of the parameter such that ||\\alpha - a||",
        "bibtex": "@inproceedings{NIPS2011_2387337b,\n author = {Carpentier, Alexandra and Maillard, Odalric-ambrym and Munos, R\\'{e}mi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sparse Recovery with Brownian Sensing},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/2387337ba1e0b0249ba90f55b2ba2521-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/2387337ba1e0b0249ba90f55b2ba2521-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/2387337ba1e0b0249ba90f55b2ba2521-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/2387337ba1e0b0249ba90f55b2ba2521-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 197476,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16825806177689027866&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "INRIALille; INRIALille; INRIALille",
        "aff_domain": "inria.fr;gmail.com;inria.fr",
        "email": "inria.fr;gmail.com;inria.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "INRIALille",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "7764eeea1d",
        "title": "Sparse recovery by thresholded non-negative least squares",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/d6723e7cd6735df68d1ce4c704c29a04-Abstract.html",
        "author": "Martin Slawski; Matthias Hein",
        "abstract": "Non-negative data are commonly encountered in numerous fields, making non-negative least squares regression (NNLS) a frequently used tool.   At least relative to its simplicity, it often performs rather well in practice. Serious doubts about its usefulness arise for modern  high-dimensional linear models. Even in this setting - unlike first intuition may suggest - we show that for a broad class of designs, NNLS is resistant to overfitting and works excellently for sparse recovery when combined with thresholding, experimentally even outperforming L1-regularization.   Since NNLS also circumvents the delicate choice of a regularization parameter, our findings suggest that NNLS may be the method of choice.",
        "bibtex": "@inproceedings{NIPS2011_d6723e7c,\n author = {Slawski, Martin and Hein, Matthias},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sparse recovery by thresholded non-negative least squares},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/d6723e7cd6735df68d1ce4c704c29a04-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/d6723e7cd6735df68d1ce4c704c29a04-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/d6723e7cd6735df68d1ce4c704c29a04-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/d6723e7cd6735df68d1ce4c704c29a04-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 465040,
        "gs_citation": 75,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1061210712858434355&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, Saarland University; Department of Computer Science, Saarland University",
        "aff_domain": "cs.uni-saarland.de;cs.uni-saarland.de",
        "email": "cs.uni-saarland.de;cs.uni-saarland.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Department of Computer Science, Saarland University",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "4b0fce4d5e",
        "title": "Spatial distance dependent Chinese restaurant processes for image segmentation",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/3d8e28caf901313a554cebc7d32e67e5-Abstract.html",
        "author": "Soumya Ghosh; Andrei B. Ungureanu; Erik B. Sudderth; David M. Blei",
        "abstract": "The distance dependent Chinese restaurant process (ddCRP) was recently introduced to accommodate random partitions of non-exchangeable data.  The ddCRP clusters data in a biased way: each data point is more likely to be clustered with other data that are near it in an external sense.  This paper examines the ddCRP in a spatial setting with the goal of natural image segmentation.  We explore the biases of the spatial ddCRP model and propose a novel hierarchical extension better suited for producing \"human-like\" segmentations. We then study the sensitivity of the models to various distance and appearance hyperparameters, and provide the first rigorous comparison of nonparametric Bayesian models in the image segmentation domain. On unsupervised image segmentation, we demonstrate that similar performance to existing nonparametric Bayesian models is possible with substantially simpler models and algorithms.",
        "bibtex": "@inproceedings{NIPS2011_3d8e28ca,\n author = {Ghosh, Soumya and Ungureanu, Andrei and Sudderth, Erik and Blei, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Spatial distance dependent Chinese restaurant processes for image segmentation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/3d8e28caf901313a554cebc7d32e67e5-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/3d8e28caf901313a554cebc7d32e67e5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/3d8e28caf901313a554cebc7d32e67e5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 892135,
        "gs_citation": 100,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1221645036600974624&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "Department of Computer Science, Brown University; Morgan Stanley + Department of Computer Science, Brown University; Department of Computer Science, Brown University; Department of Computer Science, Princeton University",
        "aff_domain": "cs.brown.edu;gmail.com;cs.brown.edu;cs.princeton.edu",
        "email": "cs.brown.edu;gmail.com;cs.brown.edu;cs.princeton.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+0;0;2",
        "aff_unique_norm": "Brown University;Morgan Stanley;Princeton University",
        "aff_unique_dep": "Department of Computer Science;;Department of Computer Science",
        "aff_unique_url": "https://www.brown.edu;https://www.morganstanley.com;https://www.princeton.edu",
        "aff_unique_abbr": "Brown;Morgan Stanley;Princeton",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2e6ad3b2ad",
        "title": "Spectral Methods for Learning Multivariate Latent Tree Structure",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/208e43f0e45c4c78cafadb83d2888cb6-Abstract.html",
        "author": "Animashree Anandkumar; Kamalika Chaudhuri; Daniel J. Hsu; Sham M. Kakade; Le Song; Tong Zhang",
        "abstract": "This work considers the problem of learning the structure of multivariate linear tree models, which include a variety of directed tree graphical models with continuous, discrete, and mixed latent variables such as linear-Gaussian models, hidden Markov models, Gaussian mixture models, and Markov evolutionary trees.  The setting is one where we only have samples from certain observed variables in the tree, and our goal is to estimate the tree structure (i.e., the graph of how the underlying hidden variables are connected to each other and to the observed variables).  We propose the Spectral Recursive Grouping algorithm, an efficient and simple bottom-up procedure for recovering the tree structure from independent samples of the observed variables.  Our finite sample size bounds for exact recovery of the tree structure  reveal certain natural dependencies on underlying statistical and structural properties of the underlying joint distribution.  Furthermore, our sample complexity guarantees have no explicit dependence on the dimensionality of the observed variables, making the algorithm applicable to many high-dimensional settings.  At the heart of our algorithm is a spectral quartet test for determining the relative topology of a quartet of variables from second-order statistics.",
        "bibtex": "@inproceedings{NIPS2011_208e43f0,\n author = {Anandkumar, Animashree and Chaudhuri, Kamalika and Hsu, Daniel J and Kakade, Sham M and Song, Le and Zhang, Tong},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Spectral Methods for Learning Multivariate Latent Tree Structure},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/208e43f0e45c4c78cafadb83d2888cb6-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/208e43f0e45c4c78cafadb83d2888cb6-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/208e43f0e45c4c78cafadb83d2888cb6-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/208e43f0e45c4c78cafadb83d2888cb6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 559730,
        "gs_citation": 66,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10014763037306269358&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 22,
        "aff": "UC Irvine; UC San Diego; Microsoft Research; Microsoft Research & University of Pennsylvania; Carnegie Mellon University; Rutgers University",
        "aff_domain": "uci.edu;cs.ucsd.edu;microsoft.com;microsoft.com;cs.cmu.edu;stat.rutgers.edu",
        "email": "uci.edu;cs.ucsd.edu;microsoft.com;microsoft.com;cs.cmu.edu;stat.rutgers.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3;4;5",
        "aff_unique_norm": "University of California, Irvine;University of California, San Diego;Microsoft Corporation;Microsoft Research & University of Pennsylvania;Carnegie Mellon University;Rutgers University",
        "aff_unique_dep": ";;Microsoft Research;;;",
        "aff_unique_url": "https://www.uci.edu;https://www.ucsd.edu;https://www.microsoft.com/en-us/research;;https://www.cmu.edu;https://www.rutgers.edu",
        "aff_unique_abbr": "UCI;UCSD;MSR;;CMU;Rutgers",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Irvine;San Diego;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "51a595197f",
        "title": "Speedy Q-Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/ab1a4d0dd4d48a2ba1077c4494791306-Abstract.html",
        "author": "Mohammad Ghavamzadeh; Hilbert J. Kappen; Mohammad G. Azar; R\u00e9mi Munos",
        "abstract": "We introduce a new convergent variant of Q-learning, called speedy Q-learning, to address the problem of slow convergence in the standard form of the Q-learning algorithm. We prove a PAC bound on the performance of SQL, which shows that for an MDP with n state-action pairs  and the discount factor \\gamma only T=O\\big(\\log(n)/(\\epsilon^{2}(1-\\gamma)^{4})\\big) steps are required for the SQL algorithm to converge to an \\epsilon-optimal action-value function with high probability. This bound has a better dependency on 1/\\epsilon and 1/(1-\\gamma), and thus, is tighter than the best available result for Q-learning. Our bound is also superior to the existing results for both model-free and model-based instances of batch Q-value iteration that are considered to be more efficient than the incremental methods like Q-learning.",
        "bibtex": "@inproceedings{NIPS2011_ab1a4d0d,\n author = {Ghavamzadeh, Mohammad and Kappen, Hilbert and Azar, Mohammad and Munos, R\\'{e}mi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Speedy Q-Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/ab1a4d0dd4d48a2ba1077c4494791306-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/ab1a4d0dd4d48a2ba1077c4494791306-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/ab1a4d0dd4d48a2ba1077c4494791306-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 130005,
        "gs_citation": 171,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9158713497457144378&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Radboud University Nijmegen; INRIA Lille, SequeL Project; INRIA Lille, SequeL Project; Radboud University Nijmegen",
        "aff_domain": "science.ru.nl;inria.fr;inria.fr;science.ru.nl",
        "email": "science.ru.nl;inria.fr;inria.fr;science.ru.nl",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "Radboud University;INRIA Lille, SequeL Project",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ru.nl/;",
        "aff_unique_abbr": "RU;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Nijmegen;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Netherlands;"
    },
    {
        "id": "41dceb4e6c",
        "title": "Spike and Slab Variational Inference for Multi-Task and Multiple Kernel Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/b495ce63ede0f4efc9eec62cb947c162-Abstract.html",
        "author": "Michalis K. Titsias; Miguel L\u00e1zaro-Gredilla",
        "abstract": "We introduce a variational Bayesian inference algorithm which can be widely applied to sparse linear models. The algorithm is based on the spike and slab prior which, from a Bayesian perspective, is the golden standard for sparse inference. We apply the method to a general multi-task and multiple kernel learning model in which a common set of Gaussian process functions is linearly combined with task-specific sparse weights, thus inducing relation between tasks. This model unifies several sparse linear models, such as generalized linear models, sparse factor analysis and matrix factorization with missing values, so that the variational algorithm can be applied to all these cases. We demonstrate our approach in multi-output Gaussian process regression, multi-class classification, image processing applications and collaborative filtering.",
        "bibtex": "@inproceedings{NIPS2011_b495ce63,\n author = {Titsias, Michalis and L\\'{a}zaro-Gredilla, Miguel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Spike and Slab Variational Inference for Multi-Task and Multiple Kernel Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/b495ce63ede0f4efc9eec62cb947c162-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/b495ce63ede0f4efc9eec62cb947c162-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/b495ce63ede0f4efc9eec62cb947c162-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/b495ce63ede0f4efc9eec62cb947c162-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 974713,
        "gs_citation": 253,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12014861840989411070&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "University of Manchester; Univ. de Cantabria & Univ. Carlos III de Madrid",
        "aff_domain": "gmail.com;tsc.uc3m.es",
        "email": "gmail.com;tsc.uc3m.es",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Manchester;Univ. de Cantabria & Univ. Carlos III de Madrid",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.manchester.ac.uk;",
        "aff_unique_abbr": "UoM;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United Kingdom;"
    },
    {
        "id": "366b306772",
        "title": "Statistical Performance of Convex Tensor Decomposition",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/b2eeb7362ef83deff5c7813a67e14f0a-Abstract.html",
        "author": "Ryota Tomioka; Taiji Suzuki; Kohei Hayashi; Hisashi Kashima",
        "abstract": "We analyze the statistical performance of a recently proposed convex tensor decomposition algorithm. Conventionally tensor decomposition has been formulated as non-convex optimization problems, which hindered the analysis of their performance. We show under some conditions that the mean squared error of the convex method scales linearly with the quantity we call the normalized rank of the true tensor. The current analysis naturally extends the analysis of convex low-rank matrix estimation to tensors. Furthermore, we show through numerical experiments that our theory can precisely predict the scaling behaviour in practice.",
        "bibtex": "@inproceedings{NIPS2011_b2eeb736,\n author = {Tomioka, Ryota and Suzuki, Taiji and Hayashi, Kohei and Kashima, Hisashi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Statistical Performance of Convex Tensor Decomposition},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/b2eeb7362ef83deff5c7813a67e14f0a-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/b2eeb7362ef83deff5c7813a67e14f0a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/b2eeb7362ef83deff5c7813a67e14f0a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 142377,
        "gs_citation": 195,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7920478182284233959&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Mathematical Informatics, The University of Tokyo; Department of Mathematical Informatics, The University of Tokyo; Graduate School of Information Science, Nara Institute of Science and Technology; Department of Mathematical Informatics, The University of Tokyo + Basic Research Programs PRESTO, Synthesis of Knowledge for Information Oriented Society, JST",
        "aff_domain": "mist.i.u-tokyo.ac.jp;stat.t.u-tokyo.ac.jp;is.naist.jp;mist.i.u-tokyo.ac.jp",
        "email": "mist.i.u-tokyo.ac.jp;stat.t.u-tokyo.ac.jp;is.naist.jp;mist.i.u-tokyo.ac.jp",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0+2",
        "aff_unique_norm": "Department of Mathematical Informatics, The University of Tokyo;Nara Institute of Science and Technology;Basic Research Programs PRESTO, Synthesis of Knowledge for Information Oriented Society, JST",
        "aff_unique_dep": ";Graduate School of Information Science;",
        "aff_unique_url": ";https://www.nist.go.jp;",
        "aff_unique_abbr": ";NIST;",
        "aff_campus_unique_index": "1;",
        "aff_campus_unique": ";Nara",
        "aff_country_unique_index": "1;",
        "aff_country_unique": ";Japan"
    },
    {
        "id": "fa7cbf692c",
        "title": "Statistical Tests for Optimization Efficiency",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/b24d516bb65a5a58079f0f3526c87c57-Abstract.html",
        "author": "Levi Boyles; Anoop Korattikara; Deva Ramanan; Max Welling",
        "abstract": "Learning problems such as logistic regression are typically formulated as pure  optimization problems defined on some loss function. We argue that this view  ignores the fact that the loss function depends on stochastically generated data  which in turn determines an intrinsic scale of precision for statistical estimation.  By considering the statistical properties of the update variables used during  the optimization (e.g. gradients), we can construct frequentist hypothesis tests  to determine the reliability of these updates. We utilize subsets of the data  for computing updates, and use the hypothesis tests for determining when the  batch-size needs to be increased. This provides computational benefits and avoids  overfitting by stopping when the batch-size has become equal to size of the full  dataset. Moreover, the proposed algorithms depend on a single interpretable  parameter \u2013 the probability for an update to be in the wrong direction \u2013 which is  set to a single value across all algorithms and datasets. In this paper, we illustrate  these ideas on three L1 regularized coordinate algorithms: L1 -regularized L2 -loss  SVMs, L1 -regularized logistic regression, and the Lasso, but we emphasize that  the underlying methods are much more generally applicable.",
        "bibtex": "@inproceedings{NIPS2011_b24d516b,\n author = {Boyles, Levi and Korattikara, Anoop and Ramanan, Deva and Welling, Max},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Statistical Tests for Optimization Efficiency},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/b24d516bb65a5a58079f0f3526c87c57-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/b24d516bb65a5a58079f0f3526c87c57-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/b24d516bb65a5a58079f0f3526c87c57-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 250713,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17275133204216302714&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Computer Science, University of California, Irvine; Department of Computer Science, University of California, Irvine; Department of Computer Science, University of California, Irvine; Department of Computer Science, University of California, Irvine",
        "aff_domain": "ics.uci.edu;ics.uci.edu;ics.uci.edu;ics.uci.edu",
        "email": "ics.uci.edu;ics.uci.edu;ics.uci.edu;ics.uci.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of California, Irvine",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.uci.edu",
        "aff_unique_abbr": "UCI",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Irvine",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "0cfcc33c4c",
        "title": "Stochastic convex optimization with bandit feedback",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/67e103b0761e60683e83c559be18d40c-Abstract.html",
        "author": "Alekh Agarwal; Dean P. Foster; Daniel J. Hsu; Sham M. Kakade; Alexander Rakhlin",
        "abstract": "This paper addresses the problem of minimizing a convex, Lipschitz  function $f$ over a convex, compact set $X$ under a stochastic bandit feedback model. In this model, the algorithm is allowed to  observe noisy realizations of the function value $f(x)$ at any query  point $x \\in X$. We demonstrate a generalization of the  ellipsoid algorithm that incurs $O(\\poly(d)\\sqrt{T})$ regret. Since any algorithm has regret at least $\\Omega(\\sqrt{T})$  on this problem, our algorithm is optimal in terms of the scaling  with $T$.",
        "bibtex": "@inproceedings{NIPS2011_67e103b0,\n author = {Agarwal, Alekh and Foster, Dean P and Hsu, Daniel J and Kakade, Sham M and Rakhlin, Alexander},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Stochastic convex optimization with bandit feedback},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/67e103b0761e60683e83c559be18d40c-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/67e103b0761e60683e83c559be18d40c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/67e103b0761e60683e83c559be18d40c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 710155,
        "gs_citation": 240,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9300116814044847343&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 26,
        "aff": "Department of EECS, UC Berkeley; Department of Statistics, University of Pennysylvania; Microsoft Research, New England; Department of Statistics, University of Pennysylvania + Microsoft Research, New England; Department of Statistics, University of Pennysylvania",
        "aff_domain": "cs.berkeley.edu;gmail.com;microsoft.com;microsoft.com;wharton.upenn.edu",
        "email": "cs.berkeley.edu;gmail.com;microsoft.com;microsoft.com;wharton.upenn.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;1+2;1",
        "aff_unique_norm": "University of California, Berkeley;Department of Statistics, University of Pennysylvania;Microsoft Research",
        "aff_unique_dep": "Department of Electrical Engineering and Computer Sciences;;",
        "aff_unique_url": "https://www.berkeley.edu;;https://www.microsoft.com/en-us/research/group/newengland",
        "aff_unique_abbr": "UC Berkeley;;MSR",
        "aff_campus_unique_index": "0;2;2",
        "aff_campus_unique": "Berkeley;;New England",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "f71bda0ecb",
        "title": "Structural equations and divisive normalization for energy-dependent component analysis",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/a89cf525e1d9f04d16ce31165e139a4b-Abstract.html",
        "author": "Jun-ichiro Hirayama; Aapo Hyv\u00e4rinen",
        "abstract": "Components estimated by independent component analysis and related methods  are typically not independent in real data. A very common form of nonlinear  dependency between the components is correlations in their variances or ener-  gies. Here, we propose a principled probabilistic model to model the energy-  correlations between the latent variables. Our two-stage model includes a linear  mixing of latent signals into the observed ones like in ICA. The main new fea-  ture is a model of the energy-correlations based on the structural equation model  (SEM), in particular, a Linear Non-Gaussian SEM. The SEM is closely related to  divisive normalization which effectively reduces energy correlation. Our new two-  stage model enables estimation of both the linear mixing and the interactions re-  lated to energy-correlations, without resorting to approximations of the likelihood  function or other non-principled approaches. We demonstrate the applicability of  our method with synthetic dataset, natural images and brain signals.",
        "bibtex": "@inproceedings{NIPS2011_a89cf525,\n author = {Hirayama, Jun-ichiro and Hyv\\\"{a}rinen, Aapo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Structural equations and divisive normalization for energy-dependent component analysis},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/a89cf525e1d9f04d16ce31165e139a4b-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/a89cf525e1d9f04d16ce31165e139a4b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/a89cf525e1d9f04d16ce31165e139a4b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/a89cf525e1d9f04d16ce31165e139a4b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 6732253,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1208565777829779974&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Dept. of Systems Science, Graduate School of Informatics, Kyoto University; Dept. of Mathematics and Statistics, Dept. of Computer Science and HIIT, University of Helsinki",
        "aff_domain": "; ",
        "email": "; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Dept. of Systems Science, Graduate School of Informatics, Kyoto University;Dept. of Mathematics and Statistics, Dept. of Computer Science and HIIT, University of Helsinki",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "7a6e1fc58e",
        "title": "Structure Learning for Optimization",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/291597a100aadd814d197af4f4bab3a7-Abstract.html",
        "author": "Shulin Yang; Ali Rahimi",
        "abstract": "We describe a family of global optimization procedures that automatically decompose optimization problems into smaller loosely coupled problems, then combine the solutions of these with message passing algorithms.  We show empirically that these methods excel in avoiding local minima and produce better solutions with fewer function evaluations than existing global optimization methods.  To develop these methods, we introduce a notion of coupling between variables of optimization that generalizes the notion of coupling that arises from factoring functions into terms that involve small subsets of the variables. It therefore subsumes the notion of independence between random variables in statistics, sparseness of the Hessian in nonlinear optimization, and the generalized distributive law. Despite being more general, this notion of coupling is easier to verify empirically -- making structure estimation easy -- yet it allows us to migrate well-established inference methods on graphical models to the setting of global optimization.",
        "bibtex": "@inproceedings{NIPS2011_291597a1,\n author = {Yang, Shulin and Rahimi, Ali},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Structure Learning for Optimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/291597a100aadd814d197af4f4bab3a7-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/291597a100aadd814d197af4f4bab3a7-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/291597a100aadd814d197af4f4bab3a7-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/291597a100aadd814d197af4f4bab3a7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 461673,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "Department of Computer Science, University of Washington; Red Bow Labs",
        "aff_domain": "cs.washington.edu;redbowlabs.com",
        "email": "cs.washington.edu;redbowlabs.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Washington;Red Bow Labs",
        "aff_unique_dep": "Department of Computer Science;",
        "aff_unique_url": "https://www.washington.edu;",
        "aff_unique_abbr": "UW;",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Seattle;",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "c78c52c7ce",
        "title": "Structured Learning for Cell Tracking",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/959a557f5f6beb411fd954f3f34b21c3-Abstract.html",
        "author": "Xinghua Lou; Fred A. Hamprecht",
        "abstract": "We study the problem of learning to track a large quantity of homogeneous objects such as cell tracking in cell culture study and developmental biology. Reliable cell tracking in time-lapse microscopic image sequences is important for modern biomedical research. Existing cell tracking methods are usually kept simple and use only a small number of features to allow for manual parameter tweaking or grid search. We propose a structured learning approach that allows to learn optimum parameters automatically from a training set. This allows for the use of a richer set of features which in turn affords improved tracking compared to recently reported methods on two public benchmark sequences.",
        "bibtex": "@inproceedings{NIPS2011_959a557f,\n author = {Lou, Xinghua and Hamprecht, Fred A},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Structured Learning for Cell Tracking},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/959a557f5f6beb411fd954f3f34b21c3-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/959a557f5f6beb411fd954f3f34b21c3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/959a557f5f6beb411fd954f3f34b21c3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1462451,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1573000892447419440&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Heidelberg Collaboratory for Image Processing (HCI) + Interdisciplinary Center for Scienti\ufb01c Computing (IWR), University of Heidelberg, Heidelberg 69115, Germany; Heidelberg Collaboratory for Image Processing (HCI) + Interdisciplinary Center for Scienti\ufb01c Computing (IWR), University of Heidelberg, Heidelberg 69115, Germany",
        "aff_domain": "iwr.uni-heidelberg.de;iwr.uni-heidelberg.de",
        "email": "iwr.uni-heidelberg.de;iwr.uni-heidelberg.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "Heidelberg Collaboratory for Image Processing;University of Heidelberg",
        "aff_unique_dep": "Image Processing;Interdisciplinary Center for Scienti\ufb01c Computing (IWR)",
        "aff_unique_url": "https://hci.iwr.uni-heidelberg.de/;https://www.uni-heidelberg.de",
        "aff_unique_abbr": "HCI;Uni Heidelberg",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Heidelberg",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "e021b8232b",
        "title": "Structured sparse coding via lateral inhibition",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/fae0b27c451c728867a567e8c1bb4e53-Abstract.html",
        "author": "Arthur D. Szlam; Karol Gregor; Yann L. Cun",
        "abstract": "This work describes a conceptually simple method for structured sparse coding and dictionary design. Supposing a dictionary with K atoms, we introduce a structure as a set of penalties or interactions between every pair of atoms. We describe modifications of standard sparse coding algorithms for inference in this setting, and describe experiments showing that these algorithms are efficient. We show that interesting dictionaries can be learned for interactions that encode tree structures or locally connected structures. Finally, we show that our framework allows us to learn the values of the interactions from the data, rather than having them pre-specified.",
        "bibtex": "@inproceedings{NIPS2011_fae0b27c,\n author = {Szlam, Arthur and Gregor, Karol and Cun, Yann},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Structured sparse coding via lateral inhibition},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/fae0b27c451c728867a567e8c1bb4e53-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/fae0b27c451c728867a567e8c1bb4e53-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/fae0b27c451c728867a567e8c1bb4e53-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/fae0b27c451c728867a567e8c1bb4e53-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 354111,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14134916040898449115&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Janelia Farm, HHMI; The City College of New York; New York University",
        "aff_domain": "gmail.com;courant.nyu.edu;cs.nyu.edu",
        "email": "gmail.com;courant.nyu.edu;cs.nyu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Janelia Farm, HHMI;City College of New York;New York University",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";https://www.ccny.cuny.edu;https://www.nyu.edu",
        "aff_unique_abbr": ";CCNY;NYU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";New York",
        "aff_country_unique_index": "1;1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "4b7b7485ac",
        "title": "Submodular Multi-Label Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/fc49306d97602c8ed1be1dfbf0835ead-Abstract.html",
        "author": "James Petterson; Tib\u00e9rio S. Caetano",
        "abstract": "In this paper we present an algorithm to learn a multi-label classifier which attempts at directly optimising the F-score. The key novelty of our formulation is that we explicitly allow for assortative (submodular) pairwise label interactions, i.e., we can leverage the co-ocurrence of pairs of labels in order to improve the quality of prediction. Prediction in this model consists of minimising a particular submodular set function, what can be accomplished exactly and efficiently via graph-cuts. Learning however is substantially more involved and requires the solution of an intractable combinatorial optimisation problem. We present an approximate algorithm for this problem and prove that it is sound in the sense that it never predicts incorrect labels. We also present a nontrivial test of a sufficient condition for our algorithm to have found an optimal solution. We present experiments on benchmark multi-label datasets, which attest the value of our proposed technique. We also make available source code that enables the reproduction of our experiments.",
        "bibtex": "@inproceedings{NIPS2011_fc49306d,\n author = {Petterson, James and Caetano, Tib\\'{e}rio},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Submodular Multi-Label Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/fc49306d97602c8ed1be1dfbf0835ead-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/fc49306d97602c8ed1be1dfbf0835ead-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/fc49306d97602c8ed1be1dfbf0835ead-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/fc49306d97602c8ed1be1dfbf0835ead-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 798974,
        "gs_citation": 89,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6204961103517045555&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "fanberraM dustralia; vydneyPfanberraM dustralia",
        "aff_domain": "qlfwdPdqx; qlfwdPdqx",
        "email": "qlfwdPdqx; qlfwdPdqx",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "fanberraM dustralia;vydneyPfanberraM dustralia",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "f7d50c978c",
        "title": "TD_gamma: Re-evaluating Complex Backups in Temporal Difference Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/fb60d411a5c5b72b2e7d3527cfc84fd0-Abstract.html",
        "author": "George Konidaris; Scott Niekum; Philip S. Thomas",
        "abstract": "We show that the lambda-return target used in the TD(lambda) family of algorithms is the maximum likelihood estimator for a specific model of how the variance of an n-step return estimate increases with n. We introduce the gamma-return estimator, an alternative target based on a more accurate model of variance, which defines the TD",
        "bibtex": "@inproceedings{NIPS2011_fb60d411,\n author = {Konidaris, George and Niekum, Scott and Thomas, Philip S.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {TD\\_gamma: Re-evaluating Complex Backups in Temporal Difference Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/fb60d411a5c5b72b2e7d3527cfc84fd0-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/fb60d411a5c5b72b2e7d3527cfc84fd0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/fb60d411a5c5b72b2e7d3527cfc84fd0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 344566,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11110754372499332127&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "MIT CSAIL\u2020; University of Massachusetts Amherst\u2021; University of Massachusetts Amherst\u2021",
        "aff_domain": "csail.mit.edu;cs.umass.edu;cs.umass.edu",
        "email": "csail.mit.edu;cs.umass.edu;cs.umass.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Massachusetts Institute of Technology;University of Massachusetts Amherst\u2021",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory;",
        "aff_unique_url": "http://www.csail.mit.edu;",
        "aff_unique_abbr": "MIT CSAIL;",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "c6b04d0865",
        "title": "Target Neighbor Consistent Feature Weighting for Nearest Neighbor Classification",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/f73b76ce8949fe29bf2a537cfa420e8f-Abstract.html",
        "author": "Ichiro Takeuchi; Masashi Sugiyama",
        "abstract": "We consider feature selection and weighting for nearest neighbor classifiers. A technical challenge in this scenario is how to cope with the discrete update of nearest neighbors when the feature space metric is changed during the learning process. This issue, called the target neighbor change, was not properly addressed in the existing feature weighting and metric learning literature.  In this paper, we propose a novel feature weighting algorithm that can exactly and efficiently keep track of the correct target neighbors via sequential quadratic programming. To the best of our knowledge, this is the first algorithm that guarantees the consistency between target neighbors and the feature space metric. We further show that the proposed algorithm can be naturally combined with regularization path tracking, allowing computationally efficient selection of the regularization parameter. We demonstrate the effectiveness of the proposed algorithm through experiments.",
        "bibtex": "@inproceedings{NIPS2011_f73b76ce,\n author = {Takeuchi, Ichiro and Sugiyama, Masashi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Target Neighbor Consistent Feature Weighting for Nearest Neighbor Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/f73b76ce8949fe29bf2a537cfa420e8f-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/f73b76ce8949fe29bf2a537cfa420e8f-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/f73b76ce8949fe29bf2a537cfa420e8f-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/f73b76ce8949fe29bf2a537cfa420e8f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 537352,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17626220231474617447&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Engineering, Nagoya Institute of Technology; Department of Computer Science, Tokyo Institute of Technology",
        "aff_domain": "nitech.ac.jp;cs.titech.ac.jp",
        "email": "nitech.ac.jp;cs.titech.ac.jp",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Department of Engineering, Nagoya Institute of Technology;Tokyo Institute of Technology",
        "aff_unique_dep": ";Department of Computer Science",
        "aff_unique_url": ";https://www.titech.ac.jp",
        "aff_unique_abbr": ";Titech",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Tokyo",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";Japan"
    },
    {
        "id": "72190a2dff",
        "title": "Testing a Bayesian Measure of Representativeness Using a Large Image Database",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/2c89109d42178de8a367c0228f169bf8-Abstract.html",
        "author": "Joshua T. Abbott; Katherine A. Heller; Zoubin Ghahramani; Thomas L. Griffiths",
        "abstract": "How do people determine which elements of a set are most representative of that set? We extend an existing Bayesian measure of representativeness, which indicates the representativeness of a sample from a distribution, to define a measure of the representativeness of an item to a set. We show that this measure is formally related to a machine learning method known as Bayesian Sets. Building on this connection, we derive an analytic expression for the representativeness of objects described by a sparse vector of binary features. We then apply this measure to a large database of images, using it to determine which images are the most representative members of different sets. Comparing the resulting predictions to human judgments of representativeness provides a test of this measure with naturalistic stimuli, and illustrates how databases that are more commonly used in computer vision and machine learning can be used to evaluate psychological theories.",
        "bibtex": "@inproceedings{NIPS2011_2c89109d,\n author = {Abbott, Joshua T and Heller, Katherine A and Ghahramani, Zoubin and Griffiths, Thomas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Testing a Bayesian Measure of Representativeness Using a Large Image Database},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/2c89109d42178de8a367c0228f169bf8-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/2c89109d42178de8a367c0228f169bf8-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/2c89109d42178de8a367c0228f169bf8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 877922,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8576570792794301292&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Psychology, University of California, Berkeley; Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology; Department of Engineering, University of Cambridge; Department of Psychology, University of California, Berkeley",
        "aff_domain": "berkeley.edu;mit.edu;eng.cam.ac.uk;berkeley.edu",
        "email": "berkeley.edu;mit.edu;eng.cam.ac.uk;berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Department of Psychology, University of California, Berkeley;Massachusetts Institute of Technology;University of Cambridge",
        "aff_unique_dep": ";Department of Brain and Cognitive Sciences;Department of Engineering",
        "aff_unique_url": ";https://web.mit.edu;https://www.cam.ac.uk",
        "aff_unique_abbr": ";MIT;Cambridge",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "1;2",
        "aff_country_unique": ";United States;United Kingdom"
    },
    {
        "id": "6d2c3efe2a",
        "title": "The Doubly Correlated Nonparametric Topic Model",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/0eec27c419d0fe24e53c90338cdc8bc6-Abstract.html",
        "author": "Dae I. Kim; Erik B. Sudderth",
        "abstract": "Topic models are learned via a statistical model of variation within document collections, but designed to extract meaningful semantic structure.  Desirable traits include the ability to incorporate annotations or metadata associated with documents; the discovery of correlated patterns of topic usage; and the avoidance of parametric assumptions, such as manual specification of the number of topics.  We propose a doubly correlated nonparametric topic (DCNT) model, the first model to simultaneously capture all three of these properties.  The DCNT models metadata via a flexible, Gaussian regression on arbitrary input features; correlations via a scalable square-root covariance representation; and nonparametric selection from an unbounded series of potential topics via a stick-breaking construction.  We validate the semantic structure and predictive performance of the DCNT using a corpus of NIPS documents annotated by various metadata.",
        "bibtex": "@inproceedings{NIPS2011_0eec27c4,\n author = {Kim, Dae and Sudderth, Erik},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The Doubly Correlated Nonparametric Topic Model},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/0eec27c419d0fe24e53c90338cdc8bc6-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/0eec27c419d0fe24e53c90338cdc8bc6-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/0eec27c419d0fe24e53c90338cdc8bc6-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/0eec27c419d0fe24e53c90338cdc8bc6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2051595,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11443796534763135346&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Computer Science, Brown University, Providence, RI 02906; Department of Computer Science, Brown University, Providence, RI 02906",
        "aff_domain": "cs.brown.edu;cs.brown.edu",
        "email": "cs.brown.edu;cs.brown.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Department of Computer Science, Brown University, Providence, RI 02906",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "2f7b494324",
        "title": "The Fast Convergence of Boosting",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/8b5040a8a5baf3e0e67386c2e3a9b903-Abstract.html",
        "author": "Matus J. Telgarsky",
        "abstract": "This manuscript considers the convergence rate of boosting under a large class of losses, including the exponential and logistic losses, where the best previous rate of convergence was O(exp(1/\u03b5\u00b2)).  First, it is established that the setting of weak learnability aids the entire class, granting a rate O(ln(1/\u03b5)).  Next, the (disjoint) conditions under which the infimal empirical risk is attainable are characterized in terms of the sample and weak learning class, and a new proof is given for the known rate O(ln(1/\u03b5)).  Finally, it is established that any instance can be decomposed into two smaller instances resembling the two preceding special cases, yielding a rate O(1/\u03b5), with a matching lower bound for the logistic loss.  The principal technical hurdle throughout this work is the potential unattainability of the infimal empirical risk; the technique for overcoming this barrier may be of general interest.",
        "bibtex": "@inproceedings{NIPS2011_8b5040a8,\n author = {Telgarsky, Matus},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The Fast Convergence of Boosting},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/8b5040a8a5baf3e0e67386c2e3a9b903-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/8b5040a8a5baf3e0e67386c2e3a9b903-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/8b5040a8a5baf3e0e67386c2e3a9b903-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/8b5040a8a5baf3e0e67386c2e3a9b903-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 408798,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12810591784887140015&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Science and Engineering, University of California, San Diego",
        "aff_domain": "cs.ucsd.edu",
        "email": "cs.ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d62526faba",
        "title": "The Fixed Points of Off-Policy TD",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/fe2d010308a6b3799a3d9c728ee74244-Abstract.html",
        "author": "J. Z. Kolter",
        "abstract": "Off-policy learning, the ability for an agent to learn about a policy other than the one it is following, is a key element of Reinforcement Learning, and in recent years there has been much work on developing Temporal Different (TD) algorithms that are guaranteed to converge under off-policy sampling.  It has remained an open question, however, whether anything can be said a priori about the quality of the TD solution when off-policy sampling is employed with function approximation.  In general the answer is no: for arbitrary off-policy sampling the error of the TD solution can be unboundedly large, even when the approximator can represent the true value function well.  In this paper we propose a novel approach to address this problem: we show that by considering a certain convex subset of off-policy distributions we can indeed provide guarantees as to the solution quality similar to the on-policy case.  Furthermore, we show that we can efficiently project on to this convex set using only samples generated from the system.  The end result is a novel TD algorithm that has approximation guarantees even in the case of off-policy sampling and which empirically outperforms existing TD methods.",
        "bibtex": "@inproceedings{NIPS2011_fe2d0103,\n author = {Kolter, J.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The Fixed Points of Off-Policy TD},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/fe2d010308a6b3799a3d9c728ee74244-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/fe2d010308a6b3799a3d9c728ee74244-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/fe2d010308a6b3799a3d9c728ee74244-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/fe2d010308a6b3799a3d9c728ee74244-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 178331,
        "gs_citation": 71,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15981228252949856357&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology",
        "aff_domain": "csail.mit.edu",
        "email": "csail.mit.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory",
        "aff_unique_url": "https://www.csail.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "be4f593288",
        "title": "The Impact of Unlabeled Patterns in Rademacher Complexity Theory for Kernel Classifiers",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/0deb1c54814305ca9ad266f53bc82511-Abstract.html",
        "author": "Luca Oneto; Davide Anguita; Alessandro Ghio; Sandro Ridella",
        "abstract": "We derive here new generalization bounds, based on Rademacher Complexity theory, for model selection and error estimation   of linear (kernel) classifiers, which exploit  the availability of unlabeled samples.   In particular, two results are obtained: the first one shows that, using the unlabeled samples, the  confidence term of the conventional bound can be reduced by a factor of three; the second one  shows that the unlabeled samples can be used to obtain much tighter bounds, by building localized versions  of the hypothesis class containing the optimal classifier.",
        "bibtex": "@inproceedings{NIPS2011_0deb1c54,\n author = {Oneto, Luca and Anguita, Davide and Ghio, Alessandro and Ridella, Sandro},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The Impact of Unlabeled Patterns in Rademacher Complexity Theory for Kernel Classifiers},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/0deb1c54814305ca9ad266f53bc82511-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/0deb1c54814305ca9ad266f53bc82511-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/0deb1c54814305ca9ad266f53bc82511-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 177717,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15643590617289254126&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "d0893675d1",
        "title": "The Kernel Beta Process",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/04ecb1fa28506ccb6f72b12c0245ddbc-Abstract.html",
        "author": "Lu Ren; Yingjian Wang; Lawrence Carin; David B. Dunson",
        "abstract": "A new Le \u0301vy process prior is proposed for an uncountable collection of covariate- dependent feature-learning measures; the model is called the kernel beta process (KBP). Available covariates are handled efficiently via the kernel construction, with covariates assumed observed with each data sample (\u201ccustomer\u201d), and latent covariates learned for each feature (\u201cdish\u201d). Each customer selects dishes from an infinite buffet, in a manner analogous to the beta process, with the added constraint that a customer first decides probabilistically whether to \u201cconsider\u201d a dish, based on the distance in covariate space between the customer and dish. If a customer does consider a particular dish, that dish is then selected probabilistically as in the beta process. The beta process is recovered as a limiting case of the KBP. An efficient Gibbs sampler is developed for computations, and state-of-the-art results are presented for image processing and music analysis tasks.",
        "bibtex": "@inproceedings{NIPS2011_04ecb1fa,\n author = {Ren, Lu and Wang, Yingjian and Carin, Lawrence and Dunson, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The Kernel Beta Process},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/04ecb1fa28506ccb6f72b12c0245ddbc-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/04ecb1fa28506ccb6f72b12c0245ddbc-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/04ecb1fa28506ccb6f72b12c0245ddbc-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/04ecb1fa28506ccb6f72b12c0245ddbc-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1096386,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3694468608078696545&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Electrical & Computer Engineering Dept., Duke University; Electrical & Computer Engineering Dept., Duke University; Department of Statistical Science, Duke University; Electrical & Computer Engineering Dept., Duke University",
        "aff_domain": "duke.edu;duke.edu;stat.duke.edu;duke.edu",
        "email": "duke.edu;duke.edu;stat.duke.edu;duke.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Electrical & Computer Engineering Dept., Duke University;Duke University",
        "aff_unique_dep": ";Department of Statistical Science",
        "aff_unique_url": ";https://www.duke.edu",
        "aff_unique_abbr": ";Duke",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "eb06cc893f",
        "title": "The Local Rademacher Complexity of Lp-Norm Multiple Kernel Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/996009f2374006606f4c0b0fda878af1-Abstract.html",
        "author": "Marius Kloft; Gilles Blanchard",
        "abstract": "We derive an upper bound on the local Rademacher complexity of Lp-norm multiple kernel learning, which yields a tighter excess risk bound than global approaches. Previous local approaches analyzed the case p=1 only while our analysis covers all cases $1\\leq p\\leq\\infty$, assuming the different feature mappings corresponding to the different kernels to be uncorrelated. We also show a lower bound that shows that the bound is tight, and derive consequences regarding excess loss, namely fast convergence rates of the order $O(n^{-\\frac{\\alpha}{1+\\alpha}})$, where $\\alpha$ is the minimum eigenvalue decay rate of the individual kernels.",
        "bibtex": "@inproceedings{NIPS2011_996009f2,\n author = {Kloft, Marius and Blanchard, Gilles},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The Local Rademacher Complexity of Lp-Norm Multiple Kernel Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/996009f2374006606f4c0b0fda878af1-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/996009f2374006606f4c0b0fda878af1-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/996009f2374006606f4c0b0fda878af1-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/996009f2374006606f4c0b0fda878af1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 435325,
        "gs_citation": 62,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5029569687643696193&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Machine Learning Laboratory, TU Berlin, Germany + Friedrich Miescher Laboratory, Max Planck Society, T\u00fcbingen; Department of Mathematics, University of Potsdam, Germany",
        "aff_domain": "tu-berlin.de;math.uni-potsdam.de",
        "email": "tu-berlin.de;math.uni-potsdam.de",
        "github": "",
        "project": "http://videolectures.net/lkasok08_whistler/",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2",
        "aff_unique_norm": "Machine Learning Laboratory, TU Berlin, Germany;Friedrich Miescher Laboratory, Max Planck Society, T\u00fcbingen;University of Potsdam",
        "aff_unique_dep": ";;Department of Mathematics",
        "aff_unique_url": ";;https://www.uni-potsdam.de",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": ";1",
        "aff_country_unique": ";Germany"
    },
    {
        "id": "00955bb9cb",
        "title": "The Manifold Tangent Classifier",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/d1f44e2f09dc172978a4d3151d11d63e-Abstract.html",
        "author": "Salah Rifai; Yann N. Dauphin; Pascal Vincent; Yoshua Bengio; Xavier Muller",
        "abstract": "We combine three important ideas present in previous work for building classi- \ufb01ers: the semi-supervised hypothesis (the input distribution contains information about the classi\ufb01er), the unsupervised manifold hypothesis (data density concen- trates near low-dimensional manifolds), and the manifold hypothesis for classi\ufb01- cation (different classes correspond to disjoint manifolds separated by low den- sity). We exploit a novel algorithm for capturing manifold structure (high-order contractive auto-encoders) and we show how it builds a topological atlas of charts, each chart being characterized by the principal singular vectors of the Jacobian of a representation mapping. This representation learning algorithm can be stacked to yield a deep architecture, and we combine it with a domain knowledge-free version of the TangentProp algorithm to encourage the classi\ufb01er to be insensitive to local directions changes along the manifold. Record-breaking classi\ufb01cation results are obtained.",
        "bibtex": "@inproceedings{NIPS2011_d1f44e2f,\n author = {Rifai, Salah and Dauphin, Yann N and Vincent, Pascal and Bengio, Yoshua and Muller, Xavier},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The Manifold Tangent Classifier},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/d1f44e2f09dc172978a4d3151d11d63e-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/d1f44e2f09dc172978a4d3151d11d63e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/d1f44e2f09dc172978a4d3151d11d63e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 324816,
        "gs_citation": 362,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8931227741868367171&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Science and Operations Research, University of Montreal; Department of Computer Science and Operations Research, University of Montreal; Department of Computer Science and Operations Research, University of Montreal; Department of Computer Science and Operations Research, University of Montreal; Department of Computer Science and Operations Research, University of Montreal",
        "aff_domain": "iro.umontreal.ca;iro.umontreal.ca;iro.umontreal.ca;iro.umontreal.ca;iro.umontreal.ca",
        "email": "iro.umontreal.ca;iro.umontreal.ca;iro.umontreal.ca;iro.umontreal.ca;iro.umontreal.ca",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Montreal",
        "aff_unique_dep": "Department of Computer Science and Operations Research",
        "aff_unique_url": "https://www.umontreal.ca",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "1a2ab8b73b",
        "title": "Thinning Measurement Models and Questionnaire Design",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/bcbe3365e6ac95ea2c0343a2395834dd-Abstract.html",
        "author": "Ricardo Silva",
        "abstract": "Inferring key unobservable features of individuals is an important task in the applied sciences. In particular, an important source of data in fields such as marketing, social sciences and medicine is questionnaires: answers in such questionnaires are noisy measures of target unobserved features. While comprehensive surveys help to better estimate the latent variables of interest, aiming at a high number of questions comes at a price: refusal to participate in surveys can go up, as well as the rate of missing data; quality of answers can decline; costs associated with applying such questionnaires can also increase. In this paper, we cast the problem of refining existing models for questionnaire data as follows: solve a constrained optimization problem of preserving the maximum amount of information found in a latent variable model using only a subset of existing questions. The goal is to find an optimal subset of a given size. For that, we first define an information theoretical measure for quantifying the quality of a reduced questionnaire. Three different approximate inference methods are introduced to solve this problem. Comparisons against a simple but powerful heuristic are presented.",
        "bibtex": "@inproceedings{NIPS2011_bcbe3365,\n author = {Silva, Ricardo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Thinning Measurement Models and Questionnaire Design},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/bcbe3365e6ac95ea2c0343a2395834dd-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/bcbe3365e6ac95ea2c0343a2395834dd-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/bcbe3365e6ac95ea2c0343a2395834dd-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/bcbe3365e6ac95ea2c0343a2395834dd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 180352,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:M7DeVkyvoegJ:scholar.google.com/&scioq=Thinning+Measurement+Models+and+Questionnaire+Design&hl=en&as_sdt=0,5",
        "gs_version_total": 11,
        "aff": "Department of Statistical Science, University College London",
        "aff_domain": "stats.ucl.ac.uk",
        "email": "stats.ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University College London",
        "aff_unique_dep": "Department of Statistical Science",
        "aff_unique_url": "https://www.ucl.ac.uk",
        "aff_unique_abbr": "UCL",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "London",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "694af86548",
        "title": "Trace Lasso: a trace norm regularization for correlated designs",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/33ceb07bf4eeb3da587e268d663aba1a-Abstract.html",
        "author": "Edouard Grave; Guillaume R. Obozinski; Francis R. Bach",
        "abstract": "Using the $\\ell_1$-norm to regularize the estimation of  the parameter vector of a linear model leads to an unstable estimator when covariates are highly correlated. In this paper, we introduce a new penalty function which takes into account the correlation of the design matrix to stabilize the estimation. This norm, called the trace Lasso, uses the trace norm of the selected covariates, which is a convex surrogate of their rank, as the criterion of model complexity. We analyze the properties of our norm, describe an optimization algorithm based on reweighted least-squares, and illustrate the behavior of this norm on synthetic data, showing that it is more adapted to strong correlations than competing methods such as the elastic net.",
        "bibtex": "@inproceedings{NIPS2011_33ceb07b,\n author = {Grave, Edouard and Obozinski, Guillaume R and Bach, Francis},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Trace Lasso: a trace norm regularization for correlated designs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/33ceb07bf4eeb3da587e268d663aba1a-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/33ceb07bf4eeb3da587e268d663aba1a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/33ceb07bf4eeb3da587e268d663aba1a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1199325,
        "gs_citation": 239,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12658853630198114719&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "INRIA, Sierra Project-team + \u00b4Ecole Normale Sup \u00b4erieure, Paris; INRIA, Sierra Project-team + \u00b4Ecole Normale Sup \u00b4erieure, Paris; INRIA, Sierra Project-team + \u00b4Ecole Normale Sup \u00b4erieure, Paris",
        "aff_domain": "inria.fr;inria.fr;inria.fr",
        "email": "inria.fr;inria.fr;inria.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;0+1",
        "aff_unique_norm": "INRIA, Sierra Project-team;Ecole Normale Sup\u00e9rieure",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://www.ens.fr",
        "aff_unique_abbr": ";ENS",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Paris",
        "aff_country_unique_index": "1;1;1",
        "aff_country_unique": ";France"
    },
    {
        "id": "6ebf00e67b",
        "title": "Transfer Learning by Borrowing Examples for Multiclass Object Detection",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/e0eacd983971634327ae1819ea8b6214-Abstract.html",
        "author": "Joseph J. Lim; Ruslan Salakhutdinov; Antonio Torralba",
        "abstract": "Despite the recent trend of increasingly large datasets for object detection, there still exist many classes with few training examples. To overcome this lack of train- ing data for certain classes, we propose a novel way of augmenting the training data for each class by borrowing and transforming examples from other classes. Our model learns which training instances from other classes to borrow and how to transform the borrowed examples so that they become more similar to instances from the target class. Our experimental results demonstrate that our new object detector, with borrowed and transformed examples, improves upon the current state-of-the-art detector on the challenging SUN09 object detection dataset.",
        "bibtex": "@inproceedings{NIPS2011_e0eacd98,\n author = {Lim, Joseph J and Salakhutdinov, Russ R and Torralba, Antonio},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Transfer Learning by Borrowing Examples for Multiclass Object Detection},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/e0eacd983971634327ae1819ea8b6214-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/e0eacd983971634327ae1819ea8b6214-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/e0eacd983971634327ae1819ea8b6214-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 6353773,
        "gs_citation": 163,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12984364456330533596&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "CSAIL, MIT; Department of Statistics, University of Toronto; CSAIL, MIT",
        "aff_domain": "csail.mit.edu;utstat.toronto.edu;csail.mit.edu",
        "email": "csail.mit.edu;utstat.toronto.edu;csail.mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Massachusetts Institute of Technology;University of Toronto",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory;Department of Statistics",
        "aff_unique_url": "https://www.csail.mit.edu;https://www.utoronto.ca",
        "aff_unique_abbr": "MIT;U of T",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Cambridge;Toronto",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;Canada"
    },
    {
        "id": "94f136837f",
        "title": "Transfer from Multiple MDPs",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/fe7ee8fc1959cc7214fa21c4840dff0a-Abstract.html",
        "author": "Alessandro Lazaric; Marcello Restelli",
        "abstract": "Transfer reinforcement learning (RL) methods leverage on the experience collected on a set of source tasks to speed-up RL algorithms. A simple and effective approach is to transfer samples from source tasks and include them in the training set used to solve a target task. In this paper, we investigate the theoretical properties of this transfer method and we introduce novel algorithms adapting the transfer process on the basis of the similarity between source and target tasks. Finally, we report illustrative experimental results in a continuous chain problem.",
        "bibtex": "@inproceedings{NIPS2011_fe7ee8fc,\n author = {Lazaric, Alessandro and Restelli, Marcello},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Transfer from Multiple MDPs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/fe7ee8fc1959cc7214fa21c4840dff0a-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/fe7ee8fc1959cc7214fa21c4840dff0a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/fe7ee8fc1959cc7214fa21c4840dff0a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 153455,
        "gs_citation": 67,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1462596213271837341&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 21,
        "aff": "INRIA Lille - Nord Europe, Team SequeL, France; Department of Electronics and Informatics, Politecnico di Milano, Italy",
        "aff_domain": "inria.fr;elet.polimi.it",
        "email": "inria.fr;elet.polimi.it",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "INRIA Lille - Nord Europe, Team SequeL, France;Department of Electronics and Informatics, Politecnico di Milano, Italy",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "cf3e855bca",
        "title": "Two is better than one: distinct roles for familiarity and recollection in retrieving palimpsest memories",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/288cc0ff022877bd3df94bc9360b9c5d-Abstract.html",
        "author": "Cristina Savin; Peter Dayan; M\u00e1t\u00e9 Lengyel",
        "abstract": "Storing a new pattern in a palimpsest memory system comes at the cost of interfering with the memory traces of previously stored items. Knowing the age of a pattern thus becomes critical for recalling it faithfully. This implies that there should be a tight coupling between estimates of age, as a form of familiarity, and the neural dynamics of recollection, something which current theories omit. Using a normative model of autoassociative memory, we show that a dual memory system, consisting of two interacting modules for familiarity and recollection, has best performance for both recollection and recognition. This finding provides a new window onto actively contentious psychological and neural aspects of recognition memory.",
        "bibtex": "@inproceedings{NIPS2011_288cc0ff,\n author = {Savin, Cristina and Dayan, Peter and Lengyel, M\\'{a}t\\'{e}},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Two is better than one: distinct roles for familiarity and recollection in retrieving palimpsest memories},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/288cc0ff022877bd3df94bc9360b9c5d-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/288cc0ff022877bd3df94bc9360b9c5d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/288cc0ff022877bd3df94bc9360b9c5d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/288cc0ff022877bd3df94bc9360b9c5d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 808348,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13349711110377735285&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Computational & Biological Learning Lab, Dept. of Engineering, University of Cambridge, UK; Gatsby Computational Neuroscience Unit, University College London, UK; Computational & Biological Learning Lab, Dept. of Engineering, University of Cambridge, UK",
        "aff_domain": "cam.ac.uk;gatsby.ucl.ac.uk;eng.cam.ac.uk",
        "email": "cam.ac.uk;gatsby.ucl.ac.uk;eng.cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Computational & Biological Learning Lab, Dept. of Engineering, University of Cambridge, UK;University College London",
        "aff_unique_dep": ";Gatsby Computational Neuroscience Unit",
        "aff_unique_url": ";https://www.ucl.ac.uk",
        "aff_unique_abbr": ";UCL",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";London",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United Kingdom"
    },
    {
        "id": "3256037fde",
        "title": "Understanding the Intrinsic Memorability of Images",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/286674e3082feb7e5afb92777e48821f-Abstract.html",
        "author": "Phillip Isola; Devi Parikh; Antonio Torralba; Aude Oliva",
        "abstract": "Artists, advertisers, and photographers are routinely presented with the task of creating an image that a viewer will remember. While it may seem like image memorability is purely subjective, recent work shows that it is not an inexplicable phenomenon: variation in memorability of images is consistent across subjects, suggesting that some images are intrinsically more memorable than others, independent of a subjects' contexts and biases. In this paper, we used the publicly available memorability dataset of Isola et al., and augmented the object and scene annotations with interpretable spatial, content, and aesthetic image properties. We used a feature-selection scheme with desirable explaining-away properties to determine a compact set of attributes that characterizes the memorability of any individual image. We find that images of enclosed spaces containing people with visible faces are memorable, while images of vistas and peaceful scenes are not. Contrary to popular belief, unusual or aesthetically pleasing scenes do not tend to be highly memorable. This work represents one of the first attempts at understanding intrinsic image memorability, and opens a new domain of investigation at the interface between human cognition and computer vision.",
        "bibtex": "@inproceedings{NIPS2011_286674e3,\n author = {Isola, Phillip and Parikh, Devi and Torralba, Antonio and Oliva, Aude},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Understanding the Intrinsic Memorability of Images},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/286674e3082feb7e5afb92777e48821f-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/286674e3082feb7e5afb92777e48821f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/286674e3082feb7e5afb92777e48821f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 7167849,
        "gs_citation": 279,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17414029046930270555&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "MIT; TTI-Chicago; MIT; MIT",
        "aff_domain": "mit.edu;ttic.edu;mit.edu;mit.edu",
        "email": "mit.edu;ttic.edu;mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology;Toyota Technological Institute at Chicago",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://web.mit.edu;https://www.tti-chicago.org",
        "aff_unique_abbr": "MIT;TTI",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Chicago",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e1fdda00b0",
        "title": "Unifying Framework for Fast Learning Rate of Non-Sparse Multiple Kernel Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/aa169b49b583a2b5af89203c2b78c67c-Abstract.html",
        "author": "Taiji Suzuki",
        "abstract": "In this paper, we give a new generalization error bound of Multiple Kernel Learning  (MKL) for a general class of regularizations. Our main target in this paper is  dense type regularizations including \u2113p-MKL that imposes \u2113p-mixed-norm regularization  instead of \u21131-mixed-norm regularization. According to the recent numerical  experiments, the sparse regularization does not necessarily show a good  performance compared with dense type regularizations. Motivated by this fact,  this paper gives a general theoretical tool to derive fast learning rates that is applicable  to arbitrary monotone norm-type regularizations in a unifying manner. As  a by-product of our general result, we show a fast learning rate of \u2113p-MKL that  is tightest among existing bounds. We also show that our general learning rate  achieves the minimax lower bound. Finally, we show that, when the complexities  of candidate reproducing kernel Hilbert spaces are inhomogeneous, dense type  regularization shows better learning rate compared with sparse \u21131 regularization.",
        "bibtex": "@inproceedings{NIPS2011_aa169b49,\n author = {Suzuki, Taiji},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Unifying Framework for Fast Learning Rate of Non-Sparse Multiple Kernel Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/aa169b49b583a2b5af89203c2b78c67c-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/aa169b49b583a2b5af89203c2b78c67c-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/aa169b49b583a2b5af89203c2b78c67c-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/aa169b49b583a2b5af89203c2b78c67c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 120457,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1969344215121394296&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Mathematical Informatics, The University of Tokyo",
        "aff_domain": "stat.t.u-tokyo.ac.jp",
        "email": "stat.t.u-tokyo.ac.jp",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Department of Mathematical Informatics, The University of Tokyo",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "b6b5b611c6",
        "title": "Unifying Non-Maximum Likelihood Learning Objectives with Minimum KL Contraction",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/a3f390d88e4c41f2747bfa2f1b5f87db-Abstract.html",
        "author": "Siwei Lyu",
        "abstract": "When used to learn high dimensional parametric probabilistic models, the clas- sical maximum likelihood (ML) learning often suffers from computational in- tractability, which motivates the active developments of non-ML learning meth- ods. Yet, because of their divergent motivations and forms, the objective func- tions of many non-ML learning methods are seemingly unrelated, and there lacks a unified framework to understand them. In this work, based on an information geometric view of parametric learning, we introduce a general non-ML learning principle termed as minimum KL contraction, where we seek optimal parameters that minimizes the contraction of the KL divergence between the two distributions after they are transformed with a KL contraction operator. We then show that the objective functions of several important or recently developed non-ML learn- ing methods, including contrastive divergence [12], noise-contrastive estimation [11], partial likelihood [7], non-local contrastive objectives [31], score match- ing [14], pseudo-likelihood [3], maximum conditional likelihood [17], maximum mutual information [2], maximum marginal likelihood [9], and conditional and marginal composite likelihood [24], can be unified under the minimum KL con- traction framework with different choices of the KL contraction operators.",
        "bibtex": "@inproceedings{NIPS2011_a3f390d8,\n author = {Lyu, Siwei},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Unifying Non-Maximum Likelihood Learning Objectives with Minimum KL Contraction},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/a3f390d88e4c41f2747bfa2f1b5f87db-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/a3f390d88e4c41f2747bfa2f1b5f87db-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/a3f390d88e4c41f2747bfa2f1b5f87db-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 439128,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15759619693379523155&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Computer Science Department, University at Albany, State University of New York",
        "aff_domain": "cs.albany.edu",
        "email": "cs.albany.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Computer Science Department, University at Albany, State University of New York",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "43d92a06c8",
        "title": "Uniqueness of Belief Propagation on Signed Graphs",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/5ec91aac30eae62f4140325d09b9afd0-Abstract.html",
        "author": "Yusuke Watanabe",
        "abstract": "While loopy Belief Propagation (LBP) has been utilized in a wide variety of applications with empirical success, it comes with few theoretical guarantees. Especially, if the interactions of random variables in a graphical model are strong, the behaviors of the algorithm can be difficult to analyze due to underlying phase transitions. In this paper, we develop a novel approach to the uniqueness problem of the LBP fixed point; our new \u201cnecessary and sufficient\u201d condition is stated in terms of graphs and signs, where the sign denotes the types (attractive/repulsive) of the interaction (i.e., compatibility function) on the edge. In all previous works, uniqueness is guaranteed only in the situations where the strength of the interactions are \u201csufficiently\u201d small in certain senses. In contrast, our condition covers arbitrary strong interactions on the specified class of signed graphs. The result of this paper is based on the recent theoretical advance in the LBP algorithm; the connection with the graph zeta function.",
        "bibtex": "@inproceedings{NIPS2011_5ec91aac,\n author = {Watanabe, Yusuke},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Uniqueness of Belief Propagation on Signed Graphs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/5ec91aac30eae62f4140325d09b9afd0-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/5ec91aac30eae62f4140325d09b9afd0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/5ec91aac30eae62f4140325d09b9afd0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 145388,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3891400582337037900&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "4ebb1180bd",
        "title": "Universal low-rank matrix recovery from Pauli measurements",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/e820a45f1dfc7b95282d10b6087e11c0-Abstract.html",
        "author": "Yi-kai Liu",
        "abstract": "We study the problem of reconstructing an unknown matrix M of rank r and dimension d using O(rd polylog d) Pauli measurements.  This has applications in quantum state tomography, and is a non-commutative analogue of a well-known problem in compressed sensing:  recovering a sparse vector from a few of its Fourier coefficients.      We show that almost all sets of O(rd log^6 d) Pauli measurements satisfy the rank-r restricted isometry property (RIP).  This implies that M can be recovered from a fixed (\"universal\") set of Pauli measurements, using nuclear-norm minimization (e.g., the matrix Lasso), with nearly-optimal bounds on the error.  A similar result holds for any class of measurements that use an orthonormal operator basis whose elements have small operator norm.  Our proof uses Dudley's inequality for Gaussian processes, together with bounds on covering numbers obtained via entropy duality.",
        "bibtex": "@inproceedings{NIPS2011_e820a45f,\n author = {Liu, Yi-kai},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Universal low-rank matrix recovery from Pauli measurements},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/e820a45f1dfc7b95282d10b6087e11c0-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/e820a45f1dfc7b95282d10b6087e11c0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/e820a45f1dfc7b95282d10b6087e11c0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 245715,
        "gs_citation": 158,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12936044100881389195&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Applied and Computational Mathematics Division, National Institute of Standards and Technology, Gaithersburg, MD, USA",
        "aff_domain": "nist.gov",
        "email": "nist.gov",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Applied and Computational Mathematics Division, National Institute of Standards and Technology, Gaithersburg, MD, USA",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "2353dd00c9",
        "title": "Unsupervised learning models of primary cortical receptive fields and receptive field plasticity",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/e19347e1c3ca0c0b97de5fb3b690855a-Abstract.html",
        "author": "Maneesh Bhand; Ritvik Mudur; Bipin Suresh; Andrew Saxe; Andrew Y. Ng",
        "abstract": "The efficient coding hypothesis holds that neural receptive fields are adapted to the statistics of the environment, but is agnostic to the timescale of this adaptation, which occurs on both evolutionary and developmental timescales. In this work we focus on that component of adaptation which occurs during an organism's lifetime, and show that a number of unsupervised feature learning algorithms can account for features of normal receptive field properties across multiple primary sensory cortices. Furthermore, we show that the same algorithms account for altered receptive field properties in response to experimentally altered environmental statistics. Based on these modeling results we propose these models as phenomenological models of receptive field plasticity during an organism's lifetime. Finally, due to the success of the same models in multiple sensory areas, we suggest that these algorithms may provide a constructive realization of the theory, first proposed by Mountcastle (1978), that a qualitatively similar learning algorithm acts throughout primary sensory cortices.",
        "bibtex": "@inproceedings{NIPS2011_e19347e1,\n author = {Bhand, Maneesh and Mudur, Ritvik and Suresh, Bipin and Saxe, Andrew and Ng, Andrew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Unsupervised learning models of primary cortical receptive fields and receptive field plasticity},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/e19347e1c3ca0c0b97de5fb3b690855a-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/e19347e1c3ca0c0b97de5fb3b690855a-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/e19347e1c3ca0c0b97de5fb3b690855a-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/e19347e1c3ca0c0b97de5fb3b690855a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2775552,
        "gs_citation": 62,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15798415369974079717&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science, Stanford University; Department of Computer Science, Stanford University; Department of Computer Science, Stanford University; Department of Computer Science, Stanford University; Department of Computer Science, Stanford University",
        "aff_domain": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "6a412953ab",
        "title": "Variance Penalizing AdaBoost",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/46072631582fc240dd2674a7d063b040-Abstract.html",
        "author": "Pannagadatta K. Shivaswamy; Tony Jebara",
        "abstract": "This paper proposes a novel boosting algorithm called VadaBoost which is motivated by recent empirical Bernstein bounds.  VadaBoost iteratively minimizes a cost function that balances the sample mean and the sample variance of the exponential loss. Each step of the proposed algorithm minimizes the cost efficiently by providing weighted data to a weak learner rather than requiring a brute force evaluation of all possible weak learners. Thus, the proposed algorithm solves a key limitation of previous empirical Bernstein boosting methods which required brute force enumeration of all possible weak learners. Experimental results confirm that the new algorithm achieves the performance improvements of EBBoost yet goes beyond decision stumps to handle any weak learner. Significant performance gains are obtained over AdaBoost for arbitrary weak learners including decision trees (CART).",
        "bibtex": "@inproceedings{NIPS2011_46072631,\n author = {Shivaswamy, Pannagadatta and Jebara, Tony},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Variance Penalizing AdaBoost},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/46072631582fc240dd2674a7d063b040-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/46072631582fc240dd2674a7d063b040-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/46072631582fc240dd2674a7d063b040-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 278801,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9911348210435324898&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science, Cornell University, Ithaca NY; Department of Compter Science, Columbia University, New York NY",
        "aff_domain": "cs.cornell.edu;cs.columbia.edu",
        "email": "cs.cornell.edu;cs.columbia.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Cornell University;Department of Compter Science, Columbia University, New York NY",
        "aff_unique_dep": "Department of Computer Science;",
        "aff_unique_url": "https://www.cornell.edu;",
        "aff_unique_abbr": "Cornell;",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Ithaca;",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "bb89a4574f",
        "title": "Variance Reduction in Monte-Carlo Tree Search",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/d736bb10d83a904aefc1d6ce93dc54b8-Abstract.html",
        "author": "Joel Veness; Marc Lanctot; Michael Bowling",
        "abstract": "Monte-Carlo Tree Search (MCTS) has proven to be a powerful, generic planning technique for decision-making in single-agent and adversarial environments. The stochastic nature of the Monte-Carlo simulations introduces errors in the value estimates, both in terms of bias and variance. Whilst reducing bias (typically through the addition of domain knowledge) has been studied in the MCTS literature, comparatively little effort has focused on reducing variance. This is somewhat surprising, since variance reduction techniques are a well-studied area in classical statistics. In this paper, we examine the application of some standard techniques for variance reduction in MCTS, including common random numbers, antithetic variates and control variates. We demonstrate how these techniques can be applied to MCTS and explore their efficacy on three different stochastic, single-agent settings: Pig, Can't Stop and Dominion.",
        "bibtex": "@inproceedings{NIPS2011_d736bb10,\n author = {Veness, Joel and Lanctot, Marc and Bowling, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Variance Reduction in Monte-Carlo Tree Search},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/d736bb10d83a904aefc1d6ce93dc54b8-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/d736bb10d83a904aefc1d6ce93dc54b8-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/d736bb10d83a904aefc1d6ce93dc54b8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 262903,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5461520266114980882&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "University of Alberta; University of Alberta; University of Alberta",
        "aff_domain": "cs.ualberta.ca;cs.ualberta.ca;cs.ualberta.ca",
        "email": "cs.ualberta.ca;cs.ualberta.ca;cs.ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Alberta",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ualberta.ca",
        "aff_unique_abbr": "UAlberta",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "0c309aa6f1",
        "title": "Variational Gaussian Process Dynamical Systems",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/af4732711661056eadbf798ba191272a-Abstract.html",
        "author": "Andreas Damianou; Michalis K. Titsias; Neil D. Lawrence",
        "abstract": "High dimensional time series are endemic in applications of machine learning  such as robotics (sensor data), computational biology (gene expression data), vision   (video sequences) and graphics (motion capture data). Practical nonlinear  probabilistic approaches to this data are required. In this paper we introduce  the variational Gaussian process dynamical system. Our work builds on recent  variational approximations for Gaussian process latent variable models to allow  for nonlinear dimensionality reduction simultaneously with learning a dynamical  prior in the latent space. The approach also allows for the appropriate dimensionality   of the latent space to be automatically determined. We demonstrate the  model on a human motion capture data set and a series of high resolution video  sequences.",
        "bibtex": "@inproceedings{NIPS2011_af473271,\n author = {Damianou, Andreas and Titsias, Michalis and Lawrence, Neil},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Variational Gaussian Process Dynamical Systems},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/af4732711661056eadbf798ba191272a-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/af4732711661056eadbf798ba191272a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/af4732711661056eadbf798ba191272a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1602604,
        "gs_citation": 151,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18414217329544379439&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Department of Computer Science, University of Sheffield, UK+Sheffield Institute for Translational Neuroscience, University of Sheffield, UK; School of Computer Science, University of Manchester, UK; Department of Computer Science, University of Sheffield, UK+Sheffield Institute for Translational Neuroscience, University of Sheffield, UK",
        "aff_domain": "sheffield.ac.uk;gmail.com;dcs.shef.ac.uk",
        "email": "sheffield.ac.uk;gmail.com;dcs.shef.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2;0+1",
        "aff_unique_norm": "University of Sheffield;Sheffield Institute for Translational Neuroscience, University of Sheffield, UK;University of Manchester",
        "aff_unique_dep": "Department of Computer Science;;School of Computer Science",
        "aff_unique_url": "https://www.sheffield.ac.uk;;https://www.manchester.ac.uk",
        "aff_unique_abbr": "Sheffield;;UoM",
        "aff_campus_unique_index": ";1;",
        "aff_campus_unique": ";Manchester",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom;"
    },
    {
        "id": "e0a4532639",
        "title": "Variational Learning for Recurrent Spiking Networks",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/069059b7ef840f0c74a814ec9237b6ec-Abstract.html",
        "author": "Danilo J. Rezende; Daan Wierstra; Wulfram Gerstner",
        "abstract": "We derive a plausible learning rule updating the synaptic efficacies   for feedforward, feedback and lateral connections between observed and latent neurons.    Operating in the context of a generative model for distributions of spike sequences,    the learning mechanism is derived from variational inference principles. The synaptic plasticity rules found   are interesting in that they are strongly reminiscent of experimentally found results on Spike Time    Dependent Plasticity, and in that they differ for excitatory and inhibitory neurons. A simulation   confirms the method's applicability to learning both stationary and temporal spike patterns.",
        "bibtex": "@inproceedings{NIPS2011_069059b7,\n author = {Rezende, Danilo and Wierstra, Daan and Gerstner, Wulfram},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Variational Learning for Recurrent Spiking Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/069059b7ef840f0c74a814ec9237b6ec-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/069059b7ef840f0c74a814ec9237b6ec-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/069059b7ef840f0c74a814ec9237b6ec-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 3535803,
        "gs_citation": 51,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7783005405611828900&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Brain Mind Institute, \u00b4Ecole Polytechnique F \u00b4ed\u00b4erale de Lausanne; School of Computer and Communication Sciences, Brain Mind Institute, \u00b4Ecole Polytechnique F \u00b4ed\u00b4erale de Lausanne; School of Computer and Communication Sciences, Brain Mind Institute, \u00b4Ecole Polytechnique F \u00b4ed\u00b4erale de Lausanne",
        "aff_domain": "epfl.ch;epfl.ch;epfl.ch",
        "email": "epfl.ch;epfl.ch;epfl.ch",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Brain Mind Institute, \u00b4Ecole Polytechnique F \u00b4ed\u00b4erale de Lausanne;School of Computer and Communication Sciences, Brain Mind Institute, \u00b4Ecole Polytechnique F \u00b4ed\u00b4erale de Lausanne",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "89ed3a6001",
        "title": "Video Annotation and Tracking with Active Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/182be0c5cdcd5072bb1864cdee4d3d6e-Abstract.html",
        "author": "Carl Vondrick; Deva Ramanan",
        "abstract": "We introduce a novel active learning framework for video annotation. By  judiciously choosing which frames a user should annotate, we can obtain highly  accurate tracks with minimal user effort.  We cast this problem as one of  active learning, and show that we can obtain excellent performance by querying  frames that, if annotated, would produce a large expected change in the  estimated object track. We implement a constrained tracker and compute the  expected change for putative annotations with efficient dynamic programming  algorithms.  We demonstrate our framework on four datasets, including two  benchmark datasets constructed with key frame annotations obtained by Amazon  Mechanical Turk. Our results indicate that we could obtain equivalent labels  for a small fraction of the original cost.",
        "bibtex": "@inproceedings{NIPS2011_182be0c5,\n author = {Vondrick, Carl and Ramanan, Deva},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Video Annotation and Tracking with Active Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/182be0c5cdcd5072bb1864cdee4d3d6e-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/182be0c5cdcd5072bb1864cdee4d3d6e-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/182be0c5cdcd5072bb1864cdee4d3d6e-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/182be0c5cdcd5072bb1864cdee4d3d6e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1416837,
        "gs_citation": 161,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8340324322546804615&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "UC Irvine; UC Irvine",
        "aff_domain": "mit.edu;ics.uci.edu",
        "email": "mit.edu;ics.uci.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Irvine",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uci.edu",
        "aff_unique_abbr": "UCI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Irvine",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2607ea844f",
        "title": "Why The Brain Separates Face Recognition From Object Recognition",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/1be3bc32e6564055d5ca3e5a354acbef-Abstract.html",
        "author": "Joel Z. Leibo; Jim Mutch; Tomaso Poggio",
        "abstract": "Many studies have uncovered evidence that visual cortex contains specialized regions involved in processing faces but not other object classes.  Recent electrophysiology studies of cells in several of these specialized regions revealed that at least some of these regions are organized in a hierarchical manner with viewpoint-specific cells projecting to downstream viewpoint-invariant identity-specific cells (Freiwald and Tsao 2010).  A separate computational line of reasoning leads to the claim that some transformations of visual inputs that preserve viewed object identity are class-specific.  In particular, the 2D images evoked by a face undergoing a 3D rotation are not produced by the same image transformation (2D) that would produce the images evoked by an object of another class undergoing the same 3D rotation.  However, within the class of faces, knowledge of the image transformation evoked by 3D rotation can be reliably transferred from previously viewed faces to help identify a novel face at a new viewpoint.  We show, through computational simulations, that an architecture which applies this method of gaining invariance to class-specific transformations is effective when restricted to faces and fails spectacularly when applied across object classes.  We argue here that in order to accomplish viewpoint-invariant face identification from a single example view, visual cortex must separate the circuitry involved in discounting 3D rotations of faces from the generic circuitry involved in processing other objects.  The resulting model of the ventral stream of visual cortex is consistent with the recent physiology results showing the hierarchical organization of the face processing network.",
        "bibtex": "@inproceedings{NIPS2011_1be3bc32,\n author = {Leibo, Joel Z and Mutch, Jim and Poggio, Tomaso},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Why The Brain Separates Face Recognition From Object Recognition},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/1be3bc32e6564055d5ca3e5a354acbef-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/1be3bc32e6564055d5ca3e5a354acbef-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/1be3bc32e6564055d5ca3e5a354acbef-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1768546,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4486918873550650717&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Brainand Cognitive Sciences; Department of Brainand Cognitive Sciences; Department of Brainand Cognitive Sciences",
        "aff_domain": "mit.edu;mit.edu;ai.mit.edu",
        "email": "mit.edu;mit.edu;ai.mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Department of Brainand Cognitive Sciences",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "b3fd0e9d7f",
        "title": "k-NN Regression Adapts to Local Intrinsic Dimension",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/05f971b5ec196b8c65b75d2ef8267331-Abstract.html",
        "author": "Samory Kpotufe",
        "abstract": "Many nonparametric regressors were recently shown to converge at rates that depend only on the intrinsic dimension of data.  These regressors thus escape the curse of dimension when high-dimensional data has low intrinsic dimension (e.g. a manifold).   We show that $k$-NN regression is also adaptive to intrinsic dimension. In particular our rates are local to a query $x$   and depend only on the way masses of balls centered at $x$ vary with radius.    Furthermore, we show a simple way to choose $k = k(x)$ locally at any $x$ so as to nearly achieve the minimax rate at $x$  in terms of the unknown intrinsic dimension in the vicinity of $x$. We also establish that the minimax rate does not depend   on a particular choice of metric space or distribution, but rather that this minimax rate holds for any metric space and doubling measure.",
        "bibtex": "@inproceedings{NIPS2011_05f971b5,\n author = {Kpotufe, Samory},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {k-NN Regression Adapts to Local Intrinsic Dimension},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/05f971b5ec196b8c65b75d2ef8267331-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/05f971b5ec196b8c65b75d2ef8267331-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/05f971b5ec196b8c65b75d2ef8267331-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 110148,
        "gs_citation": 107,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1033006009781685357&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff": "Max Planck Institute for Intelligent Systems",
        "aff_domain": "tuebingen.mpg.de",
        "email": "tuebingen.mpg.de",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Max Planck Institute for Intelligent Systems",
        "aff_unique_dep": "Intelligent Systems",
        "aff_unique_url": "https://www.mpi-is.mpg.de",
        "aff_unique_abbr": "MPI-IS",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "3373e86d97",
        "title": "t-divergence Based Approximate Inference",
        "site": "https://papers.nips.cc/paper_files/paper/2011/hash/8c235f89a8143a28a1d6067e959dd858-Abstract.html",
        "author": "Nan Ding; Yuan Qi; S.v.n. Vishwanathan",
        "abstract": "Approximate inference is an important technique for dealing with large, intractable graphical models based on the exponential family of distributions. We extend the idea of approximate inference to the t-exponential family by defining a new t-divergence. This divergence measure is obtained via convex duality between the log-partition function of the t-exponential family and a new t-entropy. We illustrate our approach on the Bayes Point Machine with a Student's t-prior.",
        "bibtex": "@inproceedings{NIPS2011_8c235f89,\n author = {Ding, Nan and Qi, Yuan and Vishwanathan, S.v.n.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {t-divergence Based Approximate Inference},\n url = {https://proceedings.neurips.cc/paper_files/paper/2011/file/8c235f89a8143a28a1d6067e959dd858-Paper.pdf},\n volume = {24},\n year = {2011}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2011/file/8c235f89a8143a28a1d6067e959dd858-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2011/file/8c235f89a8143a28a1d6067e959dd858-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2011/file/8c235f89a8143a28a1d6067e959dd858-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 596933,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8469277327033729444&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Departments of Statistics; Departments of Computer Science; Departments of Statistics + Departments of Computer Science",
        "aff_domain": "purdue.edu;stat.purdue.edu;cs.purdue.edu",
        "email": "purdue.edu;stat.purdue.edu;cs.purdue.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0+0",
        "aff_unique_norm": "University Affiliation Not Specified",
        "aff_unique_dep": "Departments of Statistics",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    }
]