[
    {
        "id": "19b36a321d",
        "title": "A Bayesian Framework for Cross-Situational Word-Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/dd8eb9f23fbd362da0e3f4e70b878c16-Abstract.html",
        "author": "Noah Goodman; Joshua B. Tenenbaum; Michael J. Black",
        "abstract": "For infants, early word learning is a chicken-and-egg problem. One way to learn a word is to observe that it co-occurs with a particular referent across different situations. Another way is to use the social context of an utterance to infer the in- tended referent of a word. Here we present a Bayesian model of cross-situational word learning, and an extension of this model that also learns which social cues are relevant to determining reference. We test our model on a small corpus of mother-infant interaction and \ufb01nd it performs better than competing models. Fi- nally, we show that our model accounts for experimental phenomena including mutual exclusivity, fast-mapping, and generalization from social cues.",
        "bibtex": "@inproceedings{NIPS2007_dd8eb9f2,\n author = {Goodman, Noah and Tenenbaum, Joshua and Black, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Bayesian Framework for Cross-Situational Word-Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/dd8eb9f23fbd362da0e3f4e70b878c16-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/dd8eb9f23fbd362da0e3f4e70b878c16-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/dd8eb9f23fbd362da0e3f4e70b878c16-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/dd8eb9f23fbd362da0e3f4e70b878c16-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 338242,
        "gs_citation": 114,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17163359261492832712&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Brain and Cognitive Science, Massachusetts Institute of Technology; Department of Brain and Cognitive Science, Massachusetts Institute of Technology; Department of Brain and Cognitive Science, Massachusetts Institute of Technology",
        "aff_domain": "mit.edu;mit.edu;mit.edu",
        "email": "mit.edu;mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Department of Brain and Cognitive Science",
        "aff_unique_url": "https://www.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "07c2596e6a",
        "title": "A Bayesian LDA-based model for semi-supervised part-of-speech tagging",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/8065d07da4a77621450aa84fee5656d9-Abstract.html",
        "author": "Kristina Toutanova; Mark Johnson",
        "abstract": "We present a novel Bayesian model for semi-supervised part-of-speech tagging. Our model extends the Latent Dirichlet Allocation model and incorporates the intuition that words\u2019 distributions over tags, p(t|w), are sparse. In addition we in- troduce a model for determining the set of possible tags of a word which captures important dependencies in the ambiguity classes of words. Our model outper- forms the best previously proposed model for this task on a standard dataset.",
        "bibtex": "@inproceedings{NIPS2007_8065d07d,\n author = {Toutanova, Kristina and Johnson, Mark},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Bayesian LDA-based model for semi-supervised part-of-speech tagging},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8065d07da4a77621450aa84fee5656d9-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/8065d07da4a77621450aa84fee5656d9-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/8065d07da4a77621450aa84fee5656d9-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/8065d07da4a77621450aa84fee5656d9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 72999,
        "gs_citation": 154,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18219372595046794294&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Microsoft Research, Redmond, WA; Brown University, Providence, RI",
        "aff_domain": "microsoft.com;brown.edu",
        "email": "microsoft.com;brown.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Microsoft Research;Brown University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.microsoft.com/en-us/research;https://www.brown.edu",
        "aff_unique_abbr": "MSR;Brown",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Redmond;Providence",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a2d37a1abb",
        "title": "A Bayesian Model of Conditioned Perception",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/08fe2621d8e716b02ec0da35256a998d-Abstract.html",
        "author": "Alan Stocker; Eero P. Simoncelli",
        "abstract": "We propose an extended probabilistic model for human perception. We argue that in many circumstances, human observers simultaneously evaluate sensory evidence under different hypotheses regarding the underlying physical process that might have generated the sensory information. Within this context, inference can be optimal if the observer weighs each hypothesis according to the correct belief in that hypothesis. But if the observer commits to a particular hypothesis, the belief in that hypothesis is converted into subjective certainty, and subsequent perceptual behavior is suboptimal, conditioned only on the chosen hypothesis. We demonstrate that this framework can explain psychophysical data of a recently reported decision-estimation experiment. The model well accounts for the data, predicting the same estimation bias as a consequence of the preceding decision step. The power of the framework is that it has no free parameters except the degree of the observer's uncertainty about its internal sensory representation. All other parameters are defined by the particular experiment which allows us to make quantitative predictions of human perception to two modifications of the original experiment.",
        "bibtex": "@inproceedings{NIPS2007_08fe2621,\n author = {Stocker, Alan A and Simoncelli, Eero},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Bayesian Model of Conditioned Perception},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/08fe2621d8e716b02ec0da35256a998d-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/08fe2621d8e716b02ec0da35256a998d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/08fe2621d8e716b02ec0da35256a998d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 388321,
        "gs_citation": 163,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13457420551087306811&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 23,
        "aff": "Howard Hughes Medical Institute + Center for Neural Science + Courant Institute of Mathematical Sciences, New York University; Howard Hughes Medical Institute + Center for Neural Science + Courant Institute of Mathematical Sciences, New York University",
        "aff_domain": "nyu.edu;nyu.edu",
        "email": "nyu.edu;nyu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1+2;0+1+2",
        "aff_unique_norm": "Howard Hughes Medical Institute;Center for Neural Science;New York University",
        "aff_unique_dep": ";;Courant Institute of Mathematical Sciences",
        "aff_unique_url": "https://www.hhmi.org;;https://www.courant.nyu.edu",
        "aff_unique_abbr": "HHMI;;NYU",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";New York",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "e007da4f99",
        "title": "A Constraint Generation Approach to Learning Stable Linear Dynamical Systems",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/13f9896df61279c928f19721878fac41-Abstract.html",
        "author": "Byron Boots; Geoffrey J. Gordon; Sajid M. Siddiqi",
        "abstract": "Stability is a desirable characteristic for linear dynamical systems, but it is often ignored by algorithms that learn these systems from data. We propose a novel method for learning stable linear dynamical systems: we formulate an approxima- tion of the problem as a convex program, start with a solution to a relaxed version of the program, and incrementally add constraints to improve stability. Rather than continuing to generate constraints until we reach a feasible solution, we test stability at each step; because the convex program is only an approximation of the desired problem, this early stopping rule can yield a higher-quality solution. We apply our algorithm to the task of learning dynamic textures from image sequences as well as to modeling biosurveillance drug-sales data. The constraint generation approach leads to noticeable improvement in the quality of simulated sequences. We compare our method to those of Lacy and Bernstein [1, 2], with positive results in terms of accuracy, quality of simulated sequences, and ef\ufb01ciency.",
        "bibtex": "@inproceedings{NIPS2007_13f9896d,\n author = {Boots, Byron and Gordon, Geoffrey J and Siddiqi, Sajid},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Constraint Generation Approach to Learning Stable Linear Dynamical Systems},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/13f9896df61279c928f19721878fac41-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/13f9896df61279c928f19721878fac41-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/13f9896df61279c928f19721878fac41-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 707483,
        "gs_citation": 161,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8609597154724649422&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 30,
        "aff": "Robotics Institute, Carnegie-Mellon University, Pittsburgh, PA 15213; Computer Science Department, Carnegie-Mellon University, Pittsburgh, PA 15213; Machine Learning Department, Carnegie-Mellon University, Pittsburgh, PA 15213",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Robotics Institute, Carnegie-Mellon University, Pittsburgh, PA 15213;Computer Science Department, Carnegie-Mellon University, Pittsburgh, PA 15213;Machine Learning Department, Carnegie-Mellon University, Pittsburgh, PA 15213",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "c71b672659",
        "title": "A Game-Theoretic Approach to Apprenticeship Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/ca3ec598002d2e7662e2ef4bdd58278b-Abstract.html",
        "author": "Umar Syed; Robert E. Schapire",
        "abstract": "We study the problem of an apprentice learning to behave in an environment with an unknown reward function by observing the behavior of an expert. We follow on the work of Abbeel and Ng [1] who considered a framework in which the true reward function is assumed to be a linear combination of a set of known and observable features. We give a new algorithm that, like theirs, is guaranteed to learn a policy that is nearly as good as the expert's, given enough examples. However, unlike their algorithm, we show that ours may produce a policy that is substantially better than the expert's. Moreover, our algorithm is computationally faster, is easier to implement, and can be applied even in the absence of an expert. The method is based on a game-theoretic view of the problem, which leads naturally to a direct application of the multiplicative-weights algorithm of Freund and Schapire [2] for playing repeated matrix games. In addition to our formal presentation and analysis of the new algorithm, we sketch how the method can be applied when the transition function itself is unknown, and we provide an experimental demonstration of the algorithm on a toy video-game environment.",
        "bibtex": "@inproceedings{NIPS2007_ca3ec598,\n author = {Syed, Umar and Schapire, Robert E},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Game-Theoretic Approach to Apprenticeship Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/ca3ec598002d2e7662e2ef4bdd58278b-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/ca3ec598002d2e7662e2ef4bdd58278b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/ca3ec598002d2e7662e2ef4bdd58278b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/ca3ec598002d2e7662e2ef4bdd58278b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 87255,
        "gs_citation": 432,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9169076903256921462&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Computer Science Department, Princeton University; Computer Science Department, Princeton University",
        "aff_domain": "cs.princeton.edu;cs.princeton.edu",
        "email": "cs.princeton.edu;cs.princeton.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Princeton University",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.princeton.edu",
        "aff_unique_abbr": "Princeton",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Princeton",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f944ce74dc",
        "title": "A General Boosting Method and its Application to Learning Ranking Functions for Web Search",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/8d317bdcf4aafcfc22149d77babee96d-Abstract.html",
        "author": "Zhaohui Zheng; Hongyuan Zha; Tong Zhang; Olivier Chapelle; Keke Chen; Gordon Sun",
        "abstract": "We present a general boosting method extending functional gradient boosting to optimize complex loss functions that are encountered in many machine learning problems. Our approach is based on optimization of quadratic upper bounds of the loss functions which allows us to present a rigorous convergence analysis of the algorithm. More importantly, this general framework enables us to use a standard regression base learner such as decision trees for fitting any loss function. We illustrate an application of the proposed method in learning ranking functions for Web search by combining both preference data and labeled data for training. We present experimental results for Web search using data from a commercial search engine that show significant improvements of our proposed methods over some existing methods.",
        "bibtex": "@inproceedings{NIPS2007_8d317bdc,\n author = {Zheng, Zhaohui and Zha, Hongyuan and Zhang, Tong and Chapelle, Olivier and Chen, Keke and Sun, Gordon},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A General Boosting Method and its Application to Learning Ranking Functions for Web Search},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8d317bdcf4aafcfc22149d77babee96d-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/8d317bdcf4aafcfc22149d77babee96d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/8d317bdcf4aafcfc22149d77babee96d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 156790,
        "gs_citation": 267,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1922994593160453504&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 20,
        "aff": "Yahoo! Inc.; College of Computing+Yahoo! Inc.; Yahoo! Inc.; Yahoo! Inc.; Yahoo! Inc.; Yahoo! Inc.",
        "aff_domain": "yahoo-inc.com;cc.gatech.edu;yahoo-inc.com;yahoo-inc.com;yahoo-inc.com;yahoo-inc.com",
        "email": "yahoo-inc.com;cc.gatech.edu;yahoo-inc.com;yahoo-inc.com;yahoo-inc.com;yahoo-inc.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+0;0;0;0;0",
        "aff_unique_norm": "Yahoo! Inc.;College of Computing",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.yahoo.com;",
        "aff_unique_abbr": "Yahoo!;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "610184df86",
        "title": "A Kernel Statistical Test of Independence",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/d5cfead94f5350c12c322b5b664544c1-Abstract.html",
        "author": "Arthur Gretton; Kenji Fukumizu; Choon H. Teo; Le Song; Bernhard Sch\u00f6lkopf; Alex J. Smola",
        "abstract": "Although kernel measures of independence have been widely applied in machine learning (notably in kernel ICA), there is as yet no method to determine whether they have detected statistically signi\ufb01cant dependence. We provide a novel test of the independence hypothesis for one particular kernel independence measure, the Hilbert-Schmidt independence criterion (HSIC). The resulting test costs O(m2), where m is the sample size. We demonstrate that this test outperforms established contingency table and functional correlation-based tests, and that this advantage is greater for multivariate data. Finally, we show the HSIC test also applies to text (and to structured data more generally), for which no other independence test presently exists.",
        "bibtex": "@inproceedings{NIPS2007_d5cfead9,\n author = {Gretton, Arthur and Fukumizu, Kenji and Teo, Choon and Song, Le and Sch\\\"{o}lkopf, Bernhard and Smola, Alex},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Kernel Statistical Test of Independence},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d5cfead94f5350c12c322b5b664544c1-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/d5cfead94f5350c12c322b5b664544c1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/d5cfead94f5350c12c322b5b664544c1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 129480,
        "gs_citation": 1194,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17493582452889469995&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff": "MPI for Biological Cybernetics, T\u00fcbingen, Germany; Inst. of Statistical Mathematics, Tokyo, Japan; NICTA, ANU, Canberra, Australia; NICTA, ANU and University of Sydney; MPI for Biological Cybernetics, T\u00fcbingen, Germany; NICTA, ANU, Canberra, Australia",
        "aff_domain": "tuebingen.mpg.de;ism.ac.jp;gmail.com;it.usyd.edu.au;tuebingen.mpg.de;gmail.com",
        "email": "tuebingen.mpg.de;ism.ac.jp;gmail.com;it.usyd.edu.au;tuebingen.mpg.de;gmail.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3;0;2",
        "aff_unique_norm": "Max Planck Institute for Biological Cybernetics;Inst. of Statistical Mathematics, Tokyo, Japan;NICTA, ANU, Canberra, Australia;NICTA, ANU and University of Sydney",
        "aff_unique_dep": "Biological Cybernetics;;;",
        "aff_unique_url": "https://www.biological-cybernetics.de;;;",
        "aff_unique_abbr": "MPIBC;;;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "T\u00fcbingen;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany;"
    },
    {
        "id": "25b0f9ebea",
        "title": "A New View of Automatic Relevance Determination",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/9c01802ddb981e6bcfbec0f0516b8e35-Abstract.html",
        "author": "David P. Wipf; Srikantan S. Nagarajan",
        "abstract": "Automatic relevance determination (ARD), and the closely-related sparse Bayesian learning (SBL) framework, are effective tools for pruning large numbers of irrelevant features. However, popular update rules used for this process are either prohibitively slow in practice and/or heuristic in nature without proven convergence properties. This paper furnishes an alternative means of optimizing a general ARD cost function using an auxiliary function that can naturally be solved using a series of re-weighted L1 problems. The result is an efficient algorithm that can be implemented using standard convex programming toolboxes and is guaranteed to converge to a stationary point unlike existing methods. The analysis also leads to additional insights into the behavior of previous ARD updates as well as the ARD cost function. For example, the standard fixed-point updates of MacKay (1992) are shown to be iteratively solving a particular min-max problem, although they are not guaranteed to lead to a stationary point. The analysis also reveals that ARD is exactly equivalent to performing MAP estimation using a particular feature- and noise-dependent \\textit{non-factorial} weight prior with several desirable properties over conventional priors with respect to feature selection. In particular, it provides a tighter approximation to the L0 quasi-norm sparsity measure than the L1 norm. Overall these results suggests alternative cost functions and update procedures for selecting features and promoting sparse solutions.",
        "bibtex": "@inproceedings{NIPS2007_9c01802d,\n author = {Wipf, David and Nagarajan, Srikantan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A New View of Automatic Relevance Determination},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9c01802ddb981e6bcfbec0f0516b8e35-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/9c01802ddb981e6bcfbec0f0516b8e35-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/9c01802ddb981e6bcfbec0f0516b8e35-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 230554,
        "gs_citation": 480,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17851019603996477128&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "d93f0735de",
        "title": "A Probabilistic Approach to Language Change",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/7ce3284b743aefde80ffd9aec500e085-Abstract.html",
        "author": "Alexandre Bouchard-c\u00f4t\u00e9; Percy Liang; Dan Klein; Thomas L. Griffiths",
        "abstract": "We present a probabilistic approach to language change in which word forms are represented by phoneme sequences that undergo stochastic edits along the branches of a phylogenetic tree. Our framework combines the advantages of the classical comparative method with the robustness of corpus-based probabilistic models. We use this framework to explore the consequences of two different schemes for defining probabilistic models of phonological change, evaluating these schemes using the reconstruction of ancient word forms in Romance languages. The result is an efficient inference procedure for automatically inferring ancient word forms from modern languages, which can be generalized to support inferences about linguistic phylogenies.",
        "bibtex": "@inproceedings{NIPS2007_7ce3284b,\n author = {Bouchard-c\\^{o}t\\'{e}, Alexandre and Liang, Percy S and Klein, Dan and Griffiths, Thomas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Probabilistic Approach to Language Change},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/7ce3284b743aefde80ffd9aec500e085-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/7ce3284b743aefde80ffd9aec500e085-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/7ce3284b743aefde80ffd9aec500e085-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 221230,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=596679071902524288&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "157cc0311e",
        "title": "A Randomized Algorithm for Large Scale Support Vector Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/ba2fd310dcaa8781a9a652a31baf3c68-Abstract.html",
        "author": "Krishnan Kumar; Chiru Bhattacharya; Ramesh Hariharan",
        "abstract": "We propose a randomized algorithm for large scale SVM learning which solves the problem by iterating over random subsets of the data. Crucial to the algorithm for scalability is the size of the subsets chosen. In the context of text classification we show that, by using ideas from random projections, a sample size of O(log n) can be used to obtain a solution which is close to the optimal with a high probability. Experiments done on synthetic and real life data sets demonstrate that the algorithm scales up SVM learners, without loss in accuracy.",
        "bibtex": "@inproceedings{NIPS2007_ba2fd310,\n author = {Kumar, Krishnan and Bhattacharya, Chiru and Hariharan, Ramesh},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Randomized Algorithm for Large Scale Support Vector Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/ba2fd310dcaa8781a9a652a31baf3c68-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/ba2fd310dcaa8781a9a652a31baf3c68-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/ba2fd310dcaa8781a9a652a31baf3c68-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 94120,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6364997012243986934&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Computer Science and Automation, Indian Institute of Science, Bangalore-12; Department of Computer Science and Automation, Indian Institute of Science, Bangalore-12; Strand Genomics, Bangalore-80",
        "aff_domain": "csa.iisc.ernet.in;csa.iisc.ernet.in;strandls.com",
        "email": "csa.iisc.ernet.in;csa.iisc.ernet.in;strandls.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Department of Computer Science and Automation, Indian Institute of Science, Bangalore-12;Strand Genomics, Bangalore-80",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "6a39aa402d",
        "title": "A Risk Minimization Principle for a Class of Parzen Estimators",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/c6e19e830859f2cb9f7c8f8cacb8d2a6-Abstract.html",
        "author": "Kristiaan Pelckmans; Johan Suykens; Bart D. Moor",
        "abstract": "This paper explores the use of a Maximal Average Margin (MAM) optimality principle for the design of learning algorithms. It is shown that the application of this risk minimization principle results in a class of (computationally) simple learning machines similar to the classical Parzen window classifier. A direct relation with the Rademacher complexities is established, as such facilitating analysis and providing a notion of certainty of prediction. This analysis is related to Support Vector Machines by means of a margin transformation. The power of the MAM principle is illustrated further by application to ordinal regression tasks, resulting in an $O(n)$ algorithm able to process large datasets in reasonable time.",
        "bibtex": "@inproceedings{NIPS2007_c6e19e83,\n author = {Pelckmans, Kristiaan and Suykens, Johan and Moor, Bart},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Risk Minimization Principle for a Class of Parzen Estimators},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c6e19e830859f2cb9f7c8f8cacb8d2a6-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/c6e19e830859f2cb9f7c8f8cacb8d2a6-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/c6e19e830859f2cb9f7c8f8cacb8d2a6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 186442,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4839720122667217978&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Electrical Engineering (ESAT) - SCD/SISTA, K.U.Leuven University; Department of Electrical Engineering (ESAT) - SCD/SISTA, K.U.Leuven University; Department of Electrical Engineering (ESAT) - SCD/SISTA, K.U.Leuven University",
        "aff_domain": "esat.kuleuven.be; ; ",
        "email": "esat.kuleuven.be; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Department of Electrical Engineering (ESAT) - SCD/SISTA, K.U.Leuven University",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "d6c73f8a36",
        "title": "A Spectral Regularization Framework for Multi-Task Structure Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/9872ed9fc22fc182d371c3e9ed316094-Abstract.html",
        "author": "Andreas Argyriou; Massimiliano Pontil; Yiming Ying; Charles A. Micchelli",
        "abstract": "Learning the common structure shared by a set of supervised tasks is an important practical and theoretical problem. Knowledge of this structure may lead to bet- ter generalization performance on the tasks and may also facilitate learning new tasks. We propose a framework for solving this problem, which is based on reg- ularization with spectral functions of matrices. This class of regularization prob- lems exhibits appealing computational properties and can be optimized ef(cid:2)ciently by an alternating minimization algorithm. In addition, we provide a necessary and suf(cid:2)cient condition for convexity of the regularizer. We analyze concrete ex- amples of the framework, which are equivalent to regularization with Lp matrix norms. Experiments on two real data sets indicate that the algorithm scales well with the number of tasks and improves on state of the art statistical performance.",
        "bibtex": "@inproceedings{NIPS2007_9872ed9f,\n author = {Argyriou, Andreas and Pontil, Massimiliano and Ying, Yiming and Micchelli, Charles},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Spectral Regularization Framework for Multi-Task Structure Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9872ed9fc22fc182d371c3e9ed316094-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/9872ed9fc22fc182d371c3e9ed316094-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/9872ed9fc22fc182d371c3e9ed316094-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 110056,
        "gs_citation": 311,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9883314250669474908&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "Department of Computer Science, University College London; Department of Mathematics and Statistics, SUNY Albany; Department of Computer Science, University College London; Department of Engineering Mathematics, University of Bristol",
        "aff_domain": "cs.ucl.ac.uk; ;cs.ucl.ac.uk;bristol.ac.uk",
        "email": "cs.ucl.ac.uk; ;cs.ucl.ac.uk;bristol.ac.uk",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;2",
        "aff_unique_norm": "University College London;Department of Mathematics and Statistics, SUNY Albany;University of Bristol",
        "aff_unique_dep": "Department of Computer Science;;Department of Engineering Mathematics",
        "aff_unique_url": "https://www.ucl.ac.uk;;https://www.bristol.ac.uk",
        "aff_unique_abbr": "UCL;;UoB",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "London;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom;"
    },
    {
        "id": "3ea80de5fd",
        "title": "A Unified Near-Optimal Estimator For Dimension Reduction in $l_\\alpha$ ($0<\\alpha\\leq 2$) Using Stable Random Projections",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/6dfe08eda761bd321f8a9b239f6f4ec3-Abstract.html",
        "author": "Ping Li; Trevor J. Hastie",
        "abstract": "Many tasks (e.g., clustering) in machine learning only require the l\u03b1 distances in- stead of the original data. For dimension reductions in the l\u03b1 norm (0 < \u03b1 \u2264 2), the method of stable random projections can ef\ufb01ciently compute the l\u03b1 distances in massive datasets (e.g., the Web or massive data streams) in one pass of the data. The estimation task for stable random projections has been an interesting topic. We propose a simple estimator based on the fractional power of the samples (pro- jected data), which is surprisingly near-optimal in terms of the asymptotic vari- ance. In fact, it achieves the Cram\u00b4er-Rao bound when \u03b1 = 2 and \u03b1 = 0+. This new result will be useful when applying stable random projections to distance- based clustering, classi\ufb01cations, kernels, massive data streams etc.",
        "bibtex": "@inproceedings{NIPS2007_6dfe08ed,\n author = {Li, Ping and Hastie, Trevor},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A Unified Near-Optimal Estimator For Dimension Reduction in l\\_\\textbackslash alpha (0<\\textbackslash alpha\\textbackslash leq 2) Using Stable Random Projections},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/6dfe08eda761bd321f8a9b239f6f4ec3-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/6dfe08eda761bd321f8a9b239f6f4ec3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/6dfe08eda761bd321f8a9b239f6f4ec3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 178159,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "Department of Statistical Science, Faculty of Computing and Information Science, Cornell University; Department of Statistics, Department of Health, Research and Policy, Stanford University",
        "aff_domain": "cornell.edu;stanford.edu",
        "email": "cornell.edu;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Department of Statistical Science, Faculty of Computing and Information Science, Cornell University;Department of Statistics, Department of Health, Research and Policy, Stanford University",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "c06b6f26e2",
        "title": "A configurable analog VLSI neural network with spiking neurons and self-regulating plastic synapses",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/500e75a036dc2d7d2fec5da1b71d36cc-Abstract.html",
        "author": "Massimiliano Giulioni; Mario Pannunzi; Davide Badoni; Vittorio Dante; Paolo D. Giudice",
        "abstract": "We summarize the implementation of an analog VLSI chip hosting a network of 32 integrate-and-fire (IF) neurons with spike-frequency adaptation and 2,048 Hebbian plastic bistable spike-driven stochastic synapses endowed with a self-regulating mechanism which stops unnecessary synaptic changes. The synaptic matrix can be flexibly configured and provides both recurrent and AER-based connectivity with external, AER compliant devices. We demonstrate the ability of the network to efficiently classify overlapping patterns, thanks to the self-regulating mechanism.",
        "bibtex": "@inproceedings{NIPS2007_500e75a0,\n author = {Giulioni, Massimiliano and Pannunzi, Mario and Badoni, Davide and Dante, Vittorio and Giudice, Paolo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A configurable analog VLSI neural network with spiking neurons and self-regulating plastic synapses},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/500e75a036dc2d7d2fec5da1b71d36cc-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/500e75a036dc2d7d2fec5da1b71d36cc-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/500e75a036dc2d7d2fec5da1b71d36cc-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 98566,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3863165826132416145&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Italian National Inst. of Health, Rome, Italy+INFN-RM2, Rome, Italy; Italian National Inst. of Health, Rome, Italy+INFN-RM1, Rome, Italy; INFN-RM2, Rome, Italy; Italian National Inst. of Health, Rome, Italy+INFN-RM1, Rome, Italy; Italian National Inst. of Health, Rome, Italy+INFN-RM1, Rome, Italy",
        "aff_domain": "roma2.infn.it; ; ; ; ",
        "email": "roma2.infn.it; ; ; ; ",
        "github": "",
        "project": "http://neural.iss.infn.it/",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+2;1;0+2;0+2",
        "aff_unique_norm": "Italian National Inst. of Health, Rome, Italy;INFN-RM2, Rome, Italy;INFN-RM1, Rome, Italy",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": ";;;",
        "aff_country_unique": ""
    },
    {
        "id": "bd0ebb47fb",
        "title": "A general agnostic active learning algorithm",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/8f85517967795eeef66c225f7883bdcb-Abstract.html",
        "author": "Sanjoy Dasgupta; Daniel J. Hsu; Claire Monteleoni",
        "abstract": "We present an agnostic active learning algorithm for any hypothesis class of bounded VC dimension under arbitrary data distributions. Most previ- ous work on active learning either makes strong distributional assumptions, or else is computationally prohibitive. Our algorithm extends the simple scheme of Cohn, Atlas, and Ladner [1] to the agnostic setting, using re- ductions to supervised learning that harness generalization bounds in a simple but subtle manner. We provide a fall-back guarantee that bounds the algorithm\u2019s label complexity by the agnostic PAC sample complexity. Our analysis yields asymptotic label complexity improvements for certain hypothesis classes and distributions. We also demonstrate improvements experimentally.",
        "bibtex": "@inproceedings{NIPS2007_8f855179,\n author = {Dasgupta, Sanjoy and Hsu, Daniel J and Monteleoni, Claire},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A general agnostic active learning algorithm},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8f85517967795eeef66c225f7883bdcb-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/8f85517967795eeef66c225f7883bdcb-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/8f85517967795eeef66c225f7883bdcb-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 173921,
        "gs_citation": 404,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4098481950826564905&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "U C San Diego; UC San Diego; UC San Diego",
        "aff_domain": "cs.ucsd.edu;cs.ucsd.edu;cs.ucsd.edu",
        "email": "cs.ucsd.edu;cs.ucsd.edu;cs.ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "U C San Diego;University of California, San Diego",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://www.ucsd.edu",
        "aff_unique_abbr": ";UCSD",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";San Diego",
        "aff_country_unique_index": "1;1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "313d8ea8de",
        "title": "A learning framework for nearest neighbor search",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/0d7de1aca9299fe63f3e0041f02638a3-Abstract.html",
        "author": "Lawrence Cayton; Sanjoy Dasgupta",
        "abstract": "Can we leverage learning techniques to build a fast nearest-neighbor (NN) retrieval data structure? We present a general learning framework for the NN problem in which sample queries are used to learn the parameters of a data structure that minimize the retrieval time and/or the miss rate. We explore the potential of this novel framework through two popular NN data structures: KD-trees and the rectilinear structures employed by locality sensitive hashing. We derive a generalization theory for these data structure classes and present simple learning algorithms for both. Experimental results reveal that learning often improves on the already strong performance of these data structures.",
        "bibtex": "@inproceedings{NIPS2007_0d7de1ac,\n author = {Cayton, Lawrence and Dasgupta, Sanjoy},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A learning framework for nearest neighbor search},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/0d7de1aca9299fe63f3e0041f02638a3-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/0d7de1aca9299fe63f3e0041f02638a3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/0d7de1aca9299fe63f3e0041f02638a3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 828741,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5333746177766907390&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Department of Computer Science, University of California, San Diego; Department of Computer Science, University of California, San Diego",
        "aff_domain": "cs.ucsd.edu;cs.ucsd.edu",
        "email": "cs.ucsd.edu;cs.ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f1c0fedd2f",
        "title": "A neural network implementing optimal state estimation based on dynamic spike train decoding",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/e44fea3bec53bcea3b7513ccef5857ac-Abstract.html",
        "author": "Omer Bobrowski; Ron Meir; Shy Shoham; Yonina Eldar",
        "abstract": "It is becoming increasingly evident that organisms acting in uncertain dynamical environments often employ exact or approximate Bayesian statistical calculations in order to continuously estimate the environmental state, integrate information from multiple sensory modalities, form predictions and choose actions. What is less clear is how these putative computations are implemented by cortical neural networks. An additional level of complexity is introduced because these networks observe the world through spike trains received from primary sensory afferents, rather than directly. A recent line of research has described mechanisms by which such computations can be implemented using a network of neurons whose activ- ity directly represents a probability distribution across the possible \u201cworld states\u201d. Much of this work, however, uses various approximations, which severely re- strict the domain of applicability of these implementations. Here we make use of rigorous mathematical results from the theory of continuous time point process \ufb01ltering, and show how optimal real-time state estimation and prediction may be implemented in a general setting using linear neural networks. We demonstrate the applicability of the approach with several examples, and relate the required network properties to the statistical nature of the environment, thereby quantify- ing the compatibility of a given network with its environment.",
        "bibtex": "@inproceedings{NIPS2007_e44fea3b,\n author = {Bobrowski, Omer and Meir, Ron and Shoham, Shy and Eldar, Yonina},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A neural network implementing optimal state estimation based on dynamic spike train decoding},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/e44fea3bec53bcea3b7513ccef5857ac-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/e44fea3bec53bcea3b7513ccef5857ac-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/e44fea3bec53bcea3b7513ccef5857ac-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 186122,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11010874031967338018&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Department of Electrical Engineering; Department of Electrical Engineering; Department of Biomedical Engineering; Department of Electrical Engineering",
        "aff_domain": "tx.technion.ac.il;ee.technion.ac.il;bm.technion.ac.il;ee.technion.ac.il",
        "email": "tx.technion.ac.il;ee.technion.ac.il;bm.technion.ac.il;ee.technion.ac.il",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Institution not specified;Department of Biomedical Engineering",
        "aff_unique_dep": "Department of Electrical Engineering;Biomedical Engineering",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "cf80c13c2d",
        "title": "A probabilistic model for generating realistic lip movements from speech",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/c203d8a151612acf12457e4d67635a95-Abstract.html",
        "author": "Gwenn Englebienne; Tim Cootes; Magnus Rattray",
        "abstract": "The present work aims to model the correspondence between facial motion and speech. The face and sound are modelled separately, with phonemes being the link between both. We propose a sequential model and evaluate its suitability for the generation of the facial animation from a sequence of phonemes, which we obtain from speech. We evaluate the results both by computing the error between generated sequences and real video, as well as with a rigorous double-blind test with human subjects. Experiments show that our model compares favourably to other existing methods and that the sequences generated are comparable to real video sequences.",
        "bibtex": "@inproceedings{NIPS2007_c203d8a1,\n author = {Englebienne, Gwenn and Cootes, Tim and Rattray, Magnus},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {A probabilistic model for generating realistic lip movements from speech},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c203d8a151612acf12457e4d67635a95-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/c203d8a151612acf12457e4d67635a95-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/c203d8a151612acf12457e4d67635a95-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 161738,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8734114095945615076&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "School of Computer Science, University of Manchester; Imaging Science and Biomedical Engineering, University of Manchester; School of Computer Science, University of Manchester",
        "aff_domain": "cs.man.ac.uk;manchester.ac.uk;manchester.ac.uk",
        "email": "cs.man.ac.uk;manchester.ac.uk;manchester.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Manchester;Imaging Science and Biomedical Engineering, University of Manchester",
        "aff_unique_dep": "School of Computer Science;",
        "aff_unique_url": "https://www.manchester.ac.uk;",
        "aff_unique_abbr": "UoM;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Manchester;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom;"
    },
    {
        "id": "dc0eefc97b",
        "title": "Active Preference Learning with Discrete Choice Data",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/b6a1085a27ab7bff7550f8a3bd017df8-Abstract.html",
        "author": "Brochu Eric; Nando D. Freitas; Abhijeet Ghosh",
        "abstract": "We propose an active learning algorithm that learns a continuous valuation model from discrete preferences. The algorithm automatically decides what items are best presented to an individual in order to find the item that they value highly in as few trials as possible, and exploits quirks of human psychology to minimize time and cognitive burden. To do this, our algorithm maximizes the expected improvement at each query without accurately modelling the entire valuation surface, which would be needlessly expensive. The problem is particularly difficult because the space of choices is infinite. We demonstrate the effectiveness of the new algorithm compared to related active learning methods. We also embed the algorithm within a decision making tool for assisting digital artists in rendering materials. The tool finds the best parameters while minimizing the number of queries.",
        "bibtex": "@inproceedings{NIPS2007_b6a1085a,\n author = {Eric, Brochu and Freitas, Nando and Ghosh, Abhijeet},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Active Preference Learning with Discrete Choice Data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/b6a1085a27ab7bff7550f8a3bd017df8-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/b6a1085a27ab7bff7550f8a3bd017df8-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/b6a1085a27ab7bff7550f8a3bd017df8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1320194,
        "gs_citation": 252,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6342005183979507443&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science, University of British Columbia, Vancouver, BC, Canada; Department of Computer Science, University of British Columbia, Vancouver, BC, Canada; Department of Computer Science, University of British Columbia, Vancouver, BC, Canada",
        "aff_domain": "cs.ubc.ca;cs.ubc.ca;cs.ubc.ca",
        "email": "cs.ubc.ca;cs.ubc.ca;cs.ubc.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of British Columbia",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ubc.ca",
        "aff_unique_abbr": "UBC",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Vancouver",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "d24e336836",
        "title": "Adaptive Embedded Subgraph Algorithms using Walk-Sum Analysis",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/5737034557ef5b8c02c0e46513b98f90-Abstract.html",
        "author": "Venkat Chandrasekaran; Alan S. Willsky; Jason K. Johnson",
        "abstract": "We consider the estimation problem in Gaussian graphical models with arbitrary structure. We analyze the Embedded Trees algorithm, which solves a sequence of problems on tractable subgraphs thereby leading to the solution of the estimation problem on an intractable graph. Our analysis is based on the recently developed walk-sum interpretation of Gaussian estimation. We show that non-stationary iterations of the Embedded Trees algorithm using any sequence of subgraphs converge in walk-summable models. Based on walk-sum calculations, we develop adaptive methods that optimize the choice of subgraphs used at each iteration with a view to achieving maximum reduction in error. These adaptive procedures provide a significant speedup in convergence over stationary iterative methods, and also appear to converge in a larger class of models.",
        "bibtex": "@inproceedings{NIPS2007_57370345,\n author = {Chandrasekaran, Venkat and Willsky, Alan and Johnson, Jason},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Adaptive Embedded Subgraph Algorithms using Walk-Sum Analysis},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/5737034557ef5b8c02c0e46513b98f90-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/5737034557ef5b8c02c0e46513b98f90-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/5737034557ef5b8c02c0e46513b98f90-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 218140,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8758222668080127419&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Electrical Engineering and Computer Science; Department of Electrical Engineering and Computer Science; Department of Electrical Engineering and Computer Science",
        "aff_domain": "mit.edu;mit.edu;mit.edu",
        "email": "mit.edu;mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Department of Electrical Engineering and Computer Science",
        "aff_unique_url": "https://web.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "6226e20616",
        "title": "Adaptive Online Gradient Descent",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/261b909dfbee5a1a09d5eb50ed7a17e0-Abstract.html",
        "author": "Peter L. Bartlett; Elad Hazan; Alexander Rakhlin",
        "abstract": "We study the rates of growth of the regret in online convex optimization. First, we show that a simple extension of the algorithm of Hazan et al eliminates the need for a priori knowledge of the lower bound on the second derivatives of the observed functions. We then provide an algorithm, Adaptive Online Gradient Descent, which interpolates between the results of Zinkevich for linear functions and of Hazan et al for strongly convex functions, achieving intermediate rates T and log T . Furthermore, we show strong optimality of the algorithm. between Finally, we provide an extension of our results to general norms.",
        "bibtex": "@inproceedings{NIPS2007_261b909d,\n author = {Bartlett, Peter and Hazan, Elad and Rakhlin, Alexander},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Adaptive Online Gradient Descent},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/261b909dfbee5a1a09d5eb50ed7a17e0-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/261b909dfbee5a1a09d5eb50ed7a17e0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/261b909dfbee5a1a09d5eb50ed7a17e0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 173808,
        "gs_citation": 300,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9180194167972832791&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff": "Division of Computer Science, Department of Statistics, UC Berkeley; IBM Almaden Research Center; Division of Computer Science, UC Berkeley",
        "aff_domain": "cs.berkeley.edu;us.ibm.com;cs.berkeley.edu",
        "email": "cs.berkeley.edu;us.ibm.com;cs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Division of Computer Science, Department of Statistics, UC Berkeley;IBM;Division of Computer Science, UC Berkeley",
        "aff_unique_dep": ";Research Center;",
        "aff_unique_url": ";https://www.ibm.com/research;",
        "aff_unique_abbr": ";IBM;",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Almaden",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "42ab63aa66",
        "title": "Agreement-Based Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/dbe272bab69f8e13f14b405e038deb64-Abstract.html",
        "author": "Percy Liang; Dan Klein; Michael I. Jordan",
        "abstract": "The learning of probabilistic models with many hidden variables and non- decomposable dependencies is an important and challenging problem. In contrast to traditional approaches based on approximate inference in a single intractable model, our approach is to train a set of tractable submodels by encouraging them to agree on the hidden variables. This allows us to capture non-decomposable aspects of the data while still maintaining tractability. We propose an objective function for our approach, derive EM-style algorithms for parameter estimation, and demonstrate their effectiveness on three challenging real-world learning tasks.",
        "bibtex": "@inproceedings{NIPS2007_dbe272ba,\n author = {Liang, Percy S and Klein, Dan and Jordan, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Agreement-Based Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/dbe272bab69f8e13f14b405e038deb64-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/dbe272bab69f8e13f14b405e038deb64-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/dbe272bab69f8e13f14b405e038deb64-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 238855,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4086147144958267502&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Computer Science Division, University of California, Berkeley, CA 94720; Computer Science Division, University of California, Berkeley, CA 94720; Computer Science Division, University of California, Berkeley, CA 94720",
        "aff_domain": "cs.berkeley.edu;cs.berkeley.edu;cs.berkeley.edu",
        "email": "cs.berkeley.edu;cs.berkeley.edu;cs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "Computer Science Division",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "92ca1fd623",
        "title": "An Analysis of Convex Relaxations for MAP Estimation",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/0084ae4bc24c0795d1e6a4f58444d39b-Abstract.html",
        "author": "Pawan Mudigonda; Vladimir Kolmogorov; Philip Torr",
        "abstract": "The problem of obtaining the maximum a posteriori estimate of a general discrete random field (i.e. a random field defined using a finite and discrete set of labels) is known to be N P-hard. However, due to its central importance in many applications, several approximate algorithms have been proposed in the literature. In this paper, we present an analysis of three such algorithms based on convex relaxations: (i) L P - S: the linear programming (L P) relaxation proposed by Schlesinger [20] for a special case and independently in [4, 12, 23] for the general case; (ii) Q P - R L: the quadratic programming (Q P) relaxation by Ravikumar and Lafferty [18]; and (iii) S O C P - M S: the second order cone programming (S O C P) relaxation first proposed by Muramatsu and Suzuki [16] for two label problems and later extended in [14] for a general label set. We show that the S O C P - M S and the Q P - R L relaxations are equivalent. Furthermore, we prove that despite the flexibility in the form of the constraints/objective function offered by Q P and S O C P, the L P - S relaxation strictly dominates (i.e. provides a better approximation than) Q P - R L and S O C P - M S. We generalize these results by defining a large class of S O C P (and equivalent Q P) relaxations which is dominated by the L P - S relaxation. Based on these results we propose some novel S O C P relaxations which strictly dominate the previous approaches.",
        "bibtex": "@inproceedings{NIPS2007_0084ae4b,\n author = {Mudigonda, Pawan and Kolmogorov, Vladimir and Torr, Philip},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {An Analysis of Convex Relaxations for MAP Estimation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/0084ae4bc24c0795d1e6a4f58444d39b-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/0084ae4bc24c0795d1e6a4f58444d39b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/0084ae4bc24c0795d1e6a4f58444d39b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/0084ae4bc24c0795d1e6a4f58444d39b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 98609,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11282246132240094909&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Dept. of Computing, Oxford Brookes University; Computer Science Dept., University College London; Dept. of Computing, Oxford Brookes University",
        "aff_domain": "brookes.ac.uk;adastral.ucl.ac.uk;brookes.ac.uk",
        "email": "brookes.ac.uk;adastral.ucl.ac.uk;brookes.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Dept. of Computing, Oxford Brookes University;Computer Science Dept., University College London",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "41b62a3662",
        "title": "An Analysis of Inference with the Universum",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/a8e864d04c95572d1aece099af852d0a-Abstract.html",
        "author": "Olivier Chapelle; Alekh Agarwal; Fabian H. Sinz; Bernhard Sch\u00f6lkopf",
        "abstract": "We study a pattern classi\ufb01cation algorithm which has recently been proposed by Vapnik and coworkers. It builds on a new inductive principle which assumes that in addition to positive and negative data, a third class of data is available, termed the Universum. We assay the behavior of the algorithm by establishing links with Fisher discriminant analysis and oriented PCA, as well as with an SVM in a pro- jected subspace (or, equivalently, with a data-dependent reduced kernel). We also provide experimental results.",
        "bibtex": "@inproceedings{NIPS2007_a8e864d0,\n author = {Chapelle, Olivier and Agarwal, Alekh and Sinz, Fabian and Sch\\\"{o}lkopf, Bernhard},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {An Analysis of Inference with the Universum},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a8e864d04c95572d1aece099af852d0a-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/a8e864d04c95572d1aece099af852d0a-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/a8e864d04c95572d1aece099af852d0a-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/a8e864d04c95572d1aece099af852d0a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 229665,
        "gs_citation": 128,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16617770744633796715&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Max Planck Institute for biological Cybernetics; Yahoo! Research; University of California Berkeley; Max Planck Institute for biological Cybernetics",
        "aff_domain": "tuebingen.mpg.de;yahoo-inc.com;eecs.berkeley.edu;tuebingen.mpg.de",
        "email": "tuebingen.mpg.de;yahoo-inc.com;eecs.berkeley.edu;tuebingen.mpg.de",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Max Planck Institute for biological Cybernetics;Yahoo!;University of California, Berkeley",
        "aff_unique_dep": ";Yahoo! Research;",
        "aff_unique_url": ";https://research.yahoo.com;https://www.berkeley.edu",
        "aff_unique_abbr": ";Yahoo!;UC Berkeley",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "1;1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "3d41835750",
        "title": "An in-silico Neural Model of Dynamic Routing through Neuronal Coherence",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/8f1d43620bc6bb580df6e80b0dc05c48-Abstract.html",
        "author": "Devarajan Sridharan; Brian Percival; John Arthur; Kwabena A. Boahen",
        "abstract": "We describe a neurobiologically plausible model to implement dynamic routing using the concept of neuronal communication through neuronal coherence. The model has a three-tier architecture: a raw input tier, a routing control tier, and an invariant output tier. The correct mapping between input and output tiers is re- alized by an appropriate alignment of the phases of their respective background oscillations by the routing control units. We present an example architecture, im- plemented on a neuromorphic chip, that is able to achieve circular-shift invariance. A simple extension to our model can accomplish circular-shift dynamic routing with only O(N) connections, compared to O(N 2) connections required by tradi- tional models.",
        "bibtex": "@inproceedings{NIPS2007_8f1d4362,\n author = {Sridharan, Devarajan and Percival, Brian and Arthur, John and Boahen, Kwabena A},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {An in-silico Neural Model of Dynamic Routing through Neuronal Coherence},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8f1d43620bc6bb580df6e80b0dc05c48-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/8f1d43620bc6bb580df6e80b0dc05c48-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/8f1d43620bc6bb580df6e80b0dc05c48-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 340284,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12250258939593910683&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Program in Neurosciences + Department of Electrical Engineering; Department of Electrical Engineering + Department of Bioengineering; Department of Bioengineering; Department of Bioengineering",
        "aff_domain": "stanford.edu;stanford.edu;stanford.edu;stanford.edu",
        "email": "stanford.edu;stanford.edu;stanford.edu;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1+2;2;2",
        "aff_unique_norm": "Program in Neurosciences;Institution not specified;Department of Bioengineering",
        "aff_unique_dep": ";Department of Electrical Engineering;Bioengineering",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": ";",
        "aff_country_unique": ""
    },
    {
        "id": "9f8b04c0bb",
        "title": "An online Hebbian learning rule that performs Independent Component Analysis",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/149e9677a5989fd342ae44213df68868-Abstract.html",
        "author": "Claudia Clopath; Andr\u00e9 Longtin; Wulfram Gerstner",
        "abstract": "Independent component analysis (ICA) is a powerful method to decouple signals. Most of the algorithms performing ICA do not consider the temporal correlations of the signal, but only higher moments of its amplitude distribution. Moreover, they require some preprocessing of the data (whitening) so as to remove second order correlations. In this paper, we are interested in understanding the neural mechanism responsible for solving ICA. We present an online learning rule that exploits delayed correlations in the input. This rule performs ICA by detecting joint variations in the firing rates of pre- and postsynaptic neurons, similar to a local rate-based Hebbian learning rule.",
        "bibtex": "@inproceedings{NIPS2007_149e9677,\n author = {Clopath, Claudia and Longtin, Andr\\'{e} and Gerstner, Wulfram},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {An online Hebbian learning rule that performs Independent Component Analysis},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/149e9677a5989fd342ae44213df68868-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/149e9677a5989fd342ae44213df68868-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/149e9677a5989fd342ae44213df68868-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/149e9677a5989fd342ae44213df68868-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 536133,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6113230856725525550&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 22,
        "aff": "School of Computer Science and Brain Mind Institute, Ecole polytechnique federale de Lausanne; Center for Neural Dynamics, University of Ottawa; School of Computer Science and Brain Mind Institute, Ecole polytechnique federale de Lausanne",
        "aff_domain": "epfl.ch;uottawa.ca;epfl.ch",
        "email": "epfl.ch;uottawa.ca;epfl.ch",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "School of Computer Science and Brain Mind Institute, Ecole polytechnique federale de Lausanne;Center for Neural Dynamics, University of Ottawa",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "3d692784d0",
        "title": "Anytime Induction of Cost-sensitive Trees",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/74db120f0a8e5646ef5a30154e9f6deb-Abstract.html",
        "author": "Saher Esmeir; Shaul Markovitch",
        "abstract": "Machine learning techniques are increasingly being used to produce a wide-range of classi\ufb01ers for complex real-world applications that involve nonuniform testing costs and misclassi\ufb01cation costs. As the complexity of these applications grows, the management of resources during the learning and classi\ufb01cation processes be- comes a challenging task. In this work we introduce ACT (Anytime Cost-sensitive Trees), a novel framework for operating in such environments. ACT is an anytime algorithm that allows trading computation time for lower classi\ufb01cation costs. It builds a tree top-down and exploits additional time resources to obtain better esti- mations for the utility of the different candidate splits. Using sampling techniques ACT approximates for each candidate split the cost of the subtree under it and fa- vors the one with a minimal cost. Due to its stochastic nature ACT is expected to be able to escape local minima, into which greedy methods may be trapped. Ex- periments with a variety of datasets were conducted to compare the performance of ACT to that of the state of the art cost-sensitive tree learners. The results show that for most domains ACT produces trees of signi\ufb01cantly lower costs. ACT is also shown to exhibit good anytime behavior with diminishing returns.",
        "bibtex": "@inproceedings{NIPS2007_74db120f,\n author = {Esmeir, Saher and Markovitch, Shaul},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Anytime Induction of Cost-sensitive Trees},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/74db120f0a8e5646ef5a30154e9f6deb-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/74db120f0a8e5646ef5a30154e9f6deb-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/74db120f0a8e5646ef5a30154e9f6deb-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 109676,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=63853430116056117&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Computer Science Department, Technion\u2014Israel Institute of Technology, Haifa 32000, Israel; Computer Science Department, Technion\u2014Israel Institute of Technology, Haifa 32000, Israel",
        "aff_domain": "cs.technion.ac.il;cs.technion.ac.il",
        "email": "cs.technion.ac.il;cs.technion.ac.il",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Computer Science Department, Technion\u2014Israel Institute of Technology, Haifa 32000, Israel",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "14c427729c",
        "title": "Augmented Functional Time Series Representation and Forecasting with Gaussian Processes",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/81e74d678581a3bb7a720b019f4f1a93-Abstract.html",
        "author": "Nicolas Chapados; Yoshua Bengio",
        "abstract": "We introduce a functional representation of time series which allows forecasts to be performed over an unspeci\ufb01ed horizon with progressively-revealed informa- tion sets. By virtue of using Gaussian processes, a complete covariance matrix between forecasts at several time-steps is available. This information is put to use in an application to actively trade price spreads between commodity futures con- tracts. The approach delivers impressive out-of-sample risk-adjusted returns after transaction costs on a portfolio of 30 spreads.",
        "bibtex": "@inproceedings{NIPS2007_81e74d67,\n author = {Chapados, Nicolas and Bengio, Yoshua},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Augmented Functional Time Series Representation and Forecasting with Gaussian Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/81e74d678581a3bb7a720b019f4f1a93-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/81e74d678581a3bb7a720b019f4f1a93-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/81e74d678581a3bb7a720b019f4f1a93-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/81e74d678581a3bb7a720b019f4f1a93-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 318437,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=284308023034315564&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Science and Operations Research, University of Montr \u00b4eal; Department of Computer Science and Operations Research, University of Montr \u00b4eal",
        "aff_domain": "iro.umontreal.ca;iro.umontreal.ca",
        "email": "iro.umontreal.ca;iro.umontreal.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Department of Computer Science and Operations Research, University of Montr \u00b4eal",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "ef6095c435",
        "title": "Automatic Generation of Social Tags for Music Recommendation",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/c2aee86157b4a40b78132f1e71a9e6f1-Abstract.html",
        "author": "Douglas Eck; Paul Lamere; Thierry Bertin-mahieux; Stephen Green",
        "abstract": "Social tags are user-generated keywords associated with some resource on the Web. In the case of music, social tags have become an important component of Web2.0\" recommender systems, allowing users to generate playlists based on use-dependent terms such as \"chill\" or \"jogging\" that have been applied to particular songs. In this paper, we propose a method for predicting these social tags directly from MP3 files. Using a set of boosted classifiers, we map audio features onto social tags collected from the Web. The resulting automatic tags (or \"autotags\") furnish information about music that is otherwise untagged or poorly tagged, allowing for insertion of previously unheard music into a social recommender. This avoids the ''cold-start problem'' common in such systems. Autotags can also be used to smooth the tag space from which similarities and recommendations are made by providing a set of comparable baseline tags for all tracks in a recommender system.\"",
        "bibtex": "@inproceedings{NIPS2007_c2aee861,\n author = {Eck, Douglas and Lamere, Paul and Bertin-mahieux, Thierry and Green, Stephen},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Automatic Generation of Social Tags for Music Recommendation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c2aee86157b4a40b78132f1e71a9e6f1-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/c2aee86157b4a40b78132f1e71a9e6f1-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/c2aee86157b4a40b78132f1e71a9e6f1-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/c2aee86157b4a40b78132f1e71a9e6f1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 160418,
        "gs_citation": 312,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15178033571005052628&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "Sun Labs, Sun Microsystems, Burlington, Mass, USA + Dept. of Computer Science, Univ. of Montreal, Montreal, Canada; Sun Labs, Sun Microsystems, Burlington, Mass, USA; Sun Labs, Sun Microsystems, Burlington, Mass, USA + Dept. of Computer Science, Univ. of Montreal, Montreal, Canada; Sun Labs, Sun Microsystems, Burlington, Mass, USA",
        "aff_domain": "umontreal.ca;sun.com;iro.umontreal.ca;sun.com",
        "email": "umontreal.ca;sun.com;iro.umontreal.ca;sun.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;0+1;0",
        "aff_unique_norm": "Sun Labs, Sun Microsystems, Burlington, Mass, USA;Dept. of Computer Science, Univ. of Montreal, Montreal, Canada",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": ";",
        "aff_country_unique": ""
    },
    {
        "id": "b9f1f6d07b",
        "title": "Bayes-Adaptive POMDPs",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/3b3dbaf68507998acd6a5a5254ab2d76-Abstract.html",
        "author": "Stephane Ross; Brahim Chaib-draa; Joelle Pineau",
        "abstract": "Bayesian Reinforcement Learning has generated substantial interest recently, as it provides an elegant solution to the exploration-exploitation trade-off in reinforce- ment learning. However most investigations of Bayesian reinforcement learning to date focus on the standard Markov Decision Processes (MDPs). Our goal is to extend these ideas to the more general Partially Observable MDP (POMDP) framework, where the state is a hidden variable. To address this problem, we in- troduce a new mathematical model, the Bayes-Adaptive POMDP. This new model allows us to (1) improve knowledge of the POMDP domain through interaction with the environment, and (2) plan optimal sequences of actions which can trade- off between improving the model, identifying the state, and gathering reward. We show how the model can be \ufb01nitely approximated while preserving the value func- tion. We describe approximations for belief tracking and planning in this model. Empirical results on two domains show that the model estimate and agent\u2019s return improve over time, as the agent learns better model estimates.",
        "bibtex": "@inproceedings{NIPS2007_3b3dbaf6,\n author = {Ross, Stephane and Chaib-draa, Brahim and Pineau, Joelle},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Bayes-Adaptive POMDPs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/3b3dbaf68507998acd6a5a5254ab2d76-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/3b3dbaf68507998acd6a5a5254ab2d76-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/3b3dbaf68507998acd6a5a5254ab2d76-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 107333,
        "gs_citation": 252,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9878061726915412180&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "McGill University, Montr\u00b4eal, Qc, Canada; Laval University, Qu\u00b4ebec, Qc, Canada; McGill University, Montr\u00b4eal, Qc, Canada",
        "aff_domain": "cs.mcgill.ca;ift.ulaval.ca;cs.mcgill.ca",
        "email": "cs.mcgill.ca;ift.ulaval.ca;cs.mcgill.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "McGill University, Montr\u00b4eal, Qc, Canada;Laval University, Qu\u00b4ebec, Qc, Canada",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "350b9b931c",
        "title": "Bayesian Agglomerative Clustering with Coalescents",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/2ca65f58e35d9ad45bf7f3ae5cfd08f1-Abstract.html",
        "author": "Yee W. Teh; Hal Daume III; Daniel M. Roy",
        "abstract": "We introduce a new Bayesian model for hierarchical clustering based on a prior over trees called Kingman\u2019s coalescent. We develop novel greedy and sequential Monte Carlo inferences which operate in a bottom-up agglomerative fashion. We show experimentally the superiority of our algorithms over the state-of-the-art, and demonstrate our approach in document clustering and phylolinguistics.",
        "bibtex": "@inproceedings{NIPS2007_2ca65f58,\n author = {Teh, Yee and Daume III, Hal and Roy, Daniel M},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Bayesian Agglomerative Clustering with Coalescents},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/2ca65f58e35d9ad45bf7f3ae5cfd08f1-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/2ca65f58e35d9ad45bf7f3ae5cfd08f1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/2ca65f58e35d9ad45bf7f3ae5cfd08f1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 876388,
        "gs_citation": 143,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8256904761551339068&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Gatsby Unit, University College London; School of Computing, University of Utah; CSAIL, MIT",
        "aff_domain": "gatsby.ucl.ac.uk;hal3.name;mit.edu",
        "email": "gatsby.ucl.ac.uk;hal3.name;mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University College London;University of Utah;Massachusetts Institute of Technology",
        "aff_unique_dep": "Gatsby Unit;School of Computing;Computer Science and Artificial Intelligence Laboratory",
        "aff_unique_url": "https://www.ucl.ac.uk;https://www.utah.edu;https://www.csail.mit.edu",
        "aff_unique_abbr": "UCL;U of U;MIT",
        "aff_campus_unique_index": "0;1;2",
        "aff_campus_unique": "London;Utah;Cambridge",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "077e03b645",
        "title": "Bayesian Co-Training",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/d54ce9de9df77c579775a7b6b1a4bdc0-Abstract.html",
        "author": "Shipeng Yu; Balaji Krishnapuram; Harald Steck; R. B. Rao; R\u00f3mer Rosales",
        "abstract": "We propose a Bayesian undirected graphical model for co-training, or more generally for semi-supervised multi-view learning. This makes explicit the previously unstated assumptions of a large class of co-training type algorithms, and also clarifies the circumstances under which these assumptions fail. Building upon new insights from this model, we propose an improved method for co-training, which is a novel co-training kernel for Gaussian process classifiers. The resulting approach is convex and avoids local-maxima problems, unlike some previous multi-view learning methods. Furthermore, it can automatically estimate how much each view should be trusted, and thus accommodate noisy or unreliable views. Experiments on toy data and real world data sets illustrate the benefits of this approach.",
        "bibtex": "@inproceedings{NIPS2007_d54ce9de,\n author = {Yu, Shipeng and Krishnapuram, Balaji and Steck, Harald and Rao, R. and Rosales, R\\'{o}mer},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Bayesian Co-Training},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d54ce9de9df77c579775a7b6b1a4bdc0-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/d54ce9de9df77c579775a7b6b1a4bdc0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/d54ce9de9df77c579775a7b6b1a4bdc0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 249101,
        "gs_citation": 262,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12215918158611462821&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 21,
        "aff": "CAD & Knowledge Solutions, Siemens Medical Solutions USA, Inc.; CAD & Knowledge Solutions, Siemens Medical Solutions USA, Inc.; CAD & Knowledge Solutions, Siemens Medical Solutions USA, Inc.; CAD & Knowledge Solutions, Siemens Medical Solutions USA, Inc.; CAD & Knowledge Solutions, Siemens Medical Solutions USA, Inc.",
        "aff_domain": "siemens.com;siemens.com;siemens.com;siemens.com;siemens.com",
        "email": "siemens.com;siemens.com;siemens.com;siemens.com;siemens.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "CAD & Knowledge Solutions, Siemens Medical Solutions USA, Inc.",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "6780724f30",
        "title": "Bayesian Inference for Spiking Neuron Models with a Sparsity Prior",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/46ba9f2a6976570b0353203ec4474217-Abstract.html",
        "author": "Sebastian Gerwinn; Matthias Bethge; Jakob H. Macke; Matthias Seeger",
        "abstract": "Generalized linear models are the most commonly used tools to describe the stim- ulus selectivity of sensory neurons. Here we present a Bayesian treatment of such models. Using the expectation propagation algorithm, we are able to approximate the full posterior distribution over all weights. In addition, we use a Laplacian prior to favor sparse solutions. Therefore, stimulus features that do not critically in\ufb02uence neural activity will be assigned zero weights and thus be effectively excluded by the model. This feature selection mechanism facilitates both the in- terpretation of the neuron model as well as its predictive abilities. The posterior distribution can be used to obtain con\ufb01dence intervals which makes it possible to assess the statistical signi\ufb01cance of the solution. In neural data analysis, the available amount of experimental measurements is often limited whereas the pa- rameter space is large. In such a situation, both regularization by a sparsity prior and uncertainty estimates for the model parameters are essential. We apply our method to multi-electrode recordings of retinal ganglion cells and use our uncer- tainty estimate to test the statistical signi\ufb01cance of functional couplings between neurons. Furthermore we used the sparsity of the Laplace prior to select those \ufb01lters from a spike-triggered covariance analysis that are most informative about the neural response.",
        "bibtex": "@inproceedings{NIPS2007_46ba9f2a,\n author = {Gerwinn, Sebastian and Bethge, Matthias and Macke, Jakob H and Seeger, Matthias},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Bayesian Inference for Spiking Neuron Models with a Sparsity Prior},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/46ba9f2a6976570b0353203ec4474217-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/46ba9f2a6976570b0353203ec4474217-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/46ba9f2a6976570b0353203ec4474217-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 327925,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2597799363994663610&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Max Planck Institute for Biological Cybernetics; Max Planck Institute for Biological Cybernetics; Max Planck Institute for Biological Cybernetics; Max Planck Institute for Biological Cybernetics",
        "aff_domain": "tuebingen.mpg.de;tuebingen.mpg.de;tuebingen.mpg.de;tuebingen.mpg.de",
        "email": "tuebingen.mpg.de;tuebingen.mpg.de;tuebingen.mpg.de;tuebingen.mpg.de",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Max Planck Institute for Biological Cybernetics",
        "aff_unique_dep": "Biological Cybernetics",
        "aff_unique_url": "https://www.biocybernetics.mpg.de",
        "aff_unique_abbr": "MPIBC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "3d79a53778",
        "title": "Bayesian Policy Learning with Trans-Dimensional MCMC",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/3a15c7d0bbe60300a39f76f8a5ba6896-Abstract.html",
        "author": "Matthew Hoffman; Arnaud Doucet; Nando D. Freitas; Ajay Jasra",
        "abstract": "A recently proposed formulation of the stochastic planning and control problem as one of parameter estimation for suitable arti\ufb01cial statistical models has led to the adoption of inference algorithms for this notoriously hard problem. At the algorithmic level, the focus has been on developing Expectation-Maximization (EM) algorithms. In this paper, we begin by making the crucial observation that the stochastic control problem can be reinterpreted as one of trans-dimensional inference. With this new interpretation, we are able to propose a novel reversible jump Markov chain Monte Carlo (MCMC) algorithm that is more ef\ufb01cient than its EM counterparts. Moreover, it enables us to implement full Bayesian policy search, without the need for gradients and with one single Markov chain. The new approach involves sampling directly from a distribution that is proportional to the reward and, consequently, performs better than classic simulations methods in situations where the reward is a rare event.",
        "bibtex": "@inproceedings{NIPS2007_3a15c7d0,\n author = {Hoffman, Matthew and Doucet, Arnaud and Freitas, Nando and Jasra, Ajay},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Bayesian Policy Learning with Trans-Dimensional MCMC},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/3a15c7d0bbe60300a39f76f8a5ba6896-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/3a15c7d0bbe60300a39f76f8a5ba6896-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/3a15c7d0bbe60300a39f76f8a5ba6896-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 199506,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18373088944315845925&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Dept. of Computer Science, University of British Columbia; Depts. of Statistics and Computer Science, University of British Columbia; Dept. of Computer Science, University of British Columbia; Dept. of Mathematics, Imperial College London",
        "aff_domain": "cs.ubc.ca;cs.ubc.ca;cs.ubc.ca;imperial.ac.uk",
        "email": "cs.ubc.ca;cs.ubc.ca;cs.ubc.ca;imperial.ac.uk",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;2",
        "aff_unique_norm": "Dept. of Computer Science, University of British Columbia;Depts. of Statistics and Computer Science, University of British Columbia;Dept. of Mathematics, Imperial College London",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "295dc0caf6",
        "title": "Bayesian binning beats approximate alternatives: estimating peri-stimulus time histograms",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/b73ce398c39f506af761d2277d853a92-Abstract.html",
        "author": "Dominik Endres; Mike Oram; Johannes Schindelin; Peter Foldiak",
        "abstract": "The peristimulus time historgram (PSTH) and its more continuous cousin, the spike density function (SDF) are staples in the analytic toolkit of neurophysiologists. The former is usually obtained by binning spiketrains, whereas the standard method for the latter is smoothing with a Gaussian kernel. Selection of a bin with or a kernel size is often done in an relatively arbitrary fashion, even though there have been recent attempts to remedy this situation \\cite{ShimazakiBinningNIPS2006,ShimazakiBinningNECO2007}. We develop an exact Bayesian, generative model approach to estimating PSHTs and demonstate its superiority to competing methods. Further advantages of our scheme include automatic complexity control and error bars on its predictions.",
        "bibtex": "@inproceedings{NIPS2007_b73ce398,\n author = {Endres, Dominik and Oram, Mike and Schindelin, Johannes and Foldiak, Peter},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Bayesian binning beats approximate alternatives: estimating peri-stimulus time histograms},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/b73ce398c39f506af761d2277d853a92-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/b73ce398c39f506af761d2277d853a92-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/b73ce398c39f506af761d2277d853a92-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/b73ce398c39f506af761d2277d853a92-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 293644,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1951688021357501744&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "School of Psychology, University of St. Andrews; School of Psychology, University of St. Andrews; School of Psychology, University of St. Andrews; School of Psychology, University of St. Andrews",
        "aff_domain": "st-andrews.ac.uk;st-andrews.ac.uk;st-andrews.ac.uk;st-andrews.ac.uk",
        "email": "st-andrews.ac.uk;st-andrews.ac.uk;st-andrews.ac.uk;st-andrews.ac.uk",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "School of Psychology, University of St. Andrews",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "e0ad0c46ea",
        "title": "Better than least squares: comparison of objective functions for estimating linear-nonlinear models",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/2f2b265625d76a6704b08093c652fd79-Abstract.html",
        "author": "Tatyana Sharpee",
        "abstract": "This paper compares a family of methods for characterizing neural feature selec- tivity with natural stimuli in the framework of the linear-nonlinear model. In this model, the neural \ufb01ring rate is a nonlinear function of a small number of relevant stimulus components. The relevant stimulus dimensions can be found by max- imizing one of the family of objective functions, R\u00b4enyi divergences of different orders [1, 2]. We show that maximizing one of them, R\u00b4enyi divergence of or- der 2, is equivalent to least-square \ufb01tting of the linear-nonlinear model to neural data. Next, we derive reconstruction errors in relevant dimensions found by max- imizing R\u00b4enyi divergences of arbitrary order in the asymptotic limit of large spike numbers. We \ufb01nd that the smallest errors are obtained with R\u00b4enyi divergence of order 1, also known as Kullback-Leibler divergence. This corresponds to \ufb01nding relevant dimensions by maximizing mutual information [2]. We numerically test how these optimization schemes perform in the regime of low signal-to-noise ra- tio (small number of spikes and increasing neural noise) for model visual neurons. We \ufb01nd that optimization schemes based on either least square \ufb01tting or informa- tion maximization perform well even when number of spikes is small. Information maximization provides slightly, but signi\ufb01cantly, better reconstructions than least square \ufb01tting. This makes the problem of \ufb01nding relevant dimensions, together with the problem of lossy compression [3], one of examples where information- theoretic measures are no more data limited than those derived from least squares.",
        "bibtex": "@inproceedings{NIPS2007_2f2b2656,\n author = {Sharpee, Tatyana},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Better than least squares: comparison of objective functions for estimating linear-nonlinear models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/2f2b265625d76a6704b08093c652fd79-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/2f2b265625d76a6704b08093c652fd79-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/2f2b265625d76a6704b08093c652fd79-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 242396,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16704348127966131840&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Computational Neurobiology Laboratory, the Salk Institute for Biological Studies, La Jolla, CA 92037",
        "aff_domain": "salk.edu",
        "email": "salk.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Computational Neurobiology Laboratory, the Salk Institute for Biological Studies, La Jolla, CA 92037",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "3169ea7391",
        "title": "Blind channel identification for speech dereverberation using l1-norm sparse learning",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/da8ce53cf0240070ce6c69c48cd588ee-Abstract.html",
        "author": "Yuanqing Lin; Jingdong Chen; Youngmoo Kim; Daniel D. Lee",
        "abstract": "Speech dereverberation remains an open problem after more than three decades of research. The most challenging step in speech dereverberation is blind chan- nel identi\ufb01cation (BCI). Although many BCI approaches have been developed, their performance is still far from satisfactory for practical applications. The main dif\ufb01culty in BCI lies in \ufb01nding an appropriate acoustic model, which not only can effectively resolve solution degeneracies due to the lack of knowledge of the source, but also robustly models real acoustic environments. This paper proposes a sparse acoustic room impulse response (RIR) model for BCI, that is, an acous- tic RIR can be modeled by a sparse FIR \ufb01lter. Under this model, we show how to formulate the BCI of a single-input multiple-output (SIMO) system into a l1- norm regularized least squares (LS) problem, which is convex and can be solved ef\ufb01ciently with guaranteed global convergence. The sparseness of solutions is controlled by l1-norm regularization parameters. We propose a sparse learning scheme that infers the optimal l1-norm regularization parameters directly from microphone observations under a Bayesian framework. Our results show that the proposed approach is effective and robust, and it yields source estimates in real acoustic environments with high \ufb01delity to anechoic chamber measurements.",
        "bibtex": "@inproceedings{NIPS2007_da8ce53c,\n author = {Lin, Yuanqing and Chen, Jingdong and Kim, Youngmoo and Lee, Daniel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Blind channel identification for speech dereverberation using l1-norm sparse learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/da8ce53cf0240070ce6c69c48cd588ee-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/da8ce53cf0240070ce6c69c48cd588ee-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/da8ce53cf0240070ce6c69c48cd588ee-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 149672,
        "gs_citation": 61,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13491088012885040353&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "72191054be",
        "title": "Boosting Algorithms for Maximizing the Soft Margin",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/cfbce4c1d7c425baf21d6b6f2babe6be-Abstract.html",
        "author": "Gunnar R\u00e4tsch; Manfred K. Warmuth; Karen A. Glocer",
        "abstract": "Gunnar R\u00a8atsch",
        "bibtex": "@inproceedings{NIPS2007_cfbce4c1,\n author = {R\\\"{a}tsch, Gunnar and Warmuth, Manfred K. K and Glocer, Karen},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Boosting Algorithms for Maximizing the Soft Margin},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/cfbce4c1d7c425baf21d6b6f2babe6be-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/cfbce4c1d7c425baf21d6b6f2babe6be-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/cfbce4c1d7c425baf21d6b6f2babe6be-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 113146,
        "gs_citation": 109,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3260496381495039448&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Dept. of Engineering, University of California, Santa Cruz, CA, U.S.A.; Dept. of Engineering, University of California, Santa Cruz, CA, U.S.A.; Friedrich Miescher Laboratory, Max Planck Society, T\u00a8ubingen, Germany",
        "aff_domain": "soe.ucsc.edu;soe.ucsc.edu;tuebingen.mpg.de",
        "email": "soe.ucsc.edu;soe.ucsc.edu;tuebingen.mpg.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Dept. of Engineering, University of California, Santa Cruz, CA, U.S.A.;Friedrich Miescher Laboratory, Max Planck Society, T\u00a8ubingen, Germany",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "80cf10370c",
        "title": "Boosting the Area under the ROC Curve",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/c5ff2543b53f4cc0ad3819a36752467b-Abstract.html",
        "author": "Phil Long; Rocco Servedio",
        "abstract": "We show that any weak ranker that can achieve an area under the ROC curve slightly better than 1/2 (which can be achieved by random guessing) can be ef\ufb01- ciently boosted to achieve an area under the ROC curve arbitrarily close to 1. We further show that this boosting can be performed even in the presence of indepen- dent misclassi\ufb01cation noise, given access to a noise-tolerant weak ranker.",
        "bibtex": "@inproceedings{NIPS2007_c5ff2543,\n author = {Long, Phil and Servedio, Rocco},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Boosting the Area under the ROC Curve},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c5ff2543b53f4cc0ad3819a36752467b-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/c5ff2543b53f4cc0ad3819a36752467b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/c5ff2543b53f4cc0ad3819a36752467b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/c5ff2543b53f4cc0ad3819a36752467b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 95531,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14721754759797567261&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 23,
        "aff": "Google; Columbia University",
        "aff_domain": "google.com;cs.columbia.edu",
        "email": "google.com;cs.columbia.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Google;Columbia University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.google.com;https://www.columbia.edu",
        "aff_unique_abbr": "Google;Columbia",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Mountain View;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "716c985661",
        "title": "Bundle Methods for Machine Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/26337353b7962f533d78c762373b3318-Abstract.html",
        "author": "Quoc V. Le; Alex J. Smola; S.v.n. Vishwanathan",
        "abstract": "We present a globally convergent method for regularized risk minimization prob- lems. Our method applies to Support Vector estimation, regression, Gaussian Processes, and any other regularized risk minimization setting which leads to a convex optimization problem. SVMPerf can be shown to be a special case of our approach. In addition to the uni\ufb01ed framework we present tight convergence bounds, which show that our algorithm converges in O(1/) steps to  precision for general convex problems and in O(log(1/)) steps for continuously differen- tiable problems. We demonstrate in experiments the performance of our approach.",
        "bibtex": "@inproceedings{NIPS2007_26337353,\n author = {Le, Quoc and Smola, Alex and Vishwanathan, S.v.n.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Bundle Methods for Machine Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/26337353b7962f533d78c762373b3318-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/26337353b7962f533d78c762373b3318-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/26337353b7962f533d78c762373b3318-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 627796,
        "gs_citation": 190,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16688059335424790276&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "NICTA and Australian National University, Canberra, Australia; NICTA and Australian National University, Canberra, Australia; NICTA and Australian National University, Canberra, Australia",
        "aff_domain": "gmail.com;nicta.com.au;nicta.com.au",
        "email": "gmail.com;nicta.com.au;nicta.com.au",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "NICTA and Australian National University, Canberra, Australia",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "7b679b189e",
        "title": "COFI RANK - Maximum Margin Matrix Factorization for Collaborative Ranking",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/f76a89f0cb91bc419542ce9fa43902dc-Abstract.html",
        "author": "Markus Weimer; Alexandros Karatzoglou; Quoc V. Le; Alex J. Smola",
        "abstract": "In this paper, we consider collaborative \ufb01ltering as a ranking problem. We present a method which uses Maximum Margin Matrix Factorization and optimizes rank- ing instead of rating. We employ structured output prediction to optimize directly for ranking scores. Experimental results show that our method gives very good ranking scores and scales well on collaborative \ufb01ltering tasks.",
        "bibtex": "@inproceedings{NIPS2007_f76a89f0,\n author = {Weimer, Markus and Karatzoglou, Alexandros and Le, Quoc and Smola, Alex},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {COFI RANK - Maximum Margin Matrix Factorization for Collaborative Ranking},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/f76a89f0cb91bc419542ce9fa43902dc-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/f76a89f0cb91bc419542ce9fa43902dc-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/f76a89f0cb91bc419542ce9fa43902dc-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/f76a89f0cb91bc419542ce9fa43902dc-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 192501,
        "gs_citation": 538,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16897453432888247641&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff": "Telecooperation Group, TU Darmstadt, Germany; Department of Statistics, TU Wien; Computer Science Department, Stanford University; SML, NICTA, Northbourne Av. 218, Canberra 2601, ACT, Australia",
        "aff_domain": "tk.informatik.tu-darmstadt.de;ci.tuwien.ac.at;stanford.edu;nicta.com.au",
        "email": "tk.informatik.tu-darmstadt.de;ci.tuwien.ac.at;stanford.edu;nicta.com.au",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "Telecooperation Group, TU Darmstadt, Germany;Department of Statistics, TU Wien;Stanford University;SML, NICTA, Northbourne Av. 218, Canberra 2601, ACT, Australia",
        "aff_unique_dep": ";;Computer Science Department;",
        "aff_unique_url": ";;https://www.stanford.edu;",
        "aff_unique_abbr": ";;Stanford;",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Stanford",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "ba2b8c31ef",
        "title": "CPR for CSPs: A Probabilistic Relaxation of Constraint Propagation",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/9908279ebbf1f9b250ba689db6a0222b-Abstract.html",
        "author": "Luis E. Ortiz",
        "abstract": "This paper proposes constraint propagation relaxation (CPR), a probabilistic approach to classical constraint propagation that provides another view on the whole parametric family of survey propagation algorithms SP(\u03c1), ranging from belief propagation (\u03c1 = 0) to (pure) survey propagation(\u03c1 = 1). More importantly, the approach elucidates the implicit, but fundamental assumptions underlying SP(\u03c1), thus shedding some light on its effectiveness and leading to applications beyond k-SAT.",
        "bibtex": "@inproceedings{NIPS2007_9908279e,\n author = {Ortiz, Luis E},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {CPR for CSPs: A Probabilistic Relaxation of Constraint Propagation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9908279ebbf1f9b250ba689db6a0222b-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/9908279ebbf1f9b250ba689db6a0222b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/9908279ebbf1f9b250ba689db6a0222b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 191133,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17608128006052120489&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 7,
        "aff": "ECE Dept, Univ. of Puerto Rico, Mayag \u00a8uez, PR 00681-9042",
        "aff_domain": "ece.uprm.edu",
        "email": "ece.uprm.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "ECE Dept, Univ. of Puerto Rico, Mayag \u00a8uez, PR 00681-9042",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "8516e380ac",
        "title": "Catching Change-points with Lasso",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/e5841df2166dd424a57127423d276bbe-Abstract.html",
        "author": "C\u00e9line Levy-leduc; Za\u00efd Harchaoui",
        "abstract": "We propose a new approach for dealing with the estimation of the location of change-points in one-dimensional piecewise constant signals observed in white noise. Our approach consists in reframing this task in a variable selection context. We use a penalized least-squares criterion with a l1-type penalty for this purpose. We prove that, in an appropriate asymptotic framework, this method provides consistent estimators of the change-points. Then, we explain how to implement this method in practice by combining the LAR algorithm and a reduced version of the dynamic programming algorithm and we apply it to synthetic and real data.",
        "bibtex": "@inproceedings{NIPS2007_e5841df2,\n author = {Levy-leduc, C\\'{e}line and Harchaoui, Za\\\"{\\i}d},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Catching Change-points with Lasso},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/e5841df2166dd424a57127423d276bbe-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/e5841df2166dd424a57127423d276bbe-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/e5841df2166dd424a57127423d276bbe-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 122034,
        "gs_citation": 102,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14450008402215501034&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "ac54208807",
        "title": "Catching Up Faster in Bayesian Model Selection and Model Averaging",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/2823f4797102ce1a1aec05359cc16dd9-Abstract.html",
        "author": "Tim V. Erven; Steven D. Rooij; Peter Gr\u00fcnwald",
        "abstract": "Bayesian model averaging, model selection and their approximations such as BIC are generally statistically consistent, but sometimes achieve slower rates of con- vergence than other methods such as AIC and leave-one-out cross-validation. On the other hand, these other methods can be inconsistent. We identify the catch-up phenomenon as a novel explanation for the slow convergence of Bayesian meth- ods. Based on this analysis we de\ufb01ne the switch-distribution, a modi\ufb01cation of the Bayesian model averaging distribution. We prove that in many situations model selection and prediction based on the switch-distribution is both consistent and achieves optimal convergence rates, thereby resolving the AIC-BIC dilemma. The method is practical; we give an ef\ufb01cient algorithm.",
        "bibtex": "@inproceedings{NIPS2007_2823f479,\n author = {Erven, Tim and Rooij, Steven and Gr\\\"{u}nwald, Peter},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Catching Up Faster in Bayesian Model Selection and Model Averaging},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/2823f4797102ce1a1aec05359cc16dd9-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/2823f4797102ce1a1aec05359cc16dd9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/2823f4797102ce1a1aec05359cc16dd9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 143471,
        "gs_citation": 56,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13928519737835355291&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 19,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "422065a355",
        "title": "Classification via Minimum Incremental Coding Length (MICL)",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/37693cfc748049e45d87b8c7d8b9aacd-Abstract.html",
        "author": "John Wright; Yangyu Tao; Zhouchen Lin; Yi Ma; Heung-yeung Shum",
        "abstract": "We present a simple new criterion for classi\ufb01cation, based on principles from lossy data compression. The criterion assigns a test sample to the class that uses the min- imum number of additional bits to code the test sample, subject to an allowable distortion. We prove asymptotic optimality of this criterion for Gaussian data and analyze its relationships to classical classi\ufb01ers. Theoretical results provide new insights into relationships among popular classi\ufb01ers such as MAP and RDA, as well as unsupervised clustering methods based on lossy compression [13]. Mini- mizing the lossy coding length induces a regularization effect which stabilizes the (implicit) density estimate in a small-sample setting. Compression also provides a uniform means of handling classes of varying dimension. This simple classi- \ufb01cation criterion and its kernel and local versions perform competitively against existing classi\ufb01ers on both synthetic examples and real imagery data such as hand- written digits and human faces, without requiring domain-speci\ufb01c information.",
        "bibtex": "@inproceedings{NIPS2007_37693cfc,\n author = {Wright, John and Tao, Yangyu and Lin, Zhouchen and Ma, Yi and Shum, Heung-yeung},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Classification via Minimum Incremental Coding Length (MICL)},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/37693cfc748049e45d87b8c7d8b9aacd-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/37693cfc748049e45d87b8c7d8b9aacd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/37693cfc748049e45d87b8c7d8b9aacd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 458038,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15687470253091297633&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 22,
        "aff": "Coordinated Science Laboratory, University of Illinois at Urbana-Champaign; Coordinated Science Laboratory, University of Illinois at Urbana-Champaign; Visual Computing Group, Microsoft Research Asia; Visual Computing Group, Microsoft Research Asia; Visual Computing Group, Microsoft Research Asia",
        "aff_domain": "uiuc.edu;uiuc.edu;microsoft.com;microsoft.com;microsoft.com",
        "email": "uiuc.edu;uiuc.edu;microsoft.com;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;1;1",
        "aff_unique_norm": "University of Illinois at Urbana-Champaign;Visual Computing Group, Microsoft Research Asia",
        "aff_unique_dep": "Coordinated Science Laboratory;",
        "aff_unique_url": "https://www illinois.edu;",
        "aff_unique_abbr": "UIUC;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Urbana-Champaign;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "04256bf7e9",
        "title": "Cluster Stability for Finite Samples",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/812b4ba287f5ee0bc9d43bbf5bbe87fb-Abstract.html",
        "author": "Ohad Shamir; Naftali Tishby",
        "abstract": "Over the past few years, the notion of stability in data clustering has received growing attention as a cluster validation criterion in a sample-based framework. However, recent work has shown that as the sample size increases, any clustering model will usually become asymptotically stable. This led to the conclusion that stability is lacking as a theoretical and practical tool. The discrepancy between this conclusion and the success of stability in practice has remained an open ques- tion, which we attempt to address. Our theoretical approach is that stability, as used by cluster validation algorithms, is similar in certain respects to measures of generalization in a model-selection framework. In such cases, the model cho- sen governs the convergence rate of generalization bounds. By arguing that these rates are more important than the sample size, we are led to the prediction that stability-based cluster validation algorithms should not degrade with increasing sample size, despite the asymptotic universal stability. This prediction is substan- tiated by a theoretical analysis as well as some empirical results. We conclude that stability remains a meaningful cluster validation criterion over \ufb01nite samples.",
        "bibtex": "@inproceedings{NIPS2007_812b4ba2,\n author = {Shamir, Ohad and Tishby, Naftali},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Cluster Stability for Finite Samples},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 324902,
        "gs_citation": 87,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6005370757296923001&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "School of Computer Science and Engineering; School of Computer Science and Engineering + Interdisciplinary Center for Neural Computation",
        "aff_domain": "cs.huji.ac.il;cs.huji.ac.il",
        "email": "cs.huji.ac.il;cs.huji.ac.il",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "University Affiliation Not Specified;Interdisciplinary Center for Neural Computation",
        "aff_unique_dep": "School of Computer Science and Engineering;Neural Computation",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "7d1714dd12",
        "title": "Collapsed Variational Inference for HDP",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/eefc9e10ebdc4a2333b42b2dbb8f27b6-Abstract.html",
        "author": "Yee W. Teh; Kenichi Kurihara; Max Welling",
        "abstract": "A wide variety of Dirichlet-multinomial \u2018topic\u2019 models have found interesting ap- plications in recent years. While Gibbs sampling remains an important method of inference in such models, variational techniques have certain advantages such as easy assessment of convergence, easy optimization without the need to maintain detailed balance, a bound on the marginal likelihood, and side-stepping of issues with topic-identi\ufb01ability. The most accurate variational technique thus far, namely collapsed variational latent Dirichlet allocation, did not deal with model selection nor did it include inference for hyperparameters. We address both issues by gen- eralizing the technique, obtaining the \ufb01rst variational algorithm to deal with the hierarchical Dirichlet process and to deal with hyperparameters of Dirichlet vari- ables. Experiments show a signi\ufb01cant improvement in accuracy.",
        "bibtex": "@inproceedings{NIPS2007_eefc9e10,\n author = {Teh, Yee and Kurihara, Kenichi and Welling, Max},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Collapsed Variational Inference for HDP},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/eefc9e10ebdc4a2333b42b2dbb8f27b6-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/eefc9e10ebdc4a2333b42b2dbb8f27b6-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/eefc9e10ebdc4a2333b42b2dbb8f27b6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 477057,
        "gs_citation": 217,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=822316481309777236&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Gatsby Unit, University College London; Dept. of Computer Science, Tokyo Institute of Technology; ICS, UC Irvine",
        "aff_domain": "gatsby.ucl.ac.uk;mi.cs.titech.ac.jp;ics.uci.edu",
        "email": "gatsby.ucl.ac.uk;mi.cs.titech.ac.jp;ics.uci.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University College London;Dept. of Computer Science, Tokyo Institute of Technology;ICS, UC Irvine",
        "aff_unique_dep": "Gatsby Unit;;",
        "aff_unique_url": "https://www.ucl.ac.uk;;",
        "aff_unique_abbr": "UCL;;",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "London;",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United Kingdom;"
    },
    {
        "id": "15b310d898",
        "title": "Collective Inference on Markov Models for Modeling Bird Migration",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/69421f032498c97020180038fddb8e24-Abstract.html",
        "author": "M.a. S. Elmohamed; Dexter Kozen; Daniel R. Sheldon",
        "abstract": "We investigate a family of inference problems on Markov models, where many sample paths are drawn from a Markov chain and partial information is revealed to an observer who attempts to reconstruct the sample paths. We present algo- rithms and hardness results for several variants of this problem which arise by re- vealing different information to the observer and imposing different requirements for the reconstruction of sample paths. Our algorithms are analogous to the clas- sical Viterbi algorithm for Hidden Markov Models, which \ufb01nds the single most probable sample path given a sequence of observations. Our work is motivated by an important application in ecology: inferring bird migration paths from a large database of observations.",
        "bibtex": "@inproceedings{NIPS2007_69421f03,\n author = {Elmohamed, M.a. and Kozen, Dexter and Sheldon, Daniel R},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Collective Inference on Markov Models for Modeling Bird Migration},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/69421f032498c97020180038fddb8e24-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/69421f032498c97020180038fddb8e24-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/69421f032498c97020180038fddb8e24-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 324080,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10007062083981415552&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Cornell University; Cornell University; Cornell University",
        "aff_domain": "cs.cornell.edu;cam.cornell.edu;cs.cornell.edu",
        "email": "cs.cornell.edu;cam.cornell.edu;cs.cornell.edu",
        "github": "",
        "project": "http://ebird.org; http://www.avianknowledge.net/visualization",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Cornell University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cornell.edu",
        "aff_unique_abbr": "Cornell",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a2ea8704f2",
        "title": "Colored Maximum Variance Unfolding",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/55a7cf9c71f1c9c495413f934dd1a158-Abstract.html",
        "author": "Le Song; Arthur Gretton; Karsten Borgwardt; Alex J. Smola",
        "abstract": "Maximum variance unfolding (MVU) is an effective heuristic for dimensionality reduction. It produces a low-dimensional representation of the data by maximiz- ing the variance of their embeddings while preserving the local distances of the original data. We show that MVU also optimizes a statistical dependence measure which aims to retain the identity of individual observations under the distance- preserving constraints. This general view allows us to design \u201ccolored\u201d variants of MVU, which produce low-dimensional representations for a given task, e.g. subject to class labels or other side information.",
        "bibtex": "@inproceedings{NIPS2007_55a7cf9c,\n author = {Song, Le and Gretton, Arthur and Borgwardt, Karsten and Smola, Alex},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Colored Maximum Variance Unfolding},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/55a7cf9c71f1c9c495413f934dd1a158-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/55a7cf9c71f1c9c495413f934dd1a158-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/55a7cf9c71f1c9c495413f934dd1a158-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/55a7cf9c71f1c9c495413f934dd1a158-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 441644,
        "gs_citation": 137,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=917366823024563782&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "National ICT Australia, Canberra, Australia; National ICT Australia, Canberra, Australia; University of Cambridge, Cambridge, United Kingdom; MPI for Biological Cybernetics, T\u00fcbingen, Germany",
        "aff_domain": "nicta.com.au;nicta.com.au;eng.cam.ac.uk;tuebingen.mpg.de",
        "email": "nicta.com.au;nicta.com.au;eng.cam.ac.uk;tuebingen.mpg.de",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2",
        "aff_unique_norm": "National ICT Australia, Canberra, Australia;University of Cambridge;Max Planck Institute for Biological Cybernetics",
        "aff_unique_dep": ";;Biological Cybernetics",
        "aff_unique_url": ";https://www.cam.ac.uk;https://www.biological-cybernetics.de",
        "aff_unique_abbr": ";Cambridge;MPIBC",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Cambridge;T\u00fcbingen",
        "aff_country_unique_index": "1;2",
        "aff_country_unique": ";United Kingdom;Germany"
    },
    {
        "id": "054ddc8749",
        "title": "Combined discriminative and generative articulated pose and non-rigid shape estimation",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/a1140a3d0df1c81e24ae954d935e8926-Abstract.html",
        "author": "Leonid Sigal; Alexandru Balan; Michael J. Black",
        "abstract": "Estimation of three-dimensional articulated human pose and motion from images is a central problem in computer vision. Much of the previous work has been limited by the use of crude generative models of humans represented as articu- lated collections of simple parts such as cylinders. Automatic initialization of such models has proved dif\ufb01cult and most approaches assume that the size and shape of the body parts are known a priori. In this paper we propose a method for automatically recovering a detailed parametric model of non-rigid body shape and pose from monocular imagery. Speci\ufb01cally, we represent the body using a param- eterized triangulated mesh model that is learned from a database of human range scans. We demonstrate a discriminative method to directly recover the model pa- rameters from monocular images using a conditional mixture of kernel regressors. This predicted pose and shape are used to initialize a generative model for more detailed pose and shape estimation. The resulting approach allows fully automatic pose and shape recovery from monocular and multi-camera imagery. Experimen- tal results show that our method is capable of robustly recovering articulated pose, shape and biometric measurements (e.g. height, weight, etc.) in both calibrated and uncalibrated camera environments.",
        "bibtex": "@inproceedings{NIPS2007_a1140a3d,\n author = {Sigal, Leonid and Balan, Alexandru and Black, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Combined discriminative and generative articulated pose and non-rigid shape estimation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a1140a3d0df1c81e24ae954d935e8926-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/a1140a3d0df1c81e24ae954d935e8926-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/a1140a3d0df1c81e24ae954d935e8926-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 280803,
        "gs_citation": 252,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7909474888434409393&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Department of Computer Science, Brown University; Department of Computer Science, Brown University; Department of Computer Science, Brown University",
        "aff_domain": "cs.brown.edu;cs.brown.edu;cs.brown.edu",
        "email": "cs.brown.edu;cs.brown.edu;cs.brown.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Brown University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.brown.edu",
        "aff_unique_abbr": "Brown",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "8e7832b0ae",
        "title": "Comparing Bayesian models for multisensory cue combination without mandatory integration",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/cf004fdc76fa1a4f25f62e0eb5261ca3-Abstract.html",
        "author": "Ulrik Beierholm; Ladan Shams; Wei J. Ma; Konrad Koerding",
        "abstract": "Bayesian models of multisensory perception traditionally address the problem of estimating an underlying variable that is assumed to be the cause of the two sen- sory signals. The brain, however, has to solve a more general problem: it also has to establish which signals come from the same source and should be integrated, and which ones do not and should be segregated. In the last couple of years, a few models have been proposed to solve this problem in a Bayesian fashion. One of these has the strength that it formalizes the causal structure of sensory signals. We \ufb01rst compare these models on a formal level. Furthermore, we conduct a psy- chophysics experiment to test human performance in an auditory-visual spatial localization task in which integration is not mandatory. We \ufb01nd that the causal Bayesian inference model accounts for the data better than other models. Keywords: causal inference, Bayesian methods, visual perception.",
        "bibtex": "@inproceedings{NIPS2007_cf004fdc,\n author = {Beierholm, Ulrik and Shams, Ladan and J., Wei and Koerding, Konrad},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Comparing Bayesian models for multisensory cue combination without mandatory integration},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/cf004fdc76fa1a4f25f62e0eb5261ca3-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/cf004fdc76fa1a4f25f62e0eb5261ca3-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/cf004fdc76fa1a4f25f62e0eb5261ca3-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/cf004fdc76fa1a4f25f62e0eb5261ca3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 235084,
        "gs_citation": 99,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16068361010824011965&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "Computation and Neural Systems, California Institute of Technology; Rehabilitation Institute of Chicago, Northwestern University, Dept. PM&R; Department of Psychology, University of California, Los Angeles; Department of Brain and Cognitive Sciences, University of Rochester",
        "aff_domain": "caltech.edu;koerding.com;psych.ucla.edu;gmail.com",
        "email": "caltech.edu;koerding.com;psych.ucla.edu;gmail.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "Computation and Neural Systems, California Institute of Technology;Rehabilitation Institute of Chicago, Northwestern University, Dept. PM&R;Department of Psychology, University of California, Los Angeles;Department of Brain and Cognitive Sciences, University of Rochester",
        "aff_unique_dep": ";;;",
        "aff_unique_url": ";;;",
        "aff_unique_abbr": ";;;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "9e5c157130",
        "title": "Competition Adds Complexity",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/3435c378bb76d4357324dd7e69f3cd18-Abstract.html",
        "author": "Judy Goldsmith; Martin Mundhenk",
        "abstract": "It is known that determinining whether a DEC-POMDP, namely, a cooperative partially observable stochastic game (POSG), has a cooperative strategy with positive expected reward is complete for NEXP. It was not known until now how cooperation affected that complexity. We show that, for competitive POSGs, the complexity of determining whether one team has a positive-expected-reward strategy is complete for the class NEXP with an oracle for NP.",
        "bibtex": "@inproceedings{NIPS2007_3435c378,\n author = {Goldsmith, Judy and Mundhenk, Martin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Competition Adds Complexity},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/3435c378bb76d4357324dd7e69f3cd18-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/3435c378bb76d4357324dd7e69f3cd18-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/3435c378bb76d4357324dd7e69f3cd18-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/3435c378bb76d4357324dd7e69f3cd18-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 67662,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2259472376641159457&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Computer Science, University of Kentucky, Lexington, KY; Friedrich-Schiller-Universit\u00a8at Jena, Jena, Germany",
        "aff_domain": "cs.uky.edu;cs.uni-jena.de",
        "email": "cs.uky.edu;cs.uni-jena.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Department of Computer Science, University of Kentucky, Lexington, KY;Friedrich-Schiller-Universit\u00a8at Jena, Jena, Germany",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "6c1f8c678b",
        "title": "Compressed Regression",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/0336dcbab05b9d5ad24f4333c7658a0e-Abstract.html",
        "author": "Shuheng Zhou; Larry Wasserman; John D. Lafferty",
        "abstract": "Recent research has studied the role of sparsity in high dimensional regression and signal reconstruction, establishing theoretical limits for recovering sparse models from sparse data. In this paper we study a variant of this problem where the original $n$ input variables are compressed by a random linear transformation to $m \\ll n$ examples in $p$ dimensions, and establish conditions under which a sparse linear model can be successfully recovered from the compressed data. A primary motivation for this compression procedure is to anonymize the data and preserve privacy by revealing little information about the original data. We characterize the number of random projections that are required for $\\ell_1$-regularized compressed regression to identify the nonzero coefficients in the true model with probability approaching one, a property called ``sparsistence.'' In addition, we show that $\\ell_1$-regularized compressed regression asymptotically predicts as well as an oracle linear model, a property called ``persistence.'' Finally, we characterize the privacy properties of the compression procedure in information-theoretic terms, establishing upper bounds on the rate of information communicated between the compressed and uncompressed data that decay to zero.",
        "bibtex": "@inproceedings{NIPS2007_0336dcba,\n author = {Zhou, Shuheng and Wasserman, Larry and Lafferty, John},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Compressed Regression},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/0336dcbab05b9d5ad24f4333c7658a0e-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/0336dcbab05b9d5ad24f4333c7658a0e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/0336dcbab05b9d5ad24f4333c7658a0e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 110903,
        "gs_citation": 60,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9885182216857617597&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 14,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "cbed58962e",
        "title": "Computational Equivalence of Fixed Points and No Regret Algorithms, and Convergence to Equilibria",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/e4bb4c5173c2ce17fd8fcd40041c068f-Abstract.html",
        "author": "Elad Hazan; Satyen Kale",
        "abstract": "We study the relation between notions of game-theoretic equilibria which are based on stability under a set of deviations, and empirical equilibria which are reached by rational players. Rational players are modelled by players using no regret algorithms, which guarantee that their payoff in the long run is almost as much as the most they could hope to achieve by consistently deviating from the algorithm's suggested action. We show that for a given set of deviations over the strategy set of a player, it is possible to efficiently approximate fixed points of a given deviation if and only if there exist efficient no regret algorithms resistant to the deviations. Further, we show that if all players use a no regret algorithm, then the empirical distribution of their plays converges to an equilibrium.",
        "bibtex": "@inproceedings{NIPS2007_e4bb4c51,\n author = {Hazan, Elad and Kale, Satyen},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Computational Equivalence of Fixed Points and No Regret Algorithms, and Convergence to Equilibria},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/e4bb4c5173c2ce17fd8fcd40041c068f-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/e4bb4c5173c2ce17fd8fcd40041c068f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/e4bb4c5173c2ce17fd8fcd40041c068f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 174914,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2461892091132381652&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "IBM Almaden Research Center; Computer Science Department, Princeton University",
        "aff_domain": "us.ibm.com;cs.princeton.edu",
        "email": "us.ibm.com;cs.princeton.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "IBM;Princeton University",
        "aff_unique_dep": "Research Center;Computer Science Department",
        "aff_unique_url": "https://www.ibm.com/research;https://www.princeton.edu",
        "aff_unique_abbr": "IBM;Princeton",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Almaden;Princeton",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "8630a9e40a",
        "title": "Computing Robust Counter-Strategies",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/6e7b33fdea3adc80ebd648fffb665bb8-Abstract.html",
        "author": "Michael Johanson; Martin Zinkevich; Michael Bowling",
        "abstract": "Adaptation to other initially unknown agents often requires computing an effective counter-strategy. In the Bayesian paradigm, one must find a good counter-strategy to the inferred posterior of the other agents' behavior. In the experts paradigm, one may want to choose experts that are good counter-strategies to the other agents' expected behavior. In this paper we introduce a technique for computing robust counter-strategies for adaptation in multiagent scenarios under a variety of paradigms. The strategies can take advantage of a suspected tendency in the decisions of the other agents, while bounding the worst-case performance when the tendency is not observed. The technique involves solving a modified game, and therefore can make use of recently developed algorithms for solving very large extensive games. We demonstrate the effectiveness of the technique in two-player Texas Hold'em. We show that the computed poker strategies are substantially more robust than best response counter-strategies, while still exploiting a suspected tendency. We also compose the generated strategies in an experts algorithm showing a dramatic improvement in performance over using simple best responses.",
        "bibtex": "@inproceedings{NIPS2007_6e7b33fd,\n author = {Johanson, Michael and Zinkevich, Martin and Bowling, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Computing Robust Counter-Strategies},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/6e7b33fdea3adc80ebd648fffb665bb8-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/6e7b33fdea3adc80ebd648fffb665bb8-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/6e7b33fdea3adc80ebd648fffb665bb8-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/6e7b33fdea3adc80ebd648fffb665bb8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 165487,
        "gs_citation": 113,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15665702085772374516&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Computing Science Department, University of Alberta; Computing Science Department, University of Alberta; Computing Science Department, University of Alberta",
        "aff_domain": "cs.ualberta.ca;cs.ualberta.ca;cs.ualberta.ca",
        "email": "cs.ualberta.ca;cs.ualberta.ca;cs.ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Alberta",
        "aff_unique_dep": "Computing Science Department",
        "aff_unique_url": "https://www.ualberta.ca",
        "aff_unique_abbr": "UAlberta",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "bf1186119f",
        "title": "Configuration Estimates Improve Pedestrian Finding",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/8c7bbbba95c1025975e548cee86dfadc-Abstract.html",
        "author": "Duan Tran; David A. Forsyth",
        "abstract": "Fair discriminative pedestrian finders are now available. In fact, these pedestrian finders make most errors on pedestrians in configurations that are uncommon in the training data, for example, mounting a bicycle. This is undesirable. However, the human configuration can itself be estimated discriminatively using structure learning. We demonstrate a pedestrian finder which first finds the most likely human pose in the window using a discriminative procedure trained with structure learning on a small dataset. We then present features (local histogram of oriented gradient and local PCA of gradient) based on that configuration to an SVM classifier. We show, using the INRIA Person dataset, that estimates of configuration significantly improve the accuracy of a discriminative pedestrian finder.",
        "bibtex": "@inproceedings{NIPS2007_8c7bbbba,\n author = {Tran, Duan and Forsyth, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Configuration Estimates Improve Pedestrian Finding},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8c7bbbba95c1025975e548cee86dfadc-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/8c7bbbba95c1025975e548cee86dfadc-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/8c7bbbba95c1025975e548cee86dfadc-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 239088,
        "gs_citation": 80,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7190437156945055572&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "U.Illinois at Urbana-Champaign; U.Illinois at Urbana-Champaign",
        "aff_domain": "uiuc.edu;uiuc.edu",
        "email": "uiuc.edu;uiuc.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "U.Illinois at Urbana-Champaign",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "7512917306",
        "title": "Congruence between model and human attention reveals unique signatures of critical visual events",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/621461af90cadfdaf0e8d4cc25129f91-Abstract.html",
        "author": "Robert Peters; Laurent Itti",
        "abstract": "Current computational models of bottom-up and top-down components of atten- tion are predictive of eye movements across a range of stimuli and of simple, \ufb01xed visual tasks (such as visual search for a target among distractors). How- ever, to date there exists no computational framework which can reliably mimic human gaze behavior in more complex environments and tasks, such as driving a vehicle through tra\ufb03c. Here, we develop a hybrid computational/behavioral framework, combining simple models for bottom-up salience and top-down rel- evance, and looking for changes in the predictive power of these components at di\ufb00erent critical event times during 4.7 hours (500,000 video frames) of observers playing car racing and \ufb02ight combat video games. This approach is motivated by our observation that the predictive strengths of the salience and relevance mod- els exhibit reliable temporal signatures during critical event windows in the task sequence\u2014for example, when the game player directly engages an enemy plane in a \ufb02ight combat game, the predictive strength of the salience model increases signi\ufb01cantly, while that of the relevance model decreases signi\ufb01cantly. Our new framework combines these temporal signatures to implement several event detec- tors. Critically, we \ufb01nd that an event detector based on fused behavioral and stim- ulus information (in the form of the model\u2019s predictive strength) is much stronger than detectors based on behavioral information alone (eye position) or image in- formation alone (model prediction maps). This approach to event detection, based on eye tracking combined with computational models applied to the visual input, may have useful applications as a less-invasive alternative to other event detection approaches based on neural signatures derived from EEG or fMRI recordings.",
        "bibtex": "@inproceedings{NIPS2007_621461af,\n author = {Peters, Robert and Itti, Laurent},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Congruence between model and human attention reveals unique signatures of critical visual events},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/621461af90cadfdaf0e8d4cc25129f91-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/621461af90cadfdaf0e8d4cc25129f91-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/621461af90cadfdaf0e8d4cc25129f91-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 425147,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17867076924253390191&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science, University of Southern California; Departments of Neuroscience and Computer Science, University of Southern California",
        "aff_domain": "usc.edu;usc.edu",
        "email": "usc.edu;usc.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Southern California;Departments of Neuroscience and Computer Science, University of Southern California",
        "aff_unique_dep": "Department of Computer Science;",
        "aff_unique_url": "https://www.usc.edu;",
        "aff_unique_abbr": "USC;",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "345857e978",
        "title": "Consistent Minimization of Clustering Objective Functions",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/faa9afea49ef2ff029a833cccc778fd0-Abstract.html",
        "author": "Ulrike V. Luxburg; Stefanie Jegelka; Michael Kaufmann; S\u00e9bastien Bubeck",
        "abstract": "Clustering is often formulated as a discrete optimization problem. The objective is to \ufb01nd, among all partitions of the data set, the best one according to some quality measure. However, in the statistical setting where we assume that the \ufb01nite data set has been sampled from some underlying space, the goal is not to \ufb01nd the best partition of the given sample, but to approximate the true partition of the under- lying space. We argue that the discrete optimization approach usually does not achieve this goal. As an alternative, we suggest the paradigm of \u201cnearest neighbor clustering\u201d. Instead of selecting the best out of all partitions of the sample, it only considers partitions in some restricted function class. Using tools from statistical learning theory we prove that nearest neighbor clustering is statistically consis- tent. Moreover, its worst case complexity is polynomial by construction, and it can be implemented with small average case complexity using branch and bound.",
        "bibtex": "@inproceedings{NIPS2007_faa9afea,\n author = {Luxburg, Ulrike and Jegelka, Stefanie and Kaufmann, Michael and Bubeck, S\\'{e}bastien},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Consistent Minimization of Clustering Objective Functions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/faa9afea49ef2ff029a833cccc778fd0-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/faa9afea49ef2ff029a833cccc778fd0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/faa9afea49ef2ff029a833cccc778fd0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 227905,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2742587759997728595&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 23,
        "aff": "Max Planck Institute for Biological Cybernetics; INRIA Futurs Lille, France; Max Planck Institute for Biological Cybernetics; University of T \u00a8ubingen, Germany",
        "aff_domain": "tuebingen.mpg.de;inria.fr;tuebingen.mpg.de;informatik.uni-tuebingen.de",
        "email": "tuebingen.mpg.de;inria.fr;tuebingen.mpg.de;informatik.uni-tuebingen.de",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;2",
        "aff_unique_norm": "Max Planck Institute for Biological Cybernetics;INRIA Futurs Lille, France;University of T\u00fcbingen",
        "aff_unique_dep": "Biological Cybernetics;;",
        "aff_unique_url": "https://www.biocybernetics.mpg.de;;https://www.uni-tuebingen.de/",
        "aff_unique_abbr": "MPIBC;;Uni T\u00fcbingen",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany;"
    },
    {
        "id": "83943a0f40",
        "title": "Continuous Time Particle Filtering for fMRI",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/6e2713a6efee97bacb63e52c54f0ada0-Abstract.html",
        "author": "Lawrence Murray; Amos J. Storkey",
        "abstract": "We construct a biologically motivated stochastic differential model of the neural and hemodynamic activity underlying the observed Blood Oxygen Level Dependent (BOLD) signal in Functional Magnetic Resonance Imaging (fMRI). The model poses a difficult parameter estimation problem, both theoretically due to the nonlinearity and divergence of the differential system, and computationally due to its time and space complexity. We adapt a particle filter and smoother to the task, and discuss some of the practical approaches used to tackle the difficulties, including use of sparse matrices and parallelisation. Results demonstrate the tractability of the approach in its application to an effective connectivity study.",
        "bibtex": "@inproceedings{NIPS2007_6e2713a6,\n author = {Murray, Lawrence and Storkey, Amos J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Continuous Time Particle Filtering for fMRI},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/6e2713a6efee97bacb63e52c54f0ada0-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/6e2713a6efee97bacb63e52c54f0ada0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/6e2713a6efee97bacb63e52c54f0ada0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 3655527,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6070730328302922778&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 10,
        "aff": "School of Informatics, University of Edinburgh; School of Informatics, University of Edinburgh",
        "aff_domain": "ed.ac.uk;ed.ac.uk",
        "email": "ed.ac.uk;ed.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Edinburgh",
        "aff_unique_dep": "School of Informatics",
        "aff_unique_url": "https://www.ed.ac.uk",
        "aff_unique_abbr": "Edinburgh",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Edinburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "791bcf62e6",
        "title": "Contraction Properties of VLSI Cooperative Competitive Neural Networks of Spiking Neurons",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/a4f23670e1833f3fdb077ca70bbd5d66-Abstract.html",
        "author": "Emre Neftci; Elisabetta Chicca; Giacomo Indiveri; Jean-jeacques Slotine; Rodney J. Douglas",
        "abstract": "A non\u2013linear dynamic system is called contracting if initial conditions are for- gotten exponentially fast, so that all trajectories converge to a single trajectory. We use contraction theory to derive an upper bound for the strength of recurrent connections that guarantees contraction for complex neural networks. Speci\ufb01- cally, we apply this theory to a special class of recurrent networks, often called Cooperative Competitive Networks (CCNs), which are an abstract representation of the cooperative-competitive connectivity observed in cortex. This speci\ufb01c type of network is believed to play a major role in shaping cortical responses and se- lecting the relevant signal among distractors and noise. In this paper, we analyze contraction of combined CCNs of linear threshold units and verify the results of our analysis in a hybrid analog/digital VLSI CCN comprising spiking neurons and dynamic synapses.",
        "bibtex": "@inproceedings{NIPS2007_a4f23670,\n author = {Neftci, Emre and Chicca, Elisabetta and Indiveri, Giacomo and Slotine, Jean-jeacques and Douglas, Rodney},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Contraction Properties of VLSI Cooperative Competitive Neural Networks of Spiking Neurons},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a4f23670e1833f3fdb077ca70bbd5d66-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/a4f23670e1833f3fdb077ca70bbd5d66-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/a4f23670e1833f3fdb077ca70bbd5d66-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/a4f23670e1833f3fdb077ca70bbd5d66-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1587268,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5514978436283024272&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Institute of Neuroinformatics, UNI| ETH, Zurich; Institute of Neuroinformatics, UNI| ETH, Zurich; Institute of Neuroinformatics, UNI| ETH, Zurich; Nonlinear Systems Laboratory, MIT, Cambridge, Massachusetts, 02139; Institute of Neuroinformatics, UNI| ETH, Zurich",
        "aff_domain": "ini.phys.ethz.ch; ; ; ; ",
        "email": "ini.phys.ethz.ch; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "Institute of Neuroinformatics, UNI| ETH, Zurich;Nonlinear Systems Laboratory, MIT, Cambridge, Massachusetts, 02139",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "31662f8394",
        "title": "Convex Clustering with Exemplar-Based Models",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/3fe94a002317b5f9259f82690aeea4cd-Abstract.html",
        "author": "Danial Lashkari; Polina Golland",
        "abstract": "Clustering is often formulated as the maximum likelihood estimation of a mixture model that explains the data. The EM algorithm widely used to solve the resulting optimization problem is inherently a gradient-descent method and is sensitive to initialization. The resulting solution is a local optimum in the neighborhood of the initial guess. This sensitivity to initialization presents a signi\ufb01cant challenge in clustering large data sets into many clusters. In this paper, we present a dif- ferent approach to approximate mixture \ufb01tting for clustering. We introduce an exemplar-based likelihood function that approximates the exact likelihood. This formulation leads to a convex minimization problem and an ef\ufb01cient algorithm with guaranteed convergence to the globally optimal solution. The resulting clus- tering can be thought of as a probabilistic mapping of the data points to the set of exemplars that minimizes the average distance and the information-theoretic cost of mapping. We present experimental results illustrating the performance of our algorithm and its comparison with the conventional approach to mixture model clustering.",
        "bibtex": "@inproceedings{NIPS2007_3fe94a00,\n author = {Lashkari, Danial and Golland, Polina},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Convex Clustering with Exemplar-Based Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/3fe94a002317b5f9259f82690aeea4cd-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/3fe94a002317b5f9259f82690aeea4cd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/3fe94a002317b5f9259f82690aeea4cd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 367536,
        "gs_citation": 136,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1817812547030896647&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "097e2a8901",
        "title": "Convex Learning with Invariances",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/20b5e1cf8694af7a3c1ba4a87f073021-Abstract.html",
        "author": "Choon H. Teo; Amir Globerson; Sam T. Roweis; Alex J. Smola",
        "abstract": "Incorporating invariances into a learning algorithm is a common problem in ma- chine learning. We provide a convex formulation which can deal with arbitrary loss functions and arbitrary losses. In addition, it is a drop-in replacement for most optimization algorithms for kernels, including solvers of the SVMStruct family. The advantage of our setting is that it relies on column generation instead of mod- ifying the underlying optimization problem directly.",
        "bibtex": "@inproceedings{NIPS2007_20b5e1cf,\n author = {Teo, Choon and Globerson, Amir and Roweis, Sam and Smola, Alex},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Convex Learning with Invariances},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/20b5e1cf8694af7a3c1ba4a87f073021-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/20b5e1cf8694af7a3c1ba4a87f073021-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/20b5e1cf8694af7a3c1ba4a87f073021-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 140597,
        "gs_citation": 145,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17937331644142205078&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Australian National University; CSAIL, MIT; Department of Computer Science, University of Toronto; NICTA, Canberra, Australia",
        "aff_domain": "anu.edu.au;csail.mit.edu;cs.toronto.edu;gmail.com",
        "email": "anu.edu.au;csail.mit.edu;cs.toronto.edu;gmail.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "Australian National University;Massachusetts Institute of Technology;University of Toronto;NICTA",
        "aff_unique_dep": ";Computer Science and Artificial Intelligence Laboratory;Department of Computer Science;",
        "aff_unique_url": "https://www.anu.edu.au;https://www.csail.mit.edu;https://www.utoronto.ca;",
        "aff_unique_abbr": "ANU;MIT;U of T;",
        "aff_campus_unique_index": "1;2;3",
        "aff_campus_unique": ";Cambridge;Toronto;Canberra",
        "aff_country_unique_index": "0;1;2;0",
        "aff_country_unique": "Australia;United States;Canada"
    },
    {
        "id": "0f75ce9f85",
        "title": "Convex Relaxations of Latent Variable Training",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/4ea83d951990d8bf07a68ec3e50f9156-Abstract.html",
        "author": "Yuhong Guo; Dale Schuurmans",
        "abstract": "We investigate a new, convex relaxation of an expectation-maximization (EM) variant that approximates a standard objective while eliminating local minima. First, a cautionary result is presented, showing that any convex relaxation of EM over hidden variables must give trivial results if any dependence on the missing values is retained. Although this appears to be a strong negative outcome, we then demonstrate how the problem can be bypassed by using equivalence relations instead of value assignments over hidden variables. In particular, we develop new algorithms for estimating exponential conditional models that only require equivalence relation information over the variable values. This reformulation leads to an exact expression for EM variants in a wide range of problems. We then develop a semidefinite relaxation that yields global training by eliminating local minima.",
        "bibtex": "@inproceedings{NIPS2007_4ea83d95,\n author = {Guo, Yuhong and Schuurmans, Dale},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Convex Relaxations of Latent Variable Training},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/4ea83d951990d8bf07a68ec3e50f9156-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/4ea83d951990d8bf07a68ec3e50f9156-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/4ea83d951990d8bf07a68ec3e50f9156-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 99803,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15287674910741047501&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computing Science, University of Alberta; Department of Computing Science, University of Alberta",
        "aff_domain": "cs.ualberta.ca;cs.ualberta.ca",
        "email": "cs.ualberta.ca;cs.ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Alberta",
        "aff_unique_dep": "Department of Computing Science",
        "aff_unique_url": "https://www.ualberta.ca",
        "aff_unique_abbr": "UAlberta",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "93d21d5b26",
        "title": "Cooled and Relaxed Survey Propagation for MRFs",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/9778d5d219c5080b9a6a17bef029331c-Abstract.html",
        "author": "Hai L. Chieu; Wee S. Lee; Yee W. Teh",
        "abstract": "We describe a new algorithm, Relaxed Survey Propagation (RSP), for \ufb01nding MAP con\ufb01gurations in Markov random \ufb01elds. We compare its performance with state-of-the-art algorithms including the max-product belief propagation, its se- quential tree-reweighted variant, residual (sum-product) belief propagation, and tree-structured expectation propagation. We show that it outperforms all ap- proaches for Ising models with mixed couplings, as well as on a web person disambiguation task formulated as a supervised clustering problem.",
        "bibtex": "@inproceedings{NIPS2007_9778d5d2,\n author = {Chieu, Hai and Lee, Wee and Teh, Yee},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Cooled and Relaxed Survey Propagation for MRFs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9778d5d219c5080b9a6a17bef029331c-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/9778d5d219c5080b9a6a17bef029331c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/9778d5d219c5080b9a6a17bef029331c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 356234,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14154814532717910113&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Singapore MIT Alliance + Department of Computer Science, National University of Singapore; Department of Computer Science, National University of Singapore; Gatsby Computational Neuroscience Unit, University College London",
        "aff_domain": "nus.edu.sg;comp.nus.edu.sg;gatsby.ucl.ac.uk",
        "email": "nus.edu.sg;comp.nus.edu.sg;gatsby.ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1;2",
        "aff_unique_norm": "Singapore MIT Alliance;National University of Singapore;University College London",
        "aff_unique_dep": ";Department of Computer Science;Gatsby Computational Neuroscience Unit",
        "aff_unique_url": ";https://www.nus.edu.sg;https://www.ucl.ac.uk",
        "aff_unique_abbr": ";NUS;UCL",
        "aff_campus_unique_index": ";1",
        "aff_campus_unique": ";London",
        "aff_country_unique_index": "1;1;2",
        "aff_country_unique": ";Singapore;United Kingdom"
    },
    {
        "id": "d518ba01dd",
        "title": "DIFFRAC: a discriminative and flexible framework for clustering",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/22fb0cee7e1f3bde58293de743871417-Abstract.html",
        "author": "Francis R. Bach; Za\u00efd Harchaoui",
        "abstract": "We present a novel linear clustering framework (Diffrac) which relies on a linear discriminative cost function and a convex relaxation of a combinatorial optimization problem. The large convex optimization problem is solved through a sequence of lower dimensional singular value decompositions. This framework has several attractive properties: (1) although apparently similar to K-means, it exhibits superior clustering performance than K-means, in particular in terms of robustness to noise. (2) It can be readily extended to non linear clustering if the discriminative cost function is based on positive definite kernels, and can then be seen as an alternative to spectral clustering. (3) Prior information on the partition is easily incorporated, leading to state-of-the-art performance for semi-supervised learning, for clustering or classification. We present empirical evaluations of our algorithms on synthetic and real medium-scale datasets.",
        "bibtex": "@inproceedings{NIPS2007_22fb0cee,\n author = {Bach, Francis and Harchaoui, Za\\\"{\\i}d},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {DIFFRAC: a discriminative and flexible framework for clustering},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/22fb0cee7e1f3bde58293de743871417-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/22fb0cee7e1f3bde58293de743871417-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/22fb0cee7e1f3bde58293de743871417-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 98499,
        "gs_citation": 172,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2612698570529900754&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "INRIA- Willow Project, \u00b4Ecole Normale Sup\u00b4erieure, 45, rue d\u2019Ulm, 75230 Paris, France; LTCI, TELECOM ParisTech and CNRS, 46, rue Barrault, 75634 Paris cedex 13, France",
        "aff_domain": "mines.org;enst.fr",
        "email": "mines.org;enst.fr",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "INRIA- Willow Project, \u00b4Ecole Normale Sup\u00b4erieure, 45, rue d\u2019Ulm, 75230 Paris, France;LTCI, TELECOM ParisTech and CNRS, 46, rue Barrault, 75634 Paris cedex 13, France",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "859909166a",
        "title": "Density Estimation under Independent Similarly Distributed Sampling Assumptions",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/d8700cbd38cc9f30cecb34f0c195b137-Abstract.html",
        "author": "Tony Jebara; Yingbo Song; Kapil Thadani",
        "abstract": "A method is proposed for semiparametric estimation where parametric and non- parametric criteria are exploited in density estimation and unsupervised learning. This is accomplished by making sampling assumptions on a dataset that smoothly interpolate between the extreme of independently distributed (or id) sample data (as in nonparametric kernel density estimators) to the extreme of independent identically distributed (or iid) sample data. This article makes independent simi- larly distributed (or isd) sampling assumptions and interpolates between these two using a scalar parameter. The parameter controls a Bhattacharyya af\ufb01nity penalty between pairs of distributions on samples. Surprisingly, the isd method maintains certain consistency and unimodality properties akin to maximum likelihood esti- mation. The proposed isd scheme is an alternative for handling nonstationarity in data without making drastic hidden variable assumptions which often make esti- mation dif\ufb01cult and laden with local optima. Experiments in density estimation on a variety of datasets con\ufb01rm the value of isd over iid estimation, id estimation and mixture modeling.",
        "bibtex": "@inproceedings{NIPS2007_d8700cbd,\n author = {Jebara, Tony and Song, Yingbo and Thadani, Kapil},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Density Estimation under Independent Similarly Distributed Sampling Assumptions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d8700cbd38cc9f30cecb34f0c195b137-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/d8700cbd38cc9f30cecb34f0c195b137-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/d8700cbd38cc9f30cecb34f0c195b137-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 961486,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17404232761348866299&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Science, Columbia University; Department of Computer Science, Columbia University; Department of Computer Science, Columbia University",
        "aff_domain": "cs.columbia.edu;cs.columbia.edu;cs.columbia.edu",
        "email": "cs.columbia.edu;cs.columbia.edu;cs.columbia.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Columbia University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.columbia.edu",
        "aff_unique_abbr": "Columbia",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c0ffc1c523",
        "title": "Direct Importance Estimation with Model Selection and Its Application to Covariate Shift Adaptation",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract.html",
        "author": "Masashi Sugiyama; Shinichi Nakajima; Hisashi Kashima; Paul V. Buenau; Motoaki Kawanabe",
        "abstract": "When training and test samples follow different input distributions (i.e., the situation called \\emph{covariate shift}), the maximum likelihood estimator is known to lose its consistency. For regaining consistency, the log-likelihood terms need to be weighted according to the \\emph{importance} (i.e., the ratio of test and training input densities). Thus, accurately estimating the importance is one of the key tasks in covariate shift adaptation. A naive approach is to first estimate training and test input densities and then estimate the importance by the ratio of the density estimates. However, since density estimation is a hard problem, this approach tends to perform poorly especially in high dimensional cases. In this paper, we propose a direct importance estimation method that does not require the input density estimates. Our method is equipped with a natural model selection procedure so tuning parameters such as the kernel width can be objectively optimized. This is an advantage over a recently developed method of direct importance estimation. Simulations illustrate the usefulness of our approach.",
        "bibtex": "@inproceedings{NIPS2007_be83ab3e,\n author = {Sugiyama, Masashi and Nakajima, Shinichi and Kashima, Hisashi and Buenau, Paul and Kawanabe, Motoaki},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Direct Importance Estimation with Model Selection and Its Application to Covariate Shift Adaptation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/be83ab3ecd0db773eb2dc1b0a17836a1-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/be83ab3ecd0db773eb2dc1b0a17836a1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/be83ab3ecd0db773eb2dc1b0a17836a1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 116265,
        "gs_citation": 1130,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12513287581742516772&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Tokyo Institute of Technology; Nikon Corporation; IBM Research; Technical University Berlin; Fraunhofer FIRST",
        "aff_domain": "cs.titech.ac.jp;nikon.co.jp;jp.ibm.com;cs.tu-berlin.de;first.fhg.de",
        "email": "cs.titech.ac.jp;nikon.co.jp;jp.ibm.com;cs.tu-berlin.de;first.fhg.de",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3;4",
        "aff_unique_norm": "Tokyo Institute of Technology;Nikon Corporation;IBM;Technical University Berlin;Fraunhofer FIRST",
        "aff_unique_dep": ";;IBM Research;;",
        "aff_unique_url": "https://www.titech.ac.jp;https://www.nikon.com;https://www.ibm.com/research;;",
        "aff_unique_abbr": "Titech;Nikon;IBM;;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "Japan;United States;"
    },
    {
        "id": "a8dbdf3236",
        "title": "Discovering Weakly-Interacting Factors in a Complex Stochastic Process",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/8597a6cfa74defcbde3047c891d78f90-Abstract.html",
        "author": "Charlie Frogner; Avi Pfeffer",
        "abstract": "Dynamic Bayesian networks are structured representations of stochastic pro- cesses. Despite their structure, exact inference in DBNs is generally intractable. One approach to approximate inference involves grouping the variables in the process into smaller factors and keeping independent beliefs over these factors. In this paper we present several techniques for decomposing a dynamic Bayesian network automatically to enable factored inference. We examine a number of fea- tures of a DBN that capture different types of dependencies that will cause error in factored inference. An empirical comparison shows that the most useful of these is a heuristic that estimates the mutual information introduced between factors by one step of belief propagation. In addition to features computed over entire factors, for ef\ufb01ciency we explored scores computed over pairs of variables. We present search methods that use these features, pairwise and not, to \ufb01nd a factor- ization, and we compare their results on several datasets. Automatic factorization extends the applicability of factored inference to large, complex models that are undesirable to factor by hand. Moreover, tests on real DBNs show that automatic factorization can achieve signi\ufb01cantly lower error in some cases.",
        "bibtex": "@inproceedings{NIPS2007_8597a6cf,\n author = {Frogner, Charlie and Pfeffer, Avi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Discovering Weakly-Interacting Factors in a Complex Stochastic Process},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8597a6cfa74defcbde3047c891d78f90-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/8597a6cfa74defcbde3047c891d78f90-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/8597a6cfa74defcbde3047c891d78f90-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 147683,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5084191863822921445&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "School of Engineering and Applied Sciences, Harvard University, Cambridge, MA 02138; School of Engineering and Applied Sciences, Harvard University, Cambridge, MA 02138",
        "aff_domain": "seas.harvard.edu;eecs.harvard.edu",
        "email": "seas.harvard.edu;eecs.harvard.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "School of Engineering and Applied Sciences, Harvard University, Cambridge, MA 02138",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "f8bbf03d1a",
        "title": "Discriminative Batch Mode Active Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/ccc0aa1b81bf81e16c676ddb977c5881-Abstract.html",
        "author": "Yuhong Guo; Dale Schuurmans",
        "abstract": "Active learning sequentially selects unlabeled instances to label with the goal of reducing the effort needed to learn a good classifier. Most previous studies in active learning have focused on selecting one unlabeled instance at one time while retraining in each iteration. However, single instance selection systems are unable to exploit a parallelized labeler when one is available. Recently a few batch mode active learning approaches have been proposed that select a set of most informative unlabeled instances in each iteration, guided by some heuristic scores. In this paper, we propose a discriminative batch mode active learning approach that formulates the instance selection task as a continuous optimization problem over auxiliary instance selection variables. The optimization is formuated to maximize the discriminative classification performance of the target classifier, while also taking the unlabeled data into account. Although the objective is not convex, we can manipulate a quasi-Newton method to obtain a good local solution. Our empirical studies on UCI datasets show that the proposed active learning is more effective than current state-of-the art batch mode active learning algorithms.",
        "bibtex": "@inproceedings{NIPS2007_ccc0aa1b,\n author = {Guo, Yuhong and Schuurmans, Dale},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Discriminative Batch Mode Active Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/ccc0aa1b81bf81e16c676ddb977c5881-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/ccc0aa1b81bf81e16c676ddb977c5881-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/ccc0aa1b81bf81e16c676ddb977c5881-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 130784,
        "gs_citation": 382,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3414871306860281126&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computing Science, University of Alberta; Department of Computing Science, University of Alberta",
        "aff_domain": "cs.ualberta.ca;cs.ualberta.ca",
        "email": "cs.ualberta.ca;cs.ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Alberta",
        "aff_unique_dep": "Department of Computing Science",
        "aff_unique_url": "https://www.ualberta.ca",
        "aff_unique_abbr": "UAlberta",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "c7433a36c3",
        "title": "Discriminative K-means for Clustering",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/a5cdd4aa0048b187f7182f1b9ce7a6a7-Abstract.html",
        "author": "Jieping Ye; Zheng Zhao; Mingrui Wu",
        "abstract": "We present a theoretical study on the discriminative clustering framework, recently proposed for simultaneous subspace selection via linear discriminant analysis (LDA) and clustering. Empirical results have shown its favorable performance in comparison with several other popular clustering algorithms. However, the inherent relationship between subspace selection and clustering in this framework is not well understood, due to the iterative nature of the algorithm. We show in this paper that this iterative subspace selection and clustering is equivalent to kernel K-means with a specific kernel Gram matrix. This provides significant and new insights into the nature of this subspace selection procedure. Based on this equivalence relationship, we propose the Discriminative K-means (DisKmeans) algorithm for simultaneous LDA subspace selection and clustering, as well as an automatic parameter estimation procedure. We also present the nonlinear extension of DisKmeans using kernels. We show that the learning of the kernel matrix over a convex set of pre-specified kernel matrices can be incorporated into the clustering formulation. The connection between DisKmeans and several other clustering algorithms is also analyzed. The presented theories and algorithms are evaluated through experiments on a collection of benchmark data sets.",
        "bibtex": "@inproceedings{NIPS2007_a5cdd4aa,\n author = {Ye, Jieping and Zhao, Zheng and Wu, Mingrui},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Discriminative K-means for Clustering},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a5cdd4aa0048b187f7182f1b9ce7a6a7-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/a5cdd4aa0048b187f7182f1b9ce7a6a7-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/a5cdd4aa0048b187f7182f1b9ce7a6a7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 251817,
        "gs_citation": 274,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7811921708077404701&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Arizona State University; Arizona State University; MPI for Biological Cybernetics",
        "aff_domain": "asu.edu;asu.edu;tuebingen.mpg.de",
        "email": "asu.edu;asu.edu;tuebingen.mpg.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Arizona State University;Max Planck Institute for Biological Cybernetics",
        "aff_unique_dep": ";Biological Cybernetics",
        "aff_unique_url": "https://www.asu.edu;https://www.biological-cybernetics.de",
        "aff_unique_abbr": "ASU;MPIBC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "United States;Germany"
    },
    {
        "id": "c6280f3f0c",
        "title": "Discriminative Keyword Selection Using Support Vector Machines",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/d6c651ddcd97183b2e40bc464231c962-Abstract.html",
        "author": "Fred Richardson; William M. Campbell",
        "abstract": "Many tasks in speech processing involve classification of long term characteristics of a speech segment such as language, speaker, dialect, or topic. A natural technique for determining these characteristics is to first convert the input speech into a sequence of tokens such as words, phones, etc. From these tokens, we can then look for distinctive phrases, keywords, that characterize the speech. In many applications, a set of distinctive keywords may not be known a priori. In this case, an automatic method of building up keywords from short context units such as phones is desirable. We propose a method for construction of keywords based upon Support Vector Machines. We cast the problem of keyword selection as a feature selection problem for n-grams of phones. We propose an alternating filter-wrapper method that builds successively longer keywords. Application of this method on a language recognition task shows that the technique produces interesting and significant qualitative and quantitative results.",
        "bibtex": "@inproceedings{NIPS2007_d6c651dd,\n author = {Richardson, Fred and Campbell, William},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Discriminative Keyword Selection Using Support Vector Machines},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d6c651ddcd97183b2e40bc464231c962-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/d6c651ddcd97183b2e40bc464231c962-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/d6c651ddcd97183b2e40bc464231c962-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 81857,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17518551127713898086&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "38b4aba937",
        "title": "Discriminative Log-Linear Grammars with Latent Variables",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/9cc138f8dc04cbf16240daa92d8d50e2-Abstract.html",
        "author": "Slav Petrov; Dan Klein",
        "abstract": "We demonstrate that log-linear grammars with latent variables can be practically trained using discriminative methods. Central to ef\ufb01cient discriminative training is a hierarchical pruning procedure which allows feature expectations to be ef\ufb01- ciently approximated in a gradient-based procedure. We compare L1 and L2 reg- ularization and show that L1 regularization is superior, requiring fewer iterations to converge, and yielding sparser solutions. On full-scale treebank parsing exper- iments, the discriminative latent models outperform both the comparable genera- tive latent models as well as the discriminative non-latent baselines.",
        "bibtex": "@inproceedings{NIPS2007_9cc138f8,\n author = {Petrov, Slav and Klein, Dan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Discriminative Log-Linear Grammars with Latent Variables},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9cc138f8dc04cbf16240daa92d8d50e2-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/9cc138f8dc04cbf16240daa92d8d50e2-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/9cc138f8dc04cbf16240daa92d8d50e2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 79629,
        "gs_citation": 102,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6024458936697100742&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Computer Science Department, EECS Division, University of California at Berkeley, Berkeley, CA, 94720; Computer Science Department, EECS Division, University of California at Berkeley, Berkeley, CA, 94720",
        "aff_domain": "cs.berkeley.edu;cs.berkeley.edu",
        "email": "cs.berkeley.edu;cs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Computer Science Department, EECS Division, University of California at Berkeley, Berkeley, CA, 94720",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "3a2b55310f",
        "title": "Distributed Inference for Latent Dirichlet Allocation",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/2dea61eed4bceec564a00115c4d21334-Abstract.html",
        "author": "David Newman; Padhraic Smyth; Max Welling; Arthur U. Asuncion",
        "abstract": "processors only sees",
        "bibtex": "@inproceedings{NIPS2007_2dea61ee,\n author = {Newman, David and Smyth, Padhraic and Welling, Max and Asuncion, Arthur},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Distributed Inference for Latent Dirichlet Allocation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/2dea61eed4bceec564a00115c4d21334-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/2dea61eed4bceec564a00115c4d21334-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/2dea61eed4bceec564a00115c4d21334-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 131109,
        "gs_citation": 345,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8671732665265325017&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Department of Computer Science, University of California, Irvine; Department of Computer Science, University of California, Irvine; Department of Computer Science, University of California, Irvine; Department of Computer Science, University of California, Irvine",
        "aff_domain": "ics.uci.edu;ics.uci.edu;ics.uci.edu;ics.uci.edu",
        "email": "ics.uci.edu;ics.uci.edu;ics.uci.edu;ics.uci.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of California, Irvine",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.uci.edu",
        "aff_unique_abbr": "UCI",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Irvine",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "8a5c5332ff",
        "title": "EEG-Based Brain-Computer Interaction: Improved Accuracy by Automatic Single-Trial Error Detection",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/6974ce5ac660610b44d9b9fed0ff9548-Abstract.html",
        "author": "Pierre Ferrez; Jos\u00e9 Mill\u00e1n",
        "abstract": "Brain-computer interfaces (BCIs), as any other interaction modality based on physiological signals and body channels (e.g., muscular activity, speech and gestures), are prone to errors in the recognition of subject's intent. An elegant approach to improve the accuracy of BCIs consists in a verification procedure directly based on the presence of error-related potentials (ErrP) in the EEG recorded right after the occurrence of an error. Six healthy volunteer subjects with no prior BCI experience participated in a new human-robot interaction experiment where they were asked to mentally move a cursor towards a target that can be reached within a few steps using motor imagination. This experiment confirms the previously reported presence of a new kind of ErrP. These Interaction ErrP\" exhibit a first sharp negative peak followed by a positive peak and a second broader negative peak (~290, ~350 and ~470 ms after the feedback, respectively). But in order to exploit these ErrP we need to detect them in each single trial using a short window following the feedback associated to the response of the classifier embedded in the BCI. We have achieved an average recognition rate of correct and erroneous single trials of 81.8% and 76.2%, respectively. Furthermore, we have achieved an average recognition rate of the subject's intent while trying to mentally drive the cursor of 73.1%. These results show that it's possible to simultaneously extract useful information for mental control to operate a brain-actuated device as well as cognitive states such as error potentials to improve the quality of the brain-computer interaction. Finally, using a well-known inverse model (sLORETA), we show that the main focus of activity at the occurrence of the ErrP are, as expected, in the pre-supplementary motor area and in the anterior cingulate cortex.\"",
        "bibtex": "@inproceedings{NIPS2007_6974ce5a,\n author = {Ferrez, Pierre and Mill\\'{a}n, Jos\\'{e}},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {EEG-Based Brain-Computer Interaction: Improved Accuracy by Automatic Single-Trial Error Detection},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/6974ce5ac660610b44d9b9fed0ff9548-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/6974ce5ac660610b44d9b9fed0ff9548-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/6974ce5ac660610b44d9b9fed0ff9548-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/6974ce5ac660610b44d9b9fed0ff9548-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 308821,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17532612363132971239&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "IDIAP Research Institute; IDIAP Research Institute",
        "aff_domain": "idiap.ch;idiap.ch",
        "email": "idiap.ch;idiap.ch",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "IDIAP Research Institute",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.idiap.ch",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "bdb6b10952",
        "title": "Efficient Bayesian Inference for Dynamically Changing Graphs",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/8757150decbd89b0f5442ca3db4d0e0e-Abstract.html",
        "author": "Ozgur Sumer; Umut Acar; Alexander T. Ihler; Ramgopal R. Mettu",
        "abstract": "Motivated by stochastic systems in which observed evidence and conditional de- pendencies between states of the network change over time, and certain quantities of interest (marginal distributions, likelihood estimates etc.) must be updated, we study the problem of adaptive inference in tree-structured Bayesian networks. We describe an algorithm for adaptive inference that handles a broad range of changes to the network and is able to maintain marginal distributions, MAP estimates, and data likelihoods in all expected logarithmic time. We give an implementation of our algorithm and provide experiments that show that the algorithm can yield up to two orders of magnitude speedups on answering queries and responding to dy- namic changes over the sum-product algorithm.",
        "bibtex": "@inproceedings{NIPS2007_8757150d,\n author = {Sumer, Ozgur and Acar, Umut and Ihler, Alexander and Mettu, Ramgopal},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Efficient Bayesian Inference for Dynamically Changing Graphs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8757150decbd89b0f5442ca3db4d0e0e-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/8757150decbd89b0f5442ca3db4d0e0e-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/8757150decbd89b0f5442ca3db4d0e0e-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/8757150decbd89b0f5442ca3db4d0e0e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 192834,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2669859237827067497&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 2,
        "aff": "Toyota Tech. Inst. Chicago, IL; U.C. Irvine Irvine, CA; Univ. of Massachusetts Amherst, MA; Uni. of Chicago Chicago, IL",
        "aff_domain": "tti-c.org;ics.uci.edu;ecs.umass.edu;cs.uchicago.edu",
        "email": "tti-c.org;ics.uci.edu;ecs.umass.edu;cs.uchicago.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "Toyota Tech. Inst. Chicago, IL;U.C. Irvine Irvine, CA;Univ. of Massachusetts Amherst, MA;Uni. of Chicago Chicago, IL",
        "aff_unique_dep": ";;;",
        "aff_unique_url": ";;;",
        "aff_unique_abbr": ";;;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "d9f4d28eff",
        "title": "Efficient Convex Relaxation for Transductive Support Vector Machine",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html",
        "author": "Zenglin Xu; Rong Jin; Jianke Zhu; Irwin King; Michael Lyu",
        "abstract": "We consider the problem of Support Vector Machine transduction, which involves a combinatorial problem with exponential computational complexity in the number of unlabeled examples. Although several studies are devoted to Transductive SVM, they suffer either from the high computation complexity or from the solutions of local optimum. To address this problem, we propose solving Transductive SVM via a convex relaxation, which converts the NP-hard problem to a semi-definite programming. Compared with the other SDP relaxation for Transductive SVM, the proposed algorithm is computationally more efficient with the number of free parameters reduced from O(n2) to O(n) where n is the number of examples. Empirical study with several benchmark data sets shows the promising performance of the proposed algorithm in comparison with other state-of-the-art implementations of Transductive SVM.",
        "bibtex": "@inproceedings{NIPS2007_c399862d,\n author = {Xu, Zenglin and Jin, Rong and Zhu, Jianke and King, Irwin and Lyu, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Efficient Convex Relaxation for Transductive Support Vector Machine},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/c399862d3b9d6b76c8436e924a68c45b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 75268,
        "gs_citation": 74,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14651963809924788452&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Dept. of Computer Science & Engineering, The Chinese University of Hong Kong; Dept. of Computer Science & Engineering, Michigan State University; Dept. of Computer Science & Engineering, The Chinese University of Hong Kong; Dept. of Computer Science & Engineering, The Chinese University of Hong Kong; Dept. of Computer Science & Engineering, The Chinese University of Hong Kong",
        "aff_domain": "cse.cuhk.edu.hk;cse.msu.edu;cse.cuhk.edu.hk;cse.cuhk.edu.hk;cse.cuhk.edu.hk",
        "email": "cse.cuhk.edu.hk;cse.msu.edu;cse.cuhk.edu.hk;cse.cuhk.edu.hk;cse.cuhk.edu.hk",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0;0",
        "aff_unique_norm": "The Chinese University of Hong Kong;Dept. of Computer Science & Engineering, Michigan State University",
        "aff_unique_dep": "Dept. of Computer Science & Engineering;",
        "aff_unique_url": "https://www.cuhk.edu.hk;",
        "aff_unique_abbr": "CUHK;",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China;"
    },
    {
        "id": "6c1cff6da1",
        "title": "Efficient Inference for Distributions on Permutations",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/d9896106ca98d3d05b8cbdf4fd8b13a1-Abstract.html",
        "author": "Jonathan Huang; Carlos Guestrin; Leonidas Guibas",
        "abstract": "Permutations are ubiquitous in many real world problems, such as voting, rankings and data association. Representing uncertainty over permutations is challenging, since there are n! possibilities, and typical compact representations such as graphical models cannot ef\ufb01ciently capture the mutual exclusivity con- straints associated with permutations. In this paper, we use the \u201clow-frequency\u201d terms of a Fourier decomposition to represent such distributions compactly. We present Kronecker conditioning, a general and ef\ufb01cient approach for maintaining these distributions directly in the Fourier domain. Low order Fourier-based approximations can lead to functions that do not correspond to valid distributions. To address this problem, we present an ef\ufb01cient quadratic program de\ufb01ned directly in the Fourier domain to project the approximation onto a relaxed form of the marginal polytope. We demonstrate the effectiveness of our approach on a real camera-based multi-people tracking setting.",
        "bibtex": "@inproceedings{NIPS2007_d9896106,\n author = {Huang, Jonathan and Guestrin, Carlos and Guibas, Leonidas J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Efficient Inference for Distributions on Permutations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d9896106ca98d3d05b8cbdf4fd8b13a1-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/d9896106ca98d3d05b8cbdf4fd8b13a1-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/d9896106ca98d3d05b8cbdf4fd8b13a1-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/d9896106ca98d3d05b8cbdf4fd8b13a1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 112485,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11481363870143170097&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Carnegie Mellon University; Carnegie Mellon University; Stanford University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;cs.stanford.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Carnegie Mellon University;Stanford University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cmu.edu;https://www.stanford.edu",
        "aff_unique_abbr": "CMU;Stanford",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Stanford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "4cd0873eee",
        "title": "Efficient Principled Learning of Thin Junction Trees",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/0768281a05da9f27df178b5c39a51263-Abstract.html",
        "author": "Anton Chechetka; Carlos Guestrin",
        "abstract": "We present the first truly polynomial algorithm for learning the structure of bounded-treewidth junction trees -- an attractive subclass of probabilistic graphical models that permits both the compact representation of probability distributions and efficient exact inference. For a constant treewidth, our algorithm has polynomial time and sample complexity, and provides strong theoretical guarantees in terms of $KL$ divergence from the true distribution. We also present a lazy extension of our approach that leads to very significant speed ups in practice, and demonstrate the viability of our method empirically, on several real world datasets. One of our key new theoretical insights is a method for bounding the conditional mutual information of arbitrarily large sets of random variables with only a polynomial number of mutual information computations on fixed-size subsets of variables, when the underlying distribution can be approximated by a bounded treewidth junction tree.",
        "bibtex": "@inproceedings{NIPS2007_0768281a,\n author = {Chechetka, Anton and Guestrin, Carlos},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Efficient Principled Learning of Thin Junction Trees},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/0768281a05da9f27df178b5c39a51263-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/0768281a05da9f27df178b5c39a51263-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/0768281a05da9f27df178b5c39a51263-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 126390,
        "gs_citation": 112,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3586863464122717911&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Carnegie Mellon University; Carnegie Mellon University",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "204c208def",
        "title": "Efficient multiple hyperparameter learning for log-linear models",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/851ddf5058cf22df63d3344ad89919cf-Abstract.html",
        "author": "Chuan-sheng Foo; Chuong B. Do; Andrew Y. Ng",
        "abstract": "Using multiple regularization hyperparameters is an effective method for managing model complexity in problems where input features have varying amounts of noise. While algorithms for choosing multiple hyperparameters are often used in neural networks and support vector machines, they are not common in structured prediction tasks, such as sequence labeling or parsing. In this paper, we consider the problem of learning regularization hyperparameters for log-linear models, a class of probabilistic models for structured prediction tasks which includes conditional random fields (CRFs). Using an implicit differentiation trick, we derive an efficient gradient-based method for learning Gaussian regularization priors with multiple hyperparameters. In both simulations and the real-world task of computational RNA secondary structure prediction, we find that multiple hyperparameter learning provides a significant boost in accuracy compared to models learned using only a single regularization hyperparameter.",
        "bibtex": "@inproceedings{NIPS2007_851ddf50,\n author = {Foo, Chuan-sheng and B., Chuong and Ng, Andrew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Efficient multiple hyperparameter learning for log-linear models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/851ddf5058cf22df63d3344ad89919cf-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/851ddf5058cf22df63d3344ad89919cf-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/851ddf5058cf22df63d3344ad89919cf-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 95391,
        "gs_citation": 161,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7723118511857312602&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Computer Science Department, Stanford University; Computer Science Department, Stanford University; Computer Science Department, Stanford University",
        "aff_domain": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "371e89be05",
        "title": "Ensemble Clustering using Semidefinite Programming",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/1385974ed5904a438616ff7bdb3f7439-Abstract.html",
        "author": "Vikas Singh; Lopamudra Mukherjee; Jiming Peng; Jinhui Xu",
        "abstract": "We consider the ensemble clustering problem where the task is to \u2018aggregate\u2019 multiple clustering solutions into a single consolidated clustering that maximizes the shared information among given clustering solutions. We obtain several new results for this problem. First, we note that the notion of agreement under such circumstances can be better captured using an agreement measure based on a 2D string encoding rather than voting strategy based methods proposed in literature. Using this generalization, we \ufb01rst derive a nonlinear optimization model to max- imize the new agreement measure. We then show that our optimization problem can be transformed into a strict 0-1 Semide\ufb01nite Program (SDP) via novel con- vexi\ufb01cation techniques which can subsequently be relaxed to a polynomial time solvable SDP. Our experiments indicate improvements not only in terms of the proposed agreement measure but also the existing agreement measures based on voting strategies. We discuss evaluations on clustering and image segmentation databases.",
        "bibtex": "@inproceedings{NIPS2007_1385974e,\n author = {Singh, Vikas and Mukherjee, Lopamudra and Peng, Jiming and Xu, Jinhui},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Ensemble Clustering using Semidefinite Programming},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/1385974ed5904a438616ff7bdb3f7439-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/1385974ed5904a438616ff7bdb3f7439-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/1385974ed5904a438616ff7bdb3f7439-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/1385974ed5904a438616ff7bdb3f7439-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 223245,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9263881692031662951&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Biostatistics and Medical Informatics, University of Wisconsin \u2013 Madison; Computer Science and Engineering, State University of New York at Buffalo; Industrial and Enterprise System Engineering, University of Illinois at Urbana-Champaign; Computer Science and Engineering, State University of New York at Buffalo",
        "aff_domain": "biostat.wisc.edu;cse.buffalo.edu;uiuc.edu;cse.buffalo.edu",
        "email": "biostat.wisc.edu;cse.buffalo.edu;uiuc.edu;cse.buffalo.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;1",
        "aff_unique_norm": "Biostatistics and Medical Informatics, University of Wisconsin \u2013 Madison;Computer Science and Engineering, State University of New York at Buffalo;Industrial and Enterprise System Engineering, University of Illinois at Urbana-Champaign",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "3055ee045a",
        "title": "Estimating disparity with confidence from energy neurons",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/8a1e808b55fde9455cb3d8857ed88389-Abstract.html",
        "author": "Eric K. Tsang; Bertram E. Shi",
        "abstract": "Binocular fusion takes place over a limited region smaller than one degree of visual angle (Panum's fusional area), which is on the order of the range of preferred disparities measured in populations of disparity-tuned neurons in the visual cortex. However, the actual range of binocular disparities encountered in natural scenes ranges over tens of degrees. This discrepancy suggests that there must be a mechanism for detecting whether the stimulus disparity is either inside or outside of the range of the preferred disparities in the population. Here, we present a statistical framework to derive feature in a population of V1 disparity neuron to determine the stimulus disparity within the preferred disparity range of the neural population. When optimized for natural images, it yields a feature that can be explained by the normalization which is a common model in V1 neurons. We further makes use of the feature to estimate the disparity in natural images. Our proposed model generates more correct estimates than coarse-to-fine multiple scales approaches and it can also identify regions with occlusion. The approach suggests another critical role for normalization in robust disparity estimation.",
        "bibtex": "@inproceedings{NIPS2007_8a1e808b,\n author = {Tsang, Eric and Shi, Bertram},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Estimating disparity with confidence from energy neurons},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8a1e808b55fde9455cb3d8857ed88389-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/8a1e808b55fde9455cb3d8857ed88389-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/8a1e808b55fde9455cb3d8857ed88389-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 364487,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11151125596746812246&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Dept. of Electronic and Computer Engr., Hong Kong Univ. of Sci. and Tech., Kowloon, HONG KONG SAR; Dept. of Electronic and Computer Engr., Hong Kong Univ. of Sci. and Tech., Kowloon, HONG KONG SAR",
        "aff_domain": "ee.ust.hk;ee.ust.hk",
        "email": "ee.ust.hk;ee.ust.hk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Dept. of Electronic and Computer Engr., Hong Kong Univ. of Sci. and Tech., Kowloon, HONG KONG SAR",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "a5e9097631",
        "title": "Estimating divergence functionals and the likelihood ratio by penalized convex risk minimization",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/72da7fd6d1302c0a159f6436d01e9eb0-Abstract.html",
        "author": "Xuanlong Nguyen; Martin J. Wainwright; Michael I. Jordan",
        "abstract": "We develop and analyze an algorithm for nonparametric estimation of divergence functionals and the density ratio of two probability distributions. Our method is based on a variational characterization of f-divergences, which turns the estima- tion into a penalized convex risk minimization problem. We present a derivation of our kernel-based estimation algorithm and an analysis of convergence rates for the estimator. Our simulation results demonstrate the convergence behavior of the method, which compares favorably with existing methods in the literature.",
        "bibtex": "@inproceedings{NIPS2007_72da7fd6,\n author = {Nguyen, XuanLong and Wainwright, Martin J and Jordan, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Estimating divergence functionals and the likelihood ratio by penalized convex risk minimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/72da7fd6d1302c0a159f6436d01e9eb0-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/72da7fd6d1302c0a159f6436d01e9eb0-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/72da7fd6d1302c0a159f6436d01e9eb0-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/72da7fd6d1302c0a159f6436d01e9eb0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 109997,
        "gs_citation": 149,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16712554677864898959&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "SAMSI & Duke University; UC Berkeley; UC Berkeley",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "SAMSI & Duke University;University of California, Berkeley",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://www.berkeley.edu",
        "aff_unique_abbr": ";UC Berkeley",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "1;1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "57eb96cf95",
        "title": "Evaluating Search Engines by Modeling the Relationship Between Relevance and Clicks",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/872488f88d1b2db54d55bc8bba2fad1b-Abstract.html",
        "author": "Ben Carterette; Rosie Jones",
        "abstract": "We propose a model that leverages the millions of clicks received by web search engines, to predict document relevance. This allows the comparison of ranking functions when clicks are available but complete relevance judgments are not. After an initial training phase using a set of relevance judgments paired with click data, we show that our model can predict the relevance score of documents that have not been judged. These predictions can be used to evaluate the performance of a search engine, using our novel formalization of the confidence of the standard evaluation metric discounted cumulative gain (DCG), so comparisons can be made across time and datasets. This contrasts with previous methods which can provide only pair-wise relevance judgements between results shown for the same query. When no relevance judgments are available, we can identify the better of two ranked lists up to 82% of the time, and with only two relevance judgments for each query, we can identify the better ranking up to 94% of the time. While our experiments are on sponsored search results, which is the financial backbone of web search, our method is general enough to be applicable to algorithmic web search results as well. Furthermore, we give an algorithm to guide the selection of additional documents to judge to improve confidence.",
        "bibtex": "@inproceedings{NIPS2007_872488f8,\n author = {Carterette, Ben and Jones, Rosie},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Evaluating Search Engines by Modeling the Relationship Between Relevance and Clicks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/872488f88d1b2db54d55bc8bba2fad1b-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/872488f88d1b2db54d55bc8bba2fad1b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/872488f88d1b2db54d55bc8bba2fad1b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 190379,
        "gs_citation": 169,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10041715697976574886&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Center for Intelligent Information Retrieval, University of Massachusetts Amherst; Yahoo! Research",
        "aff_domain": "cs.umass.edu;yahoo-inc.com",
        "email": "cs.umass.edu;yahoo-inc.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Massachusetts Amherst;Yahoo!",
        "aff_unique_dep": "Center for Intelligent Information Retrieval;Yahoo! Research",
        "aff_unique_url": "https://www.umass.edu;https://research.yahoo.com",
        "aff_unique_abbr": "UMass Amherst;Yahoo!",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Amherst;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "21b298b7c7",
        "title": "Expectation Maximization and Posterior Constraints",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/73e5080f0f3804cb9cf470a8ce895dac-Abstract.html",
        "author": "Kuzman Ganchev; Ben Taskar; Jo\u00e3o Gama",
        "abstract": "The expectation maximization (EM) algorithm is a widely used maximum likelihood estimation procedure for statistical models when the values of some of the variables in the model are not observed. Very often, however, our aim is primarily to find a model that assigns values to the latent variables that have intended meaning for our data and maximizing expected likelihood only sometimes accomplishes this. Unfortunately, it is typically difficult to add even simple a-priori information about latent variables in graphical models without making the models overly complex or intractable. In this paper, we present an efficient, principled way to inject rich constraints on the posteriors of latent variables into the EM algorithm. Our method can be used to learn tractable graphical models that satisfy additional, otherwise intractable constraints. Focusing on clustering and the alignment problem for statistical machine translation, we show that simple, intuitive posterior constraints can greatly improve the performance over standard baselines and be competitive with more complex, intractable models.",
        "bibtex": "@inproceedings{NIPS2007_73e5080f,\n author = {Ganchev, Kuzman and Taskar, Ben and Gama, Jo\\~{a}o},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Expectation Maximization and Posterior Constraints},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/73e5080f0f3804cb9cf470a8ce895dac-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/73e5080f0f3804cb9cf470a8ce895dac-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/73e5080f0f3804cb9cf470a8ce895dac-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 199317,
        "gs_citation": 203,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5588501187010085157&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "L2F INESC-ID; Computer & Information Science, University of Pennsylvania; Computer & Information Science, University of Pennsylvania",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "L2F INESC-ID;University of Pennsylvania",
        "aff_unique_dep": ";Computer & Information Science",
        "aff_unique_url": ";https://www.upenn.edu",
        "aff_unique_abbr": ";UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "1a2cfda5bf",
        "title": "Experience-Guided Search: A Theory of Attentional Control",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/efe937780e95574250dabe07151bdc23-Abstract.html",
        "author": "David Baldwin; Michael Mozer",
        "abstract": "People perform a remarkable range of tasks that require search of the visual en- vironment for a target item among distractors. The Guided Search model (Wolfe, 1994, 2007), or GS, is perhaps the best developed psychological account of hu- man visual search. To prioritize search, GS assigns saliency to locations in the visual \ufb01eld. Saliency is a linear combination of activations from retinotopic maps representing primitive visual features. GS includes heuristics for setting the gain coef\ufb01cient associated with each map. Variants of GS have formalized the notion of optimization as a principle of attentional control (e.g., Baldwin & Mozer, 2006; Cave, 1999; Navalpakkam & Itti, 2006; Rao et al., 2002), but every GS-like model must be \u2019dumbed down\u2019 to match human data, e.g., by corrupting the saliency map with noise and by imposing arbitrary restrictions on gain modulation. We propose a principled probabilistic formulation of GS, called Experience-Guided Search (EGS), based on a generative model of the environment that makes three claims: (1) Feature detectors produce Poisson spike trains whose rates are conditioned on feature type and whether the feature belongs to a target or distractor; (2) the en- vironment and/or task is nonstationary and can change over a sequence of trials; and (3) a prior speci\ufb01es that features are more likely to be present for target than for distractors. Through experience, EGS infers latent environment variables that determine the gains for guiding search. Control is thus cast as probabilistic infer- ence, not optimization. We show that EGS can replicate a range of human data from visual search, including data that GS does not address.",
        "bibtex": "@inproceedings{NIPS2007_efe93778,\n author = {Baldwin, David and Mozer, Michael C},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Experience-Guided Search: A Theory of Attentional Control},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/efe937780e95574250dabe07151bdc23-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/efe937780e95574250dabe07151bdc23-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/efe937780e95574250dabe07151bdc23-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 274927,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15593269422970259620&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Science and Institute of Cognitive Science, University of Colorado; Department of Computer Science, Indiana University",
        "aff_domain": "colorado.edu;indiana.edu",
        "email": "colorado.edu;indiana.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Department of Computer Science and Institute of Cognitive Science, University of Colorado;Indiana University",
        "aff_unique_dep": ";Department of Computer Science",
        "aff_unique_url": ";https://www.indiana.edu",
        "aff_unique_abbr": ";IU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "c477418eff",
        "title": "Exponential Family Predictive Representations of State",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/a9a1d5317a33ae8cef33961c34144f84-Abstract.html",
        "author": "David Wingate; Satinder S. Baveja",
        "abstract": "In order to represent state in controlled, partially observable, stochastic dynamical systems, some sort of suf\ufb01cient statistic for history is necessary. Predictive repre- sentations of state (PSRs) capture state as statistics of the future. We introduce a new model of such systems called the \u201cExponential family PSR,\u201d which de\ufb01nes as state the time-varying parameters of an exponential family distribution which models n sequential observations in the future. This choice of state representation explicitly connects PSRs to state-of-the-art probabilistic modeling, which allows us to take advantage of current efforts in high-dimensional density estimation, and in particular, graphical models and maximum entropy models. We present a pa- rameter learning algorithm based on maximum likelihood, and we show how a variety of current approximate inference methods apply. We evaluate the qual- ity of our model with reinforcement learning by directly evaluating the control performance of the model.",
        "bibtex": "@inproceedings{NIPS2007_a9a1d531,\n author = {Wingate, David and Baveja, Satinder},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Exponential Family Predictive Representations of State},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a9a1d5317a33ae8cef33961c34144f84-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/a9a1d5317a33ae8cef33961c34144f84-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/a9a1d5317a33ae8cef33961c34144f84-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 73551,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10889600363098032847&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Computer Science and Engineering, University of Michigan; Computer Science and Engineering, University of Michigan",
        "aff_domain": "umich.edu;umich.edu",
        "email": "umich.edu;umich.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "Computer Science and Engineering",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Ann Arbor",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "dbeb8fec7a",
        "title": "Extending position/phase-shift tuning to motion energy neurons improves velocity discrimination",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/dc82d632c9fcecb0778afbc7924494a6-Abstract.html",
        "author": "Yiu M. Lam; Bertram E. Shi",
        "abstract": "We extend position and phase-shift tuning, concepts already well established in the disparity energy neuron literature, to motion energy neurons. We show that Reichardt-like detectors can be considered examples of position tuning, and that motion energy filters whose complex valued spatio-temporal receptive fields are space-time separable can be considered examples of phase tuning. By combining these two types of detectors, we obtain an architecture for constructing motion energy neurons whose center frequencies can be adjusted by both phase and posi- tion shifts. Similar to recently described neurons in the primary visual cortex, these new motion energy neurons exhibit tuning that is between purely space- time separable and purely speed tuned. We propose a functional role for this intermediate level of tuning by demonstrating that comparisons between pairs of these motion energy neurons can reliably discriminate between inputs whose velocities lie above or below a given reference velocity.",
        "bibtex": "@inproceedings{NIPS2007_dc82d632,\n author = {Lam, Yiu and Shi, Bertram},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Extending position/phase-shift tuning to motion energy neurons improves velocity discrimination},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/dc82d632c9fcecb0778afbc7924494a6-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/dc82d632c9fcecb0778afbc7924494a6-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/dc82d632c9fcecb0778afbc7924494a6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 686179,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10156146718288140155&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "186349a39f",
        "title": "Fast Variational Inference for Large-scale Internet Diagnosis",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/aff1621254f7c1be92f64550478c56e6-Abstract.html",
        "author": "Emre Kiciman; David Maltz; John C. Platt",
        "abstract": "Web servers on the Internet need to maintain high reliability, but the cause of intermittent failures of web transactions is non-obvious. We use Bayesian inference to diagnose problems with web services. This diagnosis problem is far larger than any previously attempted: it requires inference of 10^4 possible faults from 10^5 observations. Further, such inference must be performed in less than a second. Inference can be done at this speed by combining a variational approximation, a mean-field approximation, and the use of stochastic gradient descent to optimize a variational cost function. We use this fast inference to diagnose a time series of anomalous HTTP requests taken from a real web service. The inference is fast enough to analyze network logs with billions of entries in a matter of hours.",
        "bibtex": "@inproceedings{NIPS2007_aff16212,\n author = {Kiciman, Emre and Maltz, David and Platt, John},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Fast Variational Inference for Large-scale Internet Diagnosis},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/aff1621254f7c1be92f64550478c56e6-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/aff1621254f7c1be92f64550478c56e6-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/aff1621254f7c1be92f64550478c56e6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 174614,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5112268543617572311&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Microsoft Research; Microsoft Research; Microsoft Research",
        "aff_domain": "microsoft.com;microsoft.com;microsoft.com",
        "email": "microsoft.com;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Microsoft Corporation",
        "aff_unique_dep": "Microsoft Research",
        "aff_unique_url": "https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "60f1002056",
        "title": "Fast and Scalable Training of Semi-Supervised CRFs with Application to Activity Recognition",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/19b650660b253761af189682e03501dd-Abstract.html",
        "author": "Maryam Mahdaviani; Tanzeem Choudhury",
        "abstract": "We present a new and efficient semi-supervised training method for parameter estimation and feature selection in conditional random fields (CRFs). In real-world applications such as activity recognition, unlabeled sensor traces are relatively easy to obtain whereas labeled examples are expensive and tedious to collect. Furthermore, the ability to automatically select a small subset of discriminatory features from a large pool can be advantageous in terms of computational speed as well as accuracy. In this paper, we introduce the semi-supervised virtual evidence boosting (sVEB) algorithm for training CRFs -- a semi-supervised extension to the recently developed virtual evidence boosting (VEB) method for feature selection and parameter learning. Semi-supervised VEB takes advantage of the unlabeled data via minimum entropy regularization -- the objective function combines the unlabeled conditional entropy with labeled conditional pseudo-likelihood. The sVEB algorithm reduces the overall system cost as well as the human labeling cost required during training, which are both important considerations in building real world inference systems. In a set of experiments on synthetic data and real activity traces collected from wearable sensors, we illustrate that our algorithm benefits from both the use of unlabeled data and automatic feature selection, and outperforms other semi-supervised training approaches.",
        "bibtex": "@inproceedings{NIPS2007_19b65066,\n author = {Mahdaviani, Maryam and Choudhury, Tanzeem},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Fast and Scalable Training of Semi-Supervised CRFs with Application to Activity Recognition},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/19b650660b253761af189682e03501dd-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/19b650660b253761af189682e03501dd-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/19b650660b253761af189682e03501dd-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/19b650660b253761af189682e03501dd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 600987,
        "gs_citation": 50,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5188878100127680121&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Computer Science Department, University of British Columbia, Vancouver, BC, Canada; Intel Research, 1100 NE 45th Street, Seattle, WA 98105, USA",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of British Columbia;Intel Research, 1100 NE 45th Street, Seattle, WA 98105, USA",
        "aff_unique_dep": "Computer Science Department;",
        "aff_unique_url": "https://www.ubc.ca;",
        "aff_unique_abbr": "UBC;",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Vancouver;",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Canada;"
    },
    {
        "id": "8d8f270ea8",
        "title": "Feature Selection Methods for Improving Protein Structure Prediction with Rosetta",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/e8258e5140317ff36c7f8225a3bf9590-Abstract.html",
        "author": "Ben Blum; David Baker; Michael I. Jordan; Philip Bradley; Rhiju Das; David E Kim",
        "abstract": "Rosetta is one of the leading algorithms for protein structure prediction today. It is a Monte Carlo energy minimization method requiring many random restarts to \ufb01nd structures with low energy. In this paper we present a resampling technique for structure prediction of small alpha/beta proteins using Rosetta. From an ini- tial round of Rosetta sampling, we learn properties of the energy landscape that guide a subsequent round of sampling toward lower-energy structures. Rather than attempt to \ufb01t the full energy landscape, we use feature selection methods\u2014both L1-regularized linear regression and decision trees\u2014to identify structural features that give rise to low energy. We then enrich these structural features in the second sampling round. Results are presented across a benchmark set of nine small al- pha/beta proteins demonstrating that our methods seldom impair, and frequently improve, Rosetta\u2019s performance.",
        "bibtex": "@inproceedings{NIPS2007_e8258e51,\n author = {Blum, Ben and Baker, David and Jordan, Michael and Bradley, Philip and Das, Rhiju and Kim, David E},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Feature Selection Methods for Improving Protein Structure Prediction with Rosetta},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/e8258e5140317ff36c7f8225a3bf9590-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/e8258e5140317ff36c7f8225a3bf9590-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/e8258e5140317ff36c7f8225a3bf9590-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 150920,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7687964321527328432&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Department of Electrical Engineering and Computer Science, University of California at Berkeley; Department of Electrical Engineering and Computer Science, University of California at Berkeley; Department of Genome Sciences, University of Washington; Department of Genome Sciences, University of Washington; Department of Genome Sciences, University of Washington; Department of Genome Sciences, University of Washington",
        "aff_domain": "cs.berkeley.edu;cs.berkeley.edu;u.washington.edu;u.washington.edu;u.washington.edu;u.washington.edu",
        "email": "cs.berkeley.edu;cs.berkeley.edu;u.washington.edu;u.washington.edu;u.washington.edu;u.washington.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;1;1;1",
        "aff_unique_norm": "University of California, Berkeley;Department of Genome Sciences, University of Washington",
        "aff_unique_dep": "Department of Electrical Engineering and Computer Science;",
        "aff_unique_url": "https://www.berkeley.edu;",
        "aff_unique_abbr": "UC Berkeley;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "a6f71cbd6f",
        "title": "FilterBoost: Regression and Classification on Large Datasets",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/072b030ba126b2f4b2374f342be9ed44-Abstract.html",
        "author": "Joseph K. Bradley; Robert E. Schapire",
        "abstract": "We study boosting in the \ufb01ltering setting, where the booster draws examples from an oracle instead of using a \ufb01xed training set and so may train ef\ufb01ciently on very large datasets. Our algorithm, which is based on a logistic regression technique proposed by Collins, Schapire, & Singer, requires fewer assumptions to achieve bounds equivalent to or better than previous work. Moreover, we give the \ufb01rst proof that the algorithm of Collins et al. is a strong PAC learner, albeit within the \ufb01ltering setting. Our proofs demonstrate the algorithm\u2019s strong theoretical proper- ties for both classi\ufb01cation and conditional probability estimation, and we validate these results through extensive experiments. Empirically, our algorithm proves more robust to noise and over\ufb01tting than batch boosters in conditional probability estimation and proves competitive in classi\ufb01cation.",
        "bibtex": "@inproceedings{NIPS2007_072b030b,\n author = {Bradley, Joseph K and Schapire, Robert E},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {FilterBoost: Regression and Classification on Large Datasets},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/072b030ba126b2f4b2374f342be9ed44-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/072b030ba126b2f4b2374f342be9ed44-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/072b030ba126b2f4b2374f342be9ed44-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/072b030ba126b2f4b2374f342be9ed44-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 278887,
        "gs_citation": 139,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2334873711522298750&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Machine Learning Department, Carnegie Mellon University; Department of Computer Science, Princeton University",
        "aff_domain": "cs.cmu.edu;cs.princeton.edu",
        "email": "cs.cmu.edu;cs.princeton.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Carnegie Mellon University;Princeton University",
        "aff_unique_dep": "Machine Learning Department;Department of Computer Science",
        "aff_unique_url": "https://www.cmu.edu;https://www.princeton.edu",
        "aff_unique_abbr": "CMU;Princeton",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "16f0274f1e",
        "title": "Fitted Q-iteration in continuous action-space MDPs",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/da0d1111d2dc5d489242e60ebcbaf988-Abstract.html",
        "author": "Andr\u00e1s Antos; Csaba Szepesv\u00e1ri; R\u00e9mi Munos",
        "abstract": "We consider continuous state, continuous action batch reinforcement learning where the goal is to learn a good policy from a sufficiently rich trajectory generated by another policy. We study a variant of fitted Q-iteration, where the greedy action selection is replaced by searching for a policy in a restricted set of candidate policies by maximizing the average action values. We provide a rigorous theoretical analysis of this algorithm, proving what we believe is the first finite-time bounds for value-function based algorithms for continuous state- and action-space problems.",
        "bibtex": "@inproceedings{NIPS2007_da0d1111,\n author = {Antos, Andr\\'{a}s and Szepesv\\'{a}ri, Csaba and Munos, R\\'{e}mi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Fitted Q-iteration in continuous action-space MDPs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/da0d1111d2dc5d489242e60ebcbaf988-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/da0d1111d2dc5d489242e60ebcbaf988-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/da0d1111d2dc5d489242e60ebcbaf988-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 222892,
        "gs_citation": 324,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1171441182237762017&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 24,
        "aff": "Computer and Automation Research Inst. of the Hungarian Academy of Sciences; SequeL project-team, INRIA Lille; Department of Computing Science, University of Alberta + Computer and Automation Research Inst. of the Hungarian Academy of Sciences",
        "aff_domain": "sztaki.hu;inria.fr;cs.ualberta.ca",
        "email": "sztaki.hu;inria.fr;cs.ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2+0",
        "aff_unique_norm": "Computer and Automation Research Inst. of the Hungarian Academy of Sciences;SequeL project-team, INRIA Lille;University of Alberta",
        "aff_unique_dep": ";;Department of Computing Science",
        "aff_unique_url": ";;https://www.ualberta.ca",
        "aff_unique_abbr": ";;UAlberta",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";Canada"
    },
    {
        "id": "a3bec02a52",
        "title": "Fixing Max-Product: Convergent Message Passing Algorithms for MAP LP-Relaxations",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/8d6dc35e506fc23349dd10ee68dabb64-Abstract.html",
        "author": "Amir Globerson; Tommi S. Jaakkola",
        "abstract": "We present a novel message passing algorithm for approximating the MAP problem in graphical models. The algorithm is similar in structure to max-product but unlike max-product it always converges, and can be proven to find the exact MAP solution in various settings. The algorithm is derived via block coordinate descent in a dual of the LP relaxation of MAP, but does not require any tunable parameters such as step size or tree weights. We also describe a generalization of the method to cluster based potentials. The new method is tested on synthetic and real-world problems, and compares favorably with previous approaches.",
        "bibtex": "@inproceedings{NIPS2007_8d6dc35e,\n author = {Globerson, Amir and Jaakkola, Tommi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Fixing Max-Product: Convergent Message Passing Algorithms for MAP LP-Relaxations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8d6dc35e506fc23349dd10ee68dabb64-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/8d6dc35e506fc23349dd10ee68dabb64-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/8d6dc35e506fc23349dd10ee68dabb64-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 103828,
        "gs_citation": 436,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10273107537988575748&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "c5edafef04",
        "title": "GRIFT: A graphical model for inferring visual classification features from human data",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/8e6b42f1644ecb1327dc03ab345e618b-Abstract.html",
        "author": "Michael Ross; Andrew Cohen",
        "abstract": "This paper describes a new model for human visual classification that enables the recovery of image features that explain human subjects' performance on different visual classification tasks. Unlike previous methods, this algorithm does not model their performance with a single linear classifier operating on raw image pixels. Instead, it models classification as the combination of multiple feature detectors. This approach extracts more information about human visual classification than has been previously possible with other methods and provides a foundation for further exploration.",
        "bibtex": "@inproceedings{NIPS2007_8e6b42f1,\n author = {Ross, Michael and Cohen, Andrew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {GRIFT: A graphical model for inferring visual classification features from human data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8e6b42f1644ecb1327dc03ab345e618b-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/8e6b42f1644ecb1327dc03ab345e618b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/8e6b42f1644ecb1327dc03ab345e618b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 557814,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4602883804024087002&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology; Psychology Department, University of Massachusetts Amherst",
        "aff_domain": "mit.edu;psych.umass.edu",
        "email": "mit.edu;psych.umass.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Massachusetts Institute of Technology;Psychology Department, University of Massachusetts Amherst",
        "aff_unique_dep": "Department of Brain and Cognitive Sciences;",
        "aff_unique_url": "https://web.mit.edu;",
        "aff_unique_abbr": "MIT;",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "af56ed62ac",
        "title": "Gaussian Process Models for Link Analysis and Transfer Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/d045c59a90d7587d8d671b5f5aec4e7c-Abstract.html",
        "author": "Kai Yu; Wei Chu",
        "abstract": "In this paper we develop a Gaussian process (GP) framework to model a collection of reciprocal random variables defined on the \\emph{edges} of a network. We show how to construct GP priors, i.e.,~covariance functions, on the edges of directed, undirected, and bipartite graphs. The model suggests an intimate connection between \\emph{link prediction} and \\emph{transfer learning}, which were traditionally considered two separate research topics. Though a straightforward GP inference has a very high complexity, we develop an efficient learning algorithm that can handle a large number of observations. The experimental results on several real-world data sets verify superior learning capacity.",
        "bibtex": "@inproceedings{NIPS2007_d045c59a,\n author = {Yu, Kai and Chu, Wei},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Gaussian Process Models for Link Analysis and Transfer Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d045c59a90d7587d8d671b5f5aec4e7c-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/d045c59a90d7587d8d671b5f5aec4e7c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/d045c59a90d7587d8d671b5f5aec4e7c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 165827,
        "gs_citation": 84,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14201225892049797884&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "NEC Laboratories America, Cupertino, CA 95014; Columbia University, CCLS, New York, NY 10115",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "NEC Laboratories America, Cupertino, CA 95014;Columbia University, CCLS, New York, NY 10115",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "86f02c24ac",
        "title": "HM-BiTAM: Bilingual Topic Exploration, Word Alignment, and Translation",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/9dcb88e0137649590b755372b040afad-Abstract.html",
        "author": "Bing Zhao; Eric P. Xing",
        "abstract": "We present a novel paradigm for statistical machine translation (SMT), based on joint modeling of word alignment and the topical aspects underlying bilingual document pairs via a hidden Markov Bilingual Topic AdMixture (HM-BiTAM). In this new paradigm, parallel sentence-pairs from a parallel document-pair are coupled via a certain semantic-flow, to ensure coherence of topical context in the alignment of matching words between languages, during likelihood-based training of topic-dependent translational lexicons, as well as topic representations in each language. The resulting trained HM-BiTAM can not only display topic patterns like other methods such as LDA, but now for bilingual corpora; it also offers a principled way of inferring optimal translation in a context-dependent way. Our method integrates the conventional IBM Models based on HMM --- a key component for most of the state-of-the-art SMT systems, with the recently proposed BiTAM model, and we report an extensive empirical analysis (in many way complementary to the description-oriented of our method in three aspects: word alignment, bilingual topic representation, and translation.",
        "bibtex": "@inproceedings{NIPS2007_9dcb88e0,\n author = {Zhao, Bing and Xing, Eric},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {HM-BiTAM: Bilingual Topic Exploration, Word Alignment, and Translation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9dcb88e0137649590b755372b040afad-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/9dcb88e0137649590b755372b040afad-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/9dcb88e0137649590b755372b040afad-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 679376,
        "gs_citation": 74,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11134622988035543181&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "IBM T.J. Watson Research; Carnegie Mellon University",
        "aff_domain": "us.ibm.com;cs.cmu.edu",
        "email": "us.ibm.com;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "IBM T.J. Watson Research;Carnegie Mellon University",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://www.cmu.edu",
        "aff_unique_abbr": ";CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "eb50a96cc8",
        "title": "Heterogeneous Component Analysis",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/a8abb4bb284b5b27aa7cb790dc20f80b-Abstract.html",
        "author": "Shigeyuki Oba; Motoaki Kawanabe; Klaus-Robert M\u00fcller; Shin Ishii",
        "abstract": "In bioinformatics it is often desirable to combine data from various measurement sources and thus structured feature vectors are to be analyzed that possess different intrinsic blocking characteristics (e.g., different patterns of missing values, obser- vation noise levels, effective intrinsic dimensionalities). We propose a new ma- chine learning tool, heterogeneous component analysis (HCA), for feature extrac- tion in order to better understand the factors that underlie such complex structured heterogeneous data. HCA is a linear block-wise sparse Bayesian PCA based not only on a probabilistic model with block-wise residual variance terms but also on a Bayesian treatment of a block-wise sparse factor-loading matrix. We study vari- ous algorithms that implement our HCA concept extracting sparse heterogeneous structure by obtaining common components for the blocks and speci\ufb01c compo- nents within each block. Simulations on toy and bioinformatics data underline the usefulness of the proposed structured matrix factorization concept.",
        "bibtex": "@inproceedings{NIPS2007_a8abb4bb,\n author = {Oba, Shigeyuki and Kawanabe, Motoaki and M\\\"{u}ller, Klaus-Robert and Ishii, Shin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Heterogeneous Component Analysis},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a8abb4bb284b5b27aa7cb790dc20f80b-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/a8abb4bb284b5b27aa7cb790dc20f80b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/a8abb4bb284b5b27aa7cb790dc20f80b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 197948,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4442641441636140582&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Graduate School of Information Science, Nara Institute of Science and Technology, Japan; Fraunhofer FIRST.IDA, Germany; Department of Computer Science, Technical University Berlin, Germany + Fraunhofer FIRST.IDA, Germany; Graduate School of Informatics, Kyoto University, Japan + Graduate School of Information Science, Nara Institute of Science and Technology, Japan",
        "aff_domain": "is.naist.jp; ; ; ",
        "email": "is.naist.jp; ; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2+1;3+0",
        "aff_unique_norm": "Nara Institute of Science and Technology;Fraunhofer FIRST.IDA, Germany;Department of Computer Science, Technical University Berlin, Germany;Kyoto University",
        "aff_unique_dep": "Graduate School of Information Science;;;Graduate School of Informatics",
        "aff_unique_url": "https://www.nist.go.jp;;;https://www.kyoto-u.ac.jp",
        "aff_unique_abbr": "NIST;;;Kyoto U",
        "aff_campus_unique_index": "0;;2+0",
        "aff_campus_unique": "Nara;;Kyoto",
        "aff_country_unique_index": "0;;0+0",
        "aff_country_unique": "Japan;"
    },
    {
        "id": "435da794c9",
        "title": "Hidden Common Cause Relations in Relational Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/912d2b1c7b2826caf99687388d2e8f7c-Abstract.html",
        "author": "Ricardo Silva; Wei Chu; Zoubin Ghahramani",
        "abstract": "When predicting class labels for objects within a relational database, it is often helpful to consider a model for relationships: this allows for information between class labels to be shared and to improve prediction performance. However, there are different ways by which objects can be related within a relational database. One traditional way corresponds to a Markov network structure: each existing relation is represented by an undirected edge. This encodes that, conditioned on input features, each object label is independent of other object labels given its neighbors in the graph. However, there is no reason why Markov networks should be the only representation of choice for symmetric dependence structures. Here we discuss the case when relationships are postulated to exist due to hidden com- mon causes. We discuss how the resulting graphical model differs from Markov networks, and how it describes different types of real-world relational processes. A Bayesian nonparametric classi\ufb01cation model is built upon this graphical repre- sentation and evaluated with several empirical studies.",
        "bibtex": "@inproceedings{NIPS2007_912d2b1c,\n author = {Silva, Ricardo and Chu, Wei and Ghahramani, Zoubin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Hidden Common Cause Relations in Relational Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/912d2b1c7b2826caf99687388d2e8f7c-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/912d2b1c7b2826caf99687388d2e8f7c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/912d2b1c7b2826caf99687388d2e8f7c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 96222,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14159417597343639297&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Gatsby Computational Neuroscience Unit, UCL, London, UK WC1N 3AR + Statistical Laboratory, University of Cambridge; Center for Computational Learning Systems, Columbia University, New York, NY 10115; Department of Engineering, University of Cambridge, UK CB2 1PZ",
        "aff_domain": "gatsby.ucl.ac.uk;cs.columbia.edu;eng.cam.ac.uk",
        "email": "gatsby.ucl.ac.uk;cs.columbia.edu;eng.cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2;3",
        "aff_unique_norm": "Gatsby Computational Neuroscience Unit, UCL, London, UK WC1N 3AR;University of Cambridge;Center for Computational Learning Systems, Columbia University, New York, NY 10115;Department of Engineering, University of Cambridge, UK CB2 1PZ",
        "aff_unique_dep": ";Statistical Laboratory;;",
        "aff_unique_url": ";https://www.cam.ac.uk;;",
        "aff_unique_abbr": ";Cambridge;;",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United Kingdom"
    },
    {
        "id": "bd5906f48b",
        "title": "Hierarchical Apprenticeship Learning with Application to Quadruped Locomotion",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/54a367d629152b720749e187b3eaa11b-Abstract.html",
        "author": "J. Z. Kolter; Pieter Abbeel; Andrew Y. Ng",
        "abstract": "We consider apprenticeship learning\u2014learning from expert demonstrations\u2014in the setting of large, complex domains. Past work in apprenticeship learning requires that the expert demonstrate complete trajectories through the domain. However, in many problems even an expert has dif\ufb01culty controlling the system, which makes this approach infeasible. For example, consider the task of teach- ing a quadruped robot to navigate over extreme terrain; demonstrating an optimal policy (i.e., an optimal set of foot locations over the entire terrain) is a highly non-trivial task, even for an expert. In this paper we propose a method for hier- archical apprenticeship learning, which allows the algorithm to accept isolated advice at different hierarchical levels of the control task. This type of advice is often feasible for experts to give, even if the expert is unable to demonstrate com- plete trajectories. This allows us to extend the apprenticeship learning paradigm to much larger, more challenging domains. In particular, in this paper we apply the hierarchical apprenticeship learning algorithm to the task of quadruped loco- motion over extreme terrain, and achieve, to the best of our knowledge, results superior to any previously published work.",
        "bibtex": "@inproceedings{NIPS2007_54a367d6,\n author = {Kolter, J. and Abbeel, Pieter and Ng, Andrew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Hierarchical Apprenticeship Learning with Application to Quadruped Locomotion},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/54a367d629152b720749e187b3eaa11b-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/54a367d629152b720749e187b3eaa11b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/54a367d629152b720749e187b3eaa11b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 175129,
        "gs_citation": 215,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12569187575585874575&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "Department of Computer Science, Stanford University; Department of Computer Science, Stanford University; Department of Computer Science, Stanford University",
        "aff_domain": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "534e7e7aff",
        "title": "Hierarchical Penalization",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/f29c21d4897f78948b91f03172341b7b-Abstract.html",
        "author": "Marie Szafranski; Yves Grandvalet; Pierre Morizet-mahoudeaux",
        "abstract": "Hierarchical penalization is a generic framework for incorporating prior informa- tion in the \ufb01tting of statistical models, when the explicative variables are organized in a hierarchical structure. The penalizer is a convex functional that performs soft selection at the group level, and shrinks variables within each group. This favors solutions with few leading terms in the \ufb01nal combination. The framework, orig- inally derived for taking prior knowledge into account, is shown to be useful in linear regression, when several parameters are used to model the in\ufb02uence of one feature, or in kernel regression, for learning multiple kernels. Keywords \u2013 Optimization: constrained and convex optimization. Supervised learning: regression, kernel methods, sparsity and feature selection.",
        "bibtex": "@inproceedings{NIPS2007_f29c21d4,\n author = {Szafranski, Marie and Grandvalet, Yves and Morizet-mahoudeaux, Pierre},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Hierarchical Penalization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/f29c21d4897f78948b91f03172341b7b-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/f29c21d4897f78948b91f03172341b7b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/f29c21d4897f78948b91f03172341b7b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 359929,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13309744325445380682&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 23,
        "aff": "Heudiasyc, UMR CNRS 6599, Universit \u00b4e de Technologie de Compi `egne, BP 20529, 60205 Compi `egne Cedex, France; Heudiasyc, UMR CNRS 6599, Universit \u00b4e de Technologie de Compi `egne, BP 20529, 60205 Compi `egne Cedex, France + IDIAP Research Institute, Av. des Pr \u00b4es-Beudin 20, P.O. Box 592, 1920 Martigny, Switzerland; Heudiasyc, UMR CNRS 6599, Universit \u00b4e de Technologie de Compi `egne, BP 20529, 60205 Compi `egne Cedex, France",
        "aff_domain": "hds.utc.fr; ; ",
        "email": "hds.utc.fr; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1;0",
        "aff_unique_norm": "Heudiasyc, UMR CNRS 6599, Universit \u00b4e de Technologie de Compi `egne, BP 20529, 60205 Compi `egne Cedex, France;IDIAP Research Institute, Av. des Pr \u00b4es-Beudin 20, P.O. Box 592, 1920 Martigny, Switzerland",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "b48edde922",
        "title": "Hippocampal Contributions to Control: The Third Way",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/1f4477bad7af3616c1f933a02bfabe4e-Abstract.html",
        "author": "M\u00e1t\u00e9 Lengyel; Peter Dayan",
        "abstract": "Recent experimental studies have focused on the specialization of different neural structures for different types of instrumental behavior. Recent theoretical work has provided normative accounts for why there should be more than one control system, and how the output of different controllers can be integrated. Two par- ticlar controllers have been identi\ufb01ed, one associated with a forward model and the prefrontal cortex and a second associated with computationally simpler, habit- ual, actor-critic methods and part of the striatum. We argue here for the normative appropriateness of an additional, but so far marginalized control system, associ- ated with episodic memory, and involving the hippocampus and medial temporal cortices. We analyze in depth a class of simple environments to show that episodic control should be useful in a range of cases characterized by complexity and in- ferential noise, and most particularly at the very early stages of learning, long before habitization has set in. We interpret data on the transfer of control from the hippocampus to the striatum in the light of this hypothesis.",
        "bibtex": "@inproceedings{NIPS2007_1f4477ba,\n author = {Lengyel, M\\'{a}t\\'{e} and Dayan, Peter},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Hippocampal Contributions to Control: The Third Way},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/1f4477bad7af3616c1f933a02bfabe4e-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/1f4477bad7af3616c1f933a02bfabe4e-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/1f4477bad7af3616c1f933a02bfabe4e-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/1f4477bad7af3616c1f933a02bfabe4e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 227616,
        "gs_citation": 332,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3206949976632086245&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Collegium Budapest Institute for Advanced Study+Computational & Biological Learning Lab, Cambridge University Engineering Department; Gatsby Computational Neuroscience Unit, UCL",
        "aff_domain": "gatsby.ucl.ac.uk;gatsby.ucl.ac.uk",
        "email": "gatsby.ucl.ac.uk;gatsby.ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2",
        "aff_unique_norm": "Collegium Budapest Institute for Advanced Study;Computational & Biological Learning Lab, Cambridge University Engineering Department;University College London",
        "aff_unique_dep": ";;Gatsby Computational Neuroscience Unit",
        "aff_unique_url": ";;https://www.ucl.ac.uk",
        "aff_unique_abbr": ";;UCL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": ";1",
        "aff_country_unique": ";United Kingdom"
    },
    {
        "id": "26f9a21683",
        "title": "How SVMs can estimate quantiles and the median",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/1be3bc32e6564055d5ca3e5a354acbef-Abstract.html",
        "author": "Andreas Christmann; Ingo Steinwart",
        "abstract": "We investigate quantile regression based on the pinball loss and the \u01eb-insensitive loss. For the pinball loss a condition on the data-generating distribution P is given that ensures that the conditional quantiles are approximated with respect to k \u00b7 k1. This result is then used to derive an oracle inequality for an SVM based on the pinball loss. Moreover, we show that SVMs based on the \u01eb-insensitive loss estimate the conditional median only under certain conditions on P .",
        "bibtex": "@inproceedings{NIPS2007_1be3bc32,\n author = {Christmann, Andreas and Steinwart, Ingo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {How SVMs can estimate quantiles and the median},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/1be3bc32e6564055d5ca3e5a354acbef-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/1be3bc32e6564055d5ca3e5a354acbef-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/1be3bc32e6564055d5ca3e5a354acbef-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 104528,
        "gs_citation": 102,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4896898877612809975&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Information Sciences Group CCS-3, Los Alamos National Laboratory, Los Alamos, NM 87545, USA; Department of Mathematics, Vrije Universiteit Brussel, B-1050 Brussels, Belgium",
        "aff_domain": "lanl.gov;vub.ac.be",
        "email": "lanl.gov;vub.ac.be",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Information Sciences Group CCS-3, Los Alamos National Laboratory, Los Alamos, NM 87545, USA;Department of Mathematics, Vrije Universiteit Brussel, B-1050 Brussels, Belgium",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "2d4fa9f611",
        "title": "Incremental Natural Actor-Critic Algorithms",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/6883966fd8f918a4aa29be29d2c386fb-Abstract.html",
        "author": "Shalabh Bhatnagar; Mohammad Ghavamzadeh; Mark Lee; Richard S. Sutton",
        "abstract": "We present four new reinforcement learning algorithms based on actor-critic and natural-gradient ideas, and provide their convergence proofs. Actor-critic rein- forcement learning methods are online approximations to policy iteration in which the value-function parameters are estimated using temporal difference learning and the policy parameters are updated by stochastic gradient descent. Methods based on policy gradients in this way are of special interest because of their com- patibility with function approximation methods, which are needed to handle large or in(cid:2)nite state spaces. The use of temporal difference learning in this way is of interest because in many applications it dramatically reduces the variance of the gradient estimates. The use of the natural gradient is of interest because it can produce better conditioned parameterizations and has been shown to further re- duce variance in some cases. Our results extend prior two-timescale convergence results for actor-critic methods by Konda and Tsitsiklis by using temporal differ- ence learning in the actor and by incorporating natural gradients, and they extend prior empirical studies of natural actor-critic methods by Peters, Vijayakumar and Schaal by providing the (cid:2)rst convergence proofs and the (cid:2)rst fully incremental algorithms.",
        "bibtex": "@inproceedings{NIPS2007_6883966f,\n author = {Bhatnagar, Shalabh and Ghavamzadeh, Mohammad and Lee, Mark and Sutton, Richard S},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Incremental Natural Actor-Critic Algorithms},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/6883966fd8f918a4aa29be29d2c386fb-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/6883966fd8f918a4aa29be29d2c386fb-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/6883966fd8f918a4aa29be29d2c386fb-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 126027,
        "gs_citation": 1143,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17357515309757958030&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 30,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "4263291d0b",
        "title": "Inferring Elapsed Time from Stochastic Neural Processes",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/0d352b4d3a317e3eae221199fdb49651-Abstract.html",
        "author": "Misha Ahrens; Maneesh Sahani",
        "abstract": "Many perceptual processes and neural computations, such as speech recognition, motor control and learning, depend on the ability to measure and mark the passage of time. However, the processes that make such temporal judgements possible are unknown. A number of different hypothetical mechanisms have been advanced, all of which depend on the known, temporally predictable evolution of a neural or psychological state, possibly through oscillations or the gradual decay of a memory trace. Alternatively, judgements of elapsed time might be based on observations of temporally structured, but stochastic processes. Such processes need not be specific to the sense of time; typical neural and sensory processes contain at least some statistical structure across a range of time scales. Here, we investigate the statistical properties of an estimator of elapsed time which is based on a simple family of stochastic process.",
        "bibtex": "@inproceedings{NIPS2007_0d352b4d,\n author = {Ahrens, Misha and Sahani, Maneesh},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Inferring Elapsed Time from Stochastic Neural Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/0d352b4d3a317e3eae221199fdb49651-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/0d352b4d3a317e3eae221199fdb49651-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/0d352b4d3a317e3eae221199fdb49651-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 162772,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17461359523313080965&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "ef273901fc",
        "title": "Inferring Neural Firing Rates from Spike Trains Using Gaussian Processes",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/5ef698cd9fe650923ea331c15af3b160-Abstract.html",
        "author": "John P. Cunningham; Byron M. Yu; Krishna V. Shenoy; Maneesh Sahani",
        "abstract": "Neural spike trains present challenges to analytical efforts due to their noisy, spiking nature. Many studies of neuroscienti(cid:2)c and neural prosthetic importance rely on a smoothed, denoised estimate of the spike train\u2019s underlying (cid:2)ring rate. Current techniques to (cid:2)nd time-varying (cid:2)ring rates require ad hoc choices of parameters, offer no con(cid:2)dence intervals on their estimates, and can obscure potentially important single trial variability. We present a new method, based on a Gaussian Process prior, for inferring probabilistically optimal estimates of (cid:2)ring rate functions underlying single or multiple neural spike trains. We test the performance of the method on simulated data and experimentally gathered neural spike trains, and we demonstrate improvements over conventional estimators.",
        "bibtex": "@inproceedings{NIPS2007_5ef698cd,\n author = {Cunningham, John P and Yu, Byron M and Shenoy, Krishna V and Sahani, Maneesh},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Inferring Neural Firing Rates from Spike Trains Using Gaussian Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/5ef698cd9fe650923ea331c15af3b160-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/5ef698cd9fe650923ea331c15af3b160-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/5ef698cd9fe650923ea331c15af3b160-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 340712,
        "gs_citation": 108,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1807584210361268151&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Department of Electrical Engineering; Department of Electrical Engineering + Neurosciences Program + Gatsby Computational Neuroscience Unit; Department of Electrical Engineering + Neurosciences Program; Gatsby Computational Neuroscience Unit",
        "aff_domain": "stanford.edu;stanford.edu;stanford.edu;gatsby.ucl.ac.uk",
        "email": "stanford.edu;stanford.edu;stanford.edu;gatsby.ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1+2;0+1;2",
        "aff_unique_norm": "Institution not specified;Neurosciences Program;University College London",
        "aff_unique_dep": "Department of Electrical Engineering;;Gatsby Computational Neuroscience Unit",
        "aff_unique_url": ";;https://www.ucl.ac.uk",
        "aff_unique_abbr": ";;UCL",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;;1",
        "aff_country_unique": ";United Kingdom"
    },
    {
        "id": "d6fbb39e7d",
        "title": "Infinite State Bayes-Nets for Structured Domains",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/f899139df5e1059396431415e770c6dd-Abstract.html",
        "author": "Max Welling; Ian Porteous; Evgeniy Bart",
        "abstract": "A general modeling framework is proposed that uni\ufb01es nonparametric-Bayesian models, topic-models and Bayesian networks. This class of in\ufb01nite state Bayes nets (ISBN) can be viewed as directed networks of \u2018hierarchical Dirichlet processes\u2019 (HDPs) where the domain of the variables can be structured (e.g. words in documents or features in images). We show that collapsed Gibbs sampling can be done ef\ufb01ciently in these models by leveraging the structure of the Bayes net and using the forward-\ufb01ltering-backward-sampling algorithm for junction trees. Existing models, such as nested-DP, Pachinko allocation, mixed membership sto- chastic block models as well as a number of new models are described as ISBNs. Two experiments have been performed to illustrate these ideas.",
        "bibtex": "@inproceedings{NIPS2007_f899139d,\n author = {Welling, Max and Porteous, Ian and Bart, Evgeniy},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Infinite State Bayes-Nets for Structured Domains},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/f899139df5e1059396431415e770c6dd-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/f899139df5e1059396431415e770c6dd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/f899139df5e1059396431415e770c6dd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 234277,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4218988148876470461&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Donald Bren School of Information and Computer Sciences, University of California Irvine; Donald Bren School of Information and Computer Sciences, University of California Irvine; California Institute of Technology, USA, Dept. of Electrical Engineering + Radboud University Nijmegen, Netherlands, Dept. of Biophysics",
        "aff_domain": "ics.uci.edu;ics.uci.edu;caltech.edu",
        "email": "ics.uci.edu;ics.uci.edu;caltech.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1+2",
        "aff_unique_norm": "Donald Bren School of Information and Computer Sciences, University of California Irvine;California Institute of Technology, USA, Dept. of Electrical Engineering;Radboud University Nijmegen, Netherlands, Dept. of Biophysics",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "269bebc25b",
        "title": "Invariant Common Spatial Patterns: Alleviating Nonstationarities in Brain-Computer Interfacing",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/6aab1270668d8cac7cef2566a1c5f569-Abstract.html",
        "author": "Benjamin Blankertz; Motoaki Kawanabe; Ryota Tomioka; Friederike Hohlefeld; Klaus-Robert M\u00fcller; Vadim V. Nikulin",
        "abstract": "Brain-Computer Interfaces can suffer from a large variance of the subject condi- tions within and across sessions. For example vigilance \ufb02uctuations in the indi- vidual, variable task involvement, workload etc. alter the characteristics of EEG signals and thus challenge a stable BCI operation. In the present work we aim to de\ufb01ne features based on a variant of the common spatial patterns (CSP) algorithm that are constructed invariant with respect to such nonstationarities. We enforce invariance properties by adding terms to the denominator of a Rayleigh coef\ufb01cient representation of CSP such as disturbance covariance matrices from \ufb02uctuations in visual processing. In this manner physiological prior knowledge can be used to shape the classi\ufb01cation engine for BCI. As a proof of concept we present a BCI classi\ufb01er that is robust to changes in the level of parietal a -activity. In other words, the EEG decoding still works when there are lapses in vigilance.",
        "bibtex": "@inproceedings{NIPS2007_6aab1270,\n author = {Blankertz, Benjamin and Kawanabe, Motoaki and Tomioka, Ryota and Hohlefeld, Friederike and M\\\"{u}ller, Klaus-Robert and Nikulin, Vadim},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Invariant Common Spatial Patterns: Alleviating Nonstationarities in Brain-Computer Interfacing},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/6aab1270668d8cac7cef2566a1c5f569-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/6aab1270668d8cac7cef2566a1c5f569-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/6aab1270668d8cac7cef2566a1c5f569-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 214233,
        "gs_citation": 324,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12979504199509606882&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "TUBerlin, Dept. of Computer Science, Machine Learning Laboratory, Berlin, Germany + Fraunhofer FIRST (IDA), Berlin, Germany; Fraunhofer FIRST (IDA), Berlin, Germany; Dept. Mathematical Informatics, IST, The University of Tokyo, Japan; Berlin School of Mind and Brain, Berlin, Germany; Dept. of Neurology, Campus Benjamin Franklin, Charit\u00e9 University Medicine Berlin, Germany; TUBerlin, Dept. of Computer Science, Machine Learning Laboratory, Berlin, Germany + Fraunhofer FIRST (IDA), Berlin, Germany",
        "aff_domain": "cs.tu-berlin.de;cs.tu-berlin.de; ; ; ; ",
        "email": "cs.tu-berlin.de;cs.tu-berlin.de; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1;2;3;4;0+1",
        "aff_unique_norm": "TUBerlin, Dept. of Computer Science, Machine Learning Laboratory, Berlin, Germany;Fraunhofer FIRST (IDA), Berlin, Germany;Dept. Mathematical Informatics, IST, The University of Tokyo, Japan;Berlin School of Mind and Brain, Berlin, Germany;Dept. of Neurology, Campus Benjamin Franklin, Charit\u00e9 University Medicine Berlin, Germany",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": ";;;;",
        "aff_unique_abbr": ";;;;",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": ";",
        "aff_country_unique": ""
    },
    {
        "id": "64329daff2",
        "title": "Iterative Non-linear Dimensionality Reduction with Manifold Sculpting",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/c06d06da9666a219db15cf575aff2824-Abstract.html",
        "author": "Michael Gashler; Dan Ventura; Tony Martinez",
        "abstract": "Many algorithms have been recently developed for reducing dimensionality by projecting data onto an intrinsic non-linear manifold. Unfortunately, existing algo- rithms often lose signi\ufb01cant precision in this transformation. Manifold Sculpting is a new algorithm that iteratively reduces dimensionality by simulating surface tension in local neighborhoods. We present several experiments that show Man- ifold Sculpting yields more accurate results than existing algorithms with both generated and natural data-sets. Manifold Sculpting is also able to bene\ufb01t from both prior dimensionality reduction efforts.",
        "bibtex": "@inproceedings{NIPS2007_c06d06da,\n author = {Gashler, Michael and Ventura, Dan and Martinez, Tony},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Iterative Non-linear Dimensionality Reduction with Manifold Sculpting},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c06d06da9666a219db15cf575aff2824-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/c06d06da9666a219db15cf575aff2824-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/c06d06da9666a219db15cf575aff2824-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/c06d06da9666a219db15cf575aff2824-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 458875,
        "gs_citation": 61,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9087914256415630790&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Brigham Young University; Brigham Young University; Brigham Young University",
        "aff_domain": "gmail.com;cs.byu.edu;cs.byu.edu",
        "email": "gmail.com;cs.byu.edu;cs.byu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Brigham Young University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.byu.edu",
        "aff_unique_abbr": "BYU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "8adbaa6bdf",
        "title": "Kernel Measures of Conditional Dependence",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/3a0772443a0739141292a5429b952fe6-Abstract.html",
        "author": "Kenji Fukumizu; Arthur Gretton; Xiaohai Sun; Bernhard Sch\u00f6lkopf",
        "abstract": "We propose a new measure of conditional dependence of random variables, based on normalized cross-covariance operators on reproducing kernel Hilbert spaces. Unlike previous kernel dependence measures, the proposed criterion does not de- pend on the choice of kernel in the limit of in\ufb01nite data, for a wide class of ker- nels. At the same time, it has a straightforward empirical estimate with good convergence behaviour. We discuss the theoretical properties of the measure, and demonstrate its application in experiments.",
        "bibtex": "@inproceedings{NIPS2007_3a077244,\n author = {Fukumizu, Kenji and Gretton, Arthur and Sun, Xiaohai and Sch\\\"{o}lkopf, Bernhard},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Kernel Measures of Conditional Dependence},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/3a0772443a0739141292a5429b952fe6-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/3a0772443a0739141292a5429b952fe6-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/3a0772443a0739141292a5429b952fe6-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/3a0772443a0739141292a5429b952fe6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 145411,
        "gs_citation": 805,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5416078313134509416&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Institute of Statistical Mathematics; Max-Planck Institute for Biological Cybernetics; Max-Planck Institute for Biological Cybernetics; Max-Planck Institute for Biological Cybernetics",
        "aff_domain": "ism.ac.jp;tuebingen.mpg.de;tuebingen.mpg.de;tuebingen.mpg.de",
        "email": "ism.ac.jp;tuebingen.mpg.de;tuebingen.mpg.de;tuebingen.mpg.de",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "Institute of Statistical Mathematics;Max-Planck Institute for Biological Cybernetics",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ism.ac.jp;",
        "aff_unique_abbr": "ISM;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Japan;"
    },
    {
        "id": "7db0be24d4",
        "title": "Kernels on Attributed Pointsets with Applications",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/4c56ff4ce4aaf9573aa5dff913df997a-Abstract.html",
        "author": "Mehul Parsana; Sourangshu Bhattacharya; Chiru Bhattacharya; K. Ramakrishnan",
        "abstract": "This paper introduces kernels on attributed pointsets, which are sets of vectors embedded in an euclidean space. The embedding gives the notion of neighborhood, which is used to define positive semidefinite kernels on pointsets. Two novel kernels on neighborhoods are proposed, one evaluating the attribute similarity and the other evaluating shape similarity. Shape similarity function is motivated from spectral graph matching techniques. The kernels are tested on three real life applications: face recognition, photo album tagging, and shot annotation in video sequences, with encouraging results.",
        "bibtex": "@inproceedings{NIPS2007_4c56ff4c,\n author = {Parsana, Mehul and Bhattacharya, Sourangshu and Bhattacharya, Chiru and Ramakrishnan, K.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Kernels on Attributed Pointsets with Applications},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/4c56ff4ce4aaf9573aa5dff913df997a-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/4c56ff4ce4aaf9573aa5dff913df997a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/4c56ff4ce4aaf9573aa5dff913df997a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 159078,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6506971191003099522&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Dept. of Computer Science & Automation; Dept. of Computer Science & Automation; Dept. of Computer Science & Automation; Dept. of Electrical Engineering",
        "aff_domain": "gmail.com;gmail.com;csa.iisc.ernet.in;ee.iisc.ernet.in",
        "email": "gmail.com;gmail.com;csa.iisc.ernet.in;ee.iisc.ernet.in",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Dept. of Computer Science & Automation;University Affiliation Not Specified",
        "aff_unique_dep": ";Department of Electrical Engineering",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "3a082b7369",
        "title": "Learning Bounds for Domain Adaptation",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/42e77b63637ab381e8be5f8318cc28a2-Abstract.html",
        "author": "John Blitzer; Koby Crammer; Alex Kulesza; Fernando Pereira; Jennifer Wortman",
        "abstract": "Empirical risk minimization offers well-known learning guarantees when training and test data come from the same domain. In the real world, though, we often wish to adapt a classifier from a source domain with a large amount of training data to different target domain with very little training data. In this work we give uniform convergence bounds for algorithms that minimize a convex combination of source and target empirical risk. The bounds explicitly model the inherent trade-off between training on a large but inaccurate source data set and a small but accurate target training set. Our theory also gives results when we have multiple source domains, each of which may have a different number of instances, and we exhibit cases in which minimizing a non-uniform combination of source risks can achieve much lower target error than standard empirical risk minimization.",
        "bibtex": "@inproceedings{NIPS2007_42e77b63,\n author = {Blitzer, John and Crammer, Koby and Kulesza, Alex and Pereira, Fernando and Wortman, Jennifer},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Bounds for Domain Adaptation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/42e77b63637ab381e8be5f8318cc28a2-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/42e77b63637ab381e8be5f8318cc28a2-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/42e77b63637ab381e8be5f8318cc28a2-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/42e77b63637ab381e8be5f8318cc28a2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 208748,
        "gs_citation": 616,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3426713468886567426&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 17,
        "aff": "Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA19146; Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA19146; Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA19146; Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA19146; Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA19146",
        "aff_domain": "cis.upenn.edu;cis.upenn.edu;cis.upenn.edu;cis.upenn.edu;cis.upenn.edu",
        "email": "cis.upenn.edu;cis.upenn.edu;cis.upenn.edu;cis.upenn.edu;cis.upenn.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA19146",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "75661b5e81",
        "title": "Learning Horizontal Connections in a Sparse Coding Model of Natural Images",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/f61d6947467ccd3aa5af24db320235dd-Abstract.html",
        "author": "Pierre Garrigues; Bruno A. Olshausen",
        "abstract": "It has been shown that adapting a dictionary of basis functions to the statistics of natural images so as to maximize sparsity in the coefficients results in a set of dictionary elements whose spatial properties resemble those of V1 (primary visual cortex) receptive fields. However, the resulting sparse coefficients still exhibit pronounced statistical dependencies, thus violating the independence assumption of the sparse coding model. Here, we propose a model that attempts to capture the dependencies among the basis function coefficients by including a pairwise coupling term in the prior over the coefficient activity states. When adapted to the statistics of natural images, the coupling terms learn a combination of facilitatory and inhibitory interactions among neighboring basis functions. These learned interactions may offer an explanation for the function of horizontal connections in V1, and we discuss the implications of our findings for physiological experiments.",
        "bibtex": "@inproceedings{NIPS2007_f61d6947,\n author = {Garrigues, Pierre and Olshausen, Bruno},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Horizontal Connections in a Sparse Coding Model of Natural Images},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/f61d6947467ccd3aa5af24db320235dd-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/f61d6947467ccd3aa5af24db320235dd-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/f61d6947467ccd3aa5af24db320235dd-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/f61d6947467ccd3aa5af24db320235dd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 368131,
        "gs_citation": 95,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17048539126967779278&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Department of EECS+Redwood Center for Theoretical Neuroscience+Univ. of California, Berkeley; Helen Wills Neuroscience Inst.+School of Optometry+Redwood Center for Theoretical Neuroscience+Univ. of California, Berkeley",
        "aff_domain": "eecs.berkeley.edu;berkeley.edu",
        "email": "eecs.berkeley.edu;berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+0+0;1+2+0+0",
        "aff_unique_norm": "University of California, Berkeley;Helen Wills Neuroscience Inst.;School of Optometry",
        "aff_unique_dep": "Department of Electrical Engineering and Computer Sciences;;",
        "aff_unique_url": "https://www.berkeley.edu;;",
        "aff_unique_abbr": "UC Berkeley;;",
        "aff_campus_unique_index": "0+0+0;0+0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0+0+0;0+0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "1925ba63c2",
        "title": "Learning Monotonic Transformations for Classification",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/a0e2a2c563d57df27213ede1ac4ac780-Abstract.html",
        "author": "Andrew Howard; Tony Jebara",
        "abstract": "A discriminative method is proposed for learning monotonic transforma- tions of the training data while jointly estimating a large-margin classi(cid:12)er. In many domains such as document classi(cid:12)cation, image histogram classi(cid:12)- cation and gene microarray experiments, (cid:12)xed monotonic transformations can be useful as a preprocessing step. However, most classi(cid:12)ers only explore these transformations through manual trial and error or via prior domain knowledge. The proposed method learns monotonic transformations auto- matically while training a large-margin classi(cid:12)er without any prior knowl- edge of the domain. A monotonic piecewise linear function is learned which transforms data for subsequent processing by a linear hyperplane classi(cid:12)er. Two algorithmic implementations of the method are formalized. The (cid:12)rst solves a convergent alternating sequence of quadratic and linear programs until it obtains a locally optimal solution. An improved algorithm is then derived using a convex semide(cid:12)nite relaxation that overcomes initializa- tion issues in the greedy optimization problem. The e(cid:11)ectiveness of these learned transformations on synthetic problems, text data and image data is demonstrated.",
        "bibtex": "@inproceedings{NIPS2007_a0e2a2c5,\n author = {Howard, Andrew and Jebara, Tony},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Monotonic Transformations for Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a0e2a2c563d57df27213ede1ac4ac780-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/a0e2a2c563d57df27213ede1ac4ac780-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/a0e2a2c563d57df27213ede1ac4ac780-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 369963,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5711524445465372293&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Science, Columbia University; Department of Computer Science, Columbia University",
        "aff_domain": "cs.columbia.edu;cs.columbia.edu",
        "email": "cs.columbia.edu;cs.columbia.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Columbia University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.columbia.edu",
        "aff_unique_abbr": "Columbia",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a96643013c",
        "title": "Learning Visual Attributes",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/ed265bc903a5a097f61d3ec064d96d2e-Abstract.html",
        "author": "Vittorio Ferrari; Andrew Zisserman",
        "abstract": "We present a probabilistic generative model of visual attributes, together with an ef\ufb01cient learning algorithm. Attributes are visual qualities of objects, such as \u2018red\u2019, \u2018striped\u2019, or \u2018spotted\u2019. The model sees attributes as patterns of image segments, repeatedly sharing some characteristic properties. These can be any combination of appearance, shape, or the layout of segments within the pattern. Moreover, attributes with general appearance are taken into account, such as the pattern of alternation of any two colors which is characteristic for stripes. To enable learning from unsegmented training images, the model is learnt discriminatively, by optimizing a likelihood ratio. As demonstrated in the experimental evaluation, our model can learn in a weakly supervised setting and encompasses a broad range of attributes. We show that attributes can be learnt starting from a text query to Google image search, and can then be used to recognize the attribute and determine its spatial extent in novel real-world images.",
        "bibtex": "@inproceedings{NIPS2007_ed265bc9,\n author = {Ferrari, Vittorio and Zisserman, Andrew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning Visual Attributes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/ed265bc903a5a097f61d3ec064d96d2e-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/ed265bc903a5a097f61d3ec064d96d2e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/ed265bc903a5a097f61d3ec064d96d2e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 286826,
        "gs_citation": 611,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16546677532349542280&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": "University of Oxford (UK); University of Oxford (UK)",
        "aff_domain": "; ",
        "email": "; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Oxford (UK)",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "560e11b72c",
        "title": "Learning and using relational theories",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/b0ab42fcb7133122b38521d13da7120b-Abstract.html",
        "author": "Charles Kemp; Noah Goodman; Joshua B. Tenenbaum",
        "abstract": "Much of human knowledge is organized into sophisticated systems that are often called intuitive theories. We propose that intuitive theories are mentally repre- sented in a logical language, and that the subjective complexity of a theory is determined by the length of its representation in this language. This complexity measure helps to explain how theories are learned from relational data, and how they support inductive inferences about unobserved relations. We describe two experiments that test our approach, and show that it provides a better account of human learning and reasoning than an approach developed by Goodman [1].",
        "bibtex": "@inproceedings{NIPS2007_b0ab42fc,\n author = {Kemp, Charles and Goodman, Noah and Tenenbaum, Joshua},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning and using relational theories},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/b0ab42fcb7133122b38521d13da7120b-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/b0ab42fcb7133122b38521d13da7120b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/b0ab42fcb7133122b38521d13da7120b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 261882,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8664251216952036750&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "9b09c4fcbd",
        "title": "Learning the 2-D Topology of Images",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/7fa732b517cbed14a48843d74526c11a-Abstract.html",
        "author": "Nicolas L. Roux; Yoshua Bengio; Pascal Lamblin; Marc Joliveau; Bal\u00e1zs K\u00e9gl",
        "abstract": "We study the following question: is the two-dimensional structure of images a very strong prior or is it something that can be learned with a few examples of natural images? If someone gave us a learning task involving images for which the two-dimensional topology of pixels was not known, could we discover it automatically and exploit it? For example suppose that the pixels had been permuted in a fixed but unknown way, could we recover the relative two-dimensional location of pixels on images? The surprising result presented here is that not only the answer is yes but that about as few as a thousand images are enough to approximately recover the relative locations of about a thousand pixels. This is achieved using a manifold learning algorithm applied to pixels associated with a measure of distributional similarity between pixel intensities. We compare different topology-extraction approaches and show how having the two-dimensional topology can be exploited.",
        "bibtex": "@inproceedings{NIPS2007_7fa732b5,\n author = {Roux, Nicolas and Bengio, Yoshua and Lamblin, Pascal and Joliveau, Marc and K\\'{e}gl, Bal\\'{a}zs},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning the 2-D Topology of Images},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/7fa732b517cbed14a48843d74526c11a-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/7fa732b517cbed14a48843d74526c11a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/7fa732b517cbed14a48843d74526c11a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 754266,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17871437325067047637&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "University of Montreal; University of Montreal; University of Montreal; \u00c9cole Centrale Paris; LAL/LRI, University of Paris-Sud, CNRS",
        "aff_domain": "umontreal.ca;umontreal.ca;umontreal.ca;ecp.fr;lal.in2p3.fr",
        "email": "umontreal.ca;umontreal.ca;umontreal.ca;ecp.fr;lal.in2p3.fr",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;2",
        "aff_unique_norm": "University of Montreal;\u00c9cole Centrale Paris;LAL/LRI, University of Paris-Sud, CNRS",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://wwwumontreal.ca;;",
        "aff_unique_abbr": "UM;;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada;"
    },
    {
        "id": "f38aecfebc",
        "title": "Learning the structure of manifolds using random projections",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/9fc3d7152ba9336a670e36d0ed79bc43-Abstract.html",
        "author": "Yoav Freund; Sanjoy Dasgupta; Mayank Kabra; Nakul Verma",
        "abstract": "We present a simple variant of the k-d tree which automatically adapts to intrinsic low dimensional structure in data.",
        "bibtex": "@inproceedings{NIPS2007_9fc3d715,\n author = {Freund, Yoav and Dasgupta, Sanjoy and Kabra, Mayank and Verma, Nakul},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning the structure of manifolds using random projections},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9fc3d7152ba9336a670e36d0ed79bc43-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/9fc3d7152ba9336a670e36d0ed79bc43-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/9fc3d7152ba9336a670e36d0ed79bc43-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 288786,
        "gs_citation": 150,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10127031218550544270&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "UC San Diego; UC San Diego; UC San Diego; UC San Diego",
        "aff_domain": "cs.ucsd.edu; ; ; ",
        "email": "cs.ucsd.edu; ; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "8ba2d05bb4",
        "title": "Learning to classify complex patterns using a VLSI network of spiking neurons",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/c3992e9a68c5ae12bd18488bc579b30d-Abstract.html",
        "author": "Srinjoy Mitra; Giacomo Indiveri; Stefano Fusi",
        "abstract": "We propose a compact, low power VLSI network of spiking neurons which can learn to classify complex patterns of mean \ufb01ring rates on\u2013line and in real\u2013time. The network of integrate-and-\ufb01re neurons is connected by bistable synapses that can change their weight using a local spike\u2013based plasticity mechanism. Learning is supervised by a teacher which provides an extra input to the output neurons during training. The synaptic weights are updated only if the current generated by the plastic synapses does not match the output desired by the teacher (as in the perceptron learning rule). We present experimental results that demonstrate how this VLSI network is able to robustly classify uncorrelated linearly separable spatial patterns of mean \ufb01ring rates.",
        "bibtex": "@inproceedings{NIPS2007_c3992e9a,\n author = {Mitra, Srinjoy and Indiveri, Giacomo and Fusi, Stefano},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning to classify complex patterns using a VLSI network of spiking neurons},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c3992e9a68c5ae12bd18488bc579b30d-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/c3992e9a68c5ae12bd18488bc579b30d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/c3992e9a68c5ae12bd18488bc579b30d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 275304,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6156643024594505313&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Institute of Neuroinformatics, UZH|ETH, Zurich; Institute of Neuroinformatics, UZH|ETH, Zurich; Center for Theoretical Neuroscience, Columbia University, New York",
        "aff_domain": "ini.phys.ethz.ch;ini.phys.ethz.ch;ini.phys.ethz.ch",
        "email": "ini.phys.ethz.ch;ini.phys.ethz.ch;ini.phys.ethz.ch",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Institute of Neuroinformatics, UZH|ETH, Zurich;Center for Theoretical Neuroscience, Columbia University, New York",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "e24cbc08fb",
        "title": "Learning with Transformation Invariant Kernels",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/c0e190d8267e36708f955d7ab048990d-Abstract.html",
        "author": "Christian Walder; Olivier Chapelle",
        "abstract": "This paper considers kernels invariant to translation, rotation and dilation. We show that no non-trivial positive de\ufb01nite (p.d.) kernels exist which are radial and dilation invariant, only conditionally positive de\ufb01nite (c.p.d.) ones. Accordingly, we discuss the c.p.d. case and provide some novel analysis, including an elemen- tary derivation of a c.p.d. representer theorem. On the practical side, we give a support vector machine (s.v.m.) algorithm for arbitrary c.p.d. kernels. For the thin- plate kernel this leads to a classi\ufb01er with only one parameter (the amount of regu- larisation), which we demonstrate to be as effective as an s.v.m. with the Gaussian kernel, even though the Gaussian involves a second parameter (the length scale).",
        "bibtex": "@inproceedings{NIPS2007_c0e190d8,\n author = {Walder, Christian and Chapelle, Olivier},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning with Transformation Invariant Kernels},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c0e190d8267e36708f955d7ab048990d-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/c0e190d8267e36708f955d7ab048990d-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/c0e190d8267e36708f955d7ab048990d-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/c0e190d8267e36708f955d7ab048990d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 220104,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9222293422792136405&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Max Planck Institute for Biological Cybernetics; Yahoo! Research",
        "aff_domain": "tuebingen.mpg.de;yahoo-inc.com",
        "email": "tuebingen.mpg.de;yahoo-inc.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Max Planck Institute for Biological Cybernetics;Yahoo!",
        "aff_unique_dep": "Biological Cybernetics;Yahoo! Research",
        "aff_unique_url": "https://www.biocybernetics.mpg.de;https://research.yahoo.com",
        "aff_unique_abbr": "MPIBC;Yahoo!",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Germany;United States"
    },
    {
        "id": "54e3391b8a",
        "title": "Learning with Tree-Averaged Densities and Distributions",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/731c83db8d2ff01bdc000083fd3c3740-Abstract.html",
        "author": "Sergey Kirshner",
        "abstract": "We utilize the ensemble of trees framework, a tractable mixture over super- exponential number of tree-structured distributions [1], to develop a new model for multivariate density estimation. The model is based on a construction of tree- structured copulas \u2013 multivariate distributions with uniform on [0, 1] marginals. By averaging over all possible tree structures, the new model can approximate distributions with complex variable dependencies. We propose an EM algorithm to estimate the parameters for these tree-averaged models for both the real-valued and the categorical case. Based on the tree-averaged framework, we propose a new model for joint precipitation amounts data on networks of rain stations.",
        "bibtex": "@inproceedings{NIPS2007_731c83db,\n author = {Kirshner, Sergey},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Learning with Tree-Averaged Densities and Distributions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/731c83db8d2ff01bdc000083fd3c3740-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/731c83db8d2ff01bdc000083fd3c3740-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/731c83db8d2ff01bdc000083fd3c3740-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 295807,
        "gs_citation": 77,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17340946258512561528&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "AICML and Dept of Computing Science, University of Alberta",
        "aff_domain": "cs.ualberta.ca",
        "email": "cs.ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "AICML and Dept of Computing Science, University of Alberta",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "4fb23b63f5",
        "title": "Linear programming analysis of loopy belief propagation for weighted matching",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/f57a2f557b098c43f11ab969efe1504b-Abstract.html",
        "author": "Sujay Sanghavi; Dmitry Malioutov; Alan S. Willsky",
        "abstract": "Loopy belief propagation has been employed in a wide variety of applications with great empirical success, but it comes with few theoretical guarantees. In this paper we investigate the use of the max-product form of belief propagation for weighted matching problems on general graphs. We show that max-product converges to the correct answer if the linear programming (LP) relaxation of the weighted matching problem is tight and does not converge if the LP relaxation is loose. This provides an exact characterization of max-product performance and reveals connections to the widely used optimization technique of LP relaxation. In addition, we demonstrate that max-product is effective in solving practical weighted matching problems in a distributed fashion by applying it to the problem of self-organization in sensor networks.",
        "bibtex": "@inproceedings{NIPS2007_f57a2f55,\n author = {Sanghavi, Sujay and Malioutov, Dmitry and Willsky, Alan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Linear programming analysis of loopy belief propagation for weighted matching},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/f57a2f557b098c43f11ab969efe1504b-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/f57a2f557b098c43f11ab969efe1504b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/f57a2f557b098c43f11ab969efe1504b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 104671,
        "gs_citation": 73,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4747764891305375329&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 9,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "be0cf8d235",
        "title": "Local Algorithms for Approximate Inference in Minor-Excluded Graphs",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/7143d7fbadfa4693b9eec507d9d37443-Abstract.html",
        "author": "Kyomin Jung; Devavrat Shah",
        "abstract": "We present a new local approximation algorithm for computing MAP and log-partition function for arbitrary exponential family distribution represented by a finite-valued pair-wise Markov random field (MRF), say G. Our algorithm is based on decomposing G into appropriately chosen small components; computing estimates locally in each of these components and then producing a good global solution. We prove that the algorithm can provide approximate solution within arbitrary accuracy when $G$ excludes some finite sized graph as its minor and G has bounded degree: all Planar graphs with bounded degree are examples of such graphs. The running time of the algorithm is $\\Theta(n)$ (n is the number of nodes in G), with constant dependent on accuracy, degree of graph and size of the graph that is excluded as a minor (constant for Planar graphs). Our algorithm for minor-excluded graphs uses the decomposition scheme of Klein, Plotkin and Rao (1993). In general, our algorithm works with any decomposition scheme and provides quantifiable approximation guarantee that depends on the decomposition scheme.",
        "bibtex": "@inproceedings{NIPS2007_7143d7fb,\n author = {Jung, Kyomin and Shah, Devavrat},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Local Algorithms for Approximate Inference in Minor-Excluded Graphs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/7143d7fbadfa4693b9eec507d9d37443-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/7143d7fbadfa4693b9eec507d9d37443-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/7143d7fbadfa4693b9eec507d9d37443-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/7143d7fbadfa4693b9eec507d9d37443-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 204540,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11956933749422068637&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Dept. of Mathematics, MIT; Dept. of EECS, MIT",
        "aff_domain": "mit.edu;mit.edu",
        "email": "mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Dept. of Mathematics, MIT;Dept. of EECS, MIT",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "2d0ce3a0e8",
        "title": "Locality and low-dimensions in the prediction of natural experience from fMRI",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/8b6dd7db9af49e67306feb59a8bdc52c-Abstract.html",
        "author": "Francois Meyer; Greg Stephens",
        "abstract": "Functional Magnetic Resonance Imaging (fMRI) provides an unprecedented window into the complex functioning of the human brain, typically detailing the activity of thousands of voxels during hundreds of sequential time points. Unfortunately, the interpretation of fMRI is complicated due both to the relatively unknown connection between the hemodynamic response and neural activity and the unknown spatiotemporal characteristics of the cognitive patterns themselves. Here, we use data from the Experience Based Cognition competition to compare global and local methods of prediction applying both linear and nonlinear techniques of dimensionality reduction. We build global low dimensional representations of an fMRI dataset, using linear and nonlinear methods. We learn a set of time series that are implicit functions of the fMRI data, and predict the values of these times series in the future from the knowledge of the fMRI data only. We find effective, low-dimensional models based on the principal components of cognitive activity in classically-defined anatomical regions, the Brodmann Areas. Furthermore for some of the stimuli, the top predictive regions were stable across subjects and episodes, including Wernicke\u00d5s area for verbal instructions, visual cortex for facial and body features, and visual-temporal regions (Brodmann Area 7) for velocity. These interpretations and the relative simplicity of our approach provide a transparent and conceptual basis upon which to build more sophisticated techniques for fMRI decoding. To our knowledge, this is the first time that classical areas have been used in fMRI for an effective prediction of complex natural experience.",
        "bibtex": "@inproceedings{NIPS2007_8b6dd7db,\n author = {Meyer, Francois and Stephens, Greg},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Locality and low-dimensions in the prediction of natural experience from fMRI},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8b6dd7db9af49e67306feb59a8bdc52c-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/8b6dd7db9af49e67306feb59a8bdc52c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/8b6dd7db9af49e67306feb59a8bdc52c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 288253,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15027950662615767014&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Center for the Study of Brain, Mind and Behavior, Program in Applied and Computational Mathematics, Princeton University; Center for the Study of Brain, Mind and Behavior, Department of Physics, Princeton University",
        "aff_domain": "colorado.edu;princeton.edu",
        "email": "colorado.edu;princeton.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Center for the Study of Brain, Mind and Behavior, Program in Applied and Computational Mathematics, Princeton University;Center for the Study of Brain, Mind and Behavior, Department of Physics, Princeton University",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "94a875f6fc",
        "title": "Loop Series and Bethe Variational Bounds in Attractive Graphical Models",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/062ddb6c727310e76b6200b7c71f63b5-Abstract.html",
        "author": "Alan S. Willsky; Erik B. Sudderth; Martin J. Wainwright",
        "abstract": "Variational methods are frequently used to approximate or bound the partition or likelihood function of a Markov random field. Methods based on mean field theory are guaranteed to provide lower bounds, whereas certain types of convex relaxations provide upper bounds. In general, loopy belief propagation (BP) provides (often accurate) approximations, but not bounds. We prove that for a class of attractive binary models, the value specified by any fixed point of loopy BP always provides a lower bound on the true likelihood. Empirically, this bound is much better than the naive mean field bound, and requires no further work than running BP. We establish these lower bounds using a loop series expansion due to Chertkov and Chernyak, which we show can be derived as a consequence of the tree reparameterization characterization of BP fixed points.",
        "bibtex": "@inproceedings{NIPS2007_062ddb6c,\n author = {Willsky, Alan and Sudderth, Erik and Wainwright, Martin J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Loop Series and Bethe Variational Bounds in Attractive Graphical Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/062ddb6c727310e76b6200b7c71f63b5-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/062ddb6c727310e76b6200b7c71f63b5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/062ddb6c727310e76b6200b7c71f63b5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 106306,
        "gs_citation": 88,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8282754193102370423&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Electrical Engineering & Computer Science, University of California, Berkeley; Electrical Engineering & Computer Science, University of California, Berkeley; Electrical Engineering & Computer Science, Massachusetts Institute of Technology",
        "aff_domain": "eecs.berkeley.edu;eecs.berkeley.edu;mit.edu",
        "email": "eecs.berkeley.edu;eecs.berkeley.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of California, Berkeley;Massachusetts Institute of Technology",
        "aff_unique_dep": "Electrical Engineering & Computer Science;Electrical Engineering & Computer Science",
        "aff_unique_url": "https://www.berkeley.edu;https://web.mit.edu",
        "aff_unique_abbr": "UC Berkeley;MIT",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Berkeley;Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "545a443438",
        "title": "Managing Power Consumption and Performance of Computing Systems Using Reinforcement Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/c8fbbc86abe8bd6a5eb6a3b4d0411301-Abstract.html",
        "author": "Gerald Tesauro; Rajarshi Das; Hoi Chan; Jeffrey Kephart; David Levine; Freeman Rawson; Charles Lefurgy",
        "abstract": "Electrical power management in large-scale IT systems such as commercial data- centers is an application area of rapidly growing interest from both an economic and ecological perspective, with billions of dollars and millions of metric tons of CO2 emissions at stake annually. Businesses want to save power without sac- ri\ufb01cing performance. This paper presents a reinforcement learning approach to simultaneous online management of both performance and power consumption. We apply RL in a realistic laboratory testbed using a Blade cluster and dynam- ically varying HTTP workload running on a commercial web applications mid- dleware platform. We embed a CPU frequency controller in the Blade servers\u2019 \ufb01rmware, and we train policies for this controller using a multi-criteria reward signal depending on both application performance and CPU power consumption. Our testbed scenario posed a number of challenges to successful use of RL, in- cluding multiple disparate reward functions, limited decision sampling rates, and pathologies arising when using multiple sensor readings as state variables. We describe innovative practical solutions to these challenges, and demonstrate clear performance improvements over both hand-designed policies as well as obvious \u201ccookbook\u201d RL implementations.",
        "bibtex": "@inproceedings{NIPS2007_c8fbbc86,\n author = {Tesauro, Gerald and Das, Rajarshi and Chan, Hoi and Kephart, Jeffrey and Levine, David and Rawson, Freeman and Lefurgy, Charles},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Managing Power Consumption and Performance of Computing Systems Using Reinforcement Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c8fbbc86abe8bd6a5eb6a3b4d0411301-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/c8fbbc86abe8bd6a5eb6a3b4d0411301-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/c8fbbc86abe8bd6a5eb6a3b4d0411301-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 383936,
        "gs_citation": 184,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16401193929598541491&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "IBM Watson and Austin Research Laboratories; IBM Watson and Austin Research Laboratories; IBM Watson and Austin Research Laboratories; IBM Watson and Austin Research Laboratories; IBM Watson and Austin Research Laboratories\u2217; IBM Watson and Austin Research Laboratories; IBM Watson and Austin Research Laboratories\u2217",
        "aff_domain": "us.ibm.com;us.ibm.com;us.ibm.com;us.ibm.com;us.ibm.com;us.ibm.com;us.ibm.com",
        "email": "us.ibm.com;us.ibm.com;us.ibm.com;us.ibm.com;us.ibm.com;us.ibm.com;us.ibm.com",
        "github": "",
        "project": "",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;1;0;1",
        "aff_unique_norm": "IBM Watson and Austin Research Laboratories;IBM Watson and Austin Research Laboratories\u2217",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "5acd8e0bbd",
        "title": "Markov Chain Monte Carlo with People",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/89d4402dc03d3b7318bbac10203034ab-Abstract.html",
        "author": "Adam Sanborn; Thomas L. Griffiths",
        "abstract": "Many formal models of cognition implicitly use subjective probability distributions to capture the assumptions of human learners. Most applications of these models determine these distributions indirectly. We propose a method for directly determining the assumptions of human learners by sampling from subjective probability distributions. Using a correspondence between a model of human choice and Markov chain Monte Carlo (MCMC), we describe a method for sampling from the distributions over objects that people associate with different categories. In our task, subjects choose whether to accept or reject a proposed change to an object. The task is constructed so that these decisions follow an MCMC acceptance rule, defining a Markov chain for which the stationary distribution is the category distribution. We test this procedure for both artificial categories acquired in the laboratory, and natural categories acquired from experience.",
        "bibtex": "@inproceedings{NIPS2007_89d4402d,\n author = {Sanborn, Adam and Griffiths, Thomas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Markov Chain Monte Carlo with People},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/89d4402dc03d3b7318bbac10203034ab-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/89d4402dc03d3b7318bbac10203034ab-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/89d4402dc03d3b7318bbac10203034ab-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 272052,
        "gs_citation": 84,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15682597052097530904&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Psychological and Brain Sciences, Indiana University, Bloomington, IN 47045; Department of Psychology, University of California, Berkeley, CA 94720",
        "aff_domain": "indiana.edu;berkeley.edu",
        "email": "indiana.edu;berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Psychological and Brain Sciences, Indiana University, Bloomington, IN 47045;University of California, Berkeley",
        "aff_unique_dep": ";Department of Psychology",
        "aff_unique_url": ";https://www.berkeley.edu",
        "aff_unique_abbr": ";UC Berkeley",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "320c8dbef2",
        "title": "McRank: Learning to Rank Using Multiple Classification and Gradient Boosting",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/b86e8d03fe992d1b0e19656875ee557c-Abstract.html",
        "author": "Ping Li; Qiang Wu; Christopher J. Burges",
        "abstract": "We cast the ranking problem as (1) multiple classi\ufb01cation (\u201cMc\u201d) (2) multiple or- dinal classi\ufb01cation, which lead to computationally tractable learning algorithms for relevance ranking in Web search. We consider the DCG criterion (discounted cumulative gain), a standard quality measure in information retrieval. Our ap- proach is motivated by the fact that perfect classi\ufb01cations result in perfect DCG scores and the DCG errors are bounded by classi\ufb01cation errors. We propose us- ing the Expected Relevance to convert class probabilities into ranking scores. The class probabilities are learned using a gradient boosting tree algorithm. Evalua- tions on large-scale datasets show that our approach can improve LambdaRank [5] and the regressions-based ranker [6], in terms of the (normalized) DCG scores. An ef\ufb01cient implementation of the boosting tree algorithm is also presented.",
        "bibtex": "@inproceedings{NIPS2007_b86e8d03,\n author = {Li, Ping and Wu, Qiang and Burges, Christopher},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {McRank: Learning to Rank Using Multiple Classification and Gradient Boosting},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/b86e8d03fe992d1b0e19656875ee557c-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/b86e8d03fe992d1b0e19656875ee557c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/b86e8d03fe992d1b0e19656875ee557c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 146604,
        "gs_citation": 789,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13933554473003035501&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Dept. of Statistical Science, Cornell University; Microsoft Research, Microsoft Corporation; Microsoft Research, Microsoft Corporation",
        "aff_domain": "cornell.edu;microsoft.com;microsoft.com",
        "email": "cornell.edu;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Dept. of Statistical Science, Cornell University;Microsoft Research, Microsoft Corporation",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "43a28cfbb0",
        "title": "Measuring Neural Synchrony by Message Passing",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/cbcb58ac2e496207586df2854b17995f-Abstract.html",
        "author": "Justin Dauwels; Fran\u00e7ois Vialatte; Tomasz Rutkowski; Andrzej S. Cichocki",
        "abstract": "A novel approach to measure the interdependence of two time series is proposed, referred to as \u201cstochastic event synchrony\u201d (SES); it quanti\ufb01es the alignment of two point processes by means of the following parameters: time delay, variance of the timing jitter, fraction of \u201cspurious\u201d events, and average similarity of events. SES may be applied to generic one-dimensional and multi-dimensional point pro- cesses, however, the paper mainly focusses on point processes in time-frequency domain. The average event similarity is in that case described by two parameters: the average frequency offset between events in the time-frequency plane, and the variance of the frequency offset (\u201cfrequency jitter\u201d); SES then consists of \ufb01ve pa- rameters in total. Those parameters quantify the synchrony of oscillatory events, and hence, they provide an alternative to existing synchrony measures that quan- tify amplitude or phase synchrony. The pairwise alignment of point processes is cast as a statistical inference problem, which is solved by applying the max- product algorithm on a graphical model. The SES parameters are determined from the resulting pairwise alignment by maximum a posteriori (MAP) estimation. The proposed interdependence measure is applied to the problem of detecting anoma- lies in EEG synchrony of Mild Cognitive Impairment (MCI) patients; the results indicate that SES signi\ufb01cantly improves the sensitivity of EEG in detecting MCI.",
        "bibtex": "@inproceedings{NIPS2007_cbcb58ac,\n author = {Dauwels, Justin and Vialatte, Fran\\c{c}ois and Rutkowski, Tomasz and Cichocki, Andrzej},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Measuring Neural Synchrony by Message Passing},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/cbcb58ac2e496207586df2854b17995f-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/cbcb58ac2e496207586df2854b17995f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/cbcb58ac2e496207586df2854b17995f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 216256,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3352540587521754586&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Amari Research Unit, RIKEN Brain Science Institute, Wako-shi, Saitama, Japan; Advanced Brain Signal Processing Laboratory, RIKEN Brain Science Institute, Wako-shi, Saitama, Japan; Advanced Brain Signal Processing Laboratory, RIKEN Brain Science Institute, Wako-shi, Saitama, Japan; Advanced Brain Signal Processing Laboratory, RIKEN Brain Science Institute, Wako-shi, Saitama, Japan",
        "aff_domain": "dauwels.com;brain.riken.jp;brain.riken.jp;brain.riken.jp",
        "email": "dauwels.com;brain.riken.jp;brain.riken.jp;brain.riken.jp",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "Amari Research Unit, RIKEN Brain Science Institute, Wako-shi, Saitama, Japan;Advanced Brain Signal Processing Laboratory, RIKEN Brain Science Institute, Wako-shi, Saitama, Japan",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "5ddfbe85d9",
        "title": "Message Passing for Max-weight Independent Set",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/c4015b7f368e6b4871809f49debe0579-Abstract.html",
        "author": "Sujay Sanghavi; Devavrat Shah; Alan S. Willsky",
        "abstract": "We investigate the use of message-passing algorithms for the problem of \ufb01nding the max-weight independent set (MWIS) in a graph. First, we study the perfor- mance of loopy max-product belief propagation. We show that, if it converges, the quality of the estimate is closely related to the tightness of an LP relaxation of the MWIS problem. We use this relationship to obtain suf\ufb01cient conditions for correctness of the estimate. We then develop a modi\ufb01cation of max-product \u2013 one that converges to an optimal solution of the dual of the MWIS problem. We also develop a simple iterative algorithm for estimating the max-weight independent set from this dual solution. We show that the MWIS estimate obtained using these two algorithms in conjunction is correct when the graph is bipartite and the MWIS is unique. Finally, we show that any problem of MAP estimation for probability distributions over \ufb01nite domains can be reduced to an MWIS problem. We believe this reduction will yield new insights and algorithms for MAP estimation.",
        "bibtex": "@inproceedings{NIPS2007_c4015b7f,\n author = {Sanghavi, Sujay and Shah, Devavrat and Willsky, Alan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Message Passing for Max-weight Independent Set},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c4015b7f368e6b4871809f49debe0579-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/c4015b7f368e6b4871809f49debe0579-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/c4015b7f368e6b4871809f49debe0579-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 82935,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6825719935679290453&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "IDS, MIT; Dept. of EECS, MIT; Dept. of EECS, MIT",
        "aff_domain": "mit.edu;mit.edu;mit.edu",
        "email": "mit.edu;mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "IDS, MIT;Dept. of EECS, MIT",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "cbd31f4740",
        "title": "Mining Internet-Scale Software Repositories",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/a532400ed62e772b9dc0b86f46e583ff-Abstract.html",
        "author": "Erik Linstead; Paul Rigor; Sushil Bajracharya; Cristina Lopes; Pierre F. Baldi",
        "abstract": "Large repositories of source code create new challenges and opportunities for statistical machine learning. Here we first develop an infrastructure for the automated crawling, parsing, and database storage of open source software. The infrastructure allows us to gather Internet-scale source code. For instance, in one experiment, we gather 4,632 java projects from SourceForge and Apache totaling over 38 million lines of code from 9,250 developers. Simple statistical analyses of the data first reveal robust power-law behavior for package, SLOC, and method call distributions. We then develop and apply unsupervised author-topic, probabilistic models to automatically discover the topics embedded in the code and extract topic-word and author-topic distributions. In addition to serving as a convenient summary for program function and developer activities, these and other related distributions provide a statistical and information-theoretic basis for quantifying and analyzing developer similarity and competence, topic scattering, and document tangling, with direct applications to software engineering. Finally, by combining software textual content with structural information captured by our CodeRank approach, we are able to significantly improve software retrieval performance, increasing the AUC metric to 0.86-- roughly 10-30% better than previous approaches based on text alone.",
        "bibtex": "@inproceedings{NIPS2007_a532400e,\n author = {Linstead, Erik and Rigor, Paul and Bajracharya, Sushil and Lopes, Cristina and Baldi, Pierre},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Mining Internet-Scale Software Repositories},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a532400ed62e772b9dc0b86f46e583ff-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/a532400ed62e772b9dc0b86f46e583ff-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/a532400ed62e772b9dc0b86f46e583ff-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 177822,
        "gs_citation": 356,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11561092521882100880&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 22,
        "aff": "Donald Bren School of Information and Computer Science, University of California, Irvine; Donald Bren School of Information and Computer Science, University of California, Irvine; Donald Bren School of Information and Computer Science, University of California, Irvine; Donald Bren School of Information and Computer Science, University of California, Irvine; Donald Bren School of Information and Computer Science, University of California, Irvine",
        "aff_domain": "ics.uci.edu;ics.uci.edu;ics.uci.edu;ics.uci.edu;ics.uci.edu",
        "email": "ics.uci.edu;ics.uci.edu;ics.uci.edu;ics.uci.edu;ics.uci.edu",
        "github": "",
        "project": "http://sourcerer.ics.uci.edu/nips2007/nips07.html",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of California, Irvine",
        "aff_unique_dep": "Donald Bren School of Information and Computer Science",
        "aff_unique_url": "https://www.uci.edu",
        "aff_unique_abbr": "UCI",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Irvine",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "34dd21575a",
        "title": "Modeling Natural Sounds with Modulation Cascade Processes",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/1595af6435015c77a7149e92a551338e-Abstract.html",
        "author": "Richard Turner; Maneesh Sahani",
        "abstract": "Natural sounds are structured on many time-scales. A typical segment of speech, for example, contains features that span four orders of magnitude: Sentences (~1s); phonemes (~0.1s); glottal pulses (~0.01s); and formants (<0.001s). The auditory system uses information from each of these time-scales to solve complicated tasks such as auditory scene analysis. One route toward understanding how auditory processing accomplishes this analysis is to build neuroscience-inspired algorithms which solve similar tasks and to compare the properties of these algorithms with properties of auditory processing. There is however a discord: Current machine-audition algorithms largely concentrate on the shorter time-scale structures in sounds, and the longer structures are ignored. The reason for this is two-fold. Firstly, it is a difficult technical problem to construct an algorithm that utilises both sorts of information. Secondly, it is computationally demanding to simultaneously process data both at high resolution (to extract short temporal information) and for long duration (to extract long temporal information). The contribution of this work is to develop a new statistical model for natural sounds that captures structure across a wide range of time-scales, and to provide efficient learning and inference algorithms. We demonstrate the success of this approach on a missing data task.",
        "bibtex": "@inproceedings{NIPS2007_1595af64,\n author = {Turner, Richard and Sahani, Maneesh},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Modeling Natural Sounds with Modulation Cascade Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/1595af6435015c77a7149e92a551338e-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/1595af6435015c77a7149e92a551338e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/1595af6435015c77a7149e92a551338e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 750282,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15484731413144916760&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "d3670046ab",
        "title": "Modeling homophily and stochastic equivalence in symmetric relational data",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/766ebcd59621e305170616ba3d3dac32-Abstract.html",
        "author": "Peter Hoff",
        "abstract": "This article discusses a latent variable model for inference and prediction of symmetric relational data. The model, based on the idea of the eigenvalue decomposition, represents the relationship between two nodes as the weighted inner-product of node-specific vectors of latent characteristics. This ``eigenmodel'' generalizes other popular latent variable models, such as latent class and distance models: It is shown mathematically that any latent class or distance model has a representation as an eigenmodel, but not vice-versa. The practical implications of this are examined in the context of three real datasets, for which the eigenmodel has as good or better out-of-sample predictive performance than the other two models.",
        "bibtex": "@inproceedings{NIPS2007_766ebcd5,\n author = {Hoff, Peter},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Modeling homophily and stochastic equivalence in symmetric relational data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/766ebcd59621e305170616ba3d3dac32-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/766ebcd59621e305170616ba3d3dac32-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/766ebcd59621e305170616ba3d3dac32-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 401434,
        "gs_citation": 364,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=839627185677314108&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Departments of Statistics and Biostatistics, University of Washington",
        "aff_domain": "stat.washington.edu",
        "email": "stat.washington.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Departments of Statistics and Biostatistics, University of Washington",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "35b90daee4",
        "title": "Modeling image patches with a directed hierarchy of Markov random fields",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/9232fe81225bcaef853ae32870a2b0fe-Abstract.html",
        "author": "Simon Osindero; Geoffrey E. Hinton",
        "abstract": "We describe an efficient learning procedure for multilayer generative models that combine the best aspects of Markov random fields and deep, directed belief nets. The generative models can be learned one layer at a time and when learning is complete they have a very fast inference procedure for computing a good approximation to the posterior distribution in all of the hidden layers. Each hidden layer has its own MRF whose energy function is modulated by the top-down directed connections from the layer above. To generate from the model, each layer in turn must settle to equilibrium given its top-down input. We show that this type of model is good at capturing the statistics of patches of natural images.",
        "bibtex": "@inproceedings{NIPS2007_9232fe81,\n author = {Osindero, Simon and Hinton, Geoffrey E},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Modeling image patches with a directed hierarchy of Markov random fields},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9232fe81225bcaef853ae32870a2b0fe-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/9232fe81225bcaef853ae32870a2b0fe-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/9232fe81225bcaef853ae32870a2b0fe-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 350761,
        "gs_citation": 165,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1291702162697075840&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science, University of Toronto; Department of Computer Science, University of Toronto",
        "aff_domain": "cs.toronto.edu;cs.toronto.edu",
        "email": "cs.toronto.edu;cs.toronto.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Toronto",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utoronto.ca",
        "aff_unique_abbr": "U of T",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Toronto",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "3e6a6078e1",
        "title": "Modelling motion primitives and their timing in biologically executed movements",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/84117275be999ff55a987b9381e01f96-Abstract.html",
        "author": "Ben Williams; Marc Toussaint; Amos J. Storkey",
        "abstract": "Biological movement is built up of sub-blocks or motion primitives. Such primitives provide a compact representation of movement which is also desirable in robotic control applications. We analyse handwriting data to gain a better understanding of use of primitives and their timings in biological movements. Inference of the shape and the timing of primitives can be done using a factorial HMM based model, allowing the handwriting to be represented in primitive timing space. This representation provides a distribution of spikes corresponding to the primitive activations, which can also be modelled using HMM architectures. We show how the coupling of the low level primitive model, and the higher level timing model during inference can produce good reconstructions of handwriting, with shared primitives for all characters modelled. This coupled model also captures the variance profile of the dataset which is accounted for by spike timing jitter. The timing code provides a compact representation of the movement while generating a movement without an explicit timing model produces a scribbling style of output.",
        "bibtex": "@inproceedings{NIPS2007_84117275,\n author = {Williams, Ben and Toussaint, Marc and Storkey, Amos J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Modelling motion primitives and their timing in biologically executed movements},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/84117275be999ff55a987b9381e01f96-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/84117275be999ff55a987b9381e01f96-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/84117275be999ff55a987b9381e01f96-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/84117275be999ff55a987b9381e01f96-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 611727,
        "gs_citation": 79,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11407620087441621998&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "School of Informatics, University of Edinburgh; TU Berlin; School of Informatics, University of Edinburgh",
        "aff_domain": "ed.ac.uk;cs.tu-berlin.de;ed.ac.uk",
        "email": "ed.ac.uk;cs.tu-berlin.de;ed.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Edinburgh;Technische Universit\u00e4t Berlin",
        "aff_unique_dep": "School of Informatics;",
        "aff_unique_url": "https://www.ed.ac.uk;https://www.tu-berlin.de",
        "aff_unique_abbr": "Edinburgh;TU Berlin",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Edinburgh;Berlin",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United Kingdom;Germany"
    },
    {
        "id": "c82b2c610d",
        "title": "Multi-Task Learning via Conic Programming",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/67f7fb873eaf29526a11a9b7ac33bfac-Abstract.html",
        "author": "Tsuyoshi Kato; Hisashi Kashima; Masashi Sugiyama; Kiyoshi Asai",
        "abstract": "When we have several related tasks, solving them simultaneously is shown to be more effective than solving them individually. This approach is called multi-task learning (MTL) and has been studied extensively. Existing approaches to MTL often treat all the tasks as \\emph{uniformly related to each other and the relatedness of the tasks is controlled globally. For this reason, the existing methods can lead to undesired solutions when some tasks are not highly related to each other, and some pairs of related tasks can have significantly different solutions. In this paper, we propose a novel MTL algorithm that can overcome these problems. Our method makes use of a task network, which describes the relation structure among tasks. This allows us to deal with intricate relation structures in a systematic way. Furthermore, we control the relatedness of the tasks locally, so all pairs of related tasks are guaranteed to have similar solutions. We apply the above idea to support vector machines (SVMs) and show that the optimization problem can be cast as a second order cone program, which is convex and can be solved efficiently. The usefulness of our approach is demonstrated through simulations with protein super-family classification and ordinal regression problems.",
        "bibtex": "@inproceedings{NIPS2007_67f7fb87,\n author = {Kato, Tsuyoshi and Kashima, Hisashi and Sugiyama, Masashi and Asai, Kiyoshi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multi-Task Learning via Conic Programming},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/67f7fb873eaf29526a11a9b7ac33bfac-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/67f7fb873eaf29526a11a9b7ac33bfac-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/67f7fb873eaf29526a11a9b7ac33bfac-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 148575,
        "gs_citation": 124,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=659112048846573406&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Graduate School of Frontier Sciences, The University of Tokyo + Institute for Bioinformatics Research and Development (BIRD), Japan Science and Technology Agency (JST); Tokyo Research Laboratory, IBM Research; Department of Computer Science, Tokyo Institute of Technology; AIST Computational Biology Research Center",
        "aff_domain": "cb.k.u-tokyo.ac.jp;yahoo.co.jp;cs.titech.ac.jp;cbrc.jp",
        "email": "cb.k.u-tokyo.ac.jp;yahoo.co.jp;cs.titech.ac.jp;cbrc.jp",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2;3;4",
        "aff_unique_norm": "The University of Tokyo;Institute for Bioinformatics Research and Development (BIRD), Japan Science and Technology Agency (JST);Tokyo Research Laboratory, IBM Research;Tokyo Institute of Technology;AIST Computational Biology Research Center",
        "aff_unique_dep": "Graduate School of Frontier Sciences;;;Department of Computer Science;",
        "aff_unique_url": "https://www.u-tokyo.ac.jp;;;https://www.titech.ac.jp;",
        "aff_unique_abbr": "UTokyo;;;Titech;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Tokyo;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan;"
    },
    {
        "id": "b7e096c908",
        "title": "Multi-task Gaussian Process Prediction",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/66368270ffd51418ec58bd793f2d9b1b-Abstract.html",
        "author": "Edwin V. Bonilla; Kian M. Chai; Christopher Williams",
        "abstract": "In this paper we investigate multi-task learning in the context of Gaussian Pro- cesses (GP). We propose a model that learns a shared covariance function on input-dependent features and a \u201cfree-form\u201d covariance matrix over tasks. This al- lows for good \ufb02exibility when modelling inter-task dependencies while avoiding the need for large amounts of data for training. We show that under the assump- tion of noise-free observations and a block design, predictions for a given task only depend on its target values and therefore a cancellation of inter-task trans- fer occurs. We evaluate the bene\ufb01ts of our model on two practical applications: a compiler performance prediction problem and an exam score prediction task. Additionally, we make use of GP approximations and properties of our model in order to provide scalability to large data sets.",
        "bibtex": "@inproceedings{NIPS2007_66368270,\n author = {Bonilla, Edwin V and Chai, Kian and Williams, Christopher},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multi-task Gaussian Process Prediction},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/66368270ffd51418ec58bd793f2d9b1b-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/66368270ffd51418ec58bd793f2d9b1b-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/66368270ffd51418ec58bd793f2d9b1b-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/66368270ffd51418ec58bd793f2d9b1b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 174097,
        "gs_citation": 1610,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5715381064471807401&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "School of Informatics, University of Edinburgh, 5 Forrest Hill, Edinburgh EH1 2QL, UK; School of Informatics, University of Edinburgh, 5 Forrest Hill, Edinburgh EH1 2QL, UK; School of Informatics, University of Edinburgh, 5 Forrest Hill, Edinburgh EH1 2QL, UK",
        "aff_domain": "ed.ac.uk;sms.ed.ac.uk;ed.ac.uk",
        "email": "ed.ac.uk;sms.ed.ac.uk;ed.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "School of Informatics, University of Edinburgh, 5 Forrest Hill, Edinburgh EH1 2QL, UK",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "2ef48535f3",
        "title": "Multiple-Instance Active Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/a1519de5b5d44b31a01de013b9b51a80-Abstract.html",
        "author": "Burr Settles; Mark Craven; Soumya Ray",
        "abstract": "In a multiple instance (MI) learning problem, instances are naturally organized into bags and it is the bags, instead of individual instances, that are labeled for training. MI learners assume that every instance in a bag labeled negative is actually negative, whereas at least one instance in a bag labeled positive is actually positive. We present a framework for active learning in the multiple-instance setting. In particular, we consider the case in which an MI learner is allowed to selectively query unlabeled instances in positive bags. This approach is well motivated in domains in which it is inexpensive to acquire bag labels and possible, but expensive, to acquire instance labels. We describe a method for learning from labels at mixed levels of granularity, and introduce two active query selection strategies motivated by the MI setting. Our experiments show that learning from instance labels can significantly improve performance of a basic MI learning algorithm in two multiple-instance domains: content-based image recognition and text classification.",
        "bibtex": "@inproceedings{NIPS2007_a1519de5,\n author = {Settles, Burr and Craven, Mark and Ray, Soumya},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multiple-Instance Active Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a1519de5b5d44b31a01de013b9b51a80-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/a1519de5b5d44b31a01de013b9b51a80-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/a1519de5b5d44b31a01de013b9b51a80-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 728553,
        "gs_citation": 846,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2291137710996234832&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "University of Wisconsin; University of Wisconsin; Oregon State University",
        "aff_domain": "cs.wisc.edu;biostat.wisc.edu;eecs.oregonstate.edu",
        "email": "cs.wisc.edu;biostat.wisc.edu;eecs.oregonstate.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of Wisconsin;Oregon State University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.wisc.edu;https://oregonstate.edu",
        "aff_unique_abbr": "UW;OSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "fc3884fb48",
        "title": "Multiple-Instance Pruning For Learning Efficient Cascade Detectors",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/ffeabd223de0d4eacb9a3e6e53e5448d-Abstract.html",
        "author": "Cha Zhang; Paul A. Viola",
        "abstract": "Cascade detectors have been shown to operate extremely rapidly, with high accuracy, and have important applications such as face detection. Driven by this success, cascade earning has been an area of active research in recent years. Nevertheless, there are still challenging technical problems during the training process of cascade detectors. In particular, determining the optimal target detection rate for each stage of the cascade remains an unsolved issue. In this paper, we propose the multiple instance pruning (MIP) algorithm for soft cascades. This algorithm computes a set of thresholds which aggressively terminate computation with no reduction in detection rate or increase in false positive rate on the training dataset. The algorithm is based on two key insights: i) examples that are destined to be rejected by the complete classifier can be safely pruned early; ii) face detection is a multiple instance learning problem. The MIP process is fully automatic and requires no assumptions of probability distributions, statistical independence, or ad hoc intermediate rejection targets. Experimental results on the MIT+CMU dataset demonstrate significant performance advantages.",
        "bibtex": "@inproceedings{NIPS2007_ffeabd22,\n author = {Zhang, Cha and Viola, Paul},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Multiple-Instance Pruning For Learning Efficient Cascade Detectors},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/ffeabd223de0d4eacb9a3e6e53e5448d-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/ffeabd223de0d4eacb9a3e6e53e5448d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/ffeabd223de0d4eacb9a3e6e53e5448d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 876403,
        "gs_citation": 195,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3499645488271171301&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "bd14042242",
        "title": "Near-Maximum Entropy Models for Binary Neural Representations of Natural Images",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/d296c101daa88a51f6ca8cfc1ac79b50-Abstract.html",
        "author": "Matthias Bethge; Philipp Berens",
        "abstract": "Maximum entropy analysis of binary variables provides an elegant way for study- ing the role of pairwise correlations in neural populations. Unfortunately, these approaches suffer from their poor scalability to high dimensions. In sensory cod- ing, however, high-dimensional data is ubiquitous. Here, we introduce a new approach using a near-maximum entropy model, that makes this type of analy- sis feasible for very high-dimensional data\u2014the model parameters can be derived in closed form and sampling is easy. Therefore, our NearMaxEnt approach can serve as a tool for testing predictions from a pairwise maximum entropy model not only for low-dimensional marginals, but also for high dimensional measurements of more than thousand units. We demonstrate its usefulness by studying natural images with dichotomized pixel intensities. Our results indicate that the statistics of such higher-dimensional measurements exhibit additional structure that are not predicted by pairwise correlations, despite the fact that pairwise correlations ex- plain the lower-dimensional marginal statistics surprisingly well up to the limit of dimensionality where estimation of the full joint distribution is feasible.",
        "bibtex": "@inproceedings{NIPS2007_d296c101,\n author = {Bethge, Matthias and Berens, Philipp},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Near-Maximum Entropy Models for Binary Neural Representations of Natural Images},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d296c101daa88a51f6ca8cfc1ac79b50-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/d296c101daa88a51f6ca8cfc1ac79b50-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/d296c101daa88a51f6ca8cfc1ac79b50-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/d296c101daa88a51f6ca8cfc1ac79b50-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 380696,
        "gs_citation": 56,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7215352001750101433&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "236e5d21c3",
        "title": "Nearest-Neighbor-Based Active Learning for Rare Category Detection",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/2838023a778dfaecdc212708f721b788-Abstract.html",
        "author": "Jingrui He; Jaime G. Carbonell",
        "abstract": "Rare category detection is an open challenge for active learning, especially in the de-novo case (no labeled examples), but of signi\ufb01cant practical importance for data mining - e.g. detecting new \ufb01nancial transaction fraud patterns, where normal legitimate transactions dominate. This paper develops a new method for detecting an instance of each minority class via an unsupervised local-density-differential sampling strategy. Essentially a variable-scale nearest neighbor process is used to optimize the probability of sampling tightly-grouped minority classes, subject to a local smoothness assumption of the majority class. Results on both synthetic and real data sets are very positive, detecting each minority class with only a frac- tion of the actively sampled points required by random sampling and by Pelleg\u2019s Interleave method, the prior best technique in the sparse literature on this topic.",
        "bibtex": "@inproceedings{NIPS2007_2838023a,\n author = {He, Jingrui and Carbonell, Jaime},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Nearest-Neighbor-Based Active Learning for Rare Category Detection},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/2838023a778dfaecdc212708f721b788-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/2838023a778dfaecdc212708f721b788-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/2838023a778dfaecdc212708f721b788-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 749097,
        "gs_citation": 135,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3817016852226753554&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": "School of Computer Science, Carnegie Mellon University; School of Computer Science, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2fa37cba16",
        "title": "Neural characterization in partially observed populations of spiking neurons",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/2bcab9d935d219641434683dd9d18a03-Abstract.html",
        "author": "Jonathan W. Pillow; Peter E. Latham",
        "abstract": "Point process encoding models provide powerful statistical methods for under- standing the responses of neurons to sensory stimuli. Although these models have been successfully applied to neurons in the early sensory pathway, they have fared less well capturing the response properties of neurons in deeper brain areas, ow- ing in part to the fact that they do not take into account multiple stages of pro- cessing. Here we introduce a new twist on the point-process modeling approach: we include unobserved as well as observed spiking neurons in a joint encoding model. The resulting model exhibits richer dynamics and more highly nonlinear response properties, making it more powerful and more \ufb02exible for \ufb01tting neural data. More importantly, it allows us to estimate connectivity patterns among neu- rons (both observed and unobserved), and may provide insight into how networks process sensory input. We formulate the estimation procedure using variational EM and the wake-sleep algorithm, and illustrate the model\u2019s performance using a simulated example network consisting of two coupled neurons.",
        "bibtex": "@inproceedings{NIPS2007_2bcab9d9,\n author = {Pillow, Jonathan and Latham, Peter},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Neural characterization in partially observed populations of spiking neurons},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/2bcab9d935d219641434683dd9d18a03-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/2bcab9d935d219641434683dd9d18a03-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/2bcab9d935d219641434683dd9d18a03-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 568408,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15673982796402487426&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Gatsby Computational Neuroscience Unit, UCL; Gatsby Computational Neuroscience Unit, UCL",
        "aff_domain": "gatsby.ucl.ac.uk;gatsby.ucl.ac.uk",
        "email": "gatsby.ucl.ac.uk;gatsby.ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University College London",
        "aff_unique_dep": "Gatsby Computational Neuroscience Unit",
        "aff_unique_url": "https://www.ucl.ac.uk",
        "aff_unique_abbr": "UCL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "058c01b942",
        "title": "New Outer Bounds on the Marginal Polytope",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/2b323d6eb28422cef49b266557dd31ad-Abstract.html",
        "author": "David Sontag; Tommi S. Jaakkola",
        "abstract": "We give a new class of outer bounds on the marginal polytope, and propose a cutting-plane algorithm for efficiently optimizing over these constraints. When combined with a concave upper bound on the entropy, this gives a new variational inference algorithm for probabilistic inference in discrete Markov Random Fields (MRFs). Valid constraints on the marginal polytope are derived through a series of projections onto the cut polytope. As a result, we obtain tighter upper bounds on the log-partition function. We also show empirically that the approximations of the marginals are significantly more accurate when using the tighter outer bounds. Finally, we demonstrate the advantage of the new constraints for finding the MAP assignment in protein structure prediction.",
        "bibtex": "@inproceedings{NIPS2007_2b323d6e,\n author = {Sontag, David and Jaakkola, Tommi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {New Outer Bounds on the Marginal Polytope},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/2b323d6eb28422cef49b266557dd31ad-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/2b323d6eb28422cef49b266557dd31ad-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/2b323d6eb28422cef49b266557dd31ad-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 310925,
        "gs_citation": 171,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1057106774556403846&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "356bd92f34",
        "title": "Non-parametric Modeling of Partially Ranked Data",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/fe8c15fed5f808006ce95eddb7366e35-Abstract.html",
        "author": "Guy Lebanon; Yi Mao",
        "abstract": "Statistical models on full and partial rankings of n items are often of limited prac- tical use for large n due to computational consideration. We explore the use of non-parametric models for partially ranked data and derive ef(cid:2)cient procedures for their use for large n. The derivations are largely possible through combinatorial and algebraic manipulations based on the lattice of partial rankings. In particular, we demonstrate for the (cid:2)rst time a non-parametric coherent and consistent model capable of ef(cid:2)ciently aggregating partially ranked data of different types.",
        "bibtex": "@inproceedings{NIPS2007_fe8c15fe,\n author = {Lebanon, Guy and Mao, Yi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Non-parametric Modeling of Partially Ranked Data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/fe8c15fed5f808006ce95eddb7366e35-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/fe8c15fed5f808006ce95eddb7366e35-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/fe8c15fed5f808006ce95eddb7366e35-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 165314,
        "gs_citation": 150,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12443759006631975941&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Statistics, and School of Elec. and Computer Engineering, Purdue University - West Lafayette, IN; School of Elec. and Computer Engineering, Purdue University - West Lafayette, IN",
        "aff_domain": "stat.purdue.edu;ecn.purdue.edu",
        "email": "stat.purdue.edu;ecn.purdue.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Department of Statistics, and School of Elec. and Computer Engineering, Purdue University - West Lafayette, IN;School of Elec. and Computer Engineering, Purdue University - West Lafayette, IN",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "f219e2a9f8",
        "title": "Object Recognition by Scene Alignment",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/e07413354875be01a996dc560274708e-Abstract.html",
        "author": "Bryan Russell; Antonio Torralba; Ce Liu; Rob Fergus; William T. Freeman",
        "abstract": "Current object recognition systems can only recognize a limited number of object categories; scaling up to many categories is the next challenge. We seek to build a system to recognize and localize many different object categories in complex scenes. We achieve this through a simple approach: by matching the input im- age, in an appropriate representation, to images in a large training set of labeled images. Due to regularities in object identities across similar scenes, the retrieved matches provide hypotheses for object identities and locations. We build a prob- abilistic model to transfer the labels from the retrieval set to the input image. We demonstrate the effectiveness of this approach and study algorithm component contributions using held-out test sets from the LabelMe database.",
        "bibtex": "@inproceedings{NIPS2007_e0741335,\n author = {Russell, Bryan and Torralba, Antonio and Liu, Ce and Fergus, Rob and Freeman, William},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Object Recognition by Scene Alignment},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/e07413354875be01a996dc560274708e-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/e07413354875be01a996dc560274708e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/e07413354875be01a996dc560274708e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 657492,
        "gs_citation": 167,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7733558550392047800&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": "Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA 02139 USA; Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA 02139 USA; Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA 02139 USA; Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA 02139 USA; Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA 02139 USA",
        "aff_domain": "csail.mit.edu;csail.mit.edu;csail.mit.edu;csail.mit.edu;csail.mit.edu",
        "email": "csail.mit.edu;csail.mit.edu;csail.mit.edu;csail.mit.edu;csail.mit.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA 02139 USA",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "93bb806dc4",
        "title": "On Ranking in Survival Analysis: Bounds on the Concordance Index",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/33e8075e9970de0cfea955afd4644bb2-Abstract.html",
        "author": "Harald Steck; Balaji Krishnapuram; Cary Dehing-oberije; Philippe Lambin; Vikas C. Raykar",
        "abstract": "In this paper, we show that classical survival analysis involving censored data can naturally be cast as a ranking problem. The concordance index (CI), which quantifies the quality of rankings, is the standard performance measure for model \\emph{assessment} in survival analysis. In contrast, the standard approach to \\emph{learning} the popular proportional hazard (PH) model is based on Cox's partial likelihood. In this paper we devise two bounds on CI--one of which emerges directly from the properties of PH models--and optimize them \\emph{directly}. Our experimental results suggest that both methods perform about equally well, with our new approach giving slightly better results than the Cox's method. We also explain why a method designed to maximize the Cox's partial likelihood also ends up (approximately) maximizing the CI.",
        "bibtex": "@inproceedings{NIPS2007_33e8075e,\n author = {Steck, Harald and Krishnapuram, Balaji and Dehing-oberije, Cary and Lambin, Philippe and Raykar, Vikas C},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On Ranking in Survival Analysis: Bounds on the Concordance Index},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/33e8075e9970de0cfea955afd4644bb2-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/33e8075e9970de0cfea955afd4644bb2-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/33e8075e9970de0cfea955afd4644bb2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 216300,
        "gs_citation": 334,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4515955795079494563&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "CAD and Knowledge Solutions (IKM CKS), Siemens Medical Solutions Inc., Malvern, USA; CAD and Knowledge Solutions (IKM CKS), Siemens Medical Solutions Inc., Malvern, USA; CAD and Knowledge Solutions (IKM CKS), Siemens Medical Solutions Inc., Malvern, USA; Maastro Clinic, University Hospital Maastricht, University Maastricht, GROW, The Netherlands; Maastro Clinic, University Hospital Maastricht, University Maastricht, GROW, The Netherlands",
        "aff_domain": "siemens.com;siemens.com;siemens.com;maastro.nl;maastro.nl",
        "email": "siemens.com;siemens.com;siemens.com;maastro.nl;maastro.nl",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;1",
        "aff_unique_norm": "CAD and Knowledge Solutions (IKM CKS), Siemens Medical Solutions Inc., Malvern, USA;Maastro Clinic, University Hospital Maastricht, University Maastricht, GROW, The Netherlands",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "a8856f61a1",
        "title": "On Sparsity and Overcompleteness in Image Models",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/7cce53cf90577442771720a370c3c723-Abstract.html",
        "author": "Pietro Berkes; Richard Turner; Maneesh Sahani",
        "abstract": "Computational models of visual cortex, and in particular those based on sparse coding, have enjoyed much recent attention. Despite this currency, the question of how sparse or how over-complete a sparse representation should be, has gone without principled answer. Here, we use Bayesian model-selection methods to address these questions for a sparse-coding model based on a Student-t prior. Having validated our methods on toy data, we find that natural images are indeed best modelled by extremely sparse distributions; although for the Student-t prior, the associated optimal basis size is only modestly overcomplete.",
        "bibtex": "@inproceedings{NIPS2007_7cce53cf,\n author = {Berkes, Pietro and Turner, Richard and Sahani, Maneesh},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On Sparsity and Overcompleteness in Image Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/7cce53cf90577442771720a370c3c723-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/7cce53cf90577442771720a370c3c723-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/7cce53cf90577442771720a370c3c723-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 137593,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3829569988862490524&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "00fa7d0615",
        "title": "On higher-order perceptron algorithms",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/ac1dd209cbcc5e5d1c6e28598e8cbbe8-Abstract.html",
        "author": "Claudio Gentile; Fabio Vitale; Cristian Brotto",
        "abstract": "A new algorithm for on-line learning linear-threshold functions is proposed which efficiently combines second-order statistics about the data with the logarithmic behavior\" of multiplicative/dual-norm algorithms. An initial theoretical analysis is provided suggesting that our algorithm might be viewed as a standard Perceptron algorithm operating on a transformed sequence of examples with improved margin properties. We also report on experiments carried out on datasets from diverse domains, with the goal of comparing to known Perceptron algorithms (first-order, second-order, additive, multiplicative). Our learning procedure seems to generalize quite well, and converges faster than the corresponding multiplicative baseline algorithms.\"",
        "bibtex": "@inproceedings{NIPS2007_ac1dd209,\n author = {Gentile, Claudio and Vitale, Fabio and Brotto, Cristian},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {On higher-order perceptron algorithms},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/ac1dd209cbcc5e5d1c6e28598e8cbbe8-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/ac1dd209cbcc5e5d1c6e28598e8cbbe8-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/ac1dd209cbcc5e5d1c6e28598e8cbbe8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 294675,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=778569450884377139&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "DICOM, Universit`a dell\u2019Insubria; DICOM, Universit `a dell\u2019Insubria; DICOM, Universit `a dell\u2019Insubria",
        "aff_domain": "gmail.com;uninsubria.it;yahoo.com",
        "email": "gmail.com;uninsubria.it;yahoo.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "DICOM, Universit`a dell\u2019Insubria;DICOM, Universit `a dell\u2019Insubria",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "7a7f7f7db3",
        "title": "One-Pass Boosting",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/a9a6653e48976138166de32772b1bf40-Abstract.html",
        "author": "Zafer Barutcuoglu; Phil Long; Rocco Servedio",
        "abstract": "This paper studies boosting algorithms that make a single pass over a set of base classi(cid:2)ers. We (cid:2)rst analyze a one-pass algorithm in the setting of boosting with diverse base classi(cid:2)ers. Our guarantee is the same as the best proved for any boosting algo- rithm, but our one-pass algorithm is much faster than previous approaches. We next exhibit a random source of examples for which a (cid:147)picky(cid:148) variant of Ad- aBoost that skips poor base classi(cid:2)ers can outperform the standard AdaBoost al- gorithm, which uses every base classi(cid:2)er, by an exponential factor. Experiments with Reuters and synthetic data show that one-pass boosting can sub- stantially improve on the accuracy of Naive Bayes, and that picky boosting can sometimes lead to a further improvement in accuracy.",
        "bibtex": "@inproceedings{NIPS2007_a9a6653e,\n author = {Barutcuoglu, Zafer and Long, Phil and Servedio, Rocco},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {One-Pass Boosting},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a9a6653e48976138166de32772b1bf40-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/a9a6653e48976138166de32772b1bf40-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/a9a6653e48976138166de32772b1bf40-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/a9a6653e48976138166de32772b1bf40-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 98644,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1272411079384133881&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 25,
        "aff": "Princeton University; Google; Columbia University",
        "aff_domain": "cs.princeton.edu;google.com;cs.columbia.edu",
        "email": "cs.princeton.edu;google.com;cs.columbia.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Princeton University;Google;Columbia University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.princeton.edu;https://www.google.com;https://www.columbia.edu",
        "aff_unique_abbr": "Princeton;Google;Columbia",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b7b556be78",
        "title": "Online Linear Regression and Its Application to Model-Based Reinforcement Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/b7bb35b9c6ca2aee2df08cf09d7016c2-Abstract.html",
        "author": "Alexander L. Strehl; Michael L. Littman",
        "abstract": "We provide a provably efficient algorithm for learning Markov Decision Processes (MDPs) with continuous state and action spaces in the online setting. Specifically, we take a model-based approach and show that a special type of online linear regression allows us to learn MDPs with (possibly kernalized) linearly parameterized dynamics. This result builds on Kearns and Singh's work that provides a provably efficient algorithm for finite state MDPs. Our approach is not restricted to the linear setting, and is applicable to other classes of continuous MDPs.",
        "bibtex": "@inproceedings{NIPS2007_b7bb35b9,\n author = {Strehl, Alexander and Littman, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Online Linear Regression and Its Application to Model-Based Reinforcement Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/b7bb35b9c6ca2aee2df08cf09d7016c2-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/b7bb35b9c6ca2aee2df08cf09d7016c2-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/b7bb35b9c6ca2aee2df08cf09d7016c2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 91618,
        "gs_citation": 108,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3505655968340540271&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Yahoo! Research, New York, NY + Rutgers University; Department of Computer Science, Rutgers University, Piscataway, NJ USA",
        "aff_domain": "yahoo-inc.com;cs.rutgers.edu",
        "email": "yahoo-inc.com;cs.rutgers.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2",
        "aff_unique_norm": "Yahoo! Research, New York, NY;Rutgers University;Department of Computer Science, Rutgers University, Piscataway, NJ USA",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";https://www.rutgers.edu;",
        "aff_unique_abbr": ";Rutgers;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "0b30a3d64a",
        "title": "Optimal ROC Curve for a Combination of Classifiers",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/75fc093c0ee742f6dddaa13fff98f104-Abstract.html",
        "author": "Marco Barreno; Alvaro Cardenas; J. D. Tygar",
        "abstract": "We present a new analysis for the combination of binary classifiers. We propose a theoretical framework based on the Neyman-Pearson lemma to analyze combinations of classifiers. In particular, we give a method for finding the optimal decision rule for a combination of classifiers and prove that it has the optimal ROC curve. We also show how our method generalizes and improves on previous work on combining classifiers and generating ROC curves.",
        "bibtex": "@inproceedings{NIPS2007_75fc093c,\n author = {Barreno, Marco and Cardenas, Alvaro and Tygar, J. D.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Optimal ROC Curve for a Combination of Classifiers},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/75fc093c0ee742f6dddaa13fff98f104-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/75fc093c0ee742f6dddaa13fff98f104-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/75fc093c0ee742f6dddaa13fff98f104-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 75909,
        "gs_citation": 77,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8795689073099199576&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Computer Science Division, University of California at Berkeley; Computer Science Division, University of California at Berkeley; Computer Science Division, University of California at Berkeley",
        "aff_domain": "cs.berkeley.edu;cs.berkeley.edu;cs.berkeley.edu",
        "email": "cs.berkeley.edu;cs.berkeley.edu;cs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "Computer Science Division",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b6454ddf81",
        "title": "Optimal models of sound localization by barn owls",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/a5771bce93e200c36f7cd9dfd0e5deaa-Abstract.html",
        "author": "Brian J. Fischer",
        "abstract": "Sound localization by barn owls is commonly modeled as a matching procedure where localization cues derived from auditory inputs are compared to stored templates. While the matching models can explain properties of neural responses, no model explains how the owl resolves spatial ambiguity in the localization cues to produce accurate localization near the center of gaze. Here, we examine two models for the barn owl's sound localization behavior. First, we consider a maximum likelihood estimator in order to further evaluate the cue matching model. Second, we consider a maximum a posteriori estimator to test if a Bayesian model with a prior that emphasizes directions near the center of gaze can reproduce the owl's localization behavior. We show that the maximum likelihood estimator can not reproduce the owl's behavior, while the maximum a posteriori estimator is able to match the behavior. This result suggests that the standard cue matching model will not be sufficient to explain sound localization behavior in the barn owl. The Bayesian model provides a new framework for analyzing sound localization in the barn owl and leads to predictions about the owl's localization behavior.",
        "bibtex": "@inproceedings{NIPS2007_a5771bce,\n author = {Fischer, Brian},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Optimal models of sound localization by barn owls},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a5771bce93e200c36f7cd9dfd0e5deaa-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/a5771bce93e200c36f7cd9dfd0e5deaa-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/a5771bce93e200c36f7cd9dfd0e5deaa-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/a5771bce93e200c36f7cd9dfd0e5deaa-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 221544,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9081401028784990379&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Division of Biology, California Institute of Technology, Pasadena, CA",
        "aff_domain": "caltech.edu",
        "email": "caltech.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Division of Biology, California Institute of Technology, Pasadena, CA",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "e138fd6ea7",
        "title": "Optimistic Linear Programming gives Logarithmic Regret for Irreducible MDPs",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/9f396fe44e7c05c16873b05ec425cbad-Abstract.html",
        "author": "Ambuj Tewari; Peter L. Bartlett",
        "abstract": "We present an algorithm called Optimistic Linear Programming (OLP) for learning to optimize average reward in an irreducible but otherwise unknown Markov decision process (MDP). OLP uses its experience so far to estimate the MDP. It chooses actions by optimistically maximizing estimated future rewards over a set of next-state transition probabilities that are close to the estimates: a computation that corresponds to solving linear programs. We show that the total expected reward obtained by OLP up to time $T$ is within $C(P)\\log T$ of the reward obtained by the optimal policy, where $C(P)$ is an explicit, MDP-dependent constant. OLP is closely related to an algorithm proposed by Burnetas and Katehakis with four key differences: OLP is simpler, it does not require knowledge of the supports of transition probabilities and the proof of the regret bound is simpler, but our regret bound is a constant factor larger than the regret of their algorithm. OLP is also similar in flavor to an algorithm recently proposed by Auer and Ortner. But OLP is simpler and its regret bound has a better dependence on the size of the MDP.",
        "bibtex": "@inproceedings{NIPS2007_9f396fe4,\n author = {Tewari, Ambuj and Bartlett, Peter},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Optimistic Linear Programming gives Logarithmic Regret for Irreducible MDPs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9f396fe44e7c05c16873b05ec425cbad-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/9f396fe44e7c05c16873b05ec425cbad-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/9f396fe44e7c05c16873b05ec425cbad-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/9f396fe44e7c05c16873b05ec425cbad-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 170693,
        "gs_citation": 144,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5711463119718373916&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "Computer Science Division, University of California, Berkeley; Computer Science Division and Department of Statistics, University of California, Berkeley",
        "aff_domain": "cs.berkeley.edu;cs.berkeley.edu",
        "email": "cs.berkeley.edu;cs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of California, Berkeley;Computer Science Division and Department of Statistics, University of California, Berkeley",
        "aff_unique_dep": "Computer Science Division;",
        "aff_unique_url": "https://www.berkeley.edu;",
        "aff_unique_abbr": "UC Berkeley;",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "79c3c4de05",
        "title": "Parallelizing Support Vector Machines on Distributed Computers",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/ddb30680a691d157187ee1cf9e896d03-Abstract.html",
        "author": "Kaihua Zhu; Hao Wang; Hongjie Bai; Jian Li; Zhihuan Qiu; Hang Cui; Edward Y. Chang",
        "abstract": "Support Vector Machines (SVMs) suffer from a widely recognized scalability problem in both memory use and computational time. To improve scalability, we have developed a parallel SVM algorithm (PSVM), which reduces memory use through performing a row-based, approximate matrix factorization, and which loads only essential data to each machine to perform parallel computation. Let $n$ denote the number of training instances, $p$ the reduced matrix dimension after factorization ($p$ is significantly smaller than $n$), and $m$ the number of machines. PSVM reduces the memory requirement from $\\MO$($n^2$) to $\\MO$($np/m$), and improves computation time to $\\MO$($np^2/m$). Empirical studies on up to $500$ computers shows PSVM to be effective.",
        "bibtex": "@inproceedings{NIPS2007_ddb30680,\n author = {Zhu, Kaihua and Wang, Hao and Bai, Hongjie and Li, Jian and Qiu, Zhihuan and Cui, Hang and Chang, Edward},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Parallelizing Support Vector Machines on Distributed Computers},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/ddb30680a691d157187ee1cf9e896d03-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/ddb30680a691d157187ee1cf9e896d03-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/ddb30680a691d157187ee1cf9e896d03-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 512103,
        "gs_citation": 384,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7932771764424132500&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "db75174279",
        "title": "People Tracking with the Laplacian Eigenmaps Latent Variable Model",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/5f93f983524def3dca464469d2cf9f3e-Abstract.html",
        "author": "Zhengdong Lu; Cristian Sminchisescu; Miguel \u00c1. Carreira-Perpi\u00f1\u00e1n",
        "abstract": "Reliably recovering 3D human pose from monocular video requires constraints that bias the estimates towards typical human poses and motions. We define priors for people tracking using a Laplacian Eigenmaps Latent Variable Model (LELVM). LELVM is a probabilistic dimensionality reduction model that naturally combines the advantages of latent variable models---definining a multimodal probability density for latent and observed variables, and globally differentiable nonlinear mappings for reconstruction and dimensionality reduction---with those of spectral manifold learning methods---no local optima, ability to unfold highly nonlinear manifolds, and good practical scaling to latent spaces of high dimension. LELVM is computationally efficient, simple to learn from sparse training data, and compatible with standard probabilistic trackers such as particle filters. We analyze the performance of a LELVM-based probabilistic sigma point mixture tracker in several real and synthetic human motion sequences and demonstrate that LELVM provides sufficient constraints for robust operation in the presence of missing, noisy and ambiguous image measurements.",
        "bibtex": "@inproceedings{NIPS2007_5f93f983,\n author = {Lu, Zhengdong and Sminchisescu, Cristian and Carreira-Perpi\\~{n}\\'{a}n, Miguel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {People Tracking with the Laplacian Eigenmaps Latent Variable Model},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/5f93f983524def3dca464469d2cf9f3e-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/5f93f983524def3dca464469d2cf9f3e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/5f93f983524def3dca464469d2cf9f3e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 498717,
        "gs_citation": 75,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11938419498274522629&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "CSEE, OGI, OHSU; EECS, UC Merced; University of Bonn",
        "aff_domain": "csee.ogi.edu; ; sminchisescu.ins.uni-bonn.de",
        "email": "csee.ogi.edu; ; sminchisescu.ins.uni-bonn.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "CSEE, OGI, OHSU;University of California, Merced;University of Bonn",
        "aff_unique_dep": ";Department of Electrical Engineering and Computer Science;",
        "aff_unique_url": ";https://www.ucmerced.edu;https://www.uni-bonn.de/",
        "aff_unique_abbr": ";UC Merced;UBonn",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Merced",
        "aff_country_unique_index": "1;2",
        "aff_country_unique": ";United States;Germany"
    },
    {
        "id": "02f3b7d436",
        "title": "Predicting Brain States from fMRI Data: Incremental Functional Principal Component Regression",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/db8e1af0cb3aca1ae2d0018624204529-Abstract.html",
        "author": "Sennay Ghebreab; Arnold Smeulders; Pieter Adriaans",
        "abstract": "We propose a method for reconstruction of human brain states directly from functional neuroimaging data. The method extends the traditional multivariate regression analysis of discretized fMRI data to the domain of stochastic functional measurements, facilitating evaluation of brain responses to naturalistic stimuli and boosting the power of functional imaging. The method searches for sets of voxel timecourses that optimize a multivariate functional linear model in terms of Rsquare-statistic. Population based incremental learning is used to search for spatially distributed voxel clusters, taking into account the variation in Haemodynamic lag across brain areas and among subjects by voxel-wise non-linear registration of stimuli to fMRI data. The method captures spatially distributed brain responses to naturalistic stimuli without attempting to localize function. Application of the method for prediction of naturalistic stimuli from new and unknown fMRI data shows that the approach is capable of identifying distributed clusters of brain locations that are highly predictive of a specific stimuli.",
        "bibtex": "@inproceedings{NIPS2007_db8e1af0,\n author = {Ghebreab, Sennay and Smeulders, Arnold and Adriaans, Pieter},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Predicting Brain States from fMRI Data: Incremental Functional Principal Component Regression},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/db8e1af0cb3aca1ae2d0018624204529-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/db8e1af0cb3aca1ae2d0018624204529-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/db8e1af0cb3aca1ae2d0018624204529-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 226805,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10896405059880352330&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "ISLA/HCS lab, Informatics Institute, University of Amsterdam, The Netherlands; ISLA lab, Informatics Institute, University of Amsterdam, The Netherlands; HCS lab, Informatics Institute, University of Amsterdam, The Netherlands",
        "aff_domain": "science.uva.nl;science.uva.nl;science.uva.nl",
        "email": "science.uva.nl;science.uva.nl;science.uva.nl",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "ISLA/HCS lab, Informatics Institute, University of Amsterdam, The Netherlands;ISLA lab, Informatics Institute, University of Amsterdam, The Netherlands;HCS lab, Informatics Institute, University of Amsterdam, The Netherlands",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "3fcec137c7",
        "title": "Predicting human gaze using low-level saliency combined with face detection",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/708f3cf8100d5e71834b1db77dfa15d6-Abstract.html",
        "author": "Moran Cerf; Jonathan Harel; Wolfgang Einhaeuser; Christof Koch",
        "abstract": "Under natural viewing conditions, human observers shift their gaze to allocate processing resources to subsets of the visual input. Many computational models have aimed at predicting such voluntary attentional shifts. Although the importance of high level stimulus properties (higher order statistics, semantics) stands undisputed, most models are based on low-level features of the input alone. In this study we recorded eye-movements of human observers while they viewed photographs of natural scenes. About two thirds of the stimuli contained at least one person. We demonstrate that a combined model of face detection and low-level saliency clearly outperforms a low-level model in predicting locations humans fixate. This is reflected in our finding fact that observes, even when not instructed to look for anything particular, fixate on a face with a probability of over 80% within their first two fixations (500ms). Remarkably, the model's predictive performance in images that do not contain faces is not impaired by spurious face detector responses, which is suggestive of a bottom-up mechanism for face detection. In summary, we provide a novel computational approach which combines high level object knowledge (in our case: face locations) with low-level features to successfully predict the allocation of attentional resources.",
        "bibtex": "@inproceedings{NIPS2007_708f3cf8,\n author = {Cerf, Moran and Harel, Jonathan and Einhaeuser, Wolfgang and Koch, Christof},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Predicting human gaze using low-level saliency combined with face detection},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/708f3cf8100d5e71834b1db77dfa15d6-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/708f3cf8100d5e71834b1db77dfa15d6-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/708f3cf8100d5e71834b1db77dfa15d6-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/708f3cf8100d5e71834b1db77dfa15d6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1028115,
        "gs_citation": 704,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1712023407677153928&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "Computation and Neural Systems, California Institute of Technology; Electrical Engineering, California Institute of Technology; Institute of Computational Science, Swiss Federal Institute of Technology (ETH); Computation and Neural Systems, California Institute of Technology",
        "aff_domain": "klab.caltech.edu;klab.caltech.edu;inf.ethz.ch;klab.caltech.edu",
        "email": "klab.caltech.edu;klab.caltech.edu;inf.ethz.ch;klab.caltech.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Computation and Neural Systems, California Institute of Technology;Electrical Engineering, California Institute of Technology;Institute of Computational Science, Swiss Federal Institute of Technology (ETH)",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "932c1b4d66",
        "title": "Predictive Matrix-Variate t Models",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/061412e4a03c02f9902576ec55ebbe77-Abstract.html",
        "author": "Shenghuo Zhu; Kai Yu; Yihong Gong",
        "abstract": "It is becoming increasingly important to learn from a partially-observed random matrix and predict its missing elements. We assume that the entire matrix is a single sample drawn from a matrix-variate t distribution and suggest a matrix-variate t model (MVTM) to predict those missing elements. We show that MVTM generalizes a range of known probabilistic models, and automatically performs model selection to encourage sparse predictive models. Due to the non-conjugacy of its prior, it is difficult to make predictions by computing the mode or mean of the posterior distribution. We suggest an optimization method that sequentially minimizes a convex upper-bound of the log-likelihood, which is very efficient and scalable. The experiments on a toy data and EachMovie dataset show a good predictive accuracy of the model.",
        "bibtex": "@inproceedings{NIPS2007_061412e4,\n author = {Zhu, Shenghuo and Yu, Kai and Gong, Yihong},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Predictive Matrix-Variate t Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/061412e4a03c02f9902576ec55ebbe77-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/061412e4a03c02f9902576ec55ebbe77-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/061412e4a03c02f9902576ec55ebbe77-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 143191,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=770286199866409477&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 10,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "e8f9d6c0ab",
        "title": "Privacy-Preserving Belief Propagation and Sampling",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/d516b13671a4179d9b7b458a6ebdeb92-Abstract.html",
        "author": "Michael Kearns; Jinsong Tan; Jennifer Wortman",
        "abstract": "We provide provably privacy-preserving versions of belief propagation, Gibbs sampling, and other local algorithms \u2014 distributed multiparty protocols in which each party or vertex learns only its \ufb01nal local value, and absolutely nothing else.",
        "bibtex": "@inproceedings{NIPS2007_d516b136,\n author = {Kearns, Michael and Tan, Jinsong and Wortman, Jennifer},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Privacy-Preserving Belief Propagation and Sampling},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d516b13671a4179d9b7b458a6ebdeb92-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/d516b13671a4179d9b7b458a6ebdeb92-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/d516b13671a4179d9b7b458a6ebdeb92-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/d516b13671a4179d9b7b458a6ebdeb92-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 97086,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4239580714693598322&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "431cda9fee",
        "title": "Probabilistic Matrix Factorization",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/d7322ed717dedf1eb4e6e52a37ea7bcd-Abstract.html",
        "author": "Andriy Mnih; Ruslan Salakhutdinov",
        "abstract": "Many existing approaches to collaborative \ufb01ltering can neither handle very large datasets nor easily deal with users who have very few ratings. In this paper we present the Probabilistic Matrix Factorization (PMF) model which scales linearly with the number of observations and, more importantly, performs well on the large, sparse, and very imbalanced Net\ufb02ix dataset. We further extend the PMF model to include an adaptive prior on the model parameters and show how the model capacity can be controlled automatically. Finally, we introduce a con- strained version of the PMF model that is based on the assumption that users who have rated similar sets of movies are likely to have similar preferences. The result- ing model is able to generalize considerably better for users with very few ratings. When the predictions of multiple PMF models are linearly combined with the predictions of Restricted Boltzmann Machines models, we achieve an error rate of 0.8861, that is nearly 7% better than the score of Net\ufb02ix\u2019s own system.",
        "bibtex": "@inproceedings{NIPS2007_d7322ed7,\n author = {Mnih, Andriy and Salakhutdinov, Russ R},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Probabilistic Matrix Factorization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d7322ed717dedf1eb4e6e52a37ea7bcd-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/d7322ed717dedf1eb4e6e52a37ea7bcd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/d7322ed717dedf1eb4e6e52a37ea7bcd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 85557,
        "gs_citation": 5774,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4309383935999525301&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 24,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "0f75edaaba",
        "title": "Progressive mixture rules are deviation suboptimal",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/ef575e8837d065a1683c022d2077d342-Abstract.html",
        "author": "Jean-yves Audibert",
        "abstract": "We consider the learning task consisting in predicting as well as the best function in a finite reference set G up to the smallest possible additive term. If R(g) denotes the generalization error of a prediction function g, under reasonable assumptions on the loss function (typically satisfied by the least square loss when the output is bounded), it is known that the progressive mixture rule g",
        "bibtex": "@inproceedings{NIPS2007_ef575e88,\n author = {Audibert, Jean-yves},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Progressive mixture rules are deviation suboptimal},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/ef575e8837d065a1683c022d2077d342-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/ef575e8837d065a1683c022d2077d342-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/ef575e8837d065a1683c022d2077d342-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 100621,
        "gs_citation": 90,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2359348097556706749&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "141bfb9982",
        "title": "Random Features for Large-Scale Kernel Machines",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/013a006f03dbc5392effeb8f18fda755-Abstract.html",
        "author": "Ali Rahimi; Benjamin Recht",
        "abstract": "To accelerate the training of kernel machines, we propose to map the input data to a randomized low-dimensional feature space and then apply existing fast linear methods. The features are designed so that the inner products of the transformed data are approximately equal to those in the feature space of a user speci\ufb01ed shift- invariant kernel. We explore two sets of random features, provide convergence bounds on their ability to approximate various radial basis kernels, and show that in large-scale classi\ufb01cation and regression tasks linear machine learning al- gorithms applied to these features outperform state-of-the-art large-scale kernel machines.",
        "bibtex": "@inproceedings{NIPS2007_013a006f,\n author = {Rahimi, Ali and Recht, Benjamin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Random Features for Large-Scale Kernel Machines},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 242313,
        "gs_citation": 5406,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14226915046610868571&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 19,
        "aff": "Intel Research Seattle; Caltech IST",
        "aff_domain": "intel.com;ist.caltech.edu",
        "email": "intel.com;ist.caltech.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Intel Research Seattle;Caltech IST",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "ef312a1590",
        "title": "Random Projections for Manifold Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/1e6e0a04d20f50967c64dac2d639a577-Abstract.html",
        "author": "Chinmay Hegde; Michael Wakin; Richard Baraniuk",
        "abstract": "We propose a novel method for {\\em linear} dimensionality reduction of manifold modeled data. First, we show that with a small number $M$ of {\\em random projections} of sample points in $\\reals^N$ belonging to an unknown $K$-dimensional Euclidean manifold, the intrinsic dimension (ID) of the sample set can be estimated to high accuracy. Second, we rigorously prove that using only this set of random projections, we can estimate the structure of the underlying manifold. In both cases, the number random projections required is linear in $K$ and logarithmic in $N$, meaning that $K",
        "bibtex": "@inproceedings{NIPS2007_1e6e0a04,\n author = {Hegde, Chinmay and Wakin, Michael and Baraniuk, Richard},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Random Projections for Manifold Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/1e6e0a04d20f50967c64dac2d639a577-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/1e6e0a04d20f50967c64dac2d639a577-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/1e6e0a04d20f50967c64dac2d639a577-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/1e6e0a04d20f50967c64dac2d639a577-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 222528,
        "gs_citation": 179,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13401264016587715140&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "ECE Department, Rice University; EECS Department, University of Michigan; ECE Department, Rice University",
        "aff_domain": "rice.edu;eecs.umich.edu;rice.edu",
        "email": "rice.edu;eecs.umich.edu;rice.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "ECE Department, Rice University;EECS Department, University of Michigan",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "c5976c5751",
        "title": "Random Sampling of States in Dynamic Programming",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/6faa8040da20ef399b63a72d0e4ab575-Abstract.html",
        "author": "Chris Atkeson; Benjamin Stephens",
        "abstract": "We combine two threads of research on approximate dynamic programming: random sampling of states and using local trajectory optimizers to globally optimize a policy and associated value function. This combination allows us to replace a dense multidimensional grid with a much sparser adaptive sampling of states. Our focus is on finding steady state policies for the deterministic time invariant discrete time control problems with continuous states and actions often found in robotics. In this paper we show that we can now solve problems we couldn't solve previously with regular grid-based approaches.",
        "bibtex": "@inproceedings{NIPS2007_6faa8040,\n author = {Atkeson, Chris and Stephens, Benjamin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Random Sampling of States in Dynamic Programming},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/6faa8040da20ef399b63a72d0e4ab575-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/6faa8040da20ef399b63a72d0e4ab575-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/6faa8040da20ef399b63a72d0e4ab575-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/6faa8040da20ef399b63a72d0e4ab575-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 423689,
        "gs_citation": 87,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3109742062201505303&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "Robotics Institute, Carnegie Mellon University; Robotics Institute, Carnegie Mellon University",
        "aff_domain": "cmu.edu;cmu.edu",
        "email": "cmu.edu;cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Robotics Institute",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "73c5fc2b97",
        "title": "Rapid Inference on a Novel AND/OR graph for Object Detection, Segmentation and Parsing",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/20b02dc95171540bc52912baf3aa709d-Abstract.html",
        "author": "Yuanhao Chen; Long Zhu; Chenxi Lin; Hongjiang Zhang; Alan L. Yuille",
        "abstract": "In this paper we formulate a novel AND/OR graph representation capable of describing the different configurations of deformable articulated objects such as horses. The representation makes use of the summarization principle so that lower level nodes in the graph only pass on summary statistics to the higher level nodes. The probability distributions are invariant to position, orientation, and scale. We develop a novel inference algorithm that combined a bottom-up process for proposing configurations for horses together with a top-down process for refining and validating these proposals. The strategy of surround suppression is applied to ensure that the inference time is polynomial in the size of input data. The algorithm was applied to the tasks of detecting, segmenting and parsing horses. We demonstrate that the algorithm is fast and comparable with the state of the art approaches.",
        "bibtex": "@inproceedings{NIPS2007_20b02dc9,\n author = {Chen, Yuanhao and Zhu, Long and Lin, Chenxi and Zhang, Hongjiang and Yuille, Alan L},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Rapid Inference on a Novel AND/OR graph for Object Detection, Segmentation and Parsing},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/20b02dc95171540bc52912baf3aa709d-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/20b02dc95171540bc52912baf3aa709d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/20b02dc95171540bc52912baf3aa709d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 384757,
        "gs_citation": 82,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8948139829017877568&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "9a497514d9",
        "title": "Receding Horizon Differential Dynamic Programming",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/c6bff625bdb0393992c9d4db0c6bbe45-Abstract.html",
        "author": "Yuval Tassa; Tom Erez; William D. Smart",
        "abstract": "The control of high-dimensional, continuous, non-linear systems is a key problem in reinforcement learning and control. Local, trajectory-based methods, using techniques such as Differential Dynamic Programming (DDP) are not directly subject to the curse of dimensionality, but generate only local controllers. In this paper, we introduce Receding Horizon DDP (RH-DDP), an extension to the classic DDP algorithm, which allows us to construct stable and robust controllers based on a library of local-control trajectories. We demonstrate the effectiveness of our approach on a series of high-dimensional control problems using a simulated multi-link swimming robot. These experiments show that our approach effectively circumvents dimensionality issues, and is capable of dealing effectively with problems with (at least) 34 state and 14 action dimensions.",
        "bibtex": "@inproceedings{NIPS2007_c6bff625,\n author = {Tassa, Yuval and Erez, Tom and Smart, William},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Receding Horizon Differential Dynamic Programming},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c6bff625bdb0393992c9d4db0c6bbe45-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/c6bff625bdb0393992c9d4db0c6bbe45-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/c6bff625bdb0393992c9d4db0c6bbe45-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/c6bff625bdb0393992c9d4db0c6bbe45-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 280131,
        "gs_citation": 180,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17403321062216240569&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "bee3b5d987",
        "title": "Receptive Fields without Spike-Triggering",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/289dff07669d7a23de0ef88d2f7129e7-Abstract.html",
        "author": "Guenther Zeck; Matthias Bethge; Jakob H. Macke",
        "abstract": "Stimulus selectivity of sensory neurons is often characterized by estimating their receptive field properties such as orientation selectivity. Receptive fields are usually derived from the mean (or covariance) of the spike-triggered stimulus ensemble. This approach treats each spike as an independent message but does not take into account that information might be conveyed through patterns of neural activity that are distributed across space or time. Can we find a concise description for the processing of a whole population of neurons analogous to the receptive field for single neurons? Here, we present a generalization of the linear receptive field which is not bound to be triggered on individual spikes but can be meaningfully linked to distributed response patterns. More precisely, we seek to identify those stimulus features and the corresponding patterns of neural activity that are most reliably coupled. We use an extension of reverse-correlation methods based on canonical correlation analysis. The resulting population receptive fields span the subspace of stimuli that is most informative about the population response. We evaluate our approach using both neuronal models and multi-electrode recordings from rabbit retinal ganglion cells. We show how the model can be extended to capture nonlinear stimulus-response relationships using kernel canonical correlation analysis, which makes it possible to test different coding mechanisms. Our technique can also be used to calculate receptive fields from multi-dimensional neural measurements such as those obtained from dynamic imaging methods.",
        "bibtex": "@inproceedings{NIPS2007_289dff07,\n author = {Zeck, Guenther and Bethge, Matthias and Macke, Jakob H},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Receptive Fields without Spike-Triggering},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/289dff07669d7a23de0ef88d2f7129e7-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/289dff07669d7a23de0ef88d2f7129e7-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/289dff07669d7a23de0ef88d2f7129e7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 392514,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13105845645747680940&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Max Planck Institute for Biological Cybernetics; Max Planck Institute of Neurobiology; Max Planck Institute for Biological Cybernetics",
        "aff_domain": " t ue bi nge n. mpg. de; ne ur o . mpg. de; t ue bi nge n. mpg. de",
        "email": " t ue bi nge n. mpg. de; ne ur o . mpg. de; t ue bi nge n. mpg. de",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Max Planck Institute for Biological Cybernetics;Max Planck Institute of Neurobiology",
        "aff_unique_dep": "Biological Cybernetics;",
        "aff_unique_url": "https://www.biocybernetics.mpg.de;",
        "aff_unique_abbr": "MPIBC;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany;"
    },
    {
        "id": "fb510ceba4",
        "title": "Regret Minimization in Games with Incomplete Information",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/08d98638c6fcd194a4b1e6992063e944-Abstract.html",
        "author": "Martin Zinkevich; Michael Johanson; Michael Bowling; Carmelo Piccione",
        "abstract": "Extensive games are a powerful model of multiagent decision-making scenarios with incomplete information. Finding a Nash equilibrium for very large instances of these games has received a great deal of recent attention. In this paper, we describe a new technique for solving large games based on regret minimization. In particular, we introduce the notion of counterfactual regret, which exploits the degree of incomplete information in an extensive game. We show how minimizing counterfactual regret minimizes overall regret, and therefore in self-play can be used to compute a Nash equilibrium. We demonstrate this technique in the domain of poker, showing we can solve abstractions of limit Texas Hold\u2019em with as many as 1012 states, two orders of magnitude larger than previous methods.",
        "bibtex": "@inproceedings{NIPS2007_08d98638,\n author = {Zinkevich, Martin and Johanson, Michael and Bowling, Michael and Piccione, Carmelo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Regret Minimization in Games with Incomplete Information},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/08d98638c6fcd194a4b1e6992063e944-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/08d98638c6fcd194a4b1e6992063e944-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 177180,
        "gs_citation": 1103,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=383419540884614415&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 20,
        "aff": "Computing Science Department, University of Alberta, Edmonton, AB Canada T6G2E8; Computing Science Department, University of Alberta, Edmonton, AB Canada T6G2E8; Computing Science Department, University of Alberta, Edmonton, AB Canada T6G2E8; Computing Science Department, University of Alberta, Edmonton, AB Canada T6G2E8",
        "aff_domain": "cs.ualberta.ca;cs.ualberta.ca;cs.ualberta.ca;cs.ualberta.ca",
        "email": "cs.ualberta.ca;cs.ualberta.ca;cs.ualberta.ca;cs.ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Computing Science Department, University of Alberta, Edmonton, AB Canada T6G2E8",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "5c61ce5947",
        "title": "Regularized Boost for Semi-Supervised Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/fa7cdfad1a5aaf8370ebeda47a1ff1c3-Abstract.html",
        "author": "Ke Chen; Shihai Wang",
        "abstract": "Semi-supervised inductive learning concerns how to learn a decision rule from a data set containing both labeled and unlabeled data. Several boosting algorithms have been extended to semi-supervised learning with various strategies. To our knowledge, however, none of them takes local smoothness constraints among data into account during ensemble learning. In this paper, we introduce a local smoothness regularizer to semi-supervised boosting algorithms based on the universal optimization framework of margin cost functionals. Our regularizer is applicable to existing semi-supervised boosting algorithms to improve their generalization and speed up their training. Comparative results on synthetic, benchmark and real world tasks demonstrate the effectiveness of our local smoothness regularizer. We discuss relevant issues and relate our regularizer to previous work.",
        "bibtex": "@inproceedings{NIPS2007_fa7cdfad,\n author = {Chen, Ke and Wang, Shihai},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Regularized Boost for Semi-Supervised Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/fa7cdfad1a5aaf8370ebeda47a1ff1c3-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/fa7cdfad1a5aaf8370ebeda47a1ff1c3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/fa7cdfad1a5aaf8370ebeda47a1ff1c3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 290740,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18350647287670443485&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "School of Computer Science, The University of Manchester; School of Computer Science, The University of Manchester",
        "aff_domain": "cs.manchester.ac.uk;cs.manchester.ac.uk",
        "email": "cs.manchester.ac.uk;cs.manchester.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "School of Computer Science, The University of Manchester",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "b7210f15b6",
        "title": "Regulator Discovery from Gene Expression Time Series of Malaria Parasites: a Hierachical Approach",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/3e89ebdb49f712c7d90d1b39e348bbbf-Abstract.html",
        "author": "Jos\u00e9 M. Hern\u00e1ndez-lobato; Tjeerd Dijkstra; Tom Heskes",
        "abstract": "We introduce a hierarchical Bayesian model for the discovery of putative regulators from gene expression data only. The hierarchy incorporates the knowledge that there are just a few regulators that by themselves only regulate a handful of genes. This is implemented through a so-called spike-and-slab prior, a mixture of Gaussians with different widths, with mixing weights from a hierarchical Bernoulli model. For efficient inference we implemented expectation propagation. Running the model on a malaria parasite data set, we found four genes with significant homology to transcription factors in an amoebe, one RNA regulator and three genes of unknown function (out of the top ten genes considered).",
        "bibtex": "@inproceedings{NIPS2007_3e89ebdb,\n author = {Hern\\'{a}ndez-lobato, Jos\\'{e} and Dijkstra, Tjeerd and Heskes, Tom},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Regulator Discovery from Gene Expression Time Series of Malaria Parasites: a Hierachical Approach},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/3e89ebdb49f712c7d90d1b39e348bbbf-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/3e89ebdb49f712c7d90d1b39e348bbbf-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/3e89ebdb49f712c7d90d1b39e348bbbf-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 407062,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14149800131704801239&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Escuela Polit\u00e9cnica Superior, Universidad Aut\u00f3noma de Madrid, Madrid, Spain; Leiden Malaria Research Group, LUMC, Leiden, The Netherlands; Institute for Computing and Information Sciences, Radboud University Nijmegen, Nijmegen, The Netherlands",
        "aff_domain": "uam.es;lumc.nl;science.ru.nl",
        "email": "uam.es;lumc.nl;science.ru.nl",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Escuela Polit\u00e9cnica Superior, Universidad Aut\u00f3noma de Madrid, Madrid, Spain;Leiden Malaria Research Group, LUMC, Leiden, The Netherlands;Radboud University Nijmegen",
        "aff_unique_dep": ";;Institute for Computing and Information Sciences",
        "aff_unique_url": ";;https://www.ru.nl",
        "aff_unique_abbr": ";;RU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Nijmegen",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";Netherlands"
    },
    {
        "id": "0693d350c1",
        "title": "Reinforcement Learning in Continuous Action Spaces through Sequential Monte Carlo Methods",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/0f840be9b8db4d3fbd5ba2ce59211f55-Abstract.html",
        "author": "Alessandro Lazaric; Marcello Restelli; Andrea Bonarini",
        "abstract": "Learning in real-world domains often requires to deal with continuous state and action spaces. Although many solutions have been proposed to apply Reinforce- ment Learning algorithms to continuous state problems, the same techniques can be hardly extended to continuous action spaces, where, besides the computation of a good approximation of the value function, a fast method for the identi\ufb01cation of the highest-valued action is needed. In this paper, we propose a novel actor-critic approach in which the policy of the actor is estimated through sequential Monte Carlo methods. The importance sampling step is performed on the basis of the values learned by the critic, while the resampling step modi\ufb01es the actor\u2019s policy. The proposed approach has been empirically compared to other learning algo- rithms into several domains; in this paper, we report results obtained in a control problem consisting of steering a boat across a river.",
        "bibtex": "@inproceedings{NIPS2007_0f840be9,\n author = {Lazaric, Alessandro and Restelli, Marcello and Bonarini, Andrea},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Reinforcement Learning in Continuous Action Spaces through Sequential Monte Carlo Methods},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/0f840be9b8db4d3fbd5ba2ce59211f55-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/0f840be9b8db4d3fbd5ba2ce59211f55-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/0f840be9b8db4d3fbd5ba2ce59211f55-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 183956,
        "gs_citation": 206,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8617967248744360400&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "b19353138d",
        "title": "Retrieved context and the discovery of semantic structure",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/b3967a0e938dc2a6340e258630febd5a-Abstract.html",
        "author": "Vinayak Rao; Marc Howard",
        "abstract": "Semantic memory refers to our knowledge of facts and relationships between con- cepts. A successful semantic memory depends on inferring relationships between items that are not explicitly taught. Recent mathematical modeling of episodic memory argues that episodic recall relies on retrieval of a gradually-changing rep- resentation of temporal context. We show that retrieved context enables the de- velopment of a global memory space that re\ufb02ects relationships between all items that have been previously learned. When newly-learned information is integrated into this structure, it is placed in some relationship to all other items, even if that relationship has not been explicitly learned. We demonstrate this effect for global semantic structures shaped topologically as a ring, and as a two-dimensional sheet. We also examined the utility of this learning algorithm for learning a more realistic semantic space by training it on a large pool of synonym pairs. Retrieved context enabled the model to \u201cinfer\u201d relationships between synonym pairs that had not yet been presented.",
        "bibtex": "@inproceedings{NIPS2007_b3967a0e,\n author = {Rao, Vinayak and Howard, Marc},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Retrieved context and the discovery of semantic structure},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/b3967a0e938dc2a6340e258630febd5a-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/b3967a0e938dc2a6340e258630febd5a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/b3967a0e938dc2a6340e258630febd5a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 183046,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3847311796611014913&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "Syracuse University, Department of Psychology; Syracuse University, Department of Psychology + Gatsby Computational Neuroscience Unit, University College London",
        "aff_domain": "gatsby.ucl.ac.uk;memory.syr.edu",
        "email": "gatsby.ucl.ac.uk;memory.syr.edu",
        "github": "",
        "project": "http://memory.syr.edu",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "Syracuse University, Department of Psychology;University College London",
        "aff_unique_dep": ";Gatsby Computational Neuroscience Unit",
        "aff_unique_url": ";https://www.ucl.ac.uk",
        "aff_unique_abbr": ";UCL",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";London",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United Kingdom"
    },
    {
        "id": "d1b25dc65d",
        "title": "Robust Regression with Twinned Gaussian Processes",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/2ab56412b1163ee131e1246da0955bd1-Abstract.html",
        "author": "Andrew Naish-guzman; Sean Holden",
        "abstract": "We propose a Gaussian process (GP) framework for robust inference in which a GP prior on the mixing weights of a two-component noise model augments the standard process over latent function values. This approach is a generalization of the mixture likelihood used in traditional robust GP regression, and a specialization of the GP mixture models suggested by Tresp (2000) and Rasmussen and Ghahramani (2002). The value of this restriction is in its tractable expectation propagation updates, which allow for faster inference and model selection, and better convergence than the standard mixture. An additional benefit over the latter method lies in our ability to incorporate knowledge of the noise domain to influence predictions, and to recover with the predictive distribution information about the outlier distribution via the gating process. The model has asymptotic complexity equal to that of conventional robust methods, but yields more confident predictions on benchmark problems than classical heavy-tailed models and exhibits improved stability for data with clustered corruptions, for which they fail altogether. We show further how our approach can be used without adjustment for more smoothly heteroscedastic data, and suggest how it could be extended to more general noise models. We also address similarities with the work of Goldberg et al. (1998), and the more recent contributions of Tresp, and Rasmussen and Ghahramani.",
        "bibtex": "@inproceedings{NIPS2007_2ab56412,\n author = {Naish-guzman, Andrew and Holden, Sean},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Robust Regression with Twinned Gaussian Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/2ab56412b1163ee131e1246da0955bd1-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/2ab56412b1163ee131e1246da0955bd1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/2ab56412b1163ee131e1246da0955bd1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 862133,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12719831386701877838&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Computer Laboratory, University of Cambridge; Computer Laboratory, University of Cambridge",
        "aff_domain": "cl.cam.ac.uk;cl.cam.ac.uk",
        "email": "cl.cam.ac.uk;cl.cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Cambridge",
        "aff_unique_dep": "Computer Laboratory",
        "aff_unique_url": "https://www.cam.ac.uk",
        "aff_unique_abbr": "Cambridge",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "23a8945966",
        "title": "Scan Strategies for Meteorological Radars",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/0bb4aec1710521c12ee76289d9440817-Abstract.html",
        "author": "Victoria Manfredi; Jim Kurose",
        "abstract": "We address the problem of adaptive sensor control in dynamic resource-constrained sensor networks. We focus on a meteorological sensing network comprising radars that can perform sector scanning rather than always scanning 360 degrees. We compare three sector scanning strategies. The sit-and-spin strategy always scans 360 degrees. The limited lookahead strategy additionally uses the expected environmental state K decision epochs in the future, as predicted from Kalman filters, in its decision-making. The full lookahead strategy uses all expected future states by casting the problem as a Markov decision process and using reinforcement learning to estimate the optimal scan strategy. We show that the main benefits of using a lookahead strategy are when there are multiple meteorological phenomena in the environment, and when the maximum radius of any phenomenon is sufficiently smaller than the radius of the radars. We also show that there is a trade-off between the average quality with which a phenomenon is scanned and the number of decision epochs before which a phenomenon is rescanned.",
        "bibtex": "@inproceedings{NIPS2007_0bb4aec1,\n author = {Manfredi, Victoria and Kurose, Jim},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Scan Strategies for Meteorological Radars},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/0bb4aec1710521c12ee76289d9440817-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/0bb4aec1710521c12ee76289d9440817-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/0bb4aec1710521c12ee76289d9440817-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 292448,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14535933361563101084&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science, University of Massachusetts; Department of Computer Science, University of Massachusetts",
        "aff_domain": "cs.umass.edu;cs.umass.edu",
        "email": "cs.umass.edu;cs.umass.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Massachusetts",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.umass.edu",
        "aff_unique_abbr": "UMass",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "8b7e7dbf8f",
        "title": "Scene Segmentation with CRFs Learned from Partially Labeled Images",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/cb70ab375662576bd1ac5aaf16b3fca4-Abstract.html",
        "author": "Bill Triggs; Jakob J. Verbeek",
        "abstract": "Conditional Random Fields (CRFs) are an effective tool for a variety of different data segmentation and labeling tasks including visual scene interpretation, which seeks to partition images into their constituent semantic-level regions and assign appropriate class labels to each region. For accurate labeling it is important to capture the global context of the image as well as local information. We in- troduce a CRF based scene labeling model that incorporates both local features and features aggregated over the whole image or large sections of it. Secondly, traditional CRF learning requires fully labeled datasets which can be costly and troublesome to produce. We introduce a method for learning CRFs from datasets with many unlabeled nodes by marginalizing out the unknown labels so that the log-likelihood of the known ones can be maximized by gradient ascent. Loopy Belief Propagation is used to approximate the marginals needed for the gradi- ent and log-likelihood calculations and the Bethe free-energy approximation to the log-likelihood is monitored to control the step size. Our experimental results show that effective models can be learned from fragmentary labelings and that incorporating top-down aggregate features signi\ufb01cantly improves the segmenta- tions. The resulting segmentations are compared to the state-of-the-art on three different image datasets.",
        "bibtex": "@inproceedings{NIPS2007_cb70ab37,\n author = {Triggs, Bill and Verbeek, Jakob},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Scene Segmentation with CRFs Learned from Partially Labeled Images},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/cb70ab375662576bd1ac5aaf16b3fca4-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/cb70ab375662576bd1ac5aaf16b3fca4-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/cb70ab375662576bd1ac5aaf16b3fca4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 681886,
        "gs_citation": 233,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7766015811207591490&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 37,
        "aff": "INRIA and Laboratoire Jean Kuntzmann, 655 avenue de l\u2019Europe, 38330 Montbonnot, France; INRIA and Laboratoire Jean Kuntzmann, 655 avenue de l\u2019Europe, 38330 Montbonnot, France",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "INRIA and Laboratoire Jean Kuntzmann, 655 avenue de l\u2019Europe, 38330 Montbonnot, France",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "6195b4faf2",
        "title": "Second Order Bilinear Discriminant Analysis for single trial EEG analysis",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/4ea06fbc83cdd0a06020c35d50e1e89a-Abstract.html",
        "author": "Christoforos Christoforou; Paul Sajda; Lucas C. Parra",
        "abstract": "Traditional analysis methods for single-trial classification of electro-encephalography (EEG) focus on two types of paradigms: phase locked methods, in which the amplitude of the signal is used as the feature for classification, i.e. event related potentials; and second order methods, in which the feature of interest is the power of the signal, i.e event related (de)synchronization. The process of deciding which paradigm to use is ad hoc and is driven by knowledge of neurological findings. Here we propose a unified method in which the algorithm learns the best first and second order spatial and temporal features for classification of EEG based on a bilinear model. The efficiency of the method is demonstrated in simulated and real EEG from a benchmark data set for Brain Computer Interface.",
        "bibtex": "@inproceedings{NIPS2007_4ea06fbc,\n author = {Christoforou, Christoforos and Sajda, Paul and Parra, Lucas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Second Order Bilinear Discriminant Analysis for single trial EEG analysis},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/4ea06fbc83cdd0a06020c35d50e1e89a-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/4ea06fbc83cdd0a06020c35d50e1e89a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/4ea06fbc83cdd0a06020c35d50e1e89a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 219684,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10905844826524982474&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science, The Graduate Center of the City University of New York; Department of Biomedical Engineering, Columbia University; Department of Biomedical Engineering, The City College of The City University of New York",
        "aff_domain": "gc.cuny.edu;columbia.edu;ccny.cuny.edu",
        "email": "gc.cuny.edu;columbia.edu;ccny.cuny.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Department of Computer Science, The Graduate Center of the City University of New York;Department of Biomedical Engineering, Columbia University;Department of Biomedical Engineering, The City College of The City University of New York",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "ae09092f97",
        "title": "Selecting Observations against Adversarial Objectives",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/eae27d77ca20db309e056e3d2dcd7d69-Abstract.html",
        "author": "Andreas Krause; Brendan Mcmahan; Carlos Guestrin; Anupam Gupta",
        "abstract": "In many applications, one has to actively select among a set of expensive observa- tions before making an informed decision. Often, we want to select observations which perform well when evaluated with an objective function chosen by an adver- sary. Examples include minimizing the maximum posterior variance in Gaussian Process regression, robust experimental design, and sensor placement for outbreak detection. In this paper, we present the Submodular Saturation algorithm, a sim- ple and ef\ufb01cient algorithm with strong theoretical approximation guarantees for the case where the possible objective functions exhibit submodularity, an intuitive diminishing returns property. Moreover, we prove that better approximation al- gorithms do not exist unless NP-complete problems admit ef\ufb01cient algorithms. We evaluate our algorithm on several real-world problems. For Gaussian Process regression, our algorithm compares favorably with state-of-the-art heuristics de- scribed in the geostatistics literature, while being simpler, faster and providing theoretical guarantees. For robust experimental design, our algorithm performs favorably compared to SDP-based algorithms.",
        "bibtex": "@inproceedings{NIPS2007_eae27d77,\n author = {Krause, Andreas and Mcmahan, Brendan and Guestrin, Carlos and Gupta, Anupam},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Selecting Observations against Adversarial Objectives},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/eae27d77ca20db309e056e3d2dcd7d69-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/eae27d77ca20db309e056e3d2dcd7d69-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/eae27d77ca20db309e056e3d2dcd7d69-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 211620,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3794931612896091373&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "2132d35c65",
        "title": "Semi-Supervised Multitask Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/a34bacf839b923770b2c360eefa26748-Abstract.html",
        "author": "Qiuhua Liu; Xuejun Liao; Lawrence Carin",
        "abstract": "A semi-supervised multitask learning (MTL) framework is presented, in which M parameterized semi-supervised classi\ufb01ers, each associated with one of M par- tially labeled data manifolds, are learned jointly under the constraint of a soft- sharing prior imposed over the parameters of the classi\ufb01ers. The unlabeled data are utilized by basing classi\ufb01er learning on neighborhoods, induced by a Markov random walk over a graph representation of each manifold. Experimental results on real data sets demonstrate that semi-supervised MTL yields signi\ufb01cant im- provements in generalization performance over either semi-supervised single-task learning (STL) or supervised MTL.",
        "bibtex": "@inproceedings{NIPS2007_a34bacf8,\n author = {Liu, Qiuhua and Liao, Xuejun and Carin, Lawrence},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Semi-Supervised Multitask Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/a34bacf839b923770b2c360eefa26748-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/a34bacf839b923770b2c360eefa26748-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/a34bacf839b923770b2c360eefa26748-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 216702,
        "gs_citation": 151,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13118914599436409049&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "3bc0e9a734",
        "title": "Sequential Hypothesis Testing under Stochastic Deadlines",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/9c82c7143c102b71c593d98d96093fde-Abstract.html",
        "author": "Peter Frazier; Angela J. Yu",
        "abstract": "Most models of decision-making in neuroscience assume an in\ufb01nite horizon, which yields an optimal solution that integrates evidence up to a \ufb01xed decision threshold; however, under most experimental as well as naturalistic behavioral settings, the decision has to be made before some \ufb01nite deadline, which is often experienced as a stochastic quantity, either due to variable external constraints or internal timing uncertainty. In this work, we formulate this problem as sequential hypothesis testing under a stochastic horizon. We use dynamic programming tools to show that, for a large class of deadline distributions, the Bayes-optimal solution requires integrating evidence up to a threshold that declines monotonically over time. We use numerical simulations to illustrate the optimal policy in the special cases of a \ufb01xed deadline and one that is drawn from a gamma distribution.",
        "bibtex": "@inproceedings{NIPS2007_9c82c714,\n author = {Frazier, Peter and Yu, Angela J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sequential Hypothesis Testing under Stochastic Deadlines},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9c82c7143c102b71c593d98d96093fde-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/9c82c7143c102b71c593d98d96093fde-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/9c82c7143c102b71c593d98d96093fde-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 123217,
        "gs_citation": 147,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15910798825786672417&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "ORFE, Princeton University, Princeton, NJ 08544; CSBMB, Princeton University, Princeton, NJ 08544",
        "aff_domain": "princeton.edu;princeton.edu",
        "email": "princeton.edu;princeton.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "ORFE, Princeton University, Princeton, NJ 08544;CSBMB, Princeton University, Princeton, NJ 08544",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "38c3365a4c",
        "title": "Simplified Rules and Theoretical Analysis for Information Bottleneck Optimization and PCA with Spiking Neurons",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/8fe0093bb30d6f8c31474bd0764e6ac0-Abstract.html",
        "author": "Lars Buesing; Wolfgang Maass",
        "abstract": "We show that under suitable assumptions (primarily linearization) a simple and perspicuous online learning rule for Information Bottleneck optimization with spiking neurons can be derived. This rule performs on common benchmark tasks as well as a rather complex rule that has previously been proposed \\cite{KlampflETAL:07b}. Furthermore, the transparency of this new learning rule makes a theoretical analysis of its convergence properties feasible. A variation of this learning rule (with sign changes) provides a theoretically founded method for performing Principal Component Analysis {(PCA)} with spiking neurons. By applying this rule to an ensemble of neurons, different principal components of the input can be extracted. In addition, it is possible to preferentially extract those principal components from incoming signals $X$ that are related or are not related to some additional target signal $Y_T$. In a biological interpretation, this target signal $Y_T$ (also called relevance variable) could represent proprioceptive feedback, input from other sensory modalities, or top-down signals.",
        "bibtex": "@inproceedings{NIPS2007_8fe0093b,\n author = {Buesing, Lars and Maass, Wolfgang},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Simplified Rules and Theoretical Analysis for Information Bottleneck Optimization and PCA with Spiking Neurons},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/8fe0093bb30d6f8c31474bd0764e6ac0-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/8fe0093bb30d6f8c31474bd0764e6ac0-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/8fe0093bb30d6f8c31474bd0764e6ac0-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/8fe0093bb30d6f8c31474bd0764e6ac0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 334452,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2738874293564739205&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "310c44dc81",
        "title": "Simulated Annealing: Rigorous finite-time guarantees for optimization on continuous domains",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/03afdbd66e7929b125f8597834fa83a4-Abstract.html",
        "author": "Andrea Lecchini-visintini; John Lygeros; Jan Maciejowski",
        "abstract": "Simulated annealing is a popular method for approaching the solution of a global optimization problem. Existing results on its performance apply to discrete com- binatorial optimization where the optimization variables can assume only a \ufb01nite set of possible values. We introduce a new general formulation of simulated an- nealing which allows one to guarantee \ufb01nite-time performance in the optimiza- tion of functions of continuous variables. The results hold universally for any optimization problem on a bounded domain and establish a connection between simulated annealing and up-to-date theory of convergence of Markov chain Monte Carlo methods on continuous domains. This work is inspired by the concept of \ufb01nite-time learning with known accuracy and con\ufb01dence developed in statistical learning theory.",
        "bibtex": "@inproceedings{NIPS2007_03afdbd6,\n author = {Lecchini-visintini, Andrea and Lygeros, John and Maciejowski, Jan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Simulated Annealing: Rigorous finite-time guarantees for optimization on continuous domains},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/03afdbd66e7929b125f8597834fa83a4-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/03afdbd66e7929b125f8597834fa83a4-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/03afdbd66e7929b125f8597834fa83a4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 75907,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2792705308735313681&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Engineering, University of Leicester, UK; Automatic Control Laboratory, ETH Zurich, Switzerland; Department of Engineering, University of Cambridge, UK",
        "aff_domain": "leicester.ac.uk;control.ee.ethz.ch;eng.cam.ac.uk",
        "email": "leicester.ac.uk;control.ee.ethz.ch;eng.cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Department of Engineering, University of Leicester, UK;ETH Zurich;University of Cambridge",
        "aff_unique_dep": ";Automatic Control Laboratory;Department of Engineering",
        "aff_unique_url": ";https://www.ethz.ch;https://www.cam.ac.uk",
        "aff_unique_abbr": ";ETHZ;Cambridge",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "1;2",
        "aff_country_unique": ";Switzerland;United Kingdom"
    },
    {
        "id": "e94e98ef86",
        "title": "SpAM: Sparse Additive Models",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/42e7aaa88b48137a16a1acd04ed91125-Abstract.html",
        "author": "Han Liu; Larry Wasserman; John D. Lafferty; Pradeep K. Ravikumar",
        "abstract": "We present a new class of models for high-dimensional nonparametric regression and classi\ufb01cation called sparse additive models (SpAM). Our methods combine ideas from sparse linear modeling and additive nonparametric regression. We de- rive a method for \ufb01tting the models that is effective even when the number of covariates is larger than the sample size. A statistical analysis of the properties of SpAM is given together with empirical results on synthetic and real data, show- ing that SpAM can be effective in \ufb01tting sparse nonparametric models in high dimensional data.",
        "bibtex": "@inproceedings{NIPS2007_42e7aaa8,\n author = {Liu, Han and Wasserman, Larry and Lafferty, John and Ravikumar, Pradeep},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {SpAM: Sparse Additive Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/42e7aaa88b48137a16a1acd04ed91125-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/42e7aaa88b48137a16a1acd04ed91125-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/42e7aaa88b48137a16a1acd04ed91125-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 133811,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5993057285102819716&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "1edf05ab59",
        "title": "Sparse Feature Learning for Deep Belief Networks",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/c60d060b946d6dd6145dcbad5c4ccf6f-Abstract.html",
        "author": "Marc'aurelio Ranzato; Y-lan Boureau; Yann L. Cun",
        "abstract": "Unsupervised learning algorithms aim to discover the structure hidden in the data, and to learn representations that are more suitable as input to a supervised machine than the raw input. Many unsupervised methods are based on reconstructing the input from the representation, while constraining the representation to have certain desirable properties (e.g. low dimension, sparsity, etc). Others are based on approximating density by stochastically reconstructing the input from the representation. We describe a novel and efficient algorithm to learn sparse representations, and compare it theoretically and experimentally with a similar machines trained probabilistically, namely a Restricted Boltzmann Machine. We propose a simple criterion to compare and select different unsupervised machines based on the trade-off between the reconstruction error and the information content of the representation. We demonstrate this method by extracting features from a dataset of handwritten numerals, and from a dataset of natural image patches. We show that by stacking multiple levels of such machines and by training sequentially, high-order dependencies between the input variables can be captured.",
        "bibtex": "@inproceedings{NIPS2007_c60d060b,\n author = {Ranzato, Marc\\textquotesingle aurelio and Boureau, Y-lan and Cun, Yann},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sparse Feature Learning for Deep Belief Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c60d060b946d6dd6145dcbad5c4ccf6f-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/c60d060b946d6dd6145dcbad5c4ccf6f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/c60d060b946d6dd6145dcbad5c4ccf6f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 142155,
        "gs_citation": 1203,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9883526391210411095&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 20,
        "aff": "Courant Institute of Mathematical Sciences, New York University; INRIA Rocquencourt + Courant Institute of Mathematical Sciences, New York University; Courant Institute of Mathematical Sciences, New York University",
        "aff_domain": "courant.nyu.edu;courant.nyu.edu;courant.nyu.edu",
        "email": "courant.nyu.edu;courant.nyu.edu;courant.nyu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+0;0",
        "aff_unique_norm": "New York University;INRIA Rocquencourt",
        "aff_unique_dep": "Courant Institute of Mathematical Sciences;",
        "aff_unique_url": "https://www.courant.nyu.edu;",
        "aff_unique_abbr": "NYU;",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "New York;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "7b9e7657f1",
        "title": "Sparse Overcomplete Latent Variable Decomposition of Counts Data",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/83fa5a432ae55c253d0e60dbfa716723-Abstract.html",
        "author": "Madhusudana Shashanka; Bhiksha Raj; Paris Smaragdis",
        "abstract": "An important problem in many fields is the analysis of counts data to extract meaningful latent components. Methods like Probabilistic Latent Semantic Analysis (PLSA) and Latent Dirichlet Allocation (LDA) have been proposed for this purpose. However, they are limited in the number of components they can extract and also do not have a provision to control the expressiveness\" of the extracted components. In this paper, we present a learning formulation to address these limitations by employing the notion of sparsity. We start with the PLSA framework and use an entropic prior in a maximum a posteriori formulation to enforce sparsity. We show that this allows the extraction of overcomplete sets of latent components which better characterize the data. We present experimental evidence of the utility of such representations.\"",
        "bibtex": "@inproceedings{NIPS2007_83fa5a43,\n author = {Shashanka, Madhusudana and Raj, Bhiksha and Smaragdis, Paris},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sparse Overcomplete Latent Variable Decomposition of Counts Data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/83fa5a432ae55c253d0e60dbfa716723-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/83fa5a432ae55c253d0e60dbfa716723-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/83fa5a432ae55c253d0e60dbfa716723-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/83fa5a432ae55c253d0e60dbfa716723-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 157350,
        "gs_citation": 93,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=935294783635493593&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "Mars, Incorporated, Hackettstown, NJ; Mitsubishi Electric Research Labs, Cambridge, MA; Adobe Systems, Newton, MA",
        "aff_domain": "cns.bu.edu;merl.com;adobe.com",
        "email": "cns.bu.edu;merl.com;adobe.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Mars, Incorporated, Hackettstown, NJ;Mitsubishi Electric Research Labs;Adobe Systems, Newton, MA",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";https://www.merl.com;",
        "aff_unique_abbr": ";MERL;",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "24d5bbdb43",
        "title": "Sparse deep belief net model for visual area V2",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/4daa3db355ef2b0e64b472968cb70f0d-Abstract.html",
        "author": "Honglak Lee; Chaitanya Ekanadham; Andrew Y. Ng",
        "abstract": "Motivated in part by the hierarchical organization of cortex, a number of algorithms have recently been proposed that try to learn hierarchical, or",
        "bibtex": "@inproceedings{NIPS2007_4daa3db3,\n author = {Lee, Honglak and Ekanadham, Chaitanya and Ng, Andrew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Sparse deep belief net model for visual area V2},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/4daa3db355ef2b0e64b472968cb70f0d-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/4daa3db355ef2b0e64b472968cb70f0d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/4daa3db355ef2b0e64b472968cb70f0d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 455806,
        "gs_citation": 1382,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6588035911916405091&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "Computer Science Department, Stanford University; Computer Science Department, Stanford University; Computer Science Department, Stanford University",
        "aff_domain": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "277aab979d",
        "title": "Spatial Latent Dirichlet Allocation",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/ec8956637a99787bd197eacd77acce5e-Abstract.html",
        "author": "Xiaogang Wang; Eric Grimson",
        "abstract": "In recent years, the language model Latent Dirichlet Allocation (LDA), which clusters co-occurring words into topics, has been widely appled in the computer vision field. However, many of these applications have difficulty with modeling the spatial and temporal structure among visual words, since LDA assumes that a document is a",
        "bibtex": "@inproceedings{NIPS2007_ec895663,\n author = {Wang, Xiaogang and Grimson, Eric},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Spatial Latent Dirichlet Allocation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/ec8956637a99787bd197eacd77acce5e-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/ec8956637a99787bd197eacd77acce5e-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/ec8956637a99787bd197eacd77acce5e-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/ec8956637a99787bd197eacd77acce5e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1115630,
        "gs_citation": 403,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8088587324603640073&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Computer Science and Arti\ufb01cial Intelligence Lab, Massachusetts Institute of Technology, Cambridge, MA, 02139, USA; Computer Science and Arti\ufb01cial Intelligence Lab, Massachusetts Institute of Technology, Cambridge, MA, 02139, USA",
        "aff_domain": "csail.mit.edu;csail.mit.edu",
        "email": "csail.mit.edu;csail.mit.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Computer Science and Arti\ufb01cial Intelligence Lab, Massachusetts Institute of Technology, Cambridge, MA, 02139, USA",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "63b7f3f70e",
        "title": "Stability Bounds for Non-i.i.d. Processes",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/85d8ce590ad8981ca2c8286f79f59954-Abstract.html",
        "author": "Mehryar Mohri; Afshin Rostamizadeh",
        "abstract": "The notion of algorithmic stability has been used effectively in the past to derive tight generalization bounds. A key advantage of these bounds is that they are de- signed for specific learning algorithms, exploiting their particular properties. But, as in much of learning theory, existing stability analyses and bounds apply only in the scenario where the samples are independently and identically distributed (i.i.d.). In many machine learning applications, however, this assumption does not hold. The observations received by the learning algorithm often have some inherent temporal dependence, which is clear in system diagnosis or time series prediction problems. This paper studies the scenario where the observations are drawn from a station- ary beta-mixing sequence, which implies a dependence between observations that weaken over time. It proves novel stability-based generalization bounds that hold even with this more general setting. These bounds strictly generalize the bounds given in the i.i.d. case. We also illustrate their application in the case of several general classes of learning algorithms, including Support Vector Regression and Kernel Ridge Regression.",
        "bibtex": "@inproceedings{NIPS2007_85d8ce59,\n author = {Mohri, Mehryar and Rostamizadeh, Afshin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Stability Bounds for Non-i.i.d. Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/85d8ce590ad8981ca2c8286f79f59954-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/85d8ce590ad8981ca2c8286f79f59954-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/85d8ce590ad8981ca2c8286f79f59954-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 109362,
        "gs_citation": 59,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3069008664580736878&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Courant Institute of Mathematical Sciences and Google Research; Department of Computer Science, Courant Institute of Mathematical Sciences",
        "aff_domain": "cims.nyu.edu;cs.nyu.edu",
        "email": "cims.nyu.edu;cs.nyu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Courant Institute of Mathematical Sciences and Google Research;Department of Computer Science, Courant Institute of Mathematical Sciences",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "8e58d74988",
        "title": "Stable Dual Dynamic Programming",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/069d3bb002acd8d7dd095917f9efe4cb-Abstract.html",
        "author": "Tao Wang; Michael Bowling; Dale Schuurmans; Daniel J. Lizotte",
        "abstract": "Recently, we have introduced a novel approach to dynamic programming and re- inforcement learning that is based on maintaining explicit representations of sta- tionary distributions instead of value functions. In this paper, we investigate the convergence properties of these dual algorithms both theoretically and empirically, and show how they can be scaled up by incorporating function approximation.",
        "bibtex": "@inproceedings{NIPS2007_069d3bb0,\n author = {Wang, Tao and Bowling, Michael and Schuurmans, Dale and Lizotte, Daniel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Stable Dual Dynamic Programming},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/069d3bb002acd8d7dd095917f9efe4cb-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/069d3bb002acd8d7dd095917f9efe4cb-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/069d3bb002acd8d7dd095917f9efe4cb-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 126828,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12254436017941302975&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computing Science, University of Alberta + Computer Sciences Laboratory, Australian National University; Department of Computing Science, University of Alberta; Department of Computing Science, University of Alberta; Department of Computing Science, University of Alberta",
        "aff_domain": "anu.edu.au;cs.ualberta.ca;cs.ualberta.ca;cs.ualberta.ca",
        "email": "anu.edu.au;cs.ualberta.ca;cs.ualberta.ca;cs.ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;0;0",
        "aff_unique_norm": "University of Alberta;Computer Sciences Laboratory, Australian National University",
        "aff_unique_dep": "Department of Computing Science;",
        "aff_unique_url": "https://www.ualberta.ca;",
        "aff_unique_abbr": "UAlberta;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Canada;"
    },
    {
        "id": "a2646bc888",
        "title": "Statistical Analysis of Semi-Supervised Regression",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/53c3bce66e43be4f209556518c2fcb54-Abstract.html",
        "author": "Larry Wasserman; John D. Lafferty",
        "abstract": "Semi-supervised methods use unlabeled data in addition to labeled data to con- struct predictors. While existing semi-supervised methods have shown some promising empirical performance, their development has been based largely based on heuristics. In this paper we study semi-supervised learning from the viewpoint of minimax theory. Our \ufb01rst result shows that some common methods based on regularization using graph Laplacians do not lead to faster minimax rates of con- vergence. Thus, the estimators that use the unlabeled data do not have smaller risk than the estimators that use only labeled data. We then develop several new approaches that provably lead to improved performance. The statistical tools of minimax analysis are thus used to offer some new perspective on the problem of semi-supervised learning.",
        "bibtex": "@inproceedings{NIPS2007_53c3bce6,\n author = {Wasserman, Larry and Lafferty, John},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Statistical Analysis of Semi-Supervised Regression},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/53c3bce66e43be4f209556518c2fcb54-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/53c3bce66e43be4f209556518c2fcb54-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/53c3bce66e43be4f209556518c2fcb54-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 67019,
        "gs_citation": 211,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5505986434526061205&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Computer Science Department, Carnegie Mellon University; Department of Statistics, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;stat.cmu.edu",
        "email": "cs.cmu.edu;stat.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Pittsburgh;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3573c27e55",
        "title": "Structured Learning with Approximate Inference",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/32b30a250abd6331e03a2a1f16466346-Abstract.html",
        "author": "Alex Kulesza; Fernando Pereira",
        "abstract": "In many structured prediction problems, the highest-scoring labeling is hard to compute exactly, leading to the use of approximate inference methods. However, when inference is used in a learning algorithm, a good approximation of the score may not be suf\ufb01cient. We show in particular that learning can fail even with an approximate inference method with rigorous approximation guarantees. There are two reasons for this. First, approximate methods can effectively reduce the expres- sivity of an underlying model by making it impossible to choose parameters that reliably give good predictions. Second, approximations can respond to parameter changes in such a way that standard learning algorithms are misled. In contrast, we give two positive results in the form of learning bounds for the use of LP-relaxed inference in structured perceptron and empirical risk minimization settings. We argue that without understanding combinations of inference and learning, such as these, that are appropriately compatible, learning performance under approximate inference cannot be guaranteed.",
        "bibtex": "@inproceedings{NIPS2007_32b30a25,\n author = {Kulesza, Alex and Pereira, Fernando},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Structured Learning with Approximate Inference},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/32b30a250abd6331e03a2a1f16466346-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/32b30a250abd6331e03a2a1f16466346-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/32b30a250abd6331e03a2a1f16466346-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/32b30a250abd6331e03a2a1f16466346-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 214058,
        "gs_citation": 183,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1820595124422371827&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer and Information Science, University of Pennsylvania; Department of Computer and Information Science, University of Pennsylvania",
        "aff_domain": "cis.upenn.edu;cis.upenn.edu",
        "email": "cis.upenn.edu;cis.upenn.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Pennsylvania",
        "aff_unique_dep": "Department of Computer and Information Science",
        "aff_unique_url": "https://www.upenn.edu",
        "aff_unique_abbr": "UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c84f4a7478",
        "title": "Subspace-Based Face Recognition in Analog VLSI",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/846c260d715e5b854ffad5f70a516c88-Abstract.html",
        "author": "Gonzalo Carvajal; Waldo Valenzuela; Miguel Figueroa",
        "abstract": "We describe an analog-VLSI neural network for face recognition based on subspace methods. The system uses a dimensionality-reduction network whose coe\ufb03cients can be either programmed or learned on-chip to per- form PCA, or programmed to perform LDA. A second network with user- programmed coe\ufb03cients performs classi\ufb01cation with Manhattan distances. The system uses on-chip compensation techniques to reduce the e\ufb00ects of device mismatch. Using the ORL database with 12x12-pixel images, our circuit achieves up to 85% classi\ufb01cation performance (98% of an equivalent software implementation).",
        "bibtex": "@inproceedings{NIPS2007_846c260d,\n author = {Carvajal, Gonzalo and Valenzuela, Waldo and Figueroa, Miguel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Subspace-Based Face Recognition in Analog VLSI},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/846c260d715e5b854ffad5f70a516c88-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/846c260d715e5b854ffad5f70a516c88-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/846c260d715e5b854ffad5f70a516c88-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/846c260d715e5b854ffad5f70a516c88-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 284681,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13305292099010606670&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "cbb03265f4",
        "title": "Supervised Topic Models",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/d56b9fc4b0f1be8871f5e1c40c0067e7-Abstract.html",
        "author": "Jon D. Mcauliffe; David M. Blei",
        "abstract": "We introduce supervised latent Dirichlet allocation (sLDA), a statistical model of labelled documents. The model accommodates a variety of response types. We derive a maximum-likelihood procedure for parameter estimation, which relies on variational approximations to handle intractable posterior expectations. Prediction problems motivate this research: we use the fitted model to predict response values for new documents. We test sLDA on two real-world problems: movie ratings predicted from reviews, and web page popularity predicted from text descriptions. We illustrate the benefits of sLDA versus modern regularized regression, as well as versus an unsupervised LDA analysis followed by a separate regression.",
        "bibtex": "@inproceedings{NIPS2007_d56b9fc4,\n author = {Mcauliffe, Jon and Blei, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Supervised Topic Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d56b9fc4b0f1be8871f5e1c40c0067e7-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/d56b9fc4b0f1be8871f5e1c40c0067e7-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/d56b9fc4b0f1be8871f5e1c40c0067e7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 323560,
        "gs_citation": 2369,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16996117785355797305&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 19,
        "aff": "Department of Computer Science, Princeton University, Princeton, NJ; Department of Statistics, University of Pennsylvania, Wharton School, Philadelphia, PA",
        "aff_domain": "cs.princeton.edu;wharton.upenn.edu",
        "email": "cs.princeton.edu;wharton.upenn.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Princeton University;Department of Statistics, University of Pennsylvania, Wharton School, Philadelphia, PA",
        "aff_unique_dep": "Department of Computer Science;",
        "aff_unique_url": "https://www.princeton.edu;",
        "aff_unique_abbr": "Princeton;",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Princeton;",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "d1b41e9a91",
        "title": "Support Vector Machine Classification with Indefinite Kernels",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/c0c7c76d30bd3dcaefc96f40275bdc0a-Abstract.html",
        "author": "Ronny Luss; Alexandre D'aspremont",
        "abstract": "In this paper, we propose a method for support vector machine classification using indefinite kernels. Instead of directly minimizing or stabilizing a nonconvex loss function, our method simultaneously finds the support vectors and a proxy kernel matrix used in computing the loss. This can be interpreted as a robust classification problem where the indefinite kernel matrix is treated as a noisy observation of the true positive semidefinite kernel. Our formulation keeps the problem convex and relatively large problems can be solved efficiently using the analytic center cutting plane method. We compare the performance of our technique with other methods on several data sets.",
        "bibtex": "@inproceedings{NIPS2007_c0c7c76d,\n author = {Luss, Ronny and D\\textquotesingle aspremont, Alexandre},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Support Vector Machine Classification with Indefinite Kernels},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/c0c7c76d30bd3dcaefc96f40275bdc0a-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/c0c7c76d30bd3dcaefc96f40275bdc0a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/c0c7c76d30bd3dcaefc96f40275bdc0a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 81670,
        "gs_citation": 180,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17553010807057881260&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 19,
        "aff": "ORFE, Princeton University; ORFE, Princeton University",
        "aff_domain": "princeton.edu;princeton.edu",
        "email": "princeton.edu;princeton.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Princeton University",
        "aff_unique_dep": "Operations Research and Financial Engineering",
        "aff_unique_url": "https://www.princeton.edu",
        "aff_unique_abbr": "Princeton",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Princeton",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "23bf38b16b",
        "title": "Temporal Difference Updating without a Learning Rate",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/3a30be93eb45566a90f4e95ee72a089a-Abstract.html",
        "author": "Marcus Hutter; Shane Legg",
        "abstract": "We derive an equation for temporal difference learning from statistical principles. Speci\ufb01cally, we start with the variational principle and then bootstrap to produce an updating rule for discounted state value estimates. The resulting equation is similar to the standard equation for temporal difference learning with eligibil- ity traces, so called TD(\u03bb), however it lacks the parameter \u03b1 that speci\ufb01es the learning rate. In the place of this free parameter there is now an equation for the learning rate that is speci\ufb01c to each state transition. We experimentally test this new learning rule against TD(\u03bb) and \ufb01nd that it offers superior performance in various settings. Finally, we make some preliminary investigations into how to extend our new temporal difference algorithm to reinforcement learning. To do this we combine our update equation with both Watkins\u2019 Q(\u03bb) and Sarsa(\u03bb) and \ufb01nd that it again offers superior performance without a learning rate parameter.",
        "bibtex": "@inproceedings{NIPS2007_3a30be93,\n author = {Hutter, Marcus and Legg, Shane},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Temporal Difference Updating without a Learning Rate},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/3a30be93eb45566a90f4e95ee72a089a-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/3a30be93eb45566a90f4e95ee72a089a-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/3a30be93eb45566a90f4e95ee72a089a-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/3a30be93eb45566a90f4e95ee72a089a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 188119,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7360552202231805828&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "RSISE@ANU and SML@NICTA, Canberra, ACT, 0200, Australia; IDSIA, Galleria 2, Manno-Lugano CH-6928, Switzerland",
        "aff_domain": "hutter1.net;vetta.org",
        "email": "hutter1.net;vetta.org",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "RSISE@ANU and SML@NICTA, Canberra, ACT, 0200, Australia;IDSIA, Galleria 2, Manno-Lugano CH-6928, Switzerland",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "5774c3c458",
        "title": "Testing for Homogeneity with Kernel Fisher Discriminant Analysis",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/4ca82782c5372a547c104929f03fe7a9-Abstract.html",
        "author": "Moulines Eric; Francis R. Bach; Za\u00efd Harchaoui",
        "abstract": "We propose to test for the homogeneity of two samples by using Kernel Fisher discriminant Analysis. This provides us with a consistent nonparametric test statistic, for which we derive the asymptotic distribution under the null hypothesis. We give experimental evidence of the relevance of our method on both artificial and real datasets.",
        "bibtex": "@inproceedings{NIPS2007_4ca82782,\n author = {Eric, Moulines and Bach, Francis and Harchaoui, Za\\\"{\\i}d},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Testing for Homogeneity with Kernel Fisher Discriminant Analysis},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/4ca82782c5372a547c104929f03fe7a9-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/4ca82782c5372a547c104929f03fe7a9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/4ca82782c5372a547c104929f03fe7a9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 89033,
        "gs_citation": 158,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2399629025781618205&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "LTCI, TELECOM ParisTech and CNRS; WillowProject, INRIA-ENS + LTCI, TELECOM ParisTech and CNRS; LTCI, TELECOM ParisTech and CNRS",
        "aff_domain": "enst.fr;mines.org;enst.fr",
        "email": "enst.fr;mines.org;enst.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+0;0",
        "aff_unique_norm": "LTCI, TELECOM ParisTech and CNRS;WillowProject, INRIA-ENS",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "fc5c9cfdc7",
        "title": "The Distribution Family of Similarity Distances",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/6081594975a764c8e3a691fa2b3a321d-Abstract.html",
        "author": "Gertjan Burghouts; Arnold Smeulders; Jan-mark Geusebroek",
        "abstract": "Assessing similarity between features is a key step in object recognition and scene categorization tasks. We argue that knowledge on the distribution of distances generated by similarity functions is crucial in deciding whether features are similar or not. Intuitively one would expect that similarities between features could arise from any distribution. In this paper, we will derive the contrary, and report the theoretical result that $L_p$-norms --a class of commonly applied distance metrics-- from one feature vector to other vectors are Weibull-distributed if the feature values are correlated and non-identically distributed. Besides these assumptions being realistic for images, we experimentally show them to hold for various popular feature extraction algorithms, for a diverse range of images. This fundamental insight opens new directions in the assessment of feature similarity, with projected improvements in object and scene recognition algorithms.\r\n\r\nErratum: The authors of paper have declared that they have become convinced that the reasoning in the reference is too simple as a proof of their claims. As a consequence, they withdraw their theorems.",
        "bibtex": "@inproceedings{NIPS2007_60815949,\n author = {Burghouts, Gertjan and Smeulders, Arnold and Geusebroek, Jan-mark},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The Distribution Family of Similarity Distances},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/6081594975a764c8e3a691fa2b3a321d-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/6081594975a764c8e3a691fa2b3a321d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/6081594975a764c8e3a691fa2b3a321d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 149267,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16603304893106232813&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 11,
        "aff": "Intelligent Systems Lab Amsterdam+Informatics Institute+University of Amsterdam; Intelligent Systems Lab Amsterdam+Informatics Institute+University of Amsterdam; University of Amsterdam",
        "aff_domain": "tno.nl; ;science.uva.nl",
        "email": "tno.nl; ;science.uva.nl",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1+2;0+1+2;2",
        "aff_unique_norm": "Intelligent Systems Lab Amsterdam;Informatics Institute;University of Amsterdam",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;https://www.uva.nl",
        "aff_unique_abbr": ";;UvA",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1;1",
        "aff_country_unique": ";Netherlands"
    },
    {
        "id": "81b1dbde00",
        "title": "The Epoch-Greedy Algorithm for Multi-armed Bandits with Side Information",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/4b04a686b0ad13dce35fa99fa4161c65-Abstract.html",
        "author": "John Langford; Tong Zhang",
        "abstract": "We present Epoch-Greedy, an algorithm for multi-armed bandits with observable side information. Epoch-Greedy has the following properties: No knowledge of a time horizon $T$ is necessary. The regret incurred by Epoch-Greedy is controlled by a sample complexity bound for a hypothesis class. The regret scales as $O(T^{2/3} S^{1/3})$ or better (sometimes, much better). Here $S$ is the complexity term in a sample complexity bound for standard supervised learning.",
        "bibtex": "@inproceedings{NIPS2007_4b04a686,\n author = {Langford, John and Zhang, Tong},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The Epoch-Greedy Algorithm for Multi-armed Bandits with Side Information},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/4b04a686b0ad13dce35fa99fa4161c65-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/4b04a686b0ad13dce35fa99fa4161c65-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/4b04a686b0ad13dce35fa99fa4161c65-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 167788,
        "gs_citation": 795,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18240460027483861404&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Yahoo! Research; Department of Statistics, Rutgers University",
        "aff_domain": "yahoo-inc.com;rci.rutgers.edu",
        "email": "yahoo-inc.com;rci.rutgers.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Yahoo!;Rutgers University",
        "aff_unique_dep": "Yahoo! Research;Department of Statistics",
        "aff_unique_url": "https://research.yahoo.com;https://www.rutgers.edu",
        "aff_unique_abbr": "Yahoo!;Rutgers",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "213a453df7",
        "title": "The Generalized FITC Approximation",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/94c7bb58efc3b337800875b5d382a072-Abstract.html",
        "author": "Andrew Naish-guzman; Sean Holden",
        "abstract": "We present an ef\ufb01cient generalization of the sparse pseudo-input Gaussian pro- cess (SPGP) model developed by Snelson and Ghahramani [1], applying it to binary classi\ufb01cation problems. By taking advantage of the SPGP prior covari- ance structure, we derive a numerically stable algorithm with O(N M 2) training complexity\u2014asymptotically the same as related sparse methods such as the in- formative vector machine [2], but which more faithfully represents the posterior. We present experimental results for several benchmark problems showing that in many cases this allows an exceptional degree of sparsity without compromis- ing accuracy. Following [1], we locate pseudo-inputs by gradient ascent on the marginal likelihood, but exhibit occasions when this is likely to fail, for which we suggest alternative solutions.",
        "bibtex": "@inproceedings{NIPS2007_94c7bb58,\n author = {Naish-guzman, Andrew and Holden, Sean},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The Generalized FITC Approximation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/94c7bb58efc3b337800875b5d382a072-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/94c7bb58efc3b337800875b5d382a072-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/94c7bb58efc3b337800875b5d382a072-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/94c7bb58efc3b337800875b5d382a072-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 119128,
        "gs_citation": 127,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17500041775901463950&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Computer Laboratory, University of Cambridge; Computer Laboratory, University of Cambridge",
        "aff_domain": "cl.cam.ac.uk;cl.cam.ac.uk",
        "email": "cl.cam.ac.uk;cl.cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Cambridge",
        "aff_unique_dep": "Computer Laboratory",
        "aff_unique_url": "https://www.cam.ac.uk",
        "aff_unique_abbr": "Cambridge",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "7e7a2e929d",
        "title": "The Infinite Gamma-Poisson Feature Model",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/6da37dd3139aa4d9aa55b8d237ec5d4a-Abstract.html",
        "author": "Michalis K. Titsias",
        "abstract": "We address the problem of factorial learning which associates a set of latent causes or features with the observed data. Factorial models usually assume that each feature has a single occurrence in a given data point. However, there are data such as images where latent features have multiple occurrences, e.g. a visual object class can have multiple instances shown in the same image. To deal with such cases, we present a probability model over non-negative integer valued matrices with possibly unbounded number of columns. This model can play the role of the prior in an nonparametric Bayesian learning scenario where both the latent features and the number of their occurrences are unknown. We use this prior together with a likelihood model for unsupervised learning from images using a Markov Chain Monte Carlo inference algorithm.",
        "bibtex": "@inproceedings{NIPS2007_6da37dd3,\n author = {Titsias, Michalis},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The Infinite Gamma-Poisson Feature Model},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/6da37dd3139aa4d9aa55b8d237ec5d4a-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/6da37dd3139aa4d9aa55b8d237ec5d4a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/6da37dd3139aa4d9aa55b8d237ec5d4a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 176255,
        "gs_citation": 117,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13165630499421562304&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "School of Computer Science, University of Manchester, UK",
        "aff_domain": "cs.man.ac.uk",
        "email": "cs.man.ac.uk",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Manchester",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.manchester.ac.uk",
        "aff_unique_abbr": "UoM",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Manchester",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2483bc361a",
        "title": "The Infinite Markov Model",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/b0b183c207f46f0cca7dc63b2604f5cc-Abstract.html",
        "author": "Daichi Mochihashi; Eiichiro Sumita",
        "abstract": "We present a nonparametric Bayesian method of estimating variable order Markov processes up to a theoretically in\ufb01nite order. By extending a stick-breaking prior, which is usually de\ufb01ned on a unit interval, \u201cvertically\u201d to the trees of in\ufb01nite depth associated with a hierarchical Chinese restaurant process, our model directly infers the hidden orders of Markov dependencies from which each symbol originated. Experiments on character and word sequences in natural language showed that the model has a comparative performance with an exponentially large full-order model, while computationally much ef\ufb01cient in both time and space. We expect that this basic model will also extend to the variable order hierarchical clustering of general data.",
        "bibtex": "@inproceedings{NIPS2007_b0b183c2,\n author = {Mochihashi, Daichi and Sumita, Eiichiro},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The Infinite Markov Model},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/b0b183c207f46f0cca7dc63b2604f5cc-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/b0b183c207f46f0cca7dc63b2604f5cc-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/b0b183c207f46f0cca7dc63b2604f5cc-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 95849,
        "gs_citation": 70,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6686073209523290530&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": "NTT Communication Science Laboratories; ATR / NICT + NTT Communication Science Laboratories",
        "aff_domain": "cslab.kecl.ntt.co.jp;atr.jp",
        "email": "cslab.kecl.ntt.co.jp;atr.jp",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+0",
        "aff_unique_norm": "NTT Communication Science Laboratories;ATR / NICT",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ntt-csl.com;",
        "aff_unique_abbr": "NTT CSL;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan;"
    },
    {
        "id": "13140dafb4",
        "title": "The Noisy-Logical Distribution and its Application to Causal Inference",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/2291d2ec3b3048d1a6f86c2c4591b7e0-Abstract.html",
        "author": "Alan L. Yuille; Hongjing Lu",
        "abstract": "We describe a novel noisy-logical distribution for representing the distribution of a binary output variable conditioned on multiple binary input variables. The distribution is represented in terms of noisy-or's and noisy-and-not's of causal features which are conjunctions of the binary inputs. The standard noisy-or and noisy-and-not models, used in causal reasoning and artificial intelligence, are special cases of the noisy-logical distribution. We prove that the noisy-logical distribution is complete in the sense that it can represent all conditional distributions provided a sufficient number of causal factors are used. We illustrate the noisy-logical distribution by showing that it can account for new experimental findings on how humans perform causal reasoning in more complex contexts. Finally, we speculate on the use of the noisy-logical distribution for causal reasoning and artificial intelligence.",
        "bibtex": "@inproceedings{NIPS2007_2291d2ec,\n author = {Yuille, Alan L and Lu, Hongjing},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The Noisy-Logical Distribution and its Application to Causal Inference},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/2291d2ec3b3048d1a6f86c2c4591b7e0-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/2291d2ec3b3048d1a6f86c2c4591b7e0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/2291d2ec3b3048d1a6f86c2c4591b7e0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 274480,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3588278844629998546&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Department of Statistics, University of California at Los Angeles; Department of Psychology, University of California at Los Angeles",
        "aff_domain": "stat.ucla.edu;ucla.edu",
        "email": "stat.ucla.edu;ucla.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Department of Statistics, University of California at Los Angeles;Department of Psychology, University of California at Los Angeles",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "068cc08a8c",
        "title": "The Price of Bandit Information for Online Optimization",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/bf62768ca46b6c3b5bea9515d1a1fc45-Abstract.html",
        "author": "Varsha Dani; Sham M. Kakade; Thomas P. Hayes",
        "abstract": "In the online linear optimization problem, a learner must choose, in each round, a decision from a set D \u2282 Rn in order to minimize an (unknown and chang- ing) linear cost function. We present sharp rates of convergence (with respect to additive regret) for both the full information setting (where the cost function is revealed at the end of each round) and the bandit setting (where only the scalar cost incurred is revealed). In particular, this paper is concerned with the price of bandit information, by which we mean the ratio of the best achievable regret \u221a in the bandit setting to that in the full-information setting. For the full informa- tion case, the upper bound on the regret is O\u2217( nT ), where n is the ambient \u221a dimension and T is the time horizon. For the bandit case, we present an algorithm which achieves O\u2217(n3/2 T ) regret \u2014 all previous (nontrivial) bounds here were O(poly(n)T 2/3) or worse. It is striking that the convergence rate for the bandit setting is only a factor of n worse than in the full information case \u2014 in stark \u221a contrast to the K-arm bandit setting, where the gap in the dependence on K is T log K). We also present lower bounds showing that exponential ( this gap is at least n, which we conjecture to be the correct order. The bandit algorithm we present can be implemented ef\ufb01ciently in special cases of particular interest, such as path planning and Markov Decision Problems.",
        "bibtex": "@inproceedings{NIPS2007_bf62768c,\n author = {Dani, Varsha and Kakade, Sham M and Hayes, Thomas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The Price of Bandit Information for Online Optimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/bf62768ca46b6c3b5bea9515d1a1fc45-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/bf62768ca46b6c3b5bea9515d1a1fc45-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/bf62768ca46b6c3b5bea9515d1a1fc45-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 172715,
        "gs_citation": 255,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=265980254355404014&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Department of Computer Science, University of Chicago; Toyota Technological Institute; Toyota Technological Institute",
        "aff_domain": "cs.uchicago.edu;tti-c.org;tti-c.org",
        "email": "cs.uchicago.edu;tti-c.org;tti-c.org",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of Chicago;Toyota Technological Institute",
        "aff_unique_dep": "Department of Computer Science;",
        "aff_unique_url": "https://www.uchicago.edu;https://www.tti.ac.jp",
        "aff_unique_abbr": "UChicago;TTI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "United States;Japan"
    },
    {
        "id": "76e397cb6e",
        "title": "The Tradeoffs of Large Scale Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/0d3180d672e08b4c5312dcdafdf6ef36-Abstract.html",
        "author": "L\u00e9on Bottou; Olivier Bousquet",
        "abstract": "This contribution develops a theoretical framework that takes into account the effect of approximate optimization on learning algorithms. The analysis shows distinct tradeoffs for the case of small-scale and large-scale learning problems. Small-scale learning problems are subject to the usual approximation--estimation tradeoff. Large-scale learning problems are subject to a qualitatively different tradeoff involving the computational complexity of the underlying optimization algorithms in non-trivial ways.",
        "bibtex": "@inproceedings{NIPS2007_0d3180d6,\n author = {Bottou, L\\'{e}on and Bousquet, Olivier},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The Tradeoffs of Large Scale Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/0d3180d672e08b4c5312dcdafdf6ef36-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/0d3180d672e08b4c5312dcdafdf6ef36-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/0d3180d672e08b4c5312dcdafdf6ef36-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 99403,
        "gs_citation": 2354,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10944503106683298807&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 28,
        "aff": "NEC laboratories of America, Princeton, NJ 08540, USA; Google Z \u00a8urich, 8002 Zurich, Switzerland",
        "aff_domain": "bottou.org;m4x.org",
        "email": "bottou.org;m4x.org",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "NEC laboratories of America, Princeton, NJ 08540, USA;Google Z \u00a8urich, 8002 Zurich, Switzerland",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "246a9a9033",
        "title": "The Value of Labeled and Unlabeled Examples when the Model is Imperfect",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/aa68c75c4a77c87f97fb686b2f068676-Abstract.html",
        "author": "Kaushik Sinha; Mikhail Belkin",
        "abstract": "Semi-supervised learning, i.e. learning from both labeled and unlabeled data has received signi(cid:2)cant attention in the machine learning literature in recent years. Still our understanding of the theoretical foundations of the usefulness of unla- beled data remains somewhat limited. The simplest and the best understood sit- uation is when the data is described by an identi(cid:2)able mixture model, and where each class comes from a pure component. This natural setup and its implications ware analyzed in [11, 5]. One important result was that in certain regimes, labeled data becomes exponentially more valuable than unlabeled data. However, in most realistic situations, one would not expect that the data comes from a parametric mixture distribution with identi(cid:2)able components. There have been recent efforts to analyze the non-parametric situation, for example, (cid:147)cluster(cid:148) and (cid:147)manifold(cid:148) assumptions have been suggested as a basis for analysis. Still, a satisfactory and fairly complete theoretical understanding of the nonparametric problem, similar to that in [11, 5] has not yet been developed. In this paper we investigate an intermediate situation, when the data comes from a probability distribution, which can be modeled, but not perfectly, by an identi(cid:2)able mixture distribution. This seems applicable to many situation, when, for example, a mixture of Gaussians is used to model the data. the contribution of this paper is an analysis of the role of labeled and unlabeled data depending on the amount of imperfection in the model.",
        "bibtex": "@inproceedings{NIPS2007_aa68c75c,\n author = {Sinha, Kaushik and Belkin, Mikhail},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The Value of Labeled and Unlabeled Examples when the Model is Imperfect},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/aa68c75c4a77c87f97fb686b2f068676-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/aa68c75c4a77c87f97fb686b2f068676-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/aa68c75c4a77c87f97fb686b2f068676-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 128864,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14545069268300733704&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Dept. of Computer Science and Engineering, Ohio State University; Dept. of Computer Science and Engineering, Ohio State University",
        "aff_domain": "cse.ohio-state.edu;cse.ohio-state.edu",
        "email": "cse.ohio-state.edu;cse.ohio-state.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Ohio State University",
        "aff_unique_dep": "Dept. of Computer Science and Engineering",
        "aff_unique_url": "https://www.osu.edu",
        "aff_unique_abbr": "OSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d6355bb0a4",
        "title": "The discriminant center-surround hypothesis for bottom-up saliency",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/51ef186e18dc00c2d31982567235c559-Abstract.html",
        "author": "Dashan Gao; Vijay Mahadevan; Nuno Vasconcelos",
        "abstract": "The classical hypothesis, that bottom-up saliency is a center-surround process, is combined with a more recent hypothesis that all saliency decisions are optimal in a decision-theoretic sense. The combined hypothesis is denoted as discriminant center-surround saliency, and the corresponding optimal saliency architecture is derived. This architecture equates the saliency of each image location to the discriminant power of a set of features with respect to the classification problem that opposes stimuli at center and surround, at that location. It is shown that the resulting saliency detector makes accurate quantitative predictions for various aspects of the psychophysics of human saliency, including non-linear properties beyond the reach of previous saliency models. Furthermore, it is shown that discriminant center-surround saliency can be easily generalized to various stimulus modalities (such as color, orientation and motion), and provides optimal solutions for many other saliency problems of interest for computer vision. Optimal solutions, under this hypothesis, are derived for a number of the former (including static natural images, dense motion fields, and even dynamic textures), and applied to a number of the latter (the prediction of human eye fixations, motion-based saliency in the presence of ego-motion, and motion-based saliency in the presence of highly dynamic backgrounds). In result, discriminant saliency is shown to predict eye fixations better than previous models, and produce background subtraction algorithms that outperform the state-of-the-art in computer vision.",
        "bibtex": "@inproceedings{NIPS2007_51ef186e,\n author = {Gao, Dashan and Mahadevan, Vijay and Vasconcelos, Nuno},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The discriminant center-surround hypothesis for bottom-up saliency},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/51ef186e18dc00c2d31982567235c559-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/51ef186e18dc00c2d31982567235c559-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/51ef186e18dc00c2d31982567235c559-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/51ef186e18dc00c2d31982567235c559-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 300605,
        "gs_citation": 580,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16501329982142717097&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 26,
        "aff": "Department of Electrical and Computer Engineering, University of California, San Diego; Department of Electrical and Computer Engineering, University of California, San Diego; Department of Electrical and Computer Engineering, University of California, San Diego",
        "aff_domain": "ucsd.edu;ucsd.edu;ucsd.edu",
        "email": "ucsd.edu;ucsd.edu;ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e708af5376",
        "title": "The rat as particle filter",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/fba9d88164f3e2d9109ee770223212a0-Abstract.html",
        "author": "Aaron C. Courville; Nathaniel D. Daw",
        "abstract": "Although theorists have interpreted classical conditioning as a laboratory model of Bayesian belief updating, a recent reanalysis showed that the key features that theoretical models capture about learning are artifacts of averaging over subjects. Rather than learning smoothly to asymptote (re\ufb02ecting, according to Bayesian models, the gradual tradeoff from prior to posterior as data accumulate), subjects learn suddenly and their predictions \ufb02uctuate perpetually. We suggest that abrupt and unstable learning can be modeled by assuming subjects are conducting in- ference using sequential Monte Carlo sampling with a small number of samples \u2014 one, in our simulations. Ensemble behavior resembles exact Bayesian models since, as in particle \ufb01lters, it averages over many samples. Further, the model is capable of exhibiting sophisticated behaviors like retrospective revaluation at the ensemble level, even given minimally sophisticated individuals that do not track uncertainty in their beliefs over trials.",
        "bibtex": "@inproceedings{NIPS2007_fba9d881,\n author = {Courville, Aaron C and Daw, Nathaniel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {The rat as particle filter},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/fba9d88164f3e2d9109ee770223212a0-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/fba9d88164f3e2d9109ee770223212a0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/fba9d88164f3e2d9109ee770223212a0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 234259,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4690224643971667508&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Center for Neural Science and Department of Psychology, New York University; D\u00e9partement d\u2019Informatique et de recherche op\u00e9rationnelle, Universit\u00e9 de Montr\u00e9al",
        "aff_domain": "cns.nyu.edu;gmail.com",
        "email": "cns.nyu.edu;gmail.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Center for Neural Science and Department of Psychology, New York University;D\u00e9partement d\u2019Informatique et de recherche op\u00e9rationnelle, Universit\u00e9 de Montr\u00e9al",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "43de15cd9d",
        "title": "Theoretical Analysis of Heuristic Search Methods for Online POMDPs",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/138bb0696595b338afbab333c555292a-Abstract.html",
        "author": "Stephane Ross; Joelle Pineau; Brahim Chaib-draa",
        "abstract": "Planning in partially observable environments remains a challenging problem, despite significant recent advances in offline approximation techniques. A few online methods have also been proposed recently, and proven to be remarkably scalable, but without the theoretical guarantees of their offline counterparts. Thus it seems natural to try to unify offline and online techniques, preserving the theoretical properties of the former, and exploiting the scalability of the latter. In this paper, we provide theoretical guarantees on an anytime algorithm for POMDPs which aims to reduce the error made by approximate offline value iteration algorithms through the use of an efficient online searching procedure. The algorithm uses search heuristics based on an error analysis of lookahead search, to guide the online search towards reachable beliefs with the most potential to reduce error. We provide a general theorem showing that these search heuristics are admissible, and lead to complete and epsilon-optimal algorithms. This is, to the best of our knowledge, the strongest theoretical result available for online POMDP solution methods. We also provide empirical evidence showing that our approach is also practical, and can find (provably) near-optimal solutions in reasonable time.",
        "bibtex": "@inproceedings{NIPS2007_138bb069,\n author = {Ross, Stephane and Pineau, Joelle and Chaib-draa, Brahim},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Theoretical Analysis of Heuristic Search Methods for Online POMDPs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/138bb0696595b338afbab333c555292a-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/138bb0696595b338afbab333c555292a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/138bb0696595b338afbab333c555292a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 111032,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3394934712244421483&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "McGill University, Montr\u00b4eal, Qc, Canada; McGill University, Montr\u00b4eal, Qc, Canada; Laval University, Qu\u00b4ebec, Qc, Canada",
        "aff_domain": "cs.mcgill.ca;cs.mcgill.ca;ift.ulaval.ca",
        "email": "cs.mcgill.ca;cs.mcgill.ca;ift.ulaval.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "McGill University, Montr\u00b4eal, Qc, Canada;Laval University, Qu\u00b4ebec, Qc, Canada",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "ad72d7cd5d",
        "title": "Theoretical Analysis of Learning with Reward-Modulated Spike-Timing-Dependent Plasticity",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/9b698eb3105bd82528f23d0c92dedfc0-Abstract.html",
        "author": "Dejan Pecevski; Wolfgang Maass; Robert A. Legenstein",
        "abstract": "Reward-modulated spike-timing-dependent plasticity (STDP) has recently emerged as a candidate for a learning rule that could explain how local learning rules at single synapses support behaviorally relevant adaptive changes in com- plex networks of spiking neurons. However the potential and limitations of this learning rule could so far only be tested through computer simulations. This ar- ticle provides tools for an analytic treatment of reward-modulated STDP, which allow us to predict under which conditions reward-modulated STDP will be able to achieve a desired learning effect. In particular, we can produce in this way a theoretical explanation and a computer model for a fundamental experimental \ufb01nding on biofeedback in monkeys (reported in [1]).",
        "bibtex": "@inproceedings{NIPS2007_9b698eb3,\n author = {Pecevski, Dejan and Maass, Wolfgang and Legenstein, Robert},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Theoretical Analysis of Learning with Reward-Modulated Spike-Timing-Dependent Plasticity},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9b698eb3105bd82528f23d0c92dedfc0-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/9b698eb3105bd82528f23d0c92dedfc0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/9b698eb3105bd82528f23d0c92dedfc0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 173510,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "Institute for Theoretical Computer Science, Graz University of Technology, A-8010 Graz, Austria; Institute for Theoretical Computer Science, Graz University of Technology, A-8010 Graz, Austria; Institute for Theoretical Computer Science, Graz University of Technology, A-8010 Graz, Austria",
        "aff_domain": "igi.tugraz.at;igi.tugraz.at;igi.tugraz.at",
        "email": "igi.tugraz.at;igi.tugraz.at;igi.tugraz.at",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Institute for Theoretical Computer Science, Graz University of Technology, A-8010 Graz, Austria",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "5601b019c3",
        "title": "Topmoumoute Online Natural Gradient Algorithm",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/9f61408e3afb633e50cdf1b20de6f466-Abstract.html",
        "author": "Nicolas L. Roux; Pierre-antoine Manzagol; Yoshua Bengio",
        "abstract": "Guided by the goal of obtaining an optimization algorithm that is both fast and yielding good generalization, we study the descent direction maximizing the decrease in generalization error or the probability of not increasing generalization error. The surprising result is that from both the Bayesian and frequentist perspectives this can yield the natural gradient direction. Although that direction can be very expensive to compute we develop an efficient, general, online approximation to the natural gradient descent which is suited to large scale problems. We report experimental results showing much faster convergence in computation time and in number of iterations with TONGA (Topmoumoute Online natural Gradient Algorithm) than with stochastic gradient descent, even on very large datasets.",
        "bibtex": "@inproceedings{NIPS2007_9f61408e,\n author = {Roux, Nicolas and Manzagol, Pierre-antoine and Bengio, Yoshua},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Topmoumoute Online Natural Gradient Algorithm},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9f61408e3afb633e50cdf1b20de6f466-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/9f61408e3afb633e50cdf1b20de6f466-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/9f61408e3afb633e50cdf1b20de6f466-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 275316,
        "gs_citation": 258,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12795054320160702164&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 11,
        "aff": "University of Montreal; University of Montreal; University of Montreal",
        "aff_domain": "umontreal.ca;iro.umontreal.ca;umontreal.ca",
        "email": "umontreal.ca;iro.umontreal.ca;umontreal.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Montreal",
        "aff_unique_dep": "",
        "aff_unique_url": "https://wwwumontreal.ca",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "aca2634f25",
        "title": "Transfer Learning using Kolmogorov Complexity: Basic Theory and Empirical Evaluations",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/b83aac23b9528732c23cc7352950e880-Abstract.html",
        "author": "M. M. Mahmud; Sylvian Ray",
        "abstract": "In transfer learning we aim to solve new problems using fewer examples using information gained from solving related problems. Transfer learning has been successful in practice, and extensive PAC analysis of these methods has been de- veloped. However it is not yet clear how to de\ufb01ne relatedness between tasks. This is considered as a major problem as it is conceptually troubling and it makes it unclear how much information to transfer and when and how to transfer it. In this paper we propose to measure the amount of information one task contains about another using conditional Kolmogorov complexity between the tasks. We show how existing theory neatly solves the problem of measuring relatedness and transferring the \u2018right\u2019 amount of information in sequential transfer learning in a Bayesian setting. The theory also suggests that, in a very formal and precise sense, no other reasonable transfer method can do much better than our Kolmogorov Complexity theoretic transfer method, and that sequential transfer is always justi- \ufb01ed. We also develop a practical approximation to the method and use it to transfer information between 8 arbitrarily chosen databases from the UCI ML repository.",
        "bibtex": "@inproceedings{NIPS2007_b83aac23,\n author = {Mahmud, M. and Ray, Sylvian},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Transfer Learning using Kolmogorov Complexity: Basic Theory and Empirical Evaluations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/b83aac23b9528732c23cc7352950e880-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/b83aac23b9528732c23cc7352950e880-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/b83aac23b9528732c23cc7352950e880-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 114083,
        "gs_citation": 92,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4464438874684991640&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science, University of Illinois at Urbana-Champaign; Department of Computer Science, University of Illinois at Urbana-Champaign",
        "aff_domain": "uiuc.edu;cs.uiuc.edu",
        "email": "uiuc.edu;cs.uiuc.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Illinois at Urbana-Champaign",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://illinois.edu",
        "aff_unique_abbr": "UIUC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Urbana-Champaign",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a6591d3aca",
        "title": "TrueSkill Through Time: Revisiting the History of Chess",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/9f53d83ec0691550f7d2507d57f4f5a2-Abstract.html",
        "author": "Pierre Dangauthier; Ralf Herbrich; Tom Minka; Thore Graepel",
        "abstract": "We extend the Bayesian skill rating system TrueSkill to infer entire time series of skills of players by smoothing through time instead of (cid:12)ltering. The skill of each participating player, say, every year is represented by a latent skill variable which is a(cid:11)ected by the relevant game outcomes that year, and coupled with the skill variables of the previous and subsequent year. Inference in the resulting factor graph is carried out by approximate message passing (EP) along the time series of skills. As before the system tracks the uncertainty about player skills, explicitly models draws, can deal with any number of competing entities and can infer individual skills from team results. We extend the system to estimate player-speci(cid:12)c draw mar- gins. Based on these models we present an analysis of the skill curves of important players in the history of chess over the past 150 years. Results include plots of players\u2019 lifetime skill development as well as the ability to compare the skills of di(cid:11)erent players across time. Our results indicate that a) the overall playing strength has increased over the past 150 years, and b) that modelling a player\u2019s ability to force a draw provides signi(cid:12)cantly better predictive power.",
        "bibtex": "@inproceedings{NIPS2007_9f53d83e,\n author = {Dangauthier, Pierre and Herbrich, Ralf and Minka, Tom and Graepel, Thore},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {TrueSkill Through Time: Revisiting the History of Chess},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/9f53d83ec0691550f7d2507d57f4f5a2-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/9f53d83ec0691550f7d2507d57f4f5a2-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/9f53d83ec0691550f7d2507d57f4f5a2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 175548,
        "gs_citation": 174,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10787748264046184129&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 16,
        "aff": "INRIA Rhone Alpes, Grenoble, France; Microsoft Research Ltd., Cambridge, UK; Microsoft Research Ltd., Cambridge, UK; Microsoft Research Ltd., Cambridge, UK",
        "aff_domain": "imag.fr;microsoft.com;microsoft.com;microsoft.com",
        "email": "imag.fr;microsoft.com;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "INRIA;Microsoft Research Ltd., Cambridge, UK",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.inria.fr;",
        "aff_unique_abbr": "INRIA;",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Grenoble;",
        "aff_country_unique_index": "0",
        "aff_country_unique": "France;"
    },
    {
        "id": "5d27f09190",
        "title": "Ultrafast Monte Carlo for Statistical Summations",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/5e6d27a7a8a8330df4b53240737ccc85-Abstract.html",
        "author": "Charles L. Isbell; Michael P. Holmes; Alexander G. Gray",
        "abstract": "Machine learning contains many computational bottlenecks in the form of nested summations over datasets. Kernel estimators and other methods are burdened by these expensive computations. Exact evaluation is typically O(n2 ) or higher, which severely limits application to large datasets. We present a multi-stage stratified Monte Carlo method for approximating such summations with probabilistic relative error control. The essential idea is fast approximation by sampling in trees. This method differs from many previous scalability techniques (such as standard multi-tree methods) in that its error is stochastic, but we derive conditions for error control and demonstrate that they work. Further, we give a theoretical sample complexity for the method that is independent of dataset size, and show that this appears to hold in experiments, where speedups reach as high as 1014 , many orders of magnitude beyond the previous state of the art.",
        "bibtex": "@inproceedings{NIPS2007_5e6d27a7,\n author = {Isbell, Charles and Holmes, Michael and Gray, Alexander},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Ultrafast Monte Carlo for Statistical Summations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/5e6d27a7a8a8330df4b53240737ccc85-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/5e6d27a7a8a8330df4b53240737ccc85-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/5e6d27a7a8a8330df4b53240737ccc85-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 126686,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14457678760619978017&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "College Of Computing, Georgia Institute of Technology; College Of Computing, Georgia Institute of Technology; College Of Computing, Georgia Institute of Technology",
        "aff_domain": "cc.gatech.edu;cc.gatech.edu;cc.gatech.edu",
        "email": "cc.gatech.edu;cc.gatech.edu;cc.gatech.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "College Of Computing, Georgia Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "fe440a479e",
        "title": "Unconstrained On-line Handwriting Recognition with Recurrent Neural Networks",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/4b0250793549726d5c1ea3906726ebfe-Abstract.html",
        "author": "Alex Graves; Marcus Liwicki; Horst Bunke; J\u00fcrgen Schmidhuber; Santiago Fern\u00e1ndez",
        "abstract": "On-line handwriting recognition is unusual among sequence labelling tasks in that the underlying generator of the observed data, i.e. the movement of the pen, is recorded directly. However, the raw data can be difficult to interpret because each letter is spread over many pen locations. As a consequence, sophisticated pre-processing is required to obtain inputs suitable for conventional sequence labelling algorithms, such as HMMs. In this paper we describe a system capable of directly transcribing raw on-line handwriting data. The system consists of a recurrent neural network trained for sequence labelling, combined with a probabilistic language model. In experiments on an unconstrained on-line database, we record excellent results using either raw or pre-processed data, well outperforming a benchmark HMM in both cases.",
        "bibtex": "@inproceedings{NIPS2007_4b025079,\n author = {Graves, Alex and Liwicki, Marcus and Bunke, Horst and Schmidhuber, J\\\"{u}rgen and Fern\\'{a}ndez, Santiago},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Unconstrained On-line Handwriting Recognition with Recurrent Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/4b0250793549726d5c1ea3906726ebfe-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/4b0250793549726d5c1ea3906726ebfe-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/4b0250793549726d5c1ea3906726ebfe-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 905463,
        "gs_citation": 393,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17998149229577882956&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 26,
        "aff": "TUM, Germany; IDSIA, Switzerland; University of Bern, Switzerland; University of Bern, Switzerland; IDSIA, Switzerland + TUM, Germany",
        "aff_domain": "idsia.ch;idsia.ch;iam.unibe.ch;iam.unibe.ch;idsia.ch",
        "email": "idsia.ch;idsia.ch;iam.unibe.ch;iam.unibe.ch;idsia.ch",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;2;1+0",
        "aff_unique_norm": "TUM, Germany;IDSIA, Switzerland;University of Bern",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;https://www.unibe.ch",
        "aff_unique_abbr": ";;UniBE",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1;",
        "aff_country_unique": ";Switzerland"
    },
    {
        "id": "ab86a5329b",
        "title": "Unsupervised Feature Selection for Accurate Recommendation of High-Dimensional Image Data",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/073b00ab99487b74b63c9a6d2b962ddc-Abstract.html",
        "author": "Sabri Boutemedjet; Djemel Ziou; Nizar Bouguila",
        "abstract": "Content-based image suggestion (CBIS) targets the recommendation of products based on user preferences on the visual content of images. In this paper, we mo- tivate both feature selection and model order identi\ufb01cation as two key issues for a successful CBIS. We propose a generative model in which the visual features and users are clustered into separate classes. We identify the number of both user and image classes with the simultaneous selection of relevant visual features us- ing the message length approach. The goal is to ensure an accurate prediction of ratings for multidimensional non-Gaussian and continuous image descriptors. Experiments on a collected data have demonstrated the merits of our approach.",
        "bibtex": "@inproceedings{NIPS2007_073b00ab,\n author = {Boutemedjet, Sabri and Ziou, Djemel and Bouguila, Nizar},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Unsupervised Feature Selection for Accurate Recommendation of High-Dimensional Image Data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/073b00ab99487b74b63c9a6d2b962ddc-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/073b00ab99487b74b63c9a6d2b962ddc-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/073b00ab99487b74b63c9a6d2b962ddc-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/073b00ab99487b74b63c9a6d2b962ddc-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 187882,
        "gs_citation": 65,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2969855708620541128&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "DI, Universite de Sherbrooke; DI, Universite de Sherbrooke; CIISE, Concordia University",
        "aff_domain": "usherbrooke.ca;usherbrooke.ca;ciise.concordia.ca",
        "email": "usherbrooke.ca;usherbrooke.ca;ciise.concordia.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "DI, Universite de Sherbrooke;CIISE, Concordia University",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "f707d3e80a",
        "title": "Using Deep Belief Nets to Learn Covariance Kernels for Gaussian Processes",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/4b6538a44a1dfdc2b83477cd76dee98e-Abstract.html",
        "author": "Geoffrey E. Hinton; Ruslan Salakhutdinov",
        "abstract": "We show how to use unlabeled data and a deep belief net (DBN) to learn a good covariance kernel for a Gaussian process. We first learn a deep generative model of the unlabeled data using the fast, greedy algorithm introduced by Hinton et.al. If the data is high-dimensional and highly-structured, a Gaussian kernel applied to the top layer of features in the DBN works much better than a similar kernel applied to the raw input. Performance at both regression and classification can then be further improved by using backpropagation through the DBN to discriminatively fine-tune the covariance kernel.",
        "bibtex": "@inproceedings{NIPS2007_4b6538a4,\n author = {Hinton, Geoffrey E and Salakhutdinov, Russ R},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Using Deep Belief Nets to Learn Covariance Kernels for Gaussian Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/4b6538a44a1dfdc2b83477cd76dee98e-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/4b6538a44a1dfdc2b83477cd76dee98e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/4b6538a44a1dfdc2b83477cd76dee98e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 164936,
        "gs_citation": 308,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1299878054477854748&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff": "Department of Computer Science, University of Toronto; Department of Computer Science, University of Toronto",
        "aff_domain": "cs.toronto.edu;cs.toronto.edu",
        "email": "cs.toronto.edu;cs.toronto.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Toronto",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utoronto.ca",
        "aff_unique_abbr": "U of T",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Toronto",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "49f4076b84",
        "title": "Variational Inference for Diffusion Processes",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/818f4654ed39a1c147d1e51a00ffb4cb-Abstract.html",
        "author": "C\u00e9dric Archambeau; Manfred Opper; Yuan Shen; Dan Cornford; John S. Shawe-taylor",
        "abstract": "Diffusion processes are a family of continuous-time continuous-state stochastic processes that are in general only partially observed. The joint estimation of the forcing parameters and the system noise (volatility) in these dynamical systems is a crucial, but non-trivial task, especially when the system is nonlinear and multi-modal. We propose a variational treatment of diffusion processes, which allows us to estimate these parameters by simple gradient techniques and which is computationally less demanding than most MCMC approaches. Furthermore, our parameter inference scheme does not break down when the time step gets smaller, unlike most current approaches. Finally, we show how a cheap estimate of the posterior over the parameters can be constructed based on the variational free energy.",
        "bibtex": "@inproceedings{NIPS2007_818f4654,\n author = {Archambeau, C\\'{e}dric and Opper, Manfred and Shen, Yuan and Cornford, Dan and Shawe-taylor, John},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Variational Inference for Diffusion Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/818f4654ed39a1c147d1e51a00ffb4cb-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/818f4654ed39a1c147d1e51a00ffb4cb-Paper.pdf",
        "supp": "https://papers.nips.cc/paper_files/paper/2007/file/818f4654ed39a1c147d1e51a00ffb4cb-Supplemental.zip",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/818f4654ed39a1c147d1e51a00ffb4cb-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 203591,
        "gs_citation": 120,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6038069380382365123&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": "University College London; Technical University Berlin; Aston University; Aston University; University College London",
        "aff_domain": "cs.ucl.ac.uk;cs.tu-berlin.de;aston.ac.uk;aston.ac.uk;cs.ucl.ac.uk",
        "email": "cs.ucl.ac.uk;cs.tu-berlin.de;aston.ac.uk;aston.ac.uk;cs.ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;2;0",
        "aff_unique_norm": "University College London;Technical University Berlin;Aston University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.ucl.ac.uk;;https://www.aston.ac.uk",
        "aff_unique_abbr": "UCL;;Aston",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United Kingdom;"
    },
    {
        "id": "38b34184ce",
        "title": "Variational inference for Markov jump processes",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/735b90b4568125ed6c3f678819b6e058-Abstract.html",
        "author": "Manfred Opper; Guido Sanguinetti",
        "abstract": "Markov jump processes play an important role in a large number of application domains. However, realistic systems are analytically intractable and they have traditionally been analysed using simulation based techniques, which do not provide a framework for statistical inference. We propose a mean field approximation to perform posterior inference and parameter estimation. The approximation allows a practical solution to the inference problem, {while still retaining a good degree of accuracy.} We illustrate our approach on two biologically motivated systems.",
        "bibtex": "@inproceedings{NIPS2007_735b90b4,\n author = {Opper, Manfred and Sanguinetti, Guido},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {Variational inference for Markov jump processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/735b90b4568125ed6c3f678819b6e058-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/735b90b4568125ed6c3f678819b6e058-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/735b90b4568125ed6c3f678819b6e058-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 133456,
        "gs_citation": 99,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11962478359350878372&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Computer Science, Technische Universit\u00e4t Berlin; Department of Computer Science, University of Sheffield, U.K.",
        "aff_domain": "cs.tu-berlin.de;dcs.shef.ac.uk",
        "email": "cs.tu-berlin.de;dcs.shef.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Department of Computer Science, Technische Universit\u00e4t Berlin;Department of Computer Science, University of Sheffield, U.K.",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "0cdb4e906e",
        "title": "What makes some POMDP problems easy to approximate?",
        "site": "https://papers.nips.cc/paper_files/paper/2007/hash/cfa0860e83a4c3a763a7e62d825349f7-Abstract.html",
        "author": "Wee S. Lee; Nan Rong; David Hsu",
        "abstract": "Point-based algorithms have been surprisingly successful in computing approx- imately optimal solutions for partially observable Markov decision processes (POMDPs) in high dimensional belief spaces. In this work, we seek to understand the belief-space properties that allow some POMDP problems to be approximated ef\ufb01ciently and thus help to explain the point-based algorithms\u2019 success often ob- served in the experiments. We show that an approximately optimal POMDP so- lution can be computed in time polynomial in the covering number of a reachable belief space, which is the subset of the belief space reachable from a given belief point. We also show that under the weaker condition of having a small covering number for an optimal reachable space, which is the subset of the belief space reachable under an optimal policy, computing an approximately optimal solution is NP-hard. However, given a suitable set of points that \u201ccover\u201d an optimal reach- able space well, an approximate solution can be computed in polynomial time. The covering number highlights several interesting properties that reduce the com- plexity of POMDP planning in practice, e.g., fully observed state variables, beliefs with sparse support, smooth beliefs, and circulant state-transition matrices.",
        "bibtex": "@inproceedings{NIPS2007_cfa0860e,\n author = {Lee, Wee and Rong, Nan and Hsu, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},\n pages = {},\n publisher = {Curran Associates, Inc.},\n title = {What makes some POMDP problems easy to approximate?},\n url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/cfa0860e83a4c3a763a7e62d825349f7-Paper.pdf},\n volume = {20},\n year = {2007}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2007/file/cfa0860e83a4c3a763a7e62d825349f7-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2007/file/cfa0860e83a4c3a763a7e62d825349f7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 190631,
        "gs_citation": 118,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15778689913133701992&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Computer Science, National University of Singapore, Singapore, 117590, Singapore; Department of Computer Science, National University of Singapore, Singapore, 117590, Singapore; Department of Computer Science, Cornell University, Ithaca, NY 14853, USA",
        "aff_domain": "; ; ",
        "email": "; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Department of Computer Science, National University of Singapore, Singapore, 117590, Singapore;Department of Computer Science, Cornell University, Ithaca, NY 14853, USA",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    }
]