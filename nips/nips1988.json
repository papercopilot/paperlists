[
    {
        "id": "f3a80bb869",
        "title": "A Back-Propagation Algorithm with Optimal Use of Hidden Units",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/9fc3d7152ba9336a670e36d0ed79bc43-Abstract.html",
        "author": "Yves Chauvin",
        "abstract": "This  paper  presents  a  variation  of  the  back-propagation  algo(cid:173) rithm  that makes  optimal  use  of  a  network  hidden units  by  de(cid:173) cr~asing an  \"energy\"  term written  as  a  function  of  the  squared  activations  of  these  hidden units.  The  algorithm  can automati(cid:173) cally  find  optimal  or  nearly  optimal  architectures  necessary  to  solve  known  Boolean  functions,  facilitate  the  interpretation  of  the  activation  of  the  remaining  hidden  units  and  automatically  estimate the complexity of architectures appropriate for phonetic  labeling  problems.  The  general  principle  of the  algorithm  can  also be adapted to different tasks:  for  example,  it can be used to  eliminate the  [0,  0]  local minimum  of the  [-1.  +1]  logistic  acti(cid:173) vation  function  while  preserving  a  much  faster  convergence  and  forcing  binary  activations  over the  set of hidden  units.",
        "bibtex": "@inproceedings{NIPS1988_9fc3d715,\n author = {Chauvin, Yves},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {A Back-Propagation Algorithm with Optimal Use of Hidden Units},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/9fc3d7152ba9336a670e36d0ed79bc43-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/9fc3d7152ba9336a670e36d0ed79bc43-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/9fc3d7152ba9336a670e36d0ed79bc43-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1411838,
        "gs_citation": 368,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9257529724212248390&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Thomson-CSF, Inc + Psychology Department, Stanford University",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1",
        "aff_unique_norm": "Thomson-CSF, Inc;Psychology Department, Stanford University",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "222d36c9bd",
        "title": "A Bifurcation Theory Approach to the Programming of Periodic Attractors in Network Models of Olfactory Cortex",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/2b24d495052a8ce66358eb576b8912c8-Abstract.html",
        "author": "Bill Baird",
        "abstract": "A  new  learning  algorithm  for  the  storage  of  static  and  periodic  attractors  in  biologically  inspired  recurrent  analog  neural  networks  is  introduced.  For  a  network  of  n  nodes,  n  static  or  n/2  periodic  attractors  may  be  stored.  The  algorithm  allows  programming  of  the  network  vector  field  indepen(cid:173) dent  of  the  patterns  to  be  stored.  Stability  of  patterns,  basin  geometry,  and  rates  of  convergence  may  be  controlled.  For  orthonormal  patterns,  the  l~grning operation  reduces  to  a  kind  of  periodic  outer  product  rule  that  allows  local,  additive,  commutative,  incremental  learning.  Standing  or  traveling  wave  cycles  may  be  stored  to  mimic  the  kind  of  oscillating  spatial  patterns  that  appear  in  the  neural  activity  of  the  olfactory  bulb  and  prepyriform  cortex  during  inspiration  and  suffice,  in  the  bulb,  to  predict  the  pattern  recognition  behavior  of  rabbits  in  classical  conditioning  ex(cid:173) periments.  These  attractors  arise,  during  simulat(cid:173) ed  inspiration,  through  a  multiple  Hopf  bifurca(cid:173) tion,  which  can  act  as  a  critical  \"decision  pOint\"  for  their  selection  by  a  very  small  input  pattern.",
        "bibtex": "@inproceedings{NIPS1988_2b24d495,\n author = {Baird, Bill},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {A Bifurcation Theory Approach to the Programming of Periodic Attractors in Network Models of Olfactory Cortex},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/2b24d495052a8ce66358eb576b8912c8-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/2b24d495052a8ce66358eb576b8912c8-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/2b24d495052a8ce66358eb576b8912c8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1849390,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2375497994358288941&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Biophysics, U.C. Berkeley",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Department of Biophysics, U.C. Berkeley",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "9e9d9b1af4",
        "title": "A Computationally Robust Anatomical Model for Retinal Directional Selectivity",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/98dce83da57b0395e163467c9dae521b-Abstract.html",
        "author": "Norberto M. Grzywacz; Franklin R. Amthor",
        "abstract": "We analyze a mathematical model for  retinal directionally selective  cells  based  on  recent  electrophysiological  data,  and  show  that  its  computation of motion direction is  robust against  noise  and speed.",
        "bibtex": "@inproceedings{NIPS1988_98dce83d,\n author = {Grzywacz, Norberto and Amthor, Franklin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {A Computationally Robust Anatomical Model for Retinal Directional Selectivity},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/98dce83da57b0395e163467c9dae521b-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/98dce83da57b0395e163467c9dae521b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/98dce83da57b0395e163467c9dae521b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1532896,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9985636372603487130&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Center BioI. Inf. Processing, MIT, E25-201, Cambridge, MA 02139; Dept. Psychol., Univ. Alabama Birmingham, Birmingham, AL 35294",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Center BioI. Inf. Processing, MIT, E25-201, Cambridge, MA 02139;Dept. Psychol., Univ. Alabama Birmingham, Birmingham, AL 35294",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "72013a4038",
        "title": "A Connectionist Expert System that Actually Works",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/7f1de29e6da19d22b51c68001e7e0e54-Abstract.html",
        "author": "Richard Fozzard; Gary Bradshaw; Louis Ceci",
        "abstract": "Abstract Unavailable",
        "bibtex": "@inproceedings{NIPS1988_7f1de29e,\n author = {Fozzard, Richard and Bradshaw, Gary and Ceci, Louis},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {A Connectionist Expert System that Actually Works},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/7f1de29e6da19d22b51c68001e7e0e54-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/7f1de29e6da19d22b51c68001e7e0e54-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/7f1de29e6da19d22b51c68001e7e0e54-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1688949,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14214858656302659667&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Psychology; Computer Science; Computer Science",
        "aff_domain": "boulder.colorado.edu; ; ",
        "email": "boulder.colorado.edu; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Psychology Department;Computer Science",
        "aff_unique_dep": "Department of Psychology;Computer Science Department",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "75822e242a",
        "title": "A Low-Power CMOS Circuit Which Emulates Temporal Electrical Properties of Neurons",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/1ff8a7b5dc7a7d1f0ed65aaa29c04b1e-Abstract.html",
        "author": "Jack L. Meador; Clint S. Cole",
        "abstract": "This  paper  describes  a  CMOS  artificial  neuron.  The  circuit  is \ndirectly  derived  from  the  voltage-gated  channel  model  of  neural \nmembrane,  has  low  power  dissipation,  and  small  layout  geometry. \nThe principal motivations behind this work include a desire for high \nperformance,  more  accurate  neuron  emulation,  and  the  need  for \nhigher density in practical neural network implementations.",
        "bibtex": "@inproceedings{NIPS1988_1ff8a7b5,\n author = {Meador, Jack and Cole, Clint},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {A Low-Power CMOS Circuit Which Emulates Temporal Electrical Properties of Neurons},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/1ff8a7b5dc7a7d1f0ed65aaa29c04b1e-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/1ff8a7b5dc7a7d1f0ed65aaa29c04b1e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/1ff8a7b5dc7a7d1f0ed65aaa29c04b1e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1899208,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5755083454511323679&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Electrical and Computer Engineering Dept., Washington State University, Pullman WA. 99164-2752; Electrical and Computer Engineering Dept., Washington State University, Pullman WA. 99164-2752",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Electrical and Computer Engineering Dept., Washington State University, Pullman WA. 99164-2752",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "6163e0d056",
        "title": "A Massively Parallel Self-Tuning Context-Free Parser",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/9766527f2b5d3e95d4a733fcfb77bd7e-Abstract.html",
        "author": "Eugene Santos Jr.",
        "abstract": "The  Parsing  and  Learning  System(PALS)  is  a  massively  parallel  self-tuning context-free  parser.  It  is capable  of  parsing sentences of unbounded length mainly due to its  parse-tree representation scheme. The system is capable  of  improving  its  parsing  performance  through  the  presentation of training examples.",
        "bibtex": "@inproceedings{NIPS1988_9766527f,\n author = {Santos, Eugene},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {A Massively Parallel Self-Tuning Context-Free Parser},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/9766527f2b5d3e95d4a733fcfb77bd7e-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/9766527f2b5d3e95d4a733fcfb77bd7e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/9766527f2b5d3e95d4a733fcfb77bd7e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1575554,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13290215989783115710&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science, Brown University",
        "aff_domain": "cs.brown.edu",
        "email": "cs.brown.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Brown University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.brown.edu",
        "aff_unique_abbr": "Brown",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "06bb9672ed",
        "title": "A Model for Resolution Enhancement (Hyperacuity) in Sensory Representation",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/8f85517967795eeef66c225f7883bdcb-Abstract.html",
        "author": "Jun Zhang; John P. Miller",
        "abstract": "Heiligenberg (1987)  recently proposed a  model  to explain how sen(cid:173)",
        "bibtex": "@inproceedings{NIPS1988_8f855179,\n author = {Zhang, Jun and Miller, John},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {A Model for Resolution Enhancement (Hyperacuity) in Sensory Representation},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/8f85517967795eeef66c225f7883bdcb-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/8f85517967795eeef66c225f7883bdcb-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/8f85517967795eeef66c225f7883bdcb-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 996956,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6826048980390835595&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "ca452a3ded",
        "title": "A Model of Neural Oscillator for a Unified Submodule",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/da4fb5c6e93e74d3df8527599fa62642-Abstract.html",
        "author": "Alexandr B. Kirillov; G. N. Borisyuk; R. M. Borisyuk; Ye. I. Kovalenko; V. I. Makarenko; V. A. Chulaevsky; V. I. Kryukov",
        "abstract": "Abstract Unavailable",
        "bibtex": "@inproceedings{NIPS1988_da4fb5c6,\n author = {Kirillov, Alexandr and Borisyuk, G. N. and Borisyuk, R. M. and Kovalenko, Ye. and Makarenko, V. and Chulaevsky, V. and Kryukov, V.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {A Model of Neural Oscillator for a Unified Submodule},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/da4fb5c6e93e74d3df8527599fa62642-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/da4fb5c6e93e74d3df8527599fa62642-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/da4fb5c6e93e74d3df8527599fa62642-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1521307,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17877489824174101918&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "00add95f74",
        "title": "A Network for Image Segmentation Using Color",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/8d5e957f297893487bd98fa830fa6413-Abstract.html",
        "author": "Anya Hurlbert; Tomaso Poggio",
        "abstract": "We propose a parallel network of simple processors to find  color boundaries irrespective of spatial changes in illumi(cid:173) nation, and to spread uniform colors within marked re-",
        "bibtex": "@inproceedings{NIPS1988_8d5e957f,\n author = {Hurlbert, Anya and Poggio, Tomaso},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {A Network for Image Segmentation Using Color},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/8d5e957f297893487bd98fa830fa6413-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/8d5e957f297893487bd98fa830fa6413-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/8d5e957f297893487bd98fa830fa6413-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1495692,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "Center for Biological Information Processing at Whitaker College; Department of Brain and Cognitive Science and the MIT AI Laboratory",
        "aff_domain": "wheaties.ai.mit.edu; ",
        "email": "wheaties.ai.mit.edu; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Center for Biological Information Processing at Whitaker College;Department of Brain and Cognitive Science and the MIT AI Laboratory",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "1218cd2b3d",
        "title": "A Passive Shared Element Analog Electrical Cochlea",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/0a09c8844ba8f0936c20bd791130d6b6-Abstract.html",
        "author": "David Feld; Joe Eisenberg; Edwin Lewis",
        "abstract": "We present a  simplified model  of the  micromechanics of the human  cochlea,  realized  with  electrical  elements.  Simulation  of the  model  shows that it retains four signal processing features whose importance  we argue on the basis of engineering logic and evolutionary evidence.  Furthermore, just as  the cochlea does,  the  model  achieves  massively  parallel signal processing in a structurally economic way, by means of  shared elements.  By extracting what we believe are the five essential  features of the cochlea, we hope to design a useful  front-end  filter to  process  acoustic  images and to  obtain  a  better understanding  of the  auditory system.",
        "bibtex": "@inproceedings{NIPS1988_0a09c884,\n author = {Feld, David and Eisenberg, Joe and Lewis, Edwin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {A Passive Shared Element Analog Electrical Cochlea},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/0a09c8844ba8f0936c20bd791130d6b6-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/0a09c8844ba8f0936c20bd791130d6b6-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/0a09c8844ba8f0936c20bd791130d6b6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1930010,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11269743707414963942&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "5446c3fe60",
        "title": "A Programmable Analog Neural Computer and Simulator",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/7e7757b1e12abcb736ab9a754ffb617a-Abstract.html",
        "author": "Paul Mueller; Jan Van der Spiegel; David Blackman; Timothy Chiu; Thomas Clare; Joseph Dao; Christopher Donham; Tzu-pu Hsieh; Marc Loinaz",
        "abstract": "This  report describes  the  design  of a  programmable general  purpose analog neural computer and simulator.  It is intended  primarily  for  real-world  real-time  computations  such  as  analysis  of visual  or  acoustical patterns, robotics  and the development of  special purpose  neural nets.  The machine is scalable and  composed of interconnected  modules containing arrays of neurons, modifiable synapses and switches.  It runs  entirely in analog  mode but connection architecture, synaptic  gains and time  constants as well as neuron parameters are set digitally.  Each  neuron has a limited number of inputs and can be connected to any  but not all other neurons. For the determination of synaptic gains and the  implementation  of  learning  algorithms  the  neuron  outputs  are  multiplexed, AID  converted and stored in digital  memory.  Even at  moderate size of 1()3 to IDS neurons  computational speed is expected to  exceed that of any current  digital computer.",
        "bibtex": "@inproceedings{NIPS1988_7e7757b1,\n author = {Mueller, Paul and Van der Spiegel, Jan and Blackman, David and Chiu, Timothy and Clare, Thomas and Dao, Joseph and Donham, Christopher and Hsieh, Tzu-pu and Loinaz, Marc},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {A Programmable Analog Neural Computer and Simulator},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/7e7757b1e12abcb736ab9a754ffb617a-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/7e7757b1e12abcb736ab9a754ffb617a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/7e7757b1e12abcb736ab9a754ffb617a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1530461,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12664618242668855902&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Dept.of Biochem. Biophys. + Dept. of Electrical Engineering, University of Pennsylvania, Philadelphia Pa.; Dept.of Biochem. Biophys. + Dept. of Electrical Engineering, University of Pennsylvania, Philadelphia Pa.; Dept.of Biochem. Biophys. + Dept. of Electrical Engineering, University of Pennsylvania, Philadelphia Pa.; Dept.of Biochem. Biophys. + Dept. of Electrical Engineering, University of Pennsylvania, Philadelphia Pa.; Dept.of Biochem. Biophys. + Dept. of Electrical Engineering, University of Pennsylvania, Philadelphia Pa.; Dept.of Biochem. Biophys. + Dept. of Electrical Engineering, University of Pennsylvania, Philadelphia Pa.; Dept.of Biochem. Biophys. + Dept. of Electrical Engineering, University of Pennsylvania, Philadelphia Pa.; Dept.of Biochem. Biophys. + Dept. of Electrical Engineering, University of Pennsylvania, Philadelphia Pa.; Dept.of Biochem. Biophys. + Dept. of Electrical Engineering, University of Pennsylvania, Philadelphia Pa.",
        "aff_domain": ";;;;;;;;",
        "email": ";;;;;;;;",
        "github": "",
        "project": "",
        "author_num": 9,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;0+1;0+1;0+1;0+1;0+1;0+1;0+1",
        "aff_unique_norm": "Dept.of Biochem. Biophys.;Dept. of Electrical Engineering, University of Pennsylvania, Philadelphia Pa.",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": ";;;;;;;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": ";;;;;;;;",
        "aff_country_unique": ""
    },
    {
        "id": "e89d20596a",
        "title": "A Self-Learning Neural Network",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/a2557a7b2e94197ff767970b67041697-Abstract.html",
        "author": "Allan Hartstein; R. H. Koch",
        "abstract": "Abstract Unavailable",
        "bibtex": "@inproceedings{NIPS1988_a2557a7b,\n author = {Hartstein, Allan and Koch, R.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {A Self-Learning Neural Network},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/a2557a7b2e94197ff767970b67041697-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/a2557a7b2e94197ff767970b67041697-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/a2557a7b2e94197ff767970b67041697-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1489725,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9710447752749000376&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "IBM -Thomas J. Watson Research Center; IBM -Thomas J. Watson Research Center",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "IBM -Thomas J. Watson Research Center",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "bc4eddc15b",
        "title": "ALVINN: An Autonomous Land Vehicle in a Neural Network",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/812b4ba287f5ee0bc9d43bbf5bbe87fb-Abstract.html",
        "author": "Dean A. Pomerleau",
        "abstract": "ALVINN (Autonomous Land Vehicle In a Neural Network) is a 3-layer  back-propagation network designed for the task of road following. Cur(cid:173) rently ALVINN takes images from a camera and a laser range finder as input  and produces as output the direction the vehicle should travel in order to  follow the road. Training has been conducted using simulated road images.  Successful tests on the Carnegie Mellon autonomous navigation test vehicle  indicate that the network can effectively follow real roads under certain field  conditions. The representation developed to perfOIm the task differs dra(cid:173) matically when the networlc is trained under various conditions, suggesting  the possibility of a novel adaptive autonomous navigation system capable of  tailoring its processing to the conditions at hand.",
        "bibtex": "@inproceedings{NIPS1988_812b4ba2,\n author = {Pomerleau, Dean A.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {ALVINN: An Autonomous Land Vehicle in a Neural Network},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2161959,
        "gs_citation": 3234,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3309503225122836531&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "Computer Science Department, Carnegie Mellon University",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2cb59747b3",
        "title": "Adaptive Neural Net Preprocessing for Signal Detection in Non-Gaussian Noise",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/47d1e990583c9c67424d369f3414728e-Abstract.html",
        "author": "Richard P Lippmann; Paul Beckman",
        "abstract": "A  nonlinearity  is  required  before  matched  filtering  in  mInimum  error  receivers  when  additive  noise  is  present  which  is  impulsive  and  highly  non-Gaussian.  Experiments  were  performed  to  determine  whether  the  correct clipping  nonlinearity  could  be provided  by  a  single-input  single(cid:173) output  multi-layer  perceptron  trained  with  back  propagation.  It  was  found  that a  multi-layer perceptron with one input and output node,  20  nodes  in  the  first  hidden  layer,  and  5  nodes  in  the  second  hidden  layer  could be trained to provide a  clipping nonlinearity with fewer than 5,000  presentations  of noiseless  and  corrupted  waveform  samples.  A  network  trained  at  a  relatively  high  signal-to-noise  (SIN)  ratio  and  then  used  as  a  front  end  for  a  linear  matched  filter  detector greatly  reduced  the  probability  of error.  The  clipping  nonlinearity  formed  by  this  network  was similar to that used in current receivers designed for impulsive  noise  and  provided  similar substantial  improvements in  performance.",
        "bibtex": "@inproceedings{NIPS1988_47d1e990,\n author = {Lippmann, Richard P and Beckman, Paul},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Adaptive Neural Net Preprocessing for Signal Detection in Non-Gaussian Noise},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/47d1e990583c9c67424d369f3414728e-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/47d1e990583c9c67424d369f3414728e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/47d1e990583c9c67424d369f3414728e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1624375,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1294151661938370686&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "4323f59296",
        "title": "Adaptive Neural Networks Using MOS Charge Storage",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/5f93f983524def3dca464469d2cf9f3e-Abstract.html",
        "author": "Daniel B. Schwartz; R. E. Howard; Wayne E. Hubbard",
        "abstract": "MOS charge storage has been demonstrated as an effective method to store  the  weights  in  VLSI  implementations  of  neural  network  models  by  several  workers  2 .  However,  to  achieve  the  full  power  of a  VLSI  implementation  of  an adaptive algorithm, the learning operation must built into the circuit.  We  have  fabricated  and  tested  a  circuit  ideal  for  this  purpose  by  connecting  a  pair of capacitors with  a  CCD like structure, allowing for  variable size  weight  changes  as  well  as  a  weight  decay  operation.  A  2.51-'  CMOS  version  achieves  better  than  10  bits  of dynamic  range  in  a  140/'  X  3501-'  area.  A  1.25/'  chip  based  upon  the  same  cell  has  1104  weights  on  a  3.5mm  x  6.0mm  die  and  is  capable of peak learning rates  of at least  2  x  109  weight  changes  per second.",
        "bibtex": "@inproceedings{NIPS1988_5f93f983,\n author = {Schwartz, Daniel and Howard, R. and Hubbard, Wayne},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Adaptive Neural Networks Using MOS Charge Storage},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/5f93f983524def3dca464469d2cf9f3e-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/5f93f983524def3dca464469d2cf9f3e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/5f93f983524def3dca464469d2cf9f3e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1742187,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3952595421142079419&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "AT&T Bell Laboratories; AT&T Bell Laboratories; AT&T Bell Laboratories + GTE Laboratories",
        "aff_domain": "gte.com%relay.cs.net; ; ",
        "email": "gte.com%relay.cs.net; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0+1",
        "aff_unique_norm": "AT&T Bell Laboratories;GTE Laboratories",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.att.com/labs;",
        "aff_unique_abbr": "AT&T Bell Labs;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "c2e1a10b05",
        "title": "An Adaptive Network That Learns Sequences of Transitions",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/6974ce5ac660610b44d9b9fed0ff9548-Abstract.html",
        "author": "C. L. Winter",
        "abstract": "We describe  an  adaptive  network,  TIN2,  that learns  the  transition  function of a sequential system from  observations of its behavior.  It  integrates two subnets, TIN-I  (Winter, Ryan and Turner,  1987) and  TIN-2.  TIN-2  constructs  state  representations  from  examples  of  system behavior, and  its  dynamics are the main  topics of the paper.  TIN-I abstracts transition functions from  noisy state representations  and environmental data during training, while in operation it produces  sequences of transitions in response to variations in input.  Dynamics  of both nets are based on the Adaptive Resonance Theory of Carpenter  and Grossberg (1987).  We give results from an experiment in which  TIN2 learned the behavior of a system that recognizes strings with an  even number of l's .",
        "bibtex": "@inproceedings{NIPS1988_6974ce5a,\n author = {Winter, C.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {An Adaptive Network That Learns Sequences of Transitions},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/6974ce5ac660610b44d9b9fed0ff9548-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/6974ce5ac660610b44d9b9fed0ff9548-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/6974ce5ac660610b44d9b9fed0ff9548-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1735801,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3262728169921790606&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Science Applications International Corporation",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Science Applications International Corporation",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "d070e98889",
        "title": "An Analog Self-Organizing Neural Network Chip",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/2a79ea27c279e471f4d180b08d62b00a-Abstract.html",
        "author": "James R. Mann; Sheldon Gilbert",
        "abstract": "A design for a fully analog version of a self-organizing feature map neural  network has been completed. Several parts of this design are in fabrication.  The feature map algorithm was modified to accommodate circuit solutions  to the various computations required. Performance effects were measured  by simulating the design as part of a frontend for a speech recognition  system. Circuits are included to implement both activation computations and  weight adaption 'or learning. External access to the analog weight values is  provided to facilitate weight initialization, testing and static storage. This  fully analog implementation requires an order of magnitude less area than  a comparable digital/analog hybrid version developed earlier.",
        "bibtex": "@inproceedings{NIPS1988_2a79ea27,\n author = {Mann, James and Gilbert, Sheldon},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {An Analog Self-Organizing Neural Network Chip},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/2a79ea27c279e471f4d180b08d62b00a-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/2a79ea27c279e471f4d180b08d62b00a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/2a79ea27c279e471f4d180b08d62b00a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1762393,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7930195880165184510&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "MIT Lincoln Laboratory; ",
        "aff_domain": "; ",
        "email": "; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Lincoln Laboratory",
        "aff_unique_url": "https://www.ll.mit.edu",
        "aff_unique_abbr": "MIT LL",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Lincoln Laboratory",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f9f45e4c9a",
        "title": "An Analog VLSI Chip for Thin-Plate Surface Interpolation",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/3636638817772e42b59d74cff571fbb3-Abstract.html",
        "author": "John G. Harris",
        "abstract": "Reconstructing a surface from sparse sensory data is a well-known  problem iIi computer vision. This paper describes an experimental  analog VLSI chip for smooth surface interpolation from sparse depth  data. An eight-node ID network was designed in 3J.lm CMOS and  successfully tested. The network minimizes a second-order or \"thin(cid:173) plate\" energy of the surface. The circuit directly implements the cou(cid:173) pled depth/slope model of surface reconstruction (Harris, 1987). In  addition, this chip can provide Gaussian-like smoothing of images.",
        "bibtex": "@inproceedings{NIPS1988_36366388,\n author = {Harris, John},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {An Analog VLSI Chip for Thin-Plate Surface Interpolation},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/3636638817772e42b59d74cff571fbb3-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/3636638817772e42b59d74cff571fbb3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/3636638817772e42b59d74cff571fbb3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1352342,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1103599524825693459&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "California Institute of Technology, Computation and Neural Systems Option, 216-76, Pasadena, CA 91125",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "California Institute of Technology, Computation and Neural Systems Option, 216-76, Pasadena, CA 91125",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "4f3e4a1576",
        "title": "An Application of the Principle of Maximum Information Preservation to Linear Systems",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/ec8956637a99787bd197eacd77acce5e-Abstract.html",
        "author": "Ralph Linsker",
        "abstract": "This paper addresses the problem of determining the weights for a  set  of  linear  filters  (model  \"cells\")  so  as  to  maximize  the  ensemble-averaged information that the cells' output values jointly  convey about their input values,  given  the  statistical properties of  the ensemble of input vectors.  The quantity that is maximized is the  Shannon  information  rate,  or  equivalently  the  average  mutual  information between input and output.  Several models for the role  of processing noise are analyzed, and the biological motivation for  considering them is described.  For simple models in which nearby  input  signal  values  (in  space  or  time)  are  correlated,  the  cells  resulting  from  this  optimization  process  include  center-surround  cells and cells sensitive to temporal variations in input signal.",
        "bibtex": "@inproceedings{NIPS1988_ec895663,\n author = {Linsker, Ralph},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {An Application of the Principle of Maximum Information Preservation to Linear Systems},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/ec8956637a99787bd197eacd77acce5e-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/ec8956637a99787bd197eacd77acce5e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/ec8956637a99787bd197eacd77acce5e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1990039,
        "gs_citation": 302,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3003565293250461474&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "IBM T. J. Watson Research Center, Yorktown Heights, NY 10598",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "IBM T. J. Watson Research Center",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ibm.com/research/watson",
        "aff_unique_abbr": "IBM Watson",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Yorktown Heights",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "80e477b600",
        "title": "An Electronic Photoreceptor Sensitive to Small Changes in Intensity",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/3988c7f88ebcb58c6ce932b957b6f332-Abstract.html",
        "author": "Tobi Delbr\u00fcck; C. A. Mead",
        "abstract": "We describe an electronic photoreceptor circuit that is sensitive to  small changes in incident light intensity. The sensitivity to change8  in the intensity is achieved by feeding back to the input a filtered  version of the output. The feedback loop includes a hysteretic el(cid:173) ement. The circuit behaves in a manner reminiscent of the gain  control properties and temporal responses of a variety of retinal  cells, particularly retinal bipolar cells. We compare the thresholds  for detection of intensity increments by a human and by the cir(cid:173) cuit. Both obey Weber's law and for both the temporal contrast  sensitivities are nearly identical.",
        "bibtex": "@inproceedings{NIPS1988_3988c7f8,\n author = {Delbr\\\"{u}ck, Tobi and Mead, C. A.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {An Electronic Photoreceptor Sensitive to Small Changes in Intensity},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/3988c7f88ebcb58c6ce932b957b6f332-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/3988c7f88ebcb58c6ce932b957b6f332-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/3988c7f88ebcb58c6ce932b957b6f332-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1537759,
        "gs_citation": 63,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7224636580969144105&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "California Institute of Technology; California Institute of Technology",
        "aff_domain": "; ",
        "email": "; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "California Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.caltech.edu",
        "aff_unique_abbr": "Caltech",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pasadena",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "300986cb64",
        "title": "An Information Theoretic Approach to Rule-Based Connectionist Expert Systems",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/7ef605fc8dba5425d6965fbd4c8fbe1f-Abstract.html",
        "author": "Rodney M. Goodman; John W. Miller; Padhraic Smyth",
        "abstract": "We discuss in this paper architectures for executing probabilistic rule-bases in a par(cid:173) allel manner,  using  as  a theoretical basis recently introduced information-theoretic  models.  We will begin by describing our (non-neural) learning algorithm and theory  of quantitative rule  modelling, followed  by  a discussion on  the exact nature of two  particular models.  Finally we work through an example of our approach, going from  database to rules to inference network, and compare the network's performance with  the theoretical limits for  specific  problems.",
        "bibtex": "@inproceedings{NIPS1988_7ef605fc,\n author = {Goodman, Rodney and Miller, John and Smyth, Padhraic},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {An Information Theoretic Approach to Rule-Based Connectionist Expert Systems},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/7ef605fc8dba5425d6965fbd4c8fbe1f-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/7ef605fc8dba5425d6965fbd4c8fbe1f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/7ef605fc8dba5425d6965fbd4c8fbe1f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1874498,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4531191348297451163&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "1f4370fb99",
        "title": "An Optimality Principle for Unsupervised Learning",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/e00da03b685a0dd18fb6a08af0923de0-Abstract.html",
        "author": "Terence D. Sanger",
        "abstract": "We propose an optimality  principle for  training an unsu(cid:173) pervised feedforward neural network based upon maximal  ability to reconstruct the input data from the network out(cid:173) puts.  We describe an algorithm which can be used to train  either  linear or  nonlinear  networks  with  certain  types  of  nonlinearity.  Examples of applications  to the problems of  image  coding,  feature  detection,  and analysis  of random(cid:173) dot stereograms are presented.",
        "bibtex": "@inproceedings{NIPS1988_e00da03b,\n author = {Sanger, Terence},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {An Optimality Principle for Unsupervised Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/e00da03b685a0dd18fb6a08af0923de0-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/e00da03b685a0dd18fb6a08af0923de0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/e00da03b685a0dd18fb6a08af0923de0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2235978,
        "gs_citation": 216,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10451786928460255218&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "MIT AI Laboratory, NE43-743",
        "aff_domain": "wheaties.ai.mit.edu",
        "email": "wheaties.ai.mit.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "MIT AI Laboratory, NE43-743",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "2f0c0485a7",
        "title": "Analog Implementation of Shunting Neural Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/ac627ab1ccbdb62ec96e702f07f6425b-Abstract.html",
        "author": "Bahram Nabet; Robert B. Darling; Robert B. Pinter",
        "abstract": "An  extremely  compact,  all  analog  and  fully  parallel  implementa(cid:173) tion  of a  class  of shunting  recurrent  neural  networks  that  is  ap(cid:173) plicable to a  wide variety of FET-based integration  technologies is  proposed.  While the contrast enhancement, data compression, and  adaptation to mean input intensity capabilities of the network  are  well suited for  processing of sensory information or feature  extrac(cid:173) tion for a content addressable memory (CAM) system, the network  also admits a global Liapunov function  and can thus achieve stable  CAM storage  itself.  In  addition  the model  can  readily function  as  a front-end  processor to an analog adaptive resonance  circuit.",
        "bibtex": "@inproceedings{NIPS1988_ac627ab1,\n author = {Nabet, Bahram and Darling, Robert and Pinter, Robert},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Analog Implementation of Shunting Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/ac627ab1ccbdb62ec96e702f07f6425b-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/ac627ab1ccbdb62ec96e702f07f6425b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/ac627ab1ccbdb62ec96e702f07f6425b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1341667,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16647846893208272662&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "5dcc677fab",
        "title": "Analyzing the Energy Landscapes of Distributed Winner-Take-All Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/b73ce398c39f506af761d2277d853a92-Abstract.html",
        "author": "David S. Touretzky",
        "abstract": "DCPS  (the  Distributed  Connectionist  Production System)  is  a  neural  network  with  complex  dynamical  properties.  Visualizing  the  energy  landscapes of some of its component modules leads to a  better intuitive  understanding  of the  model,  and  suggests  ways  in  which  its  dynamics  can be controlled in order to improve performance on difficult  cases.",
        "bibtex": "@inproceedings{NIPS1988_b73ce398,\n author = {Touretzky, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Analyzing the Energy Landscapes of Distributed Winner-Take-All Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/b73ce398c39f506af761d2277d853a92-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/b73ce398c39f506af761d2277d853a92-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/b73ce398c39f506af761d2277d853a92-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1866690,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15842801328209045010&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "School of Computer Science, Carnegie Mellon University",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "9d7a9046c9",
        "title": "Applications of Error Back-Propagation to Phonetic Classification",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/2723d092b63885e0d7c260cc007e8b9d-Abstract.html",
        "author": "Hong C. Leung; Victor W. Zue",
        "abstract": "This paper is concerced with the use of error back-propagation  in phonetic classification. Our objective is to investigate the ba(cid:173) sic characteristics of back-propagation, and study how the frame(cid:173) work of multi-layer perceptrons can be exploited in phonetic recog(cid:173) nition. We explore issues such as integration of heterogeneous  sources of information, conditioll~ that can affect performance of  phonetic classification, internal representations, comparisons with  traditional pattern classification techniques, comparisons of differ(cid:173) ent error metrics, and initialization of the network. Our investiga(cid:173) tion is performed within a set of experiments that attempts to rec(cid:173) ognize the 16 vowels in American English independent of speaker.  Our results are comparable to human performance.",
        "bibtex": "@inproceedings{NIPS1988_2723d092,\n author = {Leung, Hong and Zue, Victor W.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Applications of Error Back-Propagation to Phonetic Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/2723d092b63885e0d7c260cc007e8b9d-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/2723d092b63885e0d7c260cc007e8b9d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/2723d092b63885e0d7c260cc007e8b9d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1773224,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9191181045545334107&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Spoken Language Systems Group, Laboratory for Computer Science, Massachusetts Institute of Technology; Spoken Language Systems Group, Laboratory for Computer Science, Massachusetts Institute of Technology",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Spoken Language Systems Group, Laboratory for Computer Science, Massachusetts Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "8061a1c5d9",
        "title": "Associative Learning via Inhibitory Search",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/1385974ed5904a438616ff7bdb3f7439-Abstract.html",
        "author": "David H. Ackley",
        "abstract": "ALVIS is  a  reinforcement-based  connectionist  architecture  that  learns  associative  maps  in  continuous  multidimensional  environ(cid:173) ments.  The  discovered  locations  of  positive  and  negative  rein(cid:173) forcements  are  recorded  in  \"do be\"  and  \"don't  be\"  subnetworks,  respectively.  The outputs of the subnetworks relevant  to the cur(cid:173) rent goal are combined and compared with the current location to  produce  an  error  vector.  This  vector  is  backpropagated  through  a  motor-perceptual  mapping  network.  to  produce  an  action  vec(cid:173) tor that leads the system towards do-be locations  and  away from  don 't-be locations.  AL VIS is  demonstrated with a simulated robot  posed a  target-seeking task.",
        "bibtex": "@inproceedings{NIPS1988_1385974e,\n author = {Ackley, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Associative Learning via Inhibitory Search},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/1385974ed5904a438616ff7bdb3f7439-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/1385974ed5904a438616ff7bdb3f7439-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/1385974ed5904a438616ff7bdb3f7439-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1881515,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12067872846500419096&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "cad96f0e44",
        "title": "Automatic Local Annealing",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/42a0e188f5033bc65bf8d78622277c4e-Abstract.html",
        "author": "Jared Leinbach",
        "abstract": "This research involves a method for finding global maxima  in  constraint  satisfaction  networks.  It  is  an  annealing  process  butt  unlike  most  otherst  requires  no  annealing  schedule.  Temperature  is  instead  determined  locally  by  units at each updatet and thus all processing is done at the  unit  level.  There  are  two  major  practical  benefits  to  processing  this  way:  1)  processing  can continue  in  'bad t  areas of the networkt while 'good t areas remain stablet and  2)  processing  continues  in  the  'bad t  areast as  long  as  the  constraints  remain  poorly  satisfied  (i.e.  it  does  not  stop  after  some  predetermined  number of cycles).  As a  resultt  this  method  not  only  avoids  the  kludge  of requiring  an  externally determined annealing schedulet but it also finds  global  maxima  more  quickly  and  consistently  than  externally  scheduled  systems  the  to  Boltzmann machine (Ackley et alt 1985) is made).  FinallYt  implementation of this method is computationally trivial.",
        "bibtex": "@inproceedings{NIPS1988_42a0e188,\n author = {Leinbach, Jared},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Automatic Local Annealing},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/42a0e188f5033bc65bf8d78622277c4e-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/42a0e188f5033bc65bf8d78622277c4e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/42a0e188f5033bc65bf8d78622277c4e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1556268,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12339623971353946018&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Psychology, Carnegie-Mellon University",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Department of Psychology, Carnegie-Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "aa39d2cdeb",
        "title": "Backpropagation and Its Application to Handwritten Signature Verification",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/65b9eea6e1cc6bb9f0cd2a47751a186f-Abstract.html",
        "author": "Timothy S. Wilkinson; Dorothy A. Mighell; Joseph W. Goodman",
        "abstract": "A  pool  of handwritten  signatures  is  used  to  train  a  neural  net(cid:173) work for the task of deciding whether or not a  given signature is a  forgery.  The network is  a feedforward  net, with a binary image as  input.  There is a hidden layer, with a single unit output layer.  The  weights are  adjusted according to the backpropagation algorithm.  The signatures are entered into a C  software program through the  use of a Datacopy Electronic Digitizing Camera.  The binary signa(cid:173) tures  are normalized  and centered.  The performance is  examined  as  a  function of the training set  and network structure.  The  best  scores  are  on  the  order of 2%  true signature  rejection  with  2-4%  false  signature acceptance.",
        "bibtex": "@inproceedings{NIPS1988_65b9eea6,\n author = {Wilkinson, Timothy and Mighell, Dorothy and Goodman, Joseph},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Backpropagation and Its Application to Handwritten Signature Verification},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/65b9eea6e1cc6bb9f0cd2a47751a186f-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/65b9eea6e1cc6bb9f0cd2a47751a186f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/65b9eea6e1cc6bb9f0cd2a47751a186f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1577069,
        "gs_citation": 72,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10627907073576096&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "1752ea3073",
        "title": "Comparing Biases for Minimal Network Construction with Back-Propagation",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/1c9ac0159c94d8d0cbedc973445af2da-Abstract.html",
        "author": "Stephen Jose Hanson; Lorien Y. Pratt",
        "abstract": "learning",
        "bibtex": "@inproceedings{NIPS1988_1c9ac015,\n author = {Hanson, Stephen and Pratt, Lorien},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Comparing Biases for Minimal Network Construction with Back-Propagation},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/1c9ac0159c94d8d0cbedc973445af2da-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/1c9ac0159c94d8d0cbedc973445af2da-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/1c9ac0159c94d8d0cbedc973445af2da-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1578993,
        "gs_citation": 1020,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8960952083603343914&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Rutgers University; Bell Communications Research + Cognitive Science Laboratory, 221 Nassau Street, Princeton University",
        "aff_domain": "; ",
        "email": "; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+2",
        "aff_unique_norm": "Rutgers University;Bell Communications Research;Cognitive Science Laboratory, 221 Nassau Street, Princeton University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.rutgers.edu;;",
        "aff_unique_abbr": "Rutgers;;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;",
        "aff_country_unique": "United States;"
    },
    {
        "id": "4ab8d598ec",
        "title": "Computer Modeling of Associative Learning",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/069059b7ef840f0c74a814ec9237b6ec-Abstract.html",
        "author": "Daniel L. Alkon; Francis K. H. Quek; Thomas P. Vogl",
        "abstract": "Abstract Unavailable",
        "bibtex": "@inproceedings{NIPS1988_069059b7,\n author = {Alkon, Daniel and Quek, Francis and Vogl, Thomas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Computer Modeling of Associative Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/069059b7ef840f0c74a814ec9237b6ec-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/069059b7ef840f0c74a814ec9237b6ec-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/069059b7ef840f0c74a814ec9237b6ec-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 3549740,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4461084736813337268&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Laboratory for Cellular and Molecular NeurobiologYt NINCDSt NIHt Bethesdat MD 20892; Environmental Research Institute of Michigan+P.O. Box 8G18t Ann Arbort MI 48107; Environmental Research Institute of Michigan+1501 Wilson Blvd.t Suite 1105t Arlington t VA 22209",
        "aff_domain": "; ; ",
        "email": "; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+2;1+3",
        "aff_unique_norm": "Laboratory for Cellular and Molecular NeurobiologYt NINCDSt NIHt Bethesdat MD 20892;Environmental Research Institute of Michigan;P.O. Box 8G18t Ann Arbort MI 48107;1501 Wilson Blvd.t Suite 1105t Arlington t VA 22209",
        "aff_unique_dep": ";;;",
        "aff_unique_url": ";;;",
        "aff_unique_abbr": ";;;",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": ";",
        "aff_country_unique": ""
    },
    {
        "id": "f0e76180b6",
        "title": "Connectionist Learning of Expert Preferences by Comparison Training",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/a8baa56554f96369ab93e4f3bb068c22-Abstract.html",
        "author": "Gerald Tesauro",
        "abstract": "A  new  training  paradigm,  caned  the  \"eomparison  pa.radigm,\"  is  introduced  for  tasks in which  a. network must  learn  to choose  a  prdcrred  pattern from  a  set of n  alternatives,  based on  examplcs of Imma.n  expert  prderences.  In this  pa.radigm,  the inpu t  to  the network consists of t.wo  uf the  n  alterna tives,  and  the  trained  output is  the expert's judgement of which  pa.ttern is  better.  This  para.digm is  applied  to  the lea,rning  of hackgammon,  a  difficult  board ga.me in  wllieh  the  expert selects  a  move  from  a. set,  of legal  mm\u00b7es.  \\Vith  compa.rison  training,  much  higher  levels  of performance  can  hc  a.chiew~d, with  networks  that  are  much  smaller,  and  with  coding  sehemes  t.hat  are  much  simpler  and  easier  to  understand.  Furthermorf',  it  is  possible  to  set  up  the  network  so  tha.t  it  always  produces  consisten t  rank-orderings .",
        "bibtex": "@inproceedings{NIPS1988_a8baa565,\n author = {Tesauro, Gerald},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Connectionist Learning of Expert Preferences by Comparison Training},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/a8baa56554f96369ab93e4f3bb068c22-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/a8baa56554f96369ab93e4f3bb068c22-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/a8baa56554f96369ab93e4f3bb068c22-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1931567,
        "gs_citation": 125,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8666478466858282656&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "IBM Thomas J. Watson Research Center",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "IBM",
        "aff_unique_dep": "Research",
        "aff_unique_url": "https://www.ibm.com/research",
        "aff_unique_abbr": "IBM",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Yorktown Heights",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2d9f11b169",
        "title": "Consonant Recognition by Modular Construction of Large Phonemic Time-Delay Neural Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/eecca5b6365d9607ee5a9d336962c534-Abstract.html",
        "author": "Alex Waibel",
        "abstract": "In this paperl  we show that neural networks for speech recognition can be constructed in  a  modular  fashion  by  exploiting  the  hidden  structure  of previously  trained  phonetic  subcategory networks.  The performance of resulting larger phonetic nets was found to be  as  good  as  the  performance  of the  subcomponent  nets  by  themselves.  This  approach  avoids the excessive learning times  that would be necessary to  train larger networks and  allows  for  incremental  learning.  Large  time-delay  neural  networks  constructed  incrementally  by  applying  these  modular  training  techniques  achieved  a  recognition  performance of 96.0% for all consonants.",
        "bibtex": "@inproceedings{NIPS1988_eecca5b6,\n author = {Waibel, Alex},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Consonant Recognition by Modular Construction of Large Phonemic Time-Delay Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/eecca5b6365d9607ee5a9d336962c534-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/eecca5b6365d9607ee5a9d336962c534-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/eecca5b6365d9607ee5a9d336962c534-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2119177,
        "gs_citation": 202,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8804886292475741630&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "159ad5d3dc",
        "title": "Constraints on Adaptive Networks for Modeling Human Generalization",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/f0935e4cd5920aa6c7c996a5ee53a70f-Abstract.html",
        "author": "Mark A. Gluck; M. Pavel; Van Henkle",
        "abstract": "The potential of adaptive  networks  to learn categorization rules and to  model  human  performance  is  studied  by  comparing  how  natural  and  artificial systems respond to new inputs, i.e., how they generalize.  Like  humans,  networks  can  learn  a  detenninistic  categorization  task  by  a  variety  of  alternative  individual  solutions.  An  analysis  of  the  con(cid:173) straints imposed by using networks with the minimal number of hidden  units  shows  that  this  \"minimal  configuration\"  constraint  is  not  sufficient to explain and predict human performance;  only a few  solu(cid:173) tions  were  found  to be  shared by both  humans and  minimal  adaptive  networks.  A  further  analysis  of human  and  network  generalizations  indicates  that  initial  conditions  may  provide  important constraints  on  generalization.  A new  technique,  which  we  call  \"reversed learning\",  is described for finding appropriate initial conditions.",
        "bibtex": "@inproceedings{NIPS1988_f0935e4c,\n author = {Gluck, Mark and Pavel, M. and Henkle, Van},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Constraints on Adaptive Networks for Modeling Human Generalization},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/f0935e4cd5920aa6c7c996a5ee53a70f-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/f0935e4cd5920aa6c7c996a5ee53a70f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/f0935e4cd5920aa6c7c996a5ee53a70f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1997443,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15822851153648001248&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "ed69d132f5",
        "title": "Convergence and Pattern-Stabilization in the Boltzmann Machine",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/c45147dee729311ef5b5c3003946c48f-Abstract.html",
        "author": "Moshe Kam; Roger Cheng",
        "abstract": "The Boltzmann Machine has been introduced as a means to perform  global optimization for multimodal objective functions using the  principles of simulated annealing. In this paper we consider its utility  as a spurious-free content-addressable memory, and provide bounds on  its performance in this context. We show how to exploit the machine's  ability to escape local minima, in order to use it, at a constant  temperature, for unambiguous associative pattern-retrieval in noisy  environments. An association rule, which creates a sphere of influence  around each stored pattern, is used along with the Machine's dynamics  to match the machine's noisy input with one of the pre-stored patterns.  Spurious fIxed points, whose regions of attraction are not recognized by  the rule, are skipped, due to the Machine's fInite probability to escape  from any state. The results apply to the Boltzmann machine and to the  asynchronous net of binary threshold elements (Hopfield model'). They  provide the network designer with worst-case and best-case bounds for  the network's performance, and allow polynomial-time tradeoff studies  of design parameters.",
        "bibtex": "@inproceedings{NIPS1988_c45147de,\n author = {Kam, Moshe and Cheng, Roger},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Convergence and Pattern-Stabilization in the Boltzmann Machine},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/c45147dee729311ef5b5c3003946c48f-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/c45147dee729311ef5b5c3003946c48f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/c45147dee729311ef5b5c3003946c48f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1721938,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6353624268144738001&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Dept. of Electrical and Computer Eng., Drexel University, Philadelphia PA 19104; Dept. of Electrical Eng., Princeton University, NJ 08544",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Dept. of Electrical and Computer Eng., Drexel University, Philadelphia PA 19104;Dept. of Electrical Eng., Princeton University, NJ 08544",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "f3c84104b6",
        "title": "Cricket Wind Detection",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/7f6ffaa6bb0b408017b62254211691b5-Abstract.html",
        "author": "John P. Miller",
        "abstract": "Abstract Unavailable",
        "bibtex": "@inproceedings{NIPS1988_7f6ffaa6,\n author = {Miller, John},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Cricket Wind Detection},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/7f6ffaa6bb0b408017b62254211691b5-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/7f6ffaa6bb0b408017b62254211691b5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/7f6ffaa6bb0b408017b62254211691b5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1092942,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8583396564977244003&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Neurobiology Group, University of California, Berkeley, California 94720, U.S.A.",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Neurobiology Group, University of California, Berkeley, California 94720, U.S.A.",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "84f5d35fb6",
        "title": "Digital Realisation of Self-Organising Maps",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/c9e1074f5b3f9fc8ea15d152add07294-Abstract.html",
        "author": "Nigel M. Allinson; Martin J. Johnson; Kevin J. Moon",
        "abstract": "Kevin J. Moon",
        "bibtex": "@inproceedings{NIPS1988_c9e1074f,\n author = {Allinson, Nigel and Johnson, Martin and Moon, Kevin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Digital Realisation of Self-Organising Maps},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/c9e1074f5b3f9fc8ea15d152add07294-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/c9e1074f5b3f9fc8ea15d152add07294-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/c9e1074f5b3f9fc8ea15d152add07294-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1937371,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8050186024767345051&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Electronics, University of York; Department of Electronics, University of York; ",
        "aff_domain": "; ; ",
        "email": "; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Department of Electronics, University of York",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "05fbe149a3",
        "title": "Does the Neuron \"Learn\" like the Synapse?",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/6cdd60ea0045eb7a6ec44c54d29ed402-Abstract.html",
        "author": "Raoul Tawel",
        "abstract": "An improved learning paradigm that offers a significant reduction in com(cid:173)\nputation time during the supervised learning phase is described. \nIt is based on \nextending the role that the neuron plays in artificial neural systems. Prior work \nhas regarded the neuron as a strictly passive, non-linear processing element, and \nthe synapse on the other hand as the primary source of information processing and \nknowledge retention. In this work, the role of the neuron is extended insofar as allow(cid:173)\ning its parameters to adaptively participate in the learning phase. The temperature \nof the sigmoid function is an example of such a parameter. During learning, both the \nsynaptic interconnection weights w[j and the neuronal temperatures Tr are opti(cid:173)\nmized so as to capture the knowledge contained within the training set. The method \nallows each neuron to possess and update its own characteristic local temperature. \nThis algorithm has been applied to logic type of problems such as the XOR or parity \nproblem, resulting in a significant decrease in the required number of training cycles.",
        "bibtex": "@inproceedings{NIPS1988_6cdd60ea,\n author = {Tawel, Raoul},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Does the Neuron \"Learn\" like the Synapse?},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/6cdd60ea0045eb7a6ec44c54d29ed402-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/6cdd60ea0045eb7a6ec44c54d29ed402-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/6cdd60ea0045eb7a6ec44c54d29ed402-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1513205,
        "gs_citation": 56,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12718844280184987464&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "e9ce465c84",
        "title": "Dynamic, Non-Local Role Bindings and Inferencing in a Localist Network for Natural Language Understanding",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/fa7cdfad1a5aaf8370ebeda47a1ff1c3-Abstract.html",
        "author": "Trent E. Lange; Michael G. Dyer",
        "abstract": "This  paper introduces  a means to  handle the critical problem  of non(cid:173) local  role-bindings  in  localist  spreading-activation  networks.  Every  conceptual node in the network broadcasts a stable, uniquely-identifying  activation pattern, called its signature.  A dynamic role-binding is cre(cid:173) ated  when  a  role's  binding  node  has  an  activation  that  matches  the  bound concept's signature.  Most importantly, signatures are propagated  across long paths of nodes to handle the non-local role-bindings neces(cid:173) sary  for  inferencing.  Our  localist  network  model,  ROBIN  (ROle  Binding  and  Inferencing  Network),  uses  signature  activations  to  ro(cid:173) bustly represent schemata role-bindings and thus perfonn the inferenc(cid:173) ing, plan/goal analysis,  schema instantiation, word-sense disambigua(cid:173) tion, and dynamic re-interpretation portions of the natural language un(cid:173) derstanding process.",
        "bibtex": "@inproceedings{NIPS1988_fa7cdfad,\n author = {Lange, Trent and Dyer, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Dynamic, Non-Local Role Bindings and Inferencing in a Localist Network for Natural Language Understanding},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/fa7cdfad1a5aaf8370ebeda47a1ff1c3-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/fa7cdfad1a5aaf8370ebeda47a1ff1c3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/fa7cdfad1a5aaf8370ebeda47a1ff1c3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2038339,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17530618550603011038&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Artificial Intelligence Laboratory, Computer Science Department, University of California, Los Angeles; Artificial Intelligence Laboratory, Computer Science Department, University of California, Los Angeles",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Artificial Intelligence Laboratory, Computer Science Department, University of California, Los Angeles",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "c006a58d5f",
        "title": "Dynamics of Analog Neural Networks with Time Delay",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/698d51a19d8a121ce581499d7b701668-Abstract.html",
        "author": "Charles M. Marcus; R. M. Westervelt",
        "abstract": "A time delay in the response of the neurons in a network can  induce sustained oscillation and chaos. We present a stability  criterion based on local stability analysis to prevent sustained  oscillation  in  symmetric  delay  networks,  and  show  an  example  of chaotic  dynamics  in  a  non-symmetric  delay  network.",
        "bibtex": "@inproceedings{NIPS1988_698d51a1,\n author = {Marcus, Charles and Westervelt, R.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Dynamics of Analog Neural Networks with Time Delay},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/698d51a19d8a121ce581499d7b701668-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/698d51a19d8a121ce581499d7b701668-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/698d51a19d8a121ce581499d7b701668-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2114968,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6254998388266768804&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Division of Applied Sciences and Department of Physics, Harvard University, Cambridge Massachusetts 02138; Division of Applied Sciences and Department of Physics, Harvard University, Cambridge Massachusetts 02138",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Division of Applied Sciences and Department of Physics, Harvard University, Cambridge Massachusetts 02138",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "d1faeeaaab",
        "title": "Efficient Parallel Learning Algorithms for Neural Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/02522a2b2726fb0a03bb19f2d8d9524d-Abstract.html",
        "author": "Alan H. Kramer; Alberto Sangiovanni-Vincentelli",
        "abstract": "Parallelizable optimization techniques are applied to the problem of  learning in feedforward neural networks. In addition to having supe(cid:173) rior convergence properties, optimization techniques such as the Polak(cid:173) Ribiere method are also significantly more efficient than the Back(cid:173) propagation algorithm. These results are based on experiments per(cid:173) formed on small boolean learning problems and the noisy real-valued  learning problem of hand-written character recognition.",
        "bibtex": "@inproceedings{NIPS1988_02522a2b,\n author = {Kramer, Alan and Sangiovanni-Vincentelli, Alberto},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Efficient Parallel Learning Algorithms for Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/02522a2b2726fb0a03bb19f2d8d9524d-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/02522a2b2726fb0a03bb19f2d8d9524d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/02522a2b2726fb0a03bb19f2d8d9524d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2168599,
        "gs_citation": 276,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2623772148189790156&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of EECS, U.C. Berkeley; Department of EECS, U.C. Berkeley",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Department of EECS, U.C. Berkeley",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "24a5913af3",
        "title": "Electronic Receptors for Tactile/Haptic Sensing",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/76dc611d6ebaafc66cc0879c71b5db5c-Abstract.html",
        "author": "Andreas G. Andreou",
        "abstract": "We discuss synthetic receptors for  haptic sensing. These are based on  magnetic field sensors (Hall effect structures) fabricated using standard  CMOS technologies.  These receptors, biased with a small permanent  magnet can detect the presence of ferro or ferri-magnetic objects in the  vicinity of the sensor. They can also detect the magnitude and direction  of the magnetic field.",
        "bibtex": "@inproceedings{NIPS1988_76dc611d,\n author = {Andreou, Andreas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Electronic Receptors for Tactile/Haptic Sensing},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/76dc611d6ebaafc66cc0879c71b5db5c-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/76dc611d6ebaafc66cc0879c71b5db5c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/76dc611d6ebaafc66cc0879c71b5db5c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1690850,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14591972318383670322&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "a7260d62b1",
        "title": "Fast Learning in Multi-Resolution Hierarchies",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/82161242827b703e6acf9c726942a1e4-Abstract.html",
        "author": "John Moody",
        "abstract": "A class of fast, supervised learning algorithms is presented. They use lo(cid:173)",
        "bibtex": "@inproceedings{NIPS1988_82161242,\n author = {Moody, John},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Fast Learning in Multi-Resolution Hierarchies},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/82161242827b703e6acf9c726942a1e4-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/82161242827b703e6acf9c726942a1e4-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/82161242827b703e6acf9c726942a1e4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2415488,
        "gs_citation": 296,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16558197821153047915&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Yale Computer Science, P.O. Box 2158, New Haven, CT 06520",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Yale Computer Science, P.O. Box 2158, New Haven, CT 06520",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "e3430900d1",
        "title": "Fixed Point Analysis for Recurrent Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/fc221309746013ac554571fbd180e1c8-Abstract.html",
        "author": "Patrice Y. Simard; Mary B. Ottaway; Dana H. Ballard",
        "abstract": "This paper provides a systematic analysis of the recurrent backpropaga(cid:173) tion (RBP) algorithm, introducing a number of new results. The main  limitation of the RBP algorithm is that it assumes the convergence of  the network to a stable fixed point in order to backpropagate the error  signals. We show by experiment and eigenvalue analysis that this condi(cid:173) tion can be violated and that chaotic behavior can be avoided. Next we  examine the advantages of RBP over the standard backpropagation al(cid:173) gorithm. RBP is shown to build stable fixed points corresponding to the  input patterns. This makes it an appropriate tool for content address(cid:173) able memories, one-to-many function learning, and inverse problems.",
        "bibtex": "@inproceedings{NIPS1988_fc221309,\n author = {Simard, Patrice and Ottaway, Mary and Ballard, Dana},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Fixed Point Analysis for Recurrent Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/fc221309746013ac554571fbd180e1c8-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/fc221309746013ac554571fbd180e1c8-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/fc221309746013ac554571fbd180e1c8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2028833,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13942591970760131812&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Dept. of Computer Science, University of Rochester; Dept. of Computer Science, University of Rochester;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Rochester",
        "aff_unique_dep": "Dept. of Computer Science",
        "aff_unique_url": "https://www.rochester.edu",
        "aff_unique_abbr": "U of R",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "01601989fa",
        "title": "Further Explorations in Visually-Guided Reaching: Making MURPHY Smarter",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/cedebb6e872f539bef8c3f919874e9d7-Abstract.html",
        "author": "Bartlett W. Mel",
        "abstract": "MURPHY  is  a  vision-based  kinematic  controller  and  path  planner  based  on  a  connectionist  architecture,  and  implemented  with  a  video  camera and  Rhino XR-series robot  arm.  Imitative of the layout of sen(cid:173) sory  and motor maps in  cerebral cortex,  MURPHY'S internal representa(cid:173) tions  consist of four  coarse-coded populations of simple units represent(cid:173) ing both static and  dynamic aspects of the sensory-motor environment.  In previously reported work [4],  MURPHY first  learned a direct kinematic  model of his  camera-arm system during  a  period  of extended  practice,  and  then  used  this  \"mental  model\"  to  heuristically  guide  his  hand  to  unobstructed  visual  targets.  MURPHY  has  since  been  extended  in  two  ways:  First, he  now  learns the inverse differential-kinematics of his  arm  in  addition to ordinary direct  kinematics, which  allows  him to push  his  hand  directly towards  a  visual  target  without  the need  for  search.  Sec(cid:173) ondly,  he now  deals with the much more difficult problem of reaching in  the presence of obstacles.",
        "bibtex": "@inproceedings{NIPS1988_cedebb6e,\n author = {Mel, Bartlett},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Further Explorations in Visually-Guided Reaching: Making MURPHY Smarter},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/cedebb6e872f539bef8c3f919874e9d7-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/cedebb6e872f539bef8c3f919874e9d7-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/cedebb6e872f539bef8c3f919874e9d7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1819686,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16293459201692907453&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "8c7b23fdef",
        "title": "GEMINI: Gradient Estimation Through Matrix Inversion After Noise Injection",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/a0a080f42e6f13b3a2df133f073095dd-Abstract.html",
        "author": "Yann Le Cun; Conrad C. Galland; Geoffrey E. Hinton",
        "abstract": "Learning procedures that measure how random perturbations of unit ac(cid:173) tivities correlate with changes in reinforcement are inefficient but simple  to implement in hardware. Procedures like back-propagation (Rumelhart,  Hinton and Williams, 1986) which compute how changes in activities af(cid:173) fect the output error are much more efficient, but require more complex  hardware. GEMINI is a hybrid procedure for multilayer networks, which  shares many of the implementation advantages of correlational reinforce(cid:173) ment procedures but is more efficient. GEMINI injects noise only at the  first hidden layer and measures the resultant effect on the output error.  A linear network associated with each hidden layer iteratively inverts the  matrix which relates the noise to the error change, thereby obtaining  the error-derivatives. No back-propagation is involved, thus allowing un(cid:173) known non-linearities in the system. Two simulations demonstrate the  effectiveness of GEMINI.",
        "bibtex": "@inproceedings{NIPS1988_a0a080f4,\n author = {Le Cun, Yann and Galland, Conrad and Hinton, Geoffrey E},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {GEMINI: Gradient Estimation Through Matrix Inversion After Noise Injection},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/a0a080f42e6f13b3a2df133f073095dd-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/a0a080f42e6f13b3a2df133f073095dd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/a0a080f42e6f13b3a2df133f073095dd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1743419,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17993806492835719737&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science, University of Toronto; Department of Computer Science, University of Toronto; Department of Computer Science, University of Toronto + AT&T Bell Laboratories",
        "aff_domain": "cs.toronto.edu;cs.toronto.edu;cs.toronto.edu",
        "email": "cs.toronto.edu;cs.toronto.edu;cs.toronto.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0+1",
        "aff_unique_norm": "University of Toronto;AT&T Bell Laboratories",
        "aff_unique_dep": "Department of Computer Science;",
        "aff_unique_url": "https://www.utoronto.ca;https://www.att.com/labs",
        "aff_unique_abbr": "U of T;AT&T Bell Labs",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Toronto;",
        "aff_country_unique_index": "0;0;0+1",
        "aff_country_unique": "Canada;United States"
    },
    {
        "id": "5e6c744dce",
        "title": "GENESIS: A System for Simulating Neural Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/4c5bde74a8f110656874902f07378009-Abstract.html",
        "author": "Matthew A. Wilson; Upinder S. Bhalla; John D. Uhley; James M. Bower",
        "abstract": "support  simulations  at  many  it  is",
        "bibtex": "@inproceedings{NIPS1988_4c5bde74,\n author = {Wilson, Matthew and Bhalla, Upinder and Uhley, John and Bower, James},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {GENESIS: A System for Simulating Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/4c5bde74a8f110656874902f07378009-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/4c5bde74a8f110656874902f07378009-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/4c5bde74a8f110656874902f07378009-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1647554,
        "gs_citation": 170,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10364025396623247418&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "187ee59a94",
        "title": "Heterogeneous Neural Networks for Adaptive Behavior in Dynamic Environments",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/2b44928ae11fb9384c4cf38708677c48-Abstract.html",
        "author": "Randall D. Beer; Hillel J. Chiel; Leon S. Sterling",
        "abstract": "Research  in artificial neural networks has genera1ly emphasized  homogeneous architectures. In contrast, the nervous systems of natural  animals exhibit great heterogeneity in both their elements and patterns  of interconnection. This heterogeneity is crucial to the flexible  generation of behavior which is essential for survival in a complex,  dynamic environment. It may also provide powerful insights into the  design of artificial neural networks.  In this paper, we describe a  heterogeneous neural network for controlling  the wa1king of a  simulated insect. This controller is inspired by the neuroethological  It exhibits a  and neurobiological literature on insect locomotion.  variety of statically stable gaits at different speeds simply by varying  the tonic activity of a single cell. It can also adapt to perturbations as a  natural consequence of its design.",
        "bibtex": "@inproceedings{NIPS1988_2b44928a,\n author = {Beer, Randall and Chiel, Hillel and Sterling, Leon S.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Heterogeneous Neural Networks for Adaptive Behavior in Dynamic Environments},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/2b44928ae11fb9384c4cf38708677c48-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/2b44928ae11fb9384c4cf38708677c48-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/2b44928ae11fb9384c4cf38708677c48-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1998803,
        "gs_citation": 78,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3708598706954093562&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Biology Dept. & CAISR CWRU; Dept. of Computer Engineering and Science and Center for Automation and Intelligent Systems Research Case Western Reserve University; CS Dept. & CAISR CWRU",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Biology Dept. & CAISR CWRU;Dept. of Computer Engineering and Science and Center for Automation and Intelligent Systems Research Case Western Reserve University;CS Dept. & CAISR CWRU",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "033a079fe4",
        "title": "Implications of Recursive Distributed Representations",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/5fd0b37cd7dbbb00f97ba6ce92bf5add-Abstract.html",
        "author": "Jordan B. Pollack",
        "abstract": "I  will  describe  my  recent  results  on  the  automatic  development  of fixed(cid:173) width recursive  distributed representations  of variable-sized  hierarchal data  structures.  One  implication  of this  wolk  is  that  certain  types  of AI-style  data-structures can now be represented in fixed-width analog vectors. Simple  inferences  can  be  perfonned  using  the  type  of pattern  associations  that  neural  networks excel  at  Another implication arises from  noting that these  representations  become  self-similar in  the  limit Once  this door to  chaos is  opened.  many  interesting new  questions  about  the  representational  basis  of  intelligence emerge, and can (and will) be discussed.",
        "bibtex": "@inproceedings{NIPS1988_5fd0b37c,\n author = {Pollack, Jordan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Implications of Recursive Distributed Representations},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/5fd0b37cd7dbbb00f97ba6ce92bf5add-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/5fd0b37cd7dbbb00f97ba6ce92bf5add-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/5fd0b37cd7dbbb00f97ba6ce92bf5add-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2523165,
        "gs_citation": 96,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16917916792743785498&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "Laboratory for A I Research, Ohio State University",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Laboratory for A I Research, Ohio State University",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "bbe977a027",
        "title": "Learning Sequential Structure in Simple Recurrent Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/9dcb88e0137649590b755372b040afad-Abstract.html",
        "author": "David Servan-Schreiber; Axel Cleeremans; James L. McClelland",
        "abstract": "We explore a network architecture introduced by Elman (1988) for  predicting successive elements of a sequence. The network uses the  pattern of activation over a set of hidden units from time-step t-l,  together with element t, to predict element t+ 1. When the network is  trained with strings from a particular finite-state grammar, it can learn  to be a perfect finite-state recognizer for the grammar. Cluster analyses  of the hidden-layer patterns of activation showed that they encode  prediction-relevant information about the entire path traversed through  the network. We illustrate the phases of learning with cluster analyses  performed at different points during training.",
        "bibtex": "@inproceedings{NIPS1988_9dcb88e0,\n author = {Servan-Schreiber, David and Cleeremans, Axel and McClelland, James},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Learning Sequential Structure in Simple Recurrent Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/9dcb88e0137649590b755372b040afad-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/9dcb88e0137649590b755372b040afad-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/9dcb88e0137649590b755372b040afad-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1970061,
        "gs_citation": 128,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2613124034738692285&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "54e7bdf5c5",
        "title": "Learning by Choice of Internal Representations",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/5ef059938ba799aaa845e1c2e8a762bd-Abstract.html",
        "author": "Tal Grossman; Ronny Meir; Eytan Domany",
        "abstract": "We  introduce  a  learning algorithm for  multilayer neural  net(cid:173) works  composed of binary linear threshold elements.  Whereas ex(cid:173) isting algorithms reduce  the learning process  to minimizing a  cost  function  over  the  weights,  our  method  treats  the  internal  repre(cid:173) sentations  as  the fundamental entities  to  be  determined.  Once  a  correct set of internal representations is  arrived at, the weights are  found  by  the  local  aild  biologically plausible Perceptron  Learning  Rule  (PLR).  We tested  our  learning algorithm on  four  problems:  adjacency, symmetry, parity and  combined symmetry-parity.",
        "bibtex": "@inproceedings{NIPS1988_5ef05993,\n author = {Grossman, Tal and Meir, Ronny and Domany, Eytan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Learning by Choice of Internal Representations},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/5ef059938ba799aaa845e1c2e8a762bd-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/5ef059938ba799aaa845e1c2e8a762bd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/5ef059938ba799aaa845e1c2e8a762bd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1806138,
        "gs_citation": 94,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1402302056267165447&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "ef928118bd",
        "title": "Learning the Solution to the Aperture Problem for Pattern Motion with a Hebb Rule",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/4c56ff4ce4aaf9573aa5dff913df997a-Abstract.html",
        "author": "Martin I. Sereno",
        "abstract": "to",
        "bibtex": "@inproceedings{NIPS1988_4c56ff4c,\n author = {Sereno, Martin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Learning the Solution to the Aperture Problem for Pattern Motion with a Hebb Rule},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/4c56ff4ce4aaf9573aa5dff913df997a-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/4c56ff4ce4aaf9573aa5dff913df997a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/4c56ff4ce4aaf9573aa5dff913df997a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1982555,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17083203528566641650&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Cognitive Science C-015, University of California, San Diego, La Jolla, CA 92093-0115",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Cognitive Science C-015, University of California, San Diego, La Jolla, CA 92093-0115",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "76062a81cd",
        "title": "Learning with Temporal Derivatives in Pulse-Coded Neuronal Systems",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/f4b9ec30ad9f68f89b29639786cb62ef-Abstract.html",
        "author": "David B. Parker; Mark Gluck; Eric S. Reifsnider",
        "abstract": "A number of learning models have recently been proposed which  involve calculations of temporal differences (or derivatives in  continuous-time models). These models. like most adaptive network  models. are formulated in tenns of frequency (or activation), a useful  abstraction of neuronal firing rates. To more precisely evaluate the  implications of a neuronal model. it may be preferable to develop a  model which transmits discrete pulse-coded information. We point out  that many functions and properties of neuronal processing and learning  may depend. in subtle ways. on the pulse-coded nature of the informa(cid:173) tion coding and transmission properties of neuron systems. When com(cid:173) pared to formulations in terms of activation. computing with temporal  derivatives (or differences) as proposed by Kosko (1986). Klopf  (1988). and Sutton (1988). is both more stable and easier when refor(cid:173) mulated for a more neuronally realistic pulse-coded system. In refor(cid:173) mulating these models in terms of pulse-coding. our motivation has  been to enable us to draw further parallels and connections between  real-time behavioral models of learning and biological circuit models  of the substrates underlying learning and memory.",
        "bibtex": "@inproceedings{NIPS1988_f4b9ec30,\n author = {Parker, David and Gluck, Mark and Reifsnider, Eric},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Learning with Temporal Derivatives in Pulse-Coded Neuronal Systems},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/f4b9ec30ad9f68f89b29639786cb62ef-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/f4b9ec30ad9f68f89b29639786cb62ef-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/f4b9ec30ad9f68f89b29639786cb62ef-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2040131,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13731256934847042780&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Psychology, Stanford University; Department of Psychology, Stanford University; ",
        "aff_domain": "; ; ",
        "email": "; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Department of Psychology",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "50c3067681",
        "title": "Linear Learning: Landscapes and Algorithms",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/202cb962ac59075b964b07152d234b70-Abstract.html",
        "author": "Pierre Baldi",
        "abstract": "Abstract Unavailable",
        "bibtex": "@inproceedings{NIPS1988_202cb962,\n author = {Baldi, Pierre},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Linear Learning: Landscapes and Algorithms},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/202cb962ac59075b964b07152d234b70-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/202cb962ac59075b964b07152d234b70-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/202cb962ac59075b964b07152d234b70-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1455326,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1558052255870535802&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "8a6b9512c3",
        "title": "Links Between Markov Models and Multilayer Perceptrons",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/0777d5c17d4066b82ab86dff8a46af6f-Abstract.html",
        "author": "Herv\u00e9 Bourlard; C. J. Wellekens",
        "abstract": "Hidden Markov models are widely used for automatic speech recog(cid:173) nition. They inherently incorporate the sequential character of the  speech signal and are statistically trained. However, the a-priori  choice of the model topology limits their flexibility. Another draw(cid:173) back of these models is their weak discriminating power. Multilayer  perceptrons are now promising tools in the connectionist approach  for classification problems and have already been successfully tested  on speech recognition problems. However, the sequential nature of  the speech signal remains difficult to handle in that kind of ma(cid:173) chine. In this paper, a discriminant hidden Markov model is de(cid:173) fined and it is shown how a particular multilayer perceptron with  contextual and extra feedback input units can be considered as a  general form of such Markov models.",
        "bibtex": "@inproceedings{NIPS1988_0777d5c1,\n author = {Bourlard, Herv\\'{e} and Wellekens, C. J.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Links Between Markov Models and Multilayer Perceptrons},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/0777d5c17d4066b82ab86dff8a46af6f-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/0777d5c17d4066b82ab86dff8a46af6f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/0777d5c17d4066b82ab86dff8a46af6f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1992509,
        "gs_citation": 449,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=887266263130957852&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Philips Research Laboratory Brussels, B-1170 Belgium + Int. Compo Science Institute Berkeley, CA 94704 USA; Philips Research Laboratory Brussels, B-1170 Belgium + Int. Compo Science Institute Berkeley, CA 94704 USA",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "Philips Research Laboratory Brussels, B-1170 Belgium;Int. Compo Science Institute Berkeley, CA 94704 USA",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": ";",
        "aff_country_unique": ""
    },
    {
        "id": "98be54dc66",
        "title": "Mapping Classifier Systems Into Neural Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/82aa4b0af34c2313a562076992e50aa3-Abstract.html",
        "author": "Lawrence Davis",
        "abstract": "Classifier systems  are  machine  learning  systems  incotporating  a  genetic  al(cid:173)",
        "bibtex": "@inproceedings{NIPS1988_82aa4b0a,\n author = {Davis, Lawrence},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Mapping Classifier Systems Into Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/82aa4b0af34c2313a562076992e50aa3-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/82aa4b0af34c2313a562076992e50aa3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/82aa4b0af34c2313a562076992e50aa3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1806469,
        "gs_citation": 55,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16537004939768968176&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 5,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "fe06a4d033",
        "title": "Modeling Small Oscillating Biological Networks in Analog VLSI",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/1afa34a7f984eeabdbb0a7d494132ee5-Abstract.html",
        "author": "Sylvie Ryckebusch; James M. Bower; Carver Mead",
        "abstract": "We  have used analog VLSI technology to model a class of small os(cid:173) cillating  biological neural circuits  known  as  central pattern  gener(cid:173) ators  (CPG). These circuits generate rhythmic patterns of activity  which drive locomotor behaviour in the animal.  We  have designed,  fabricated,  and tested a model neuron circuit which relies on many  of the  same  mechanisms  as  a  biological  central pattern  generator  neuron,  such  as  delays  and  internal feedback.  We  show  that  this  neuron can be used  to build several small circuits based on known  biological CPG circuits, and that these circuits produce patterns of  output which  are very similar to the observed biological patterns.",
        "bibtex": "@inproceedings{NIPS1988_1afa34a7,\n author = {Ryckebusch, Sylvie and Bower, James and Mead, Carver},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Modeling Small Oscillating Biological Networks in Analog VLSI},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/1afa34a7f984eeabdbb0a7d494132ee5-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/1afa34a7f984eeabdbb0a7d494132ee5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/1afa34a7f984eeabdbb0a7d494132ee5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1689187,
        "gs_citation": 61,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12609781165889753663&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "d2c2582a3a",
        "title": "Modeling the Olfactory Bulb - Coupled Nonlinear Oscillators",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/013d407166ec4fa56eb1e1f8cbe183b9-Abstract.html",
        "author": "Zhaoping Li; John J. Hopfield",
        "abstract": "The olfactory  bulb of mammals  aids  in  the  discrimination  of  odors.  A  mathematical  model  based  on  the  bulbar  anatomy  and  electrophysiology  is  described.  Simulations  produce  a  35-60  Hz  modulated activity coherent across the bulb, mimicing the observed  field  potentials.  The  decision  states  (for  the  odor  information)  here  can  be  thought  of as  stable  cycles,  rather  than  point  stable  states  typical  of simpler  neuro-computing  models.  Analysis  and  simulations show that a  group of coupled non-linear oscillators are  responsible for the oscillatory activities determined by the odor in(cid:173) put, and that the bulb, with appropriate inputs from higher centers,  can  enhance  or suppress  the  sensitivity  to  partiCUlar  odors.  The  model provides a framework  in which to understand the transform  between odor input and the bulbar output to olfactory cortex.",
        "bibtex": "@inproceedings{NIPS1988_013d4071,\n author = {Li, Zhaoping and Hopfield, John J.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Modeling the Olfactory Bulb - Coupled Nonlinear Oscillators},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/013d407166ec4fa56eb1e1f8cbe183b9-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/013d407166ec4fa56eb1e1f8cbe183b9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/013d407166ec4fa56eb1e1f8cbe183b9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1831796,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16948132255605954482&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Division of Physics, Mathematics and Astronomy + Division of Biology + Division of Chemistry and Chemical Engineering, California Institute of Technology, Pasadena, CA 91125, USA; AT&T Bell Laboratories",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1+2;3",
        "aff_unique_norm": "Division of Physics, Mathematics and Astronomy;Division of Biology;Division of Chemistry and Chemical Engineering, California Institute of Technology, Pasadena, CA 91125, USA;AT&T Bell Laboratories",
        "aff_unique_dep": ";;;",
        "aff_unique_url": ";;;https://www.att.com/labs",
        "aff_unique_abbr": ";;;AT&T Bell Labs",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": ";1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "347fa92899",
        "title": "Models of Ocular Dominance Column Formation: Analytical and Computational Results",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/c8ffe9a587b126f152ed3d89a146b445-Abstract.html",
        "author": "Kenneth D. Miller; Joseph B. Keller; Michael P. Stryker",
        "abstract": "We  have  previously  developed  a  simple  mathemati(cid:173)",
        "bibtex": "@inproceedings{NIPS1988_c8ffe9a5,\n author = {Miller, Kenneth and Keller, Joseph and Stryker, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Models of Ocular Dominance Column Formation: Analytical and Computational Results},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/c8ffe9a587b126f152ed3d89a146b445-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/c8ffe9a587b126f152ed3d89a146b445-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/c8ffe9a587b126f152ed3d89a146b445-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 3538900,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9429487358861904771&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "f7e73f6e8b",
        "title": "Neural Analog Diffusion-Enhancement Layer and Spatio-Temporal Grouping in Early Vision",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/e2ef524fbf3d9fe611d5a8e90fefdc9c-Abstract.html",
        "author": "Allen M. Waxman; Michael Seibert; Robert K. Cunningham; Jian Wu",
        "abstract": "A  new  class of neural  network  aimed  at  early  visual  processing  is  described; we call it a Neural  Analog Diffusion-Enhancement Layer or  \"NADEL.\" The  network  consists  of two  levels  which  are  coupled  through feedfoward and shunted feedback connections. The lower level  is  a  two-dimensional  diffusion map which  accepts  visual  features  as  input, and spreads activity over larger scales as a function of time. The  upper layer is periodically fed the  activity from  the diffusion layer and  locates local maxima in it (an extreme form  of contrast enhancement)  using a network of local comparators. These local maxima are fed back  to  the  diffusion  layer  using  an  on-center/off-surround  shunting  anatomy. The maxima are also available  as output of the network.  The  network dynamics  serves  to  cluster features  on  multiple  scales  as  a  function of time, and can be used in a variety of early visual processing  tasks such  as:  extraction of comers  and high  curvature  points  along  edge contours, line end detection, gap filling in contours, generation of  fixation points, perceptual grouping on multiple scales, correspondence  and path impletion  in long-range  apparent  motion,  and building  2-D  shape representations that are invariant to  location, orientation, scale,  and small deformation on the visual field.",
        "bibtex": "@inproceedings{NIPS1988_e2ef524f,\n author = {Waxman, Allen and Seibert, Michael and Cunningham, Robert and Wu, Jian},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Neural Analog Diffusion-Enhancement Layer and Spatio-Temporal Grouping in Early Vision},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/e2ef524fbf3d9fe611d5a8e90fefdc9c-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/e2ef524fbf3d9fe611d5a8e90fefdc9c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/e2ef524fbf3d9fe611d5a8e90fefdc9c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1708936,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7057352378756770171&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "0923620b09",
        "title": "Neural Approach for TV Image Compression Using a Hopfield Type Network",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/bd4c9ab730f5513206b999ec0d90d1fb-Abstract.html",
        "author": "Martine Naillon; Jean-Bernard Theeten",
        "abstract": "A self-organizing Hopfield network has been  developed in the context of Vector Ouantiza(cid:173) -tion, aiming at compression of  television  images. The metastable states of the spin  glass-like network are used as  an extra  the Minimal Overlap  storage resource using  and Mezard 1987) to  rule (Krauth  learning  the organization of the attractors.  optimize  The sel f-organi zi ng  that we have  scheme  devised  the generation of an  in  adaptive codebook for any qiven TV image.",
        "bibtex": "@inproceedings{NIPS1988_bd4c9ab7,\n author = {Naillon, Martine and Theeten, Jean-Bernard},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Neural Approach for TV Image Compression Using a Hopfield Type Network},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/bd4c9ab730f5513206b999ec0d90d1fb-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/bd4c9ab730f5513206b999ec0d90d1fb-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/bd4c9ab730f5513206b999ec0d90d1fb-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1550029,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=70212874630684552&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Laboratoire d'Electronique et de Physique Appliquee*; Laboratoire d'Electronique et de Physique Appliquee*",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Laboratoire d'Electronique et de Physique Appliquee*",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "14dcbeffe5",
        "title": "Neural Architecture",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/5878a7ab84fb43402106c575658472fa-Abstract.html",
        "author": "Valentino Braitenberg",
        "abstract": "Abstract Unavailable",
        "bibtex": "@inproceedings{NIPS1988_5878a7ab,\n author = {Braitenberg, Valentino},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Neural Architecture},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/5878a7ab84fb43402106c575658472fa-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/5878a7ab84fb43402106c575658472fa-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/5878a7ab84fb43402106c575658472fa-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 197098,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "Max Planck Institute",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Max Planck Institute",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.mpiwg-berlin.mpg.de",
        "aff_unique_abbr": "MPI",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "da4ce2b07c",
        "title": "Neural Control of Sensory Acquisition: The Vestibulo-Ocular Reflex",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/006f52e9102a8d3be2fe5614f42ba989-Abstract.html",
        "author": "Michael G. Paulin; Mark E. Nelson; James M. Bower",
        "abstract": "We present a new hypothesis that the cerebellum plays a key role in ac(cid:173) tively controlling the acquisition of sensory infonnation by the nervous  system.  In this paper we explore this idea by examining the function of  a  simple  cerebellar-related  behavior,  the  vestibula-ocular  reflex  or  VOR, in  which  eye movements  are generated to minimize image slip  on  the  retina  during  rapid  head  movements.  Considering  this  system  from  the point of view of statistical estimation theory, our results  sug(cid:173) gest that the transfer function of the VOR, often regarded as a static or  slowly  modifiable  feature  of the  system,  should  actually  be  continu(cid:173) ously and rapidly changed during head movements. We further suggest  that these changes are under the direct control of the cerebellar cortex  and propose experiments to test this hypothesis.",
        "bibtex": "@inproceedings{NIPS1988_006f52e9,\n author = {Paulin, Michael and Nelson, Mark and Bower, James},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Neural Control of Sensory Acquisition: The Vestibulo-Ocular Reflex},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/006f52e9102a8d3be2fe5614f42ba989-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/006f52e9102a8d3be2fe5614f42ba989-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/006f52e9102a8d3be2fe5614f42ba989-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1988374,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9564452058641577398&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "e2641c1e90",
        "title": "Neural Net Receivers in Multiple Access-Communications",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/9872ed9fc22fc182d371c3e9ed316094-Abstract.html",
        "author": "Bernd-Peter Paris; Geoffrey Orsak; Mahesh Varanasi; Behnaam Aazhang",
        "abstract": "The application of neural networks to the demodulation of  spread-spectrum signals in a multiple-access environment is  considered. This study is motivated in large part by the fact  that, in a multiuser system, the conventional (matched fil(cid:173) ter) receiver suffers severe performance degradation as the  relative powers of the interfering signals become large (the  \"near-far\" problem). Furthermore, the optimum receiver,  which alleviates the near-far problem, is too complex to be  of practical use. Receivers based on multi-layer perceptrons  are considered as a simple and robust alternative to the opti(cid:173) mum solution. The optimum receiver is used to benchmark  the performance of the neural net receiver; in particular, it is  proven to be instrumental in identifying the decision regions  of the neural networks. The back-propagation algorithm and  a modified version of it are used to train the neural net. An  importance sampling technique is introduced to reduce the  number of simulations necessary to evaluate the performance  of neural nets. In all examples considered the proposed neu(cid:173) ral ~et receiver significantly outperforms the conventional  recelver.",
        "bibtex": "@inproceedings{NIPS1988_9872ed9f,\n author = {Paris, Bernd-Peter and Orsak, Geoffrey and Varanasi, Mahesh and Aazhang, Behnaam},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Neural Net Receivers in Multiple Access-Communications},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/9872ed9fc22fc182d371c3e9ed316094-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/9872ed9fc22fc182d371c3e9ed316094-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/9872ed9fc22fc182d371c3e9ed316094-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2056768,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9337644477840784236&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "6eb4cd3b01",
        "title": "Neural Network Recognizer for Hand-Written Zip Code Digits",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/a97da629b098b75c294dffdc3e463904-Abstract.html",
        "author": "John S. Denker; W. R. Gardner; Hans Peter Graf; Donnie Henderson; R. E. Howard; W. Hubbard; L. D. Jackel; Henry S. Baird; Isabelle Guyon",
        "abstract": "This paper describes the construction of a system that recognizes hand-printed  digits, using a combination of classical techniques and neural-net methods. The  system has been trained and tested on real-world data, derived from zip codes seen  on actual U.S. Mail. The system rejects a small percentage of the examples as  unclassifiable, and achieves a very low error rate on the remaining examples. The  system compares favorably with other state-of-the art recognizers. While some of  the methods are specific to this task, it is hoped that many of the techniques will  be applicable to a wide range of recognition tasks.",
        "bibtex": "@inproceedings{NIPS1988_a97da629,\n author = {Denker, John and Gardner, W. and Graf, Hans and Henderson, Donnie and Howard, R. and Hubbard, W. and Jackel, L. D. and Baird, Henry and Guyon, Isabelle},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Neural Network Recognizer for Hand-Written Zip Code Digits},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/a97da629b098b75c294dffdc3e463904-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/a97da629b098b75c294dffdc3e463904-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/a97da629b098b75c294dffdc3e463904-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1925690,
        "gs_citation": 303,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2335381669319025209&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "AT &T Bell Laboratories; AT &T Bell Laboratories; AT &T Bell Laboratories; AT &T Bell Laboratories; AT &T Bell Laboratories; AT &T Bell Laboratories; AT &T Bell Laboratories; AT &T Bell Laboratories; AT &T Bell Laboratories",
        "aff_domain": ";;;;;;;;",
        "email": ";;;;;;;;",
        "github": "",
        "project": "",
        "author_num": 9,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_unique_norm": "AT &T Bell Laboratories",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "25d640453f",
        "title": "Neural Network Star Pattern Recognition for Spacecraft Attitude Determination and Control",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/96da2f590cd7246bbde0051047b0d6f7-Abstract.html",
        "author": "Phillip Alvelda; A. Miguel San Martin",
        "abstract": "computational  bottlenecks",
        "bibtex": "@inproceedings{NIPS1988_96da2f59,\n author = {Alvelda, Phillip and San Martin, A.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Neural Network Star Pattern Recognition for Spacecraft Attitude Determination and Control},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/96da2f590cd7246bbde0051047b0d6f7-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/96da2f590cd7246bbde0051047b0d6f7-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/96da2f590cd7246bbde0051047b0d6f7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1572400,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3396561937594821336&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "The Jet Propulsion Laboratory, California Institute of Technology, Pasadena, Ca. 91109; The Jet Propulsion Laboratory, California Institute of Technology, Pasadena, Ca. 91109",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "The Jet Propulsion Laboratory, California Institute of Technology, Pasadena, Ca. 91109",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "fa094c5594",
        "title": "Neural Networks for Model Matching and Perceptual Organization",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/a3c65c2974270fd093ee8a9bf8ae7d0b-Abstract.html",
        "author": "Eric Mjolsness; Gene Gindi; P. Anandan",
        "abstract": "We introduce an optimization approach for solving problems in com(cid:173)",
        "bibtex": "@inproceedings{NIPS1988_a3c65c29,\n author = {Mjolsness, Eric and Gindi, Gene and Anandan, P.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Neural Networks for Model Matching and Perceptual Organization},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/a3c65c2974270fd093ee8a9bf8ae7d0b-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/a3c65c2974270fd093ee8a9bf8ae7d0b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/a3c65c2974270fd093ee8a9bf8ae7d0b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1659941,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17102588147092045958&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "2c4d0df0bd",
        "title": "Neural Networks that Learn to Discriminate Similar Kanji Characters",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/06409663226af2f3114485aa4e0a23b4-Abstract.html",
        "author": "Yoshihiro Mori; Kazuhiko Yokosawa",
        "abstract": "is",
        "bibtex": "@inproceedings{NIPS1988_06409663,\n author = {Mori, Yoshihiro and Yokosawa, Kazuhiko},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Neural Networks that Learn to Discriminate Similar Kanji Characters},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/06409663226af2f3114485aa4e0a23b4-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/06409663226af2f3114485aa4e0a23b4-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/06409663226af2f3114485aa4e0a23b4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1523305,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10732701246378502251&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "e07d0602a9",
        "title": "Neuronal Maps for Sensory-Motor Control in the Barn Owl",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/a4a042cf4fd6bfb47701cbc8a1653ada-Abstract.html",
        "author": "Clay D. Spence; John C. Pearson; J. J. Gelfand; R. M. Peterson; W. E. Sullivan",
        "abstract": "The bam owl has fused visual/auditory/motor representations of  space in its midbrain which are used to orient the head so that visu(cid:173) al or auditory stimuli are centered in the visual field of view. We  present models and computer simulations of these structures which  address various problems, inclu",
        "bibtex": "@inproceedings{NIPS1988_a4a042cf,\n author = {Spence, Clay D. and Pearson, John and Gelfand, J. and Peterson, R. and Sullivan, W.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Neuronal Maps for Sensory-Motor Control in the Barn Owl},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/a4a042cf4fd6bfb47701cbc8a1653ada-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/a4a042cf4fd6bfb47701cbc8a1653ada-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/a4a042cf4fd6bfb47701cbc8a1653ada-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1854899,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=395796760309227106&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "David Sarnoff Research Center, Subsidiary of SRI International, CN5300, Princeton, New Jersey 08543-5300; David Sarnoff Research Center, Subsidiary of SRI International, CN5300, Princeton, New Jersey 08543-5300; David Sarnoff Research Center, Subsidiary of SRI International, CN5300, Princeton, New Jersey 08543-5300; David Sarnoff Research Center, Subsidiary of SRI International, CN5300, Princeton, New Jersey 08543-5300; Department of Biology, Princeton University, Princeton, New Jersey 08544",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;1",
        "aff_unique_norm": "David Sarnoff Research Center, Subsidiary of SRI International, CN5300, Princeton, New Jersey 08543-5300;Department of Biology, Princeton University, Princeton, New Jersey 08544",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "84ce1b6b87",
        "title": "On the K-Winners-Take-All Network",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/6c4b761a28b734fe93831e3fb400ce87-Abstract.html",
        "author": "E. Majani; Ruth Erlanson; Yaser S. Abu-Mostafa",
        "abstract": "We present  and  rigorously  analyze a generalization of the Winner(cid:173) Take-All  Network:  the  K-Winners-Take-All  Network.  This  net(cid:173) work  identifies  the  K  largest  of a  set  of N  real  numbers.  The  network  model used  is  the continuous Hopfield  model.",
        "bibtex": "@inproceedings{NIPS1988_6c4b761a,\n author = {Majani, E. and Erlanson, Ruth and Abu-Mostafa, Yaser},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {On the K-Winners-Take-All Network},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/6c4b761a28b734fe93831e3fb400ce87-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/6c4b761a28b734fe93831e3fb400ce87-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/6c4b761a28b734fe93831e3fb400ce87-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1443497,
        "gs_citation": 211,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9705517903445828365&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "8edca119dd",
        "title": "Optimization by Mean Field Annealing",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/ec5decca5ed3d6b8079e2e7e7bacc9f2-Abstract.html",
        "author": "Griff Bilbro; Reinhold Mann; Thomas K. Miller; Wesley E. Snyder; David E. van den Bout; Mark White",
        "abstract": "Nearly optimal solutions to many combinatorial problems can be  found using stochastic simulated annealing. This paper extends  the concept of simulated annealing from its original formulation  as a Markov process to a new formulation based on mean field  theory. Mean field annealing essentially replaces the discrete de(cid:173) grees of freedom in simulated annealing with their average values  as computed by the mean field approximation. The net result is  that equilibrium at a given temperature is achieved 1-2 orders of  magnitude faster than with simulated annealing. A general frame(cid:173) work for the mean field annealing algorithm is derived, and its re(cid:173) lationship to Hopfield networks is shown. The behavior of MFA is  examined both analytically and experimentally for a generic combi(cid:173) natorial optimization problem: graph bipartitioning. This analysis  indicates the presence of critical temperatures which could be im(cid:173) portant in improving the performance of neural networks.",
        "bibtex": "@inproceedings{NIPS1988_ec5decca,\n author = {Bilbro, Griff and Mann, Reinhold and Miller, Thomas and Snyder, Wesley and van den Bout, David and White, Mark},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Optimization by Mean Field Annealing},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/ec5decca5ed3d6b8079e2e7e7bacc9f2-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/ec5decca5ed3d6b8079e2e7e7bacc9f2-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/ec5decca5ed3d6b8079e2e7e7bacc9f2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1576392,
        "gs_citation": 213,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5611261711612944651&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "dee318943a",
        "title": "Performance of Synthetic Neural Network Classification of Noisy Radar Signals",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/a5e00132373a7031000fd987a3c9f87b-Abstract.html",
        "author": "Stanley C. Ahalt; F. D. Garber; I. Jouny; Ashok K. Krishnamurthy",
        "abstract": "This study evaluates the performance of the multilayer-perceptron  and  the frequency-sensitive  competitive  learning network  in  iden(cid:173) tifying  five  commercial  aircraft  from  radar  backscatter  measure(cid:173) ments.  The performance  of the neural network  classifiers  is  com(cid:173) pared  with  that of the  nearest-neighbor  and  maximum-likelihood  classifiers.  Our  results  indicate  that  for  this  problem,  the  neural  network  classifiers  are  relatively  insensitive  to  changes  in  the  net(cid:173) work  topology,  and  to the noise  level  in  the  training data.  While,  for  this  problem,  the traditional algorithms outperform these sim(cid:173) ple neural classifiers,  we  feel  that neural networks show  the poten(cid:173) tial for  improved performance.",
        "bibtex": "@inproceedings{NIPS1988_a5e00132,\n author = {Ahalt, Stanley and Garber, F. and Jouny, I. and Krishnamurthy, Ashok},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Performance of Synthetic Neural Network Classification of Noisy Radar Signals},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/a5e00132373a7031000fd987a3c9f87b-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/a5e00132373a7031000fd987a3c9f87b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/a5e00132373a7031000fd987a3c9f87b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1371445,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8202122611724581855&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "cd0224a826",
        "title": "Performance of a Stochastic Learning Microchip",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/140f6969d5213fd0ece03148e62e461e-Abstract.html",
        "author": "Joshua Alspector; Bhusan Gupta; Robert B. Allen",
        "abstract": "Abstract Unavailable",
        "bibtex": "@inproceedings{NIPS1988_140f6969,\n author = {Alspector, Joshua and Gupta, Bhusan and Allen, Robert},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Performance of a Stochastic Learning Microchip},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/140f6969d5213fd0ece03148e62e461e-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/140f6969d5213fd0ece03148e62e461e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/140f6969d5213fd0ece03148e62e461e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2556948,
        "gs_citation": 110,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2811426162399619073&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "146c554789",
        "title": "Programmable Analog Pulse-Firing Neural Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/31fefc0e570cb3860f2a6d4b38c6490d-Abstract.html",
        "author": "Alister Hamilton; Alan F. Murray; Lionel Tarassenko",
        "abstract": "We  describe  pulse  - stream  firing  integrated  circuits  that  imple(cid:173) ment asynchronous analog neural networks.  Synaptic weights are  stored  dynamically,  and  weighting  uses  time-division  of  the  neural  pulses  from  a  signalling  neuron  to  a  receiving  neuron.  MOS  transistors  in  their  \"ON\"  state  act  as  variable  resistors  to  control  a  capacitive  discharge,  and  time-division  is  thus  achieved  by  a  small  synapse  circuit  cell.  The  VLSI  chip  set  design  uses  2.5J.1.m  CMOS technology.",
        "bibtex": "@inproceedings{NIPS1988_31fefc0e,\n author = {Hamilton, Alister and Murray, Alan and Tarassenko, Lionel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Programmable Analog Pulse-Firing Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/31fefc0e570cb3860f2a6d4b38c6490d-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/31fefc0e570cb3860f2a6d4b38c6490d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/31fefc0e570cb3860f2a6d4b38c6490d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1309567,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3961584130080215913&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "a9ca37e8d6",
        "title": "Range Image Restoration Using Mean Field Annealing",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/b3e3e393c77e35a4a3f3cbd1e429b5dc-Abstract.html",
        "author": "Griff L. Bilbro; Wesley E. Snyder",
        "abstract": "A  new  optimization strategy,  Mean  Field  Annealing, is  presented.  Its application to MAP restoration of noisy range images is derived  and experimentally verified.",
        "bibtex": "@inproceedings{NIPS1988_b3e3e393,\n author = {Bilbro, Griff and Snyder, Wesley},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Range Image Restoration Using Mean Field Annealing},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/b3e3e393c77e35a4a3f3cbd1e429b5dc-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/b3e3e393c77e35a4a3f3cbd1e429b5dc-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/b3e3e393c77e35a4a3f3cbd1e429b5dc-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1454321,
        "gs_citation": 79,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2101700061871890935&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Center for Communications and Signal Processing, North Carolina State University, Raleigh, NC; Center for Communications and Signal Processing, North Carolina State University, Raleigh, NC",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Center for Communications and Signal Processing, North Carolina State University, Raleigh, NC",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "814c98b9e1",
        "title": "Scaling and Generalization in Neural Networks: A Case Study",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/d1f491a404d6854880943e5c3cd9ca25-Abstract.html",
        "author": "Subutai Ahmad; Gerald Tesauro",
        "abstract": "The  issues  of scaling  and  generalization  have  emerged  as  key  issues  in  current studies of supervised learning from examples in neural networks.  Questions such  as  how  many  training  patterns  and  training  cycles  are  needed for  a problem of a given size  and difficulty,  how  to represent the  inllUh  and how  to choose useful training exemplars,  are of considerable  theoretical  and  practical  importance.  Several  intuitive  rules  of thumb  have been obtained from empirical studies, but as yet there are few  rig(cid:173) orous  results.  In  this  paper we  summarize  a  study Qf generalization in  the simplest possible case-perceptron networks learning linearly separa(cid:173) ble  functions.  The  task  chosen  was  the majority function  (i.e.  return  a  1  if a  majority  of the  input  units  are  on),  a  predicate  with  a  num(cid:173) ber  of useful  properties.  We  find  that  many  aspects  of.generalization  in  multilayer  networks  learning  large,  difficult  tasks  are  reproduced  in  this simple domain, in which  concrete numerical results and even some  analytic understanding can be achieved.",
        "bibtex": "@inproceedings{NIPS1988_d1f491a4,\n author = {Ahmad, Subutai and Tesauro, Gerald},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Scaling and Generalization in Neural Networks: A Case Study},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/d1f491a404d6854880943e5c3cd9ca25-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/d1f491a404d6854880943e5c3cd9ca25-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/d1f491a404d6854880943e5c3cd9ca25-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1704819,
        "gs_citation": 130,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8913821709550862345&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Center for Complex Systems Research, University of Illinois at Urbana-Champaign; IBM Watson Research Center",
        "aff_domain": "; ",
        "email": "; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Center for Complex Systems Research, University of Illinois at Urbana-Champaign;IBM",
        "aff_unique_dep": ";Watson Research Center",
        "aff_unique_url": ";https://www.ibm.com/watson",
        "aff_unique_abbr": ";IBM Watson",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Yorktown Heights",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "c3a6a04030",
        "title": "Self Organizing Neural Networks for the Identification Problem",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/f2217062e9a397a1dca429e7d70bc6ca-Abstract.html",
        "author": "Manoel Fernando Tenorio; Wei-Tsih Lee",
        "abstract": "This  work  introduces  a  new  method called Self Organizing  Neural  Network  (SONN)  algorithm  and  demonstrates  its  use  in  a  system  identification  task.  The  algorithm  constructs  the  network,  chooses the neuron functions, and adjusts the weights. It is compared to  the Back-Propagation algorithm in the identification of the chaotic time  series.  The  results  shows  that  SONN  constructs  a  simpler,  more  accurate model. requiring less training data and epochs. The algorithm  can be applied and generalized to appilications as a classifier.",
        "bibtex": "@inproceedings{NIPS1988_f2217062,\n author = {Tenorio, Manoel and Lee, Wei-Tsih},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Self Organizing Neural Networks for the Identification Problem},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/f2217062e9a397a1dca429e7d70bc6ca-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/f2217062e9a397a1dca429e7d70bc6ca-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/f2217062e9a397a1dca429e7d70bc6ca-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1617491,
        "gs_citation": 85,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11990206145080143752&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "School of Electrical Engineering, Purdue University; School of Electrical Engineering, Purdue University",
        "aff_domain": "ee.ecn.purdue.edu;ed.ecn.purdue.edu",
        "email": "ee.ecn.purdue.edu;ed.ecn.purdue.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "School of Electrical Engineering, Purdue University",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "1f8e881301",
        "title": "Simulation and Measurement of the Electric Fields Generated by Weakly Electric Fish",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/37a749d808e46495a8da1e5352d03cae-Abstract.html",
        "author": "Brian Rasnow; Christopher Assad; Mark E. Nelson; James M. Bower",
        "abstract": "The weakly electric fish, Gnathonemus peters;;, explores its environment by gener(cid:173) ating pulsed elecbic fields  and detecting small pertwbations in the fields  resulting from  nearby objects.  Accordingly, the fISh  detects and discriminates objects on  the basis of a  sequence of elecbic \"images\" whose temporal and spatial properties depend on  the  tim(cid:173) ing of the fish's electric organ discharge and its body position relative to objects in its en(cid:173) vironmenl  We are interested in investigating how these fish utilize timing and body-po(cid:173) sition during exploration to aid in object discrimination.  We have developed a fmite-ele(cid:173) ment simulation of the fish's self-generated electric  fields  so as  to  reconstruct the elec(cid:173) trosensory consequences of body position and electric organ discharge timing in the fish.  This paper describes this finite-element simulation system and presents preliminary elec(cid:173) tric field measurements which are being used to tune the simulation.",
        "bibtex": "@inproceedings{NIPS1988_37a749d8,\n author = {Rasnow, Brian and Assad, Christopher and Nelson, Mark and Bower, James},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Simulation and Measurement of the Electric Fields Generated by Weakly Electric Fish},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/37a749d808e46495a8da1e5352d03cae-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/37a749d808e46495a8da1e5352d03cae-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/37a749d808e46495a8da1e5352d03cae-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1736269,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14332556425135292741&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "720299b344",
        "title": "Skeletonization: A Technique for Trimming the Fat from a Network via Relevance Assessment",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/07e1cd7dca89a1678042477183b7ac3f-Abstract.html",
        "author": "Michael Mozer; Paul Smolensky",
        "abstract": "This paper proposes a means of using the knowledge in a network to  determine the functionality or relevance of individual units, both for  the purpose of understanding the network's behavior and improving its  performance. The basic idea is to iteratively train the network to a cer(cid:173) tain performance criterion, compute a measure of relevance that identi(cid:173) fies which input or hidden units are most critical to performance, and  automatically trim the least relevant units. This skeletonization tech(cid:173) nique can be used to simplify networks by eliminating units that con(cid:173) vey redundant information; to improve learning performance by first  learning with spare hidden units and then trimming the unnecessary  ones away, thereby constraining generalization; and to understand the  behavior of networks in terms of minimal \"rules.\"",
        "bibtex": "@inproceedings{NIPS1988_07e1cd7d,\n author = {Mozer, Michael C and Smolensky, Paul},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Skeletonization: A Technique for Trimming the Fat from a Network via Relevance Assessment},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/07e1cd7dca89a1678042477183b7ac3f-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/07e1cd7dca89a1678042477183b7ac3f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/07e1cd7dca89a1678042477183b7ac3f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2008041,
        "gs_citation": 1064,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7392176995477104969&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science & Institute of Cognitive Science, University of Colorado, Boulder, CO 80309-0430; Department of Computer Science & Institute of Cognitive Science, University of Colorado, Boulder, CO 80309-0430",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Department of Computer Science & Institute of Cognitive Science, University of Colorado, Boulder, CO 80309-0430",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "b1cbcc6b91",
        "title": "Song Learning in Birds",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/149e9677a5989fd342ae44213df68868-Abstract.html",
        "author": "M. Konishi",
        "abstract": "Abstract Unavailable",
        "bibtex": "@inproceedings{NIPS1988_149e9677,\n author = {Konishi, M.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Song Learning in Birds},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/149e9677a5989fd342ae44213df68868-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/149e9677a5989fd342ae44213df68868-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/149e9677a5989fd342ae44213df68868-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 227548,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "Division of Biology, California Institute of Technology",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Division of Biology, California Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "e60b8ac775",
        "title": "Speech Production Using A Neural Network with a Cooperative Learning Mechanism",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/0f28b5d49b3020afeecd95b4009adf4c-Abstract.html",
        "author": "Mitsuo Komura; Akio Tanaka",
        "abstract": "We  propose  a  new  neural  network  model  and  its  learning  algorithm. The proposed neural network consists of four layers  - input, hidden, output and final output layers. The hidden and  output layers are multiple.  Using the proposed  SICL(Spread  Pattern Information and Cooperative  Learning)  algorithm,  it  is  possible  to  learn  analog  data  accurately  and  to  obtain  smooth outputs. Using this neural network, we have developed  a  speech production system consisting of a  phonemic  symbol  production  subsystem  and  a  speech  parameter  production  subsystem.  We  have  succeeded  in  producing  natural  speech  waves with high accuracy.",
        "bibtex": "@inproceedings{NIPS1988_0f28b5d4,\n author = {Komura, Mitsuo and Tanaka, Akio},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Speech Production Using A Neural Network with a Cooperative Learning Mechanism},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/0f28b5d49b3020afeecd95b4009adf4c-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/0f28b5d49b3020afeecd95b4009adf4c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/0f28b5d49b3020afeecd95b4009adf4c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1615365,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5120964770048972570&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "International Institute for Advanced Study of Social Information Science, Fujitsu Limited; International Institute for Advanced Study of Social Information Science, Fujitsu Limited",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "International Institute for Advanced Study of Social Information Science, Fujitsu Limited",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "db60429da2",
        "title": "Speech Recognition: Statistical and Neural Information Processing Approaches",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/bf8229696f7a3bb4700cfddef19fa23f-Abstract.html",
        "author": "John S. Bridle",
        "abstract": "Abstract Unavailable",
        "bibtex": "@inproceedings{NIPS1988_bf822969,\n author = {Bridle, John},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Speech Recognition: Statistical and Neural Information Processing Approaches},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/bf8229696f7a3bb4700cfddef19fa23f-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/bf8229696f7a3bb4700cfddef19fa23f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/bf8229696f7a3bb4700cfddef19fa23f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1265322,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17913229133688949366&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "4d0ba9b733",
        "title": "Spreading Activation over Distributed Microfeatures",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/ed3d2c21991e3bef5e069713af9fa6ca-Abstract.html",
        "author": "James Hendler",
        "abstract": "One att\u00b7empt at explaining human inferencing is that of spread(cid:173) ing activat,ion, particularly in the st.ructured connectionist para(cid:173) digm. This has resulted in t.he building of systems with semanti(cid:173) cally nameable nodes which perform inferencing by examining  t.he pat,t.erns of activation spread. In this paper we demonst.rate  t.hat simple structured network infert'ncing can be p(>rformed by  passing art.iva.t.ion over the weights learned by a distributed alga(cid:173) rit,hm. Thus , an account, is provided which explains a well(cid:173) behaved rela t ionship bet.ween structured and distri butt'd conn('c(cid:173) t.ionist. a.pproachrs.",
        "bibtex": "@inproceedings{NIPS1988_ed3d2c21,\n author = {Hendler, James},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Spreading Activation over Distributed Microfeatures},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/ed3d2c21991e3bef5e069713af9fa6ca-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/ed3d2c21991e3bef5e069713af9fa6ca-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/ed3d2c21991e3bef5e069713af9fa6ca-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1233803,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2660985512742575864&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "05c6a6398b",
        "title": "Statistical Prediction with Kanerva's Sparse Distributed Memory",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/9b8619251a19057cff70779273e95aa6-Abstract.html",
        "author": "David Rogers",
        "abstract": "A  new  viewpoint  of  the  processing  performed  by  Kanerva's  sparse  distributed  memory  (SDM)  is  presented.  In  conditions  of  near- or  over- capacity,  where  the  associative-memory  behavior  of the  mod(cid:173) el  breaks  down,  the  processing  performed by  the  model  can  be  inter(cid:173) preted  as  that  of  a  statistical  predictor.  Mathematical  results  are  presented  which  serve  as  the  framework  for  a  new  statistical  view(cid:173) point  of  sparse  distributed  memory  and  for  which  the  standard  for(cid:173) mulation  of SDM  is  a  special  case.  This  viewpoint  suggests  possi(cid:173) ble  enhancements  to  the  SDM  model,  including  a  procedure  for  improving  the  predictiveness  of  the  system  based  on  Holland's  work  with  'Genetic  Algorithms',  and  a  method  for  improving  the  capacity of SDM even when used as an associative memory.",
        "bibtex": "@inproceedings{NIPS1988_9b861925,\n author = {Rogers, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Statistical Prediction with Kanerva\\textquotesingle s Sparse Distributed Memory},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/9b8619251a19057cff70779273e95aa6-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/9b8619251a19057cff70779273e95aa6-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/9b8619251a19057cff70779273e95aa6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1781902,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3617066068303726581&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 9,
        "aff": "Research Institute for Advanced Computer Science MS 230-5, NASA Ames Research Center Moffett Field, CA 94035",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Research Institute for Advanced Computer Science MS 230-5, NASA Ames Research Center Moffett Field, CA 94035",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "ab441c3522",
        "title": "Storing Covariance by the Associative Long-Term Potentiation and Depression of Synaptic Strengths in the Hippocampus",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/f899139df5e1059396431415e770c6dd-Abstract.html",
        "author": "Patric K. Stanton; Terrence J. Sejnowski",
        "abstract": "In modeling studies or memory based on neural networks, both the selective  enhancement and depression or synaptic strengths are required ror effident storage  or inrormation (Sejnowski, 1977a,b; Kohonen, 1984; Bienenstock et aI, 1982;  Sejnowski and Tesauro, 1989). We have tested this assumption in the hippocampus,  a cortical structure or the brain that is involved in long-term memory. A brier,  high-frequency activation or excitatory synapses in the hippocampus produces an  increase in synaptic strength known as long-term potentiation, or L TP (BUss and  Lomo, 1973), that can last ror many days. LTP is known to be Hebbian since it  requires the simultaneous release or neurotransmitter from presynaptic terminals  coupled with postsynaptic depolarization (Kelso et al, 1986; Malinow and Miller,  1986; Gustatrson et al, 1987). However, a mechanism ror the persistent reduction or  synaptic strength that could balance LTP has not yet been demonstrated. We stu(cid:173) died the associative interactions between separate inputs onto the same dendritic  trees or hippocampal pyramidal cells or field CAl, and round that a low-frequency  input which, by itselr, does not persistently change synaptic strength, can either  increase (associative L TP) or decrease in strength (associative long-term depression  or LTD) depending upon whether it is positively or negatively correlated in time  with a second, high-frequency bursting input. LTP or synaptic strength is Hebbian,  and LTD is anti-Hebbian since it is elicited by pairing presynaptic firing with post(cid:173) synaptic hyperpolarization sufficient to block postsynaptic activity. Thus, associa(cid:173) tive L TP and associative L TO are capable or storing inrormation contained in the  covariance between separate, converging hippocampal inputs \u2022",
        "bibtex": "@inproceedings{NIPS1988_f899139d,\n author = {Stanton, Patric and Sejnowski, Terrence J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Storing Covariance by the Associative Long-Term Potentiation and Depression of Synaptic Strengths in the Hippocampus},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/f899139df5e1059396431415e770c6dd-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/f899139df5e1059396431415e770c6dd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/f899139df5e1059396431415e770c6dd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1759177,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=780764567445155290&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Biophysics, Johns Hopkins University; Department of Biophysics, Johns Hopkins University",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Department of Biophysics, Johns Hopkins University",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "f42e918077",
        "title": "Temporal Representations in a Connectionist Speech System",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/903ce9225fca3e988c2af215d4e544d3-Abstract.html",
        "author": "Erich J. Smythe",
        "abstract": "SYREN  is  a  connectionist model  that uses  temporal  information  in  a  speech signal  for  syllable  recognition.  It classifies  the  rates  and directions of formant center transitions,  and uses an adaptive  method  to  associate  transition  events  with  each  syllable.  The  system  uses  explicit  spatial  temporal  representations through  de(cid:173) lay  lines.  SYREN  uses  implicit  parametric  temporal  representa(cid:173) tions  in  formant  transition  classification  through  node  activation  onset,  decay,  and transition delays  in sub-networks analogous to  visual  motion detector cells.  SYREN  recognizes 79% of six repe(cid:173) titions  of  24  consonant-vowel  syllables  when  tested  on  unseen  data,  and  recognizes  100%  of  its  training  syllables.",
        "bibtex": "@inproceedings{NIPS1988_903ce922,\n author = {Smythe, Erich},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Temporal Representations in a Connectionist Speech System},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/903ce9225fca3e988c2af215d4e544d3-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/903ce9225fca3e988c2af215d4e544d3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/903ce9225fca3e988c2af215d4e544d3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1886987,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6448215978142193464&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "207 Greenmanville Ave, #6 Mystic, CT 06355",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "207 Greenmanville Ave, #6 Mystic, CT 06355",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "ebad76de04",
        "title": "The Boltzmann Perceptron Network: A Multi-Layered Feed-Forward Network Equivalent to the Boltzmann Machine",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/045117b0e0a11a242b9765e79cbf113f-Abstract.html",
        "author": "Eyal Yair; Allen Gersho",
        "abstract": "The  concept  of  the  stochastic  Boltzmann  machine  (BM)  is  auractive  for  decision  making  and  pattern  classification  purposes  since  the  probability  of  attaining  the network  states  is a  function  of the network energy.  Hence,  the  probability of attaining particular energy minima  may be associated  with  the  probabilities  of  making  certain  decisions  (or  classifications).  However,  because of its stochastic  nature,  the complexity of the BM is fairly  high and  therefore  such  networks  are  not  very  likely  to  be  used  in  practice.  In  this  paper  we  suggest  a  way  to  alleviate  this  drawback  by  converting  the  sto(cid:173) chastic  BM into  a  deterministic  network  which  we  call  the  Boltzmann  Per(cid:173) ceptron  Network  (BPN).  The BPN is functionally  equivalent  to  the  BM but  has  a  feed-forward  structure  and  low  complexity.  No annealing  is required.  The  conditions  under  which  such  a  convmion  is  feasible  are  given.  A  learning  algorithm  for  the  BPN based  on  the  conjugate  gradient  method  is  also provided which is somewhat akin  to the backpropagation algorithm.",
        "bibtex": "@inproceedings{NIPS1988_045117b0,\n author = {Yair, Eyal and Gersho, Allen},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {The Boltzmann Perceptron Network: A Multi-Layered Feed-Forward Network Equivalent to the Boltzmann Machine},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/045117b0e0a11a242b9765e79cbf113f-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/045117b0e0a11a242b9765e79cbf113f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/045117b0e0a11a242b9765e79cbf113f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2060234,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2883669441489319229&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Center for Information Processing Research, Department of Electrical & Computer Engineering, University of California, Santa Barbara, CA 93106; Center for Information Processing Research, Department of Electrical & Computer Engineering, University of California, Santa Barbara, CA 93106",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Center for Information Processing Research, Department of Electrical & Computer Engineering, University of California, Santa Barbara, CA 93106",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "2df3fb7414",
        "title": "Theory of Self-Organization of Cortical Maps",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/65ded5353c5ee48d0b7d48c591b8f430-Abstract.html",
        "author": "Shigeru Tanaka",
        "abstract": "We  have  mathematically  shown  that  cortical  maps  in  the  primary sensory  cortices  can  be  reproduced  by  using  three  hypotheses  which  have  physiological  basis  and  meaning.  Here, our main focus is on ocular.dominance column formation  in the primary visual cortex.  Monte Carlo simulations on the  segregation of ipsilateral and contralateral afferent terminals  are carried out.  Based on  these,  we  show that almost all  the  physiological  experimental  results  concerning  the  ocular  dominance patterns of cats and monkeys reared under normal  or various abnormal visual conditions can be explained from a  viewpoint of the phase transition phenomena.",
        "bibtex": "@inproceedings{NIPS1988_65ded535,\n author = {Tanaka, Shigeru},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Theory of Self-Organization of Cortical Maps},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/65ded5353c5ee48d0b7d48c591b8f430-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/65ded5353c5ee48d0b7d48c591b8f430-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/65ded5353c5ee48d0b7d48c591b8f430-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1605091,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11139330694750398360&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Fundamental Research Laboratorys, NEC Corporation",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Fundamental Research Laboratorys, NEC Corporation",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "485b894cc3",
        "title": "Training Multilayer Perceptrons with the Extended Kalman Algorithm",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/38b3eff8baf56627478ec76a704e9b52-Abstract.html",
        "author": "Sharad Singhal; Lance Wu",
        "abstract": "trained with",
        "bibtex": "@inproceedings{NIPS1988_38b3eff8,\n author = {Singhal, Sharad and Wu, Lance},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Training Multilayer Perceptrons with the Extended Kalman Algorithm},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/38b3eff8baf56627478ec76a704e9b52-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/38b3eff8baf56627478ec76a704e9b52-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/38b3eff8baf56627478ec76a704e9b52-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1513443,
        "gs_citation": 557,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5218064003808608040&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 10,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "f4c9f93ad9",
        "title": "Training a 3-Node Neural Network is NP-Complete",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/3def184ad8f4755ff269862ea77393dd-Abstract.html",
        "author": "Avrim Blum; Ronald L. Rivest",
        "abstract": "We consider  a  2-layer,  3-node,  n-input neural network whose  nodes  compute linear threshold functions  of their inputs.  We  show  that it  is NP-complete to decide whether there exist weights and thresholds  for the three nodes of this network so that it will produce output con(cid:173) sistent  with  a  given set of training examples.  We  extend  the result  to other simple networks.  This result suggests that those looking for  perfect  training  algorithms  cannot  escape  inherent  computational  difficulties just by  considering only simple or very  regular networks.  It also suggests the importance, given a training problem, of finding  an  appropriate network  and input encoding for  that problem.  It is  left as an open problem to extend our result to nodes with non-linear  functions such as  sigmoids.",
        "bibtex": "@inproceedings{NIPS1988_3def184a,\n author = {Blum, Avrim and Rivest, Ronald},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Training a 3-Node Neural Network is NP-Complete},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/3def184ad8f4755ff269862ea77393dd-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/3def184ad8f4755ff269862ea77393dd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/3def184ad8f4755ff269862ea77393dd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1501409,
        "gs_citation": 1476,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8025971942740054674&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 21,
        "aff": "MIT Lab. for Computer Science; MIT Lab. for Computer Science",
        "aff_domain": "; ",
        "email": "; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "MIT Lab. for Computer Science",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "175a47ec12",
        "title": "Training a Limited-Interconnect, Synthetic Neural IC",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/8f53295a73878494e9bc8dd6c3c7104f-Abstract.html",
        "author": "M. R. Walker; S. Haghighi; A. Afghan; Larry A. Akers",
        "abstract": "Hardware implementation of neuromorphic algorithms is hampered by  high  degrees of connectivity.  Functionally equivalent feedforward  networks may be formed by using limited fan-in nodes and additional  layers.  but  this  complicates  procedures  for  determining  weight  magnitudes.  No direct mapping of weights exists between fully and  limited-interconnect  nets.  Low-level  nonlinearities  prevent  the  formation  of internal  representations  of widely  separated  spatial  features and the use of gradient descent methods to minimize output  error is hampered by error magnitude dissipation.  The judicious use  of linear summations or collection units is proposed as a solution.",
        "bibtex": "@inproceedings{NIPS1988_8f53295a,\n author = {Walker, M. and Haghighi, S. and Afghan, A. and Akers, Larry},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Training a Limited-Interconnect, Synthetic Neural IC},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/8f53295a73878494e9bc8dd6c3c7104f-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/8f53295a73878494e9bc8dd6c3c7104f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/8f53295a73878494e9bc8dd6c3c7104f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1358841,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10104839651049101853&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Center for Solid State Electronics Research, Arizona State University; Center for Solid State Electronics Research, Arizona State University; Center for Solid State Electronics Research, Arizona State University; Center for Solid State Electronics Research, Arizona State University",
        "aff_domain": "enuxha.eas.asu.edu; ; ; ",
        "email": "enuxha.eas.asu.edu; ; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Center for Solid State Electronics Research, Arizona State University",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "fd4f3f80c4",
        "title": "Use of Multi-Layered Networks for Coding Speech with Phonetic Features",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/73278a4a86960eeb576a8fd4c9ec6997-Abstract.html",
        "author": "Yoshua Bengio; R\u00e9gis Cardin; Renato de Mori; Piero Cosi",
        "abstract": "Preliminary  results  on  speaker-independant  speech  recognition  are  reported.  A method  that combines  expertise  on  neural  networks  with  expertise  on  speech  recognition  is  used  to  build  the  recognition  systems.  For  transient  sounds,  event(cid:173) driven  property  extractors  with  variable  resolution  in  the  time  and  frequency  domains  are  used.  For  sonorant  speech,  a  model  of the  human  auditory  system  is  preferred  to  FFT  as  a  front-end  module.",
        "bibtex": "@inproceedings{NIPS1988_73278a4a,\n author = {Bengio, Yoshua and Cardin, R\\'{e}gis and de Mori, Renato and Cosi, Piero},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Use of Multi-Layered Networks for Coding Speech with Phonetic Features},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/73278a4a86960eeb576a8fd4c9ec6997-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/73278a4a86960eeb576a8fd4c9ec6997-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/73278a4a86960eeb576a8fd4c9ec6997-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1808318,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17839482341811556074&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "a0eca7ffdd",
        "title": "Using Backpropagation with Temporal Windows to Learn the Dynamics of the CMU Direct-Drive Arm II",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/f7e6c85504ce6e82442c770f7c8606f0-Abstract.html",
        "author": "Kenneth Y. Goldberg; Barak A. Pearlmutter",
        "abstract": "Computing the inverse dynamics  of a robot ann is an active area of research  in the control literature.  We hope to  learn the  inverse dynamics  by training  a neural network on the  measured  response of a physical ann.  The  input  to  the  network is  a  temporal  window of measured positions;  output is  a vector  of torques.  We  train  the  network on  data measured from  the  first  two joints  of the CMU Direct-Drive Arm II  as  it moves  through a randomly-generated  sample  of \"pick-and-place\"  trajectories.  We  then  test  generalization  with  a  new  trajectory  and  compare  its  output  with  the  torque  measured  at  the  physical arm.  The network  is  shown  to  generalize with  a root mean  square  error/standard deviation  (RMSS)  of 0.10.  We  interpreted the weights  of the  network in tenns of the velocity and acceleration filters  used in  conventional  control  theory.",
        "bibtex": "@inproceedings{NIPS1988_f7e6c855,\n author = {Goldberg, Kenneth and Pearlmutter, Barak},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Using Backpropagation with Temporal Windows to Learn the Dynamics of the CMU Direct-Drive Arm II},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/f7e6c85504ce6e82442c770f7c8606f0-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/f7e6c85504ce6e82442c770f7c8606f0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/f7e6c85504ce6e82442c770f7c8606f0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 210870,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14163669143832845056&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "School of Computer Science, Carnegie Mellon University; School of Computer Science, Carnegie Mellon University",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "15b8d38b84",
        "title": "What Size Net Gives Valid Generalization?",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/1d7f7abc18fcb43975065399b0d1e48e-Abstract.html",
        "author": "Eric B. Baum; David Haussler",
        "abstract": "We  address  the  question  of when  a  network  can  be  expected  to  generalize from m  random training examples chosen from some ar(cid:173) bitrary probability distribution, assuming that future test examples  are drawn from  the same  distribution.  Among  our  results are  the  following  bounds on appropriate sample vs.  network size.  Assume  o <  \u00a3  $  1/8.  We  show  that  if m  >  O( ~log~) random  exam(cid:173) ples  can  be  loaded  on  a  feedforward  network  of linear  threshold  functions  with N  nodes and W  weights,  so that at least a  fraction  1 - t of the examples  are  correctly  classified,  then one  has confi(cid:173) dence approaching certainty that the network will correctly classify  a  fraction  1 - \u00a3  of future  test  examples drawn from  the same  dis(cid:173) tribution.  Conversely,  for  fully-connected  feedforward  nets  with  one  hidden  layer,  any learning  algorithm  using  fewer  than  O( '!')  random training examples  will,  for  some distributions of examples  consistent  with  an  appropriate  weight  choice,  fail  at  least  some  fixed fraction of the time to find  a  weight choice that will correctly  classify more  than a 1 - \u00a3  fraction of the future  test examples.",
        "bibtex": "@inproceedings{NIPS1988_1d7f7abc,\n author = {Baum, Eric and Haussler, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {What Size Net Gives Valid Generalization?},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/1d7f7abc18fcb43975065399b0d1e48e-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/1d7f7abc18fcb43975065399b0d1e48e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/1d7f7abc18fcb43975065399b0d1e48e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2299758,
        "gs_citation": 2715,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11315461090020639134&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Physics, Princeton University, Princeton NJ 08540; Computer and Information Science, University of California, Santa Cruz, CA 95064",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Department of Physics, Princeton University, Princeton NJ 08540;Computer and Information Science, University of California, Santa Cruz, CA 95064",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "d3f658e9fe",
        "title": "Winner-Take-All Networks of O(N) Complexity",
        "site": "https://papers.nips.cc/paper_files/paper/1988/hash/a8f15eda80c50adb0e71943adc8015cf-Abstract.html",
        "author": "J. Lazzaro; S. Ryckebusch; M.A. Mahowald; C. A. Mead",
        "abstract": "We have designed, fabricated, and tested a series of compact CMOS  integrated circuits that realize the winner-take-all function. These  analog, continuous-time circuits use only O(n) of interconnect to  perform this function. We have also modified the winner-take-all  circuit, realizing a circuit that computes local nonlinear inhibition.",
        "bibtex": "@inproceedings{NIPS1988_a8f15eda,\n author = {Lazzaro, J. and Ryckebusch, S. and Mahowald, M.A. and Mead, C. A.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {D. Touretzky},\n pages = {},\n publisher = {Morgan-Kaufmann},\n title = {Winner-Take-All Networks of O(N) Complexity},\n url = {https://proceedings.neurips.cc/paper_files/paper/1988/file/a8f15eda80c50adb0e71943adc8015cf-Paper.pdf},\n volume = {1},\n year = {1988}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1988/file/a8f15eda80c50adb0e71943adc8015cf-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1988/file/a8f15eda80c50adb0e71943adc8015cf-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1454563,
        "gs_citation": 884,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1109155958238173962&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 22,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    }
]