[
    {
        "id": "20366d2dc0",
        "title": "A Charge-Based CMOS Parallel Analog Vector Quantizer",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/c4b31ce7d95c75ca70d50c19aef08bf1-Abstract.html",
        "author": "Gert Cauwenberghs; Volnei Pedroni",
        "abstract": "We present an analog VLSI chip for parallel analog vector quantiza(cid:173) tion. The MOSIS 2.0 J..Lm double-poly CMOS Tiny chip contains an  array of 16 x 16 charge-based distance estimation cells, implementing a  mean absolute difference (MAD) metric operating on a 16-input analog  vector field and 16 analog template vectors. The distance cell includ(cid:173) ing dynamic template storage measures 60 x 78 J..Lm2 \u2022 Additionally,  the chip features a winner-take-all (WTA) output circuit of linear com(cid:173) plexity, with global positive feedback for fast and decisive settling of a  single winner output. Experimental results on the complete 16 x 16 VQ  system demonstrate correct operation with 34 dB analog input dynamic  range and 3 J..Lsec cycle time at 0.7 mW power dissipation.",
        "bibtex": "@inproceedings{NIPS1994_c4b31ce7,\n author = {Cauwenberghs, Gert and Pedroni, Volnei},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {A Charge-Based CMOS Parallel Analog Vector Quantizer},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/c4b31ce7d95c75ca70d50c19aef08bf1-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/c4b31ce7d95c75ca70d50c19aef08bf1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/c4b31ce7d95c75ca70d50c19aef08bf1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1380899,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3363271992332002782&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 8,
        "aff": "Johns Hopkins University ECE Department; California Institute of Technology EE Department",
        "aff_domain": "jhunix.hcf.jhu.edu;romeo.caltech.edu",
        "email": "jhunix.hcf.jhu.edu;romeo.caltech.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Johns Hopkins University ECE Department;California Institute of Technology EE Department",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "1ddfacd13f",
        "title": "A Comparison of Discrete-Time Operator Models for Nonlinear System Identification",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/0efe32849d230d7f53049ddc4a4b0c60-Abstract.html",
        "author": "Andrew D. Back; Ah Chung Tsoi",
        "abstract": "We present a unifying view of discrete-time operator models used in the  context of finite word length linear signal processing.  Comparisons are  made between the recently presented gamma operator model, and the delta  and rho operator models for performing nonlinear system identification  and prediction using neural networks.  A new model based on an adaptive  bilinear  transformation  which  generalizes  all  of the  above  models  is  presented.",
        "bibtex": "@inproceedings{NIPS1994_0efe3284,\n author = {Back, Andrew and Tsoi, Ah},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {A Comparison of Discrete-Time Operator Models for Nonlinear System Identification},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/0efe32849d230d7f53049ddc4a4b0c60-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/0efe32849d230d7f53049ddc4a4b0c60-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/0efe32849d230d7f53049ddc4a4b0c60-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1595944,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7411731000680772450&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Electrical and Computer Engineering, University of Queensland; Department of Electrical and Computer Engineering, University of Queensland",
        "aff_domain": "elec.uq.oz.au;elec.uq.oz.au",
        "email": "elec.uq.oz.au;elec.uq.oz.au",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Department of Electrical and Computer Engineering, University of Queensland",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "9ec3573bc7",
        "title": "A Computational Model of Prefrontal Cortex Function",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/65cc2c8205a05d7379fa3a6386f710e1-Abstract.html",
        "author": "Todd S. Braver; Jonathan D. Cohen; David Servan-Schreiber",
        "abstract": "Accumulating  data  from  neurophysiology  and  neuropsychology  have  suggested  two  information processing roles  for  prefrontal cor(cid:173) tex  (PFC):  1)  short-term  active  memory;  and  2)  inhibition.  We  present  a  new  behavioral  task  and  a  computational model  which  were  developed  in  parallel.  The  task  was  developed  to probe  both  of these  prefrontal  functions  simultaneously,  and  produces  a  rich  set  of behavioral  data  that  act  as  constraints  on  the  model.  The  model is  implemented in continuous-time, thus providing a  natural  framework  in  which  to study  the  temporal dynamics of processing  in the task.  We show how the model can be used to examine the be(cid:173) havioral  consequences of neuromodulation in PFC . Specifically, we  use  the model to make novel and testable predictions regarding the  behavioral performance of schizophrenics,  who  are hypothesized  to  suffer  from  reduced  dopaminergic tone  in  this brain  area.",
        "bibtex": "@inproceedings{NIPS1994_65cc2c82,\n author = {Braver, Todd and Cohen, Jonathan D and Servan-Schreiber, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {A Computational Model of Prefrontal Cortex Function},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/65cc2c8205a05d7379fa3a6386f710e1-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/65cc2c8205a05d7379fa3a6386f710e1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/65cc2c8205a05d7379fa3a6386f710e1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1779488,
        "gs_citation": 63,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12191338227531740256&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "6fa88f7eca",
        "title": "A Connectionist Technique for Accelerated Textual Input: Letting a Network Do the Typing",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/298923c8190045e91288b430794814c4-Abstract.html",
        "author": "Dean Pomerleau",
        "abstract": "Each year people spend a huge amount of time typing. The text people type  typically contains a tremendous amount of redundancy due to predictable  word  usage  patterns  and  the  text's  structure.  This  paper  describes  a  neural network system call AutoTypist that monitors a person's typing and  predicts what will be entered  next.  AutoTypist displays the most likely  subsequent word to the typist, who can accept it with a single keystroke,  instead of typing it in its entirety.  The multi-layer perceptron at the heart  of Auto'JYpist adapts its predictions of likely subsequent text to the user's  word usage pattern,  and to the characteristics of the text currently being  typed.  Increases in typing speed of 2-3% when typing English prose and  10-20% when typing C code have been demonstrated using the system,  suggesting a potential time savings of more than 20 hours per user per year.  In addition to increasing typing speed, AutoTypist reduces the number of  keystrokes a user must type by a similar amount (2-3% for English,  10- 20% for computer programs).  This keystroke savings has the potential to  significantly reduce the frequency  and severity of repeated stress injuries  caused by typing, which are the most common injury suffered in today's  office environment.",
        "bibtex": "@inproceedings{NIPS1994_298923c8,\n author = {Pomerleau, Dean},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {A Connectionist Technique for Accelerated Textual Input: Letting a Network Do the Typing},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/298923c8190045e91288b430794814c4-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/298923c8190045e91288b430794814c4-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/298923c8190045e91288b430794814c4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1776351,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2872622221233602156&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "7d814dfd7d",
        "title": "A Convolutional Neural Network Hand Tracker",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/d93ed5b6db83be78efb0d05ae420158e-Abstract.html",
        "author": "Steven J. Nowlan; John C. Platt",
        "abstract": "We  describe  a system that can track  a hand in a sequence of video  frames  and recognize hand gestures  in  a user-independent  manner.  The system  locates  the  hand in  each  video  frame  and  determines  if the hand is open or closed.  The tracking system is  able to track  the  hand  to  within  \u00b110  pixels  of its  correct  location  in  99.7%  of  the frames  from a  test set  containing video sequences  from  18  dif(cid:173) ferent  individuals captured in 18  different  room environments.  The  gesture  recognition network correctly  determines if the hand being  tracked  is  open  or  closed  in  99.1 % of the  frames  in this  test  set .  The system has been designed to operate in real time with existing  hardware.",
        "bibtex": "@inproceedings{NIPS1994_d93ed5b6,\n author = {Nowlan, Steven and Platt, John},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {A Convolutional Neural Network Hand Tracker},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/d93ed5b6db83be78efb0d05ae420158e-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/d93ed5b6db83be78efb0d05ae420158e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/d93ed5b6db83be78efb0d05ae420158e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 516285,
        "gs_citation": 158,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12636409539154622233&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Synaptics, Inc.; Synaptics, Inc.",
        "aff_domain": "synaptics.com;synaptics.com",
        "email": "synaptics.com;synaptics.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Synaptics, Inc.",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "3da5204018",
        "title": "A Critical Comparison of Models for Orientation and Ocular Dominance Columns in the Striate Cortex",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/5d616dd38211ebb5d6ec52986674b6e4-Abstract.html",
        "author": "E. Erwin; K. Obermayer; K. Schulten",
        "abstract": "More than ten of the most prominent models for the structure  and for the activity dependent formation of orientation and ocu(cid:173) lar dominance columns in the striate cort(>x have been evaluated.  We implemented those models on parallel machines, we extensively  explored parameter space, and we quantitatively compared model  predictions with experimental data which were recorded optically  from macaque striate cortex.  In our contribution we present a summary of our results to date.  Briefly, we find that (i) despite apparent differences, many models  are based on similar principles and, consequently, make similar pre(cid:173) dictions, (ii) certain \"pattern models\" as well as the developmental  \"correlation-based learning\" models disagree with the experimen(cid:173) tal data, and (iii) of the models we have investigated, \"competitive  Hebbian\" models and the recent model of Swindale provide the  best match with experimental data.",
        "bibtex": "@inproceedings{NIPS1994_5d616dd3,\n author = {Erwin, E. and Obermayer, K. and Schulten, K.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {A Critical Comparison of Models for Orientation and Ocular Dominance Columns in the Striate Cortex},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/5d616dd38211ebb5d6ec52986674b6e4-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/5d616dd38211ebb5d6ec52986674b6e4-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/5d616dd38211ebb5d6ec52986674b6e4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1604612,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=810113307196787504&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "6cf582ebc3",
        "title": "A Growing Neural Gas Network Learns Topologies",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/d56b9fc4b0f1be8871f5e1c40c0067e7-Abstract.html",
        "author": "Bernd Fritzke",
        "abstract": "An incremental network model is  introduced which is  able to learn  the important topological relations in a given set of input vectors by  means of a  simple Hebb-like learning rule.  In contrast to previous  approaches like the \"neural gas\"  method of Martinetz and Schulten  (1991,  1994), this model has no parameters which change over time  and is able to continue learning, adding units and connections, until  a  performance criterion has been met.  Applications of the model  include vector quantization, clustering, and interpolation.",
        "bibtex": "@inproceedings{NIPS1994_d56b9fc4,\n author = {Fritzke, Bernd},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {A Growing Neural Gas Network Learns Topologies},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/d56b9fc4b0f1be8871f5e1c40c0067e7-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/d56b9fc4b0f1be8871f5e1c40c0067e7-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/d56b9fc4b0f1be8871f5e1c40c0067e7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1533148,
        "gs_citation": 2799,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7671823782648986477&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Institut fur Neuroinformatik, Ruhr-Universitat Bochum",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Institut fur Neuroinformatik, Ruhr-Universitat Bochum",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "aa5d2c6644",
        "title": "A Lagrangian Formulation For Optical Backpropagation Training In Kerr-Type Optical Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/d240e3d38a8882ecad8633c8f9c78c9b-Abstract.html",
        "author": "James Edward Steck; Steven R. Skinner; Alvaro A. Cruz-Cabrara; Elizabeth C. Behrman",
        "abstract": "A training method based on a form of continuous spatially distributed  optical error back-propagation is presented for an all optical network  composed of nondiscrete neurons and weighted interconnections. The all  optical network is feed-forward and is composed of thin layers of a Kerr(cid:173) type self focusing/defocusing nonlinear optical material. The training  method is derived from a Lagrangian formulation of the constrained  minimization of the network error at the output. This leads to a  formulation that describes training as a calculation of the distributed error  of the optical signal at the output which is then reflected back through the  device to assign a spatially distributed error to the internal layers. This  error is then used to modify the internal weighting values. Results from  several computer simulations of the training are presented, and a simple  optical table demonstration of the network is discussed.",
        "bibtex": "@inproceedings{NIPS1994_d240e3d3,\n author = {Steck, James and Skinner, Steven and Cruz-Cabrara, Alvaro and Behrman, Elizabeth},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {A Lagrangian Formulation For Optical Backpropagation Training In Kerr-Type Optical Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/d240e3d38a8882ecad8633c8f9c78c9b-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/d240e3d38a8882ecad8633c8f9c78c9b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/d240e3d38a8882ecad8633c8f9c78c9b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1373641,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:f2QI6Rv0QsAJ:scholar.google.com/&scioq=A+Lagrangian+Formulation+For+Optical+Backpropagation+Training+In+Kerr-Type+Optical+Networks&hl=en&as_sdt=0,5",
        "gs_version_total": 4,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "a827cf72e4",
        "title": "A Mixture Model System for Medical and Machine Diagnosis",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/03e0704b5690a2dee1861dc3ad3316c9-Abstract.html",
        "author": "Magnus Stensmo; Terrence J. Sejnowski",
        "abstract": "Diagnosis of human disease or machine fault is a missing data problem  since many variables are initially unknown. Additional information needs  to be obtained. The j oint probability distribution of the data can be used to  solve this problem. We model this with mixture models whose parameters  are estimated by the EM algorithm.  This gives the benefit that missing  data in the database itself can also be handled correctly.  The request for  new information to refine the diagnosis is performed using the maximum  utility  principle.  Since  the  system  is  based  on  learning  it  is  domain  independent and less labor intensive than expert systems or probabilistic  networks.  An example using a heart disease database is presented.",
        "bibtex": "@inproceedings{NIPS1994_03e0704b,\n author = {Stensmo, Magnus and Sejnowski, Terrence J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {A Mixture Model System for Medical and Machine Diagnosis},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/03e0704b5690a2dee1861dc3ad3316c9-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/03e0704b5690a2dee1861dc3ad3316c9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/03e0704b5690a2dee1861dc3ad3316c9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1600621,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=294209118911616479&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "ac4e3bc556",
        "title": "A Model for Chemosensory Reception",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/e820a45f1dfc7b95282d10b6087e11c0-Abstract.html",
        "author": "Rainer Malaka; Thomas Ragg; Martin Hammer",
        "abstract": "A new  model for chemosensory reception is presented.  It models reacti(cid:173) ons between odor molecules and receptor proteins and the activation of  second  messenger  by receptor proteins.  The mathematical  formulation  of the  reaction  kinetics  is  transformed  into an  artificial  neural  network  (ANN). The resulting feed-forward network provides a powerful means  for parameter fitting by applying learning algorithms. The weights of the  network corresponding to chemical parameters can be trained by presen(cid:173) ting experimental data.  We demonstrate the simulation capabilities of the  model with experimental data from honey bee chemosensory neurons.  It  can be shown that our model is sufficient to rebuild the observed data and  that simpler models are not able to do this task.",
        "bibtex": "@inproceedings{NIPS1994_e820a45f,\n author = {Malaka, Rainer and Ragg, Thomas and Hammer, Martin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {A Model for Chemosensory Reception},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/e820a45f1dfc7b95282d10b6087e11c0-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/e820a45f1dfc7b95282d10b6087e11c0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/e820a45f1dfc7b95282d10b6087e11c0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1502076,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6686028434526323823&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Institut fUr Logik, Komplexitat und Oeduktionssysteme, Universitat Karlsruhe, PO Box 0-76128 Karlsruhe, Germany; Institut fUr Logik, Komplexitat und Oeduktionssysteme, Universitat Karlsruhe, PO Box 0-76128 Karlsruhe, Germany; Institut fur Neurobiologie, Freie Universitat Berlin, 0-14195 Berlin, Germany",
        "aff_domain": "ira.uka.de;ira.uka.de;castor.zedat.fu-berlin.de",
        "email": "ira.uka.de;ira.uka.de;castor.zedat.fu-berlin.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Institut fUr Logik, Komplexitat und Oeduktionssysteme, Universitat Karlsruhe, PO Box 0-76128 Karlsruhe, Germany;Institut fur Neurobiologie, Freie Universitat Berlin, 0-14195 Berlin, Germany",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "fe394d5891",
        "title": "A Model of the Neural Basis of the Rat's Sense of Direction",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/024d7f84fff11dd7e8d9c510137a2381-Abstract.html",
        "author": "William E. Skaggs; James J. Knierim; Hemant S. Kudrimoti; Bruce L. McNaughton",
        "abstract": "In  the last decade  the outlines  of the neural  structures  subserving  the sense of direction have begun to emerge.  Several investigations  have shed  light on  the effects  of vestibular  input  and visual  input  on  the  head  direction  representation.  In  this  paper,  a  model  is  formulated of the neural mechanisms underlying the head direction  system.  The model is  built out of simple ingredients,  depending  on  nothing  more complicated than  connectional  specificity,  attractor  dynamics,  Hebbian  learning,  and  sigmoidal  nonlinearities,  but  it  behaves  in  a  sophisticated  way  and  is  consistent  with  most of the  observed properties ofreal head direction cells.  In addition it makes  a  number  of predictions  that  ought  to  be  testable  by  reasonably  straightforward experiments.",
        "bibtex": "@inproceedings{NIPS1994_024d7f84,\n author = {Skaggs, William and Knierim, James and Kudrimoti, Hemant and McNaughton, Bruce},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {A Model of the Neural Basis of the Rat\\textquotesingle s Sense of Direction},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/024d7f84fff11dd7e8d9c510137a2381-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/024d7f84fff11dd7e8d9c510137a2381-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/024d7f84fff11dd7e8d9c510137a2381-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1766516,
        "gs_citation": 594,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15187181922153445090&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "ARL Division of Neural Systems, Memory, And Aging; ARL Division of Neural Systems, Memory, And Aging; ARL Division of Neural Systems, Memory, And Aging; ARL Division of Neural Systems, Memory, And Aging",
        "aff_domain": "nsma.arizona.edu;nsma.arizona.edu;nsma.arizona.edu;nsma.arizona.edu",
        "email": "nsma.arizona.edu;nsma.arizona.edu;nsma.arizona.edu;nsma.arizona.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "ARL Division of Neural Systems, Memory, And Aging",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "5f8ed4dc96",
        "title": "A Neural Model of Delusions and Hallucinations in Schizophrenia",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/b6a1085a27ab7bff7550f8a3bd017df8-Abstract.html",
        "author": "Eytan Ruppin; James A. Reggia; David Horn",
        "abstract": "We  implement and study a computational model of Stevens'  [19921  theory  of the  pathogenesis  of schizophrenia.  This  theory  hypoth(cid:173) esizes  that  the  onset  of schizophrenia  is  associated  with  reactive  synaptic regeneration occurring in brain regions receiving degener(cid:173) ating temporal lobe  projections.  Concentrating on  one  such  area,  the  frontal  cortex,  we  model  a  frontal  module  as  an  associative  memory neural  network  whose  input synapses  represent  incoming  temporal  projections.  We  analyze  how,  in  the  face  of weakened  external input projections, compensatory strengthening of internal  synaptic connections and increased  noise levels can maintain mem(cid:173) ory  capacities  (which  are  generally  preserved  in  schizophrenia) .  However,  These  compensatory  changes  adversely  lead  to  sponta(cid:173) neous,  biased  retrieval  of stored  memories,  which  corresponds  to  the occurrence  of schizophrenic  delusions  and hallucinations with(cid:173) out  any  apparent  external  trigger,  and  for  their  tendency  to  con(cid:173) centrate on just few  central themes.  Our  results explain why  these  symptoms tend  to wane  as  schizophrenia progresses,  and why  de(cid:173) layed  therapeutical intervention leads to a  much slower  response.",
        "bibtex": "@inproceedings{NIPS1994_b6a1085a,\n author = {Ruppin, Eytan and Reggia, James and Horn, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {A Neural Model of Delusions and Hallucinations in Schizophrenia},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/b6a1085a27ab7bff7550f8a3bd017df8-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/b6a1085a27ab7bff7550f8a3bd017df8-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/b6a1085a27ab7bff7550f8a3bd017df8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1469269,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13445428990067333031&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science, University of Maryland, College Park, MD 20742; Department of Computer Science, University of Maryland, College Park, MD 20742; School of Physics and Astronomy, Tel Aviv University, Tel Aviv 69978, Israel",
        "aff_domain": "cs.umd.edu;cs.umd.edu;vm.tau.ac.il",
        "email": "cs.umd.edu;cs.umd.edu;vm.tau.ac.il",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of Maryland;School of Physics and Astronomy, Tel Aviv University, Tel Aviv 69978, Israel",
        "aff_unique_dep": "Department of Computer Science;",
        "aff_unique_url": "https://www/umd.edu;",
        "aff_unique_abbr": "UMD;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "College Park;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "27f4ea396f",
        "title": "A Non-linear Information Maximisation Algorithm that Performs Blind Separation",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/9232fe81225bcaef853ae32870a2b0fe-Abstract.html",
        "author": "Anthony J. Bell; Terrence J. Sejnowski",
        "abstract": "A new learning algorithm is derived which performs online stochas(cid:173) tic gradient ascent in the mutual information between outputs and  inputs  of a  network.  In  the  absence  of a  priori knowledge  about  the  'signal'  and  'noise'  components  of the  input,  propagation  of  information depends  on  calibrating network  non-linearities to  the  detailed  higher-order  moments of the  input  density functions.  By  incidentally  minimising  mutual  information  between  outputs,  as  well  as  maximising  their  individual  entropies,  the  network  'fac(cid:173) torises'  the  input  into  independent  components.  As  an  example  application,  we  have  achieved  near-perfect  separation of ten  digi(cid:173) tally mixed speech signals.  Our simulations lead us to believe  that  our network performs better at blind separation than the Herault(cid:173) J utten network,  reflecting the fact that it is derived rigorously from  the  mutual information objective.",
        "bibtex": "@inproceedings{NIPS1994_9232fe81,\n author = {Bell, Anthony and Sejnowski, Terrence J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {A Non-linear Information Maximisation Algorithm that Performs Blind Separation},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/9232fe81225bcaef853ae32870a2b0fe-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/9232fe81225bcaef853ae32870a2b0fe-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/9232fe81225bcaef853ae32870a2b0fe-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1576873,
        "gs_citation": 102,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13502043403136655530&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Computational Neurobiology Laboratory, The Salk Institute, 10010 N. Torrey Pines Road, La Jolla, California 92037-1099 + Department of Biology, University of California at San Diego, La Jolla CA 92093; Computational Neurobiology Laboratory, The Salk Institute, 10010 N. Torrey Pines Road, La Jolla, California 92037-1099 + Department of Biology, University of California at San Diego, La Jolla CA 92093",
        "aff_domain": "tonylOsalk.edu; terrylOsalk.edu",
        "email": "tonylOsalk.edu; terrylOsalk.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "Computational Neurobiology Laboratory, The Salk Institute, 10010 N. Torrey Pines Road, La Jolla, California 92037-1099;Department of Biology, University of California at San Diego, La Jolla CA 92093",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": ";",
        "aff_country_unique": ""
    },
    {
        "id": "82eb6a7628",
        "title": "A Novel Reinforcement Model of Birdsong Vocalization Learning",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/0a113ef6b61820daa5611c870ed8d5ee-Abstract.html",
        "author": "Kenji Doya; Terrence J. Sejnowski",
        "abstract": "Songbirds learn to imitate a tutor song through auditory and motor learn(cid:173) ing.  We  have developed a theoretical framework  for song learning that  accounts  for response properties of neurons that have been  observed in  many  of the nuclei  that are involved in  song learning.  Specifically,  we  suggest that the anteriorforebrain pathway, which is not needed for song  production  in  the  adult  but is  essential  for  song  acquisition,  provides  synaptic perturbations and adaptive evaluations for syllable vocalization  learning.  A computer model based on reinforcement learning was  con(cid:173) structed  that could replicate a real  zebra finch  song with 90% accuracy  based on a spectrographic measure.  The second generation of the bird(cid:173) song model replicated the tutor song with 96% accuracy.",
        "bibtex": "@inproceedings{NIPS1994_0a113ef6,\n author = {Doya, Kenji and Sejnowski, Terrence J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {A Novel Reinforcement Model of Birdsong Vocalization Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/0a113ef6b61820daa5611c870ed8d5ee-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/0a113ef6b61820daa5611c870ed8d5ee-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/0a113ef6b61820daa5611c870ed8d5ee-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1544450,
        "gs_citation": 140,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10483091704455190806&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "ATR Human Information Processing Research Laboratories; Howard Hughes Medical Institute UCSD and Salk Institute",
        "aff_domain": "; ",
        "email": "; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "ATR Human Information Processing Research Laboratories;Howard Hughes Medical Institute UCSD and Salk Institute",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "9fd8ffdfaa",
        "title": "A Rapid Graph-based Method for Arbitrary Transformation-Invariant Pattern Classification",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/f33ba15effa5c10e873bf3842afb46a6-Abstract.html",
        "author": "Alessandro Sperduti; David G. Stork",
        "abstract": "We  present  a  graph-based  method  for  rapid,  accurate  search  through prototypes for  transformation-invariant pattern classifica(cid:173) tion.  Our method has in theory the same recognition accuracy as  other recent  methods  based  on  ''tangent  distance\"  [Simard et al.,  1994],  since it uses the same categorization rule.  Nevertheless ours  is  significantly  faster  during  classification  because  far  fewer  tan(cid:173) gent  distances  need  be  computed.  Crucial  to  the success  of our  system  are  1)  a  novel  graph  architecture in  which  transformation  constraints and geometric  relationships  among  prototypes are en(cid:173) coded  during learning,  and 2)  an improved graph search criterion,  used during classification.  These architectural insights are applica(cid:173) ble to a wide range of problem domains.  Here we demonstrate that  on  a  handwriting  recognition task,  a  basic implementation of our  system  requires  less  than  half the  computation  of the  Euclidean  sorting method.",
        "bibtex": "@inproceedings{NIPS1994_f33ba15e,\n author = {Sperduti, Alessandro and Stork, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {A Rapid Graph-based Method for Arbitrary Transformation-Invariant Pattern Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/f33ba15effa5c10e873bf3842afb46a6-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/f33ba15effa5c10e873bf3842afb46a6-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/f33ba15effa5c10e873bf3842afb46a6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1599733,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7186798691509682640&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Dipartimento di Informatica, Universita di Pisa; Machine Learning and Perception Group, Ricoh California Research Center",
        "aff_domain": "perso~di.unipi.it; stork~crc.ricoh.com",
        "email": "perso~di.unipi.it; stork~crc.ricoh.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Dipartimento di Informatica, Universita di Pisa;Machine Learning and Perception Group, Ricoh California Research Center",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "c8e4af019b",
        "title": "A Real Time Clustering CMOS Neural Engine",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/ec5aa0b7846082a2415f0902f0da88f2-Abstract.html",
        "author": "Teresa Serrano-Gotarredona; Bernab\u00e9 Linares-Barranco; Jos\u00e9 Luis Huertas",
        "abstract": "We  describe  an  analog  VLSI  implementation  of the ARTI  algorithm  (Carpenter, 1987). A prototype chip has been fabricated in a standard low  cost  1.5~m double-metal single-poly CMOS process. It has a die area of  lcm2  and  is  mounted in  a  12O-pins PGA package.  The chip realizes  a  modified version of the original ARTI  architecture.  Such modification  has been shown to preserve all computational properties of the original  algorithm  (Serrano,  1994a),  while  being  more  appropriate  for  VLSI  realizations. The chip implements an ARTI network with 100 F 1 nodes  and 18 F2 nodes. It can therefore cluster 100 binary pixels input patterns  into up to 18 different categories. Modular expansibility of the system is  possible  by  assembling  an  NxM  array  of  chips  without  any  extra  interfacing circuitry, resulting in an F 1 layer with  l00xN nodes, and an  F2  layer  with  18xM nodes.  Pattern  classification  is  performed  in  less  than  1.8~s, which  means  an  equivalent  computing  power of 2.2x109  connections and connection-updates per second. Although internally the  chip is analog in nature, it interfaces to the outside world through digital  signals, thus having a true asynchrounous digital behavior. Experimental  chip  test  results  are  available,  which  have  been  obtained  through  test  equipments for digital chips.",
        "bibtex": "@inproceedings{NIPS1994_ec5aa0b7,\n author = {Serrano-Gotarredona, Teresa and Linares-Barranco, Bernab\\'{e} and Huertas, Jos\\'{e}},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {A Real Time Clustering CMOS Neural Engine},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/ec5aa0b7846082a2415f0902f0da88f2-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/ec5aa0b7846082a2415f0902f0da88f2-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/ec5aa0b7846082a2415f0902f0da88f2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1581379,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6569216722156574175&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Dept. of Analog Design, National Microelectronics Center (CNM), Ed. CICA, Av. Reina Mercedes sIn, 41012 Sevilla, SPAIN; Dept. of Analog Design, National Microelectronics Center (CNM), Ed. CICA, Av. Reina Mercedes sIn, 41012 Sevilla, SPAIN; Dept. of Analog Design, National Microelectronics Center (CNM), Ed. CICA, Av. Reina Mercedes sIn, 41012 Sevilla, SPAIN",
        "aff_domain": "cnm.us.es;cnm.us.es;cnm.us.es",
        "email": "cnm.us.es;cnm.us.es;cnm.us.es",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Dept. of Analog Design, National Microelectronics Center (CNM), Ed. CICA, Av. Reina Mercedes sIn, 41012 Sevilla, SPAIN",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "05299be975",
        "title": "A Rigorous Analysis of Linsker-type Hebbian Learning",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/7634ea65a4e6d9041cfd3f7de18e334a-Abstract.html",
        "author": "J. Feng; H. Pan; V. P. Roychowdhury",
        "abstract": "We propose a novel rigorous approach for the analysis of Linsker's  unsupervised Hebbian learning network. The behavior of this  model is determined by the underlying nonlinear dynamics which  are parameterized by a set of parameters originating from the Heb(cid:173) bian rule and the arbor density of the synapses. These parameters  determine the presence or absence of a specific receptive field (also  referred to as a 'connection pattern') as a saturated fixed point  attractor of the model. In this paper, we perform a qualitative  analysis of the underlying nonlinear dynamics over the parameter  space, determine the effects of the system parameters on the emer(cid:173) gence of various receptive fields, and predict precisely within which  parameter regime the network will have the potential to develop  a specially designated connection pattern. In particular, this ap(cid:173) proach exposes, for the first time, the crucial role played by the  synaptic density functions, and provides a complete precise picture  of the parameter space that defines the relationships among the  different receptive fields. Our theoretical predictions are confirmed  by numerical simulations.",
        "bibtex": "@inproceedings{NIPS1994_7634ea65,\n author = {Feng, J. and Pan, H. and Roychowdhury, V. P.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {A Rigorous Analysis of Linsker-type Hebbian Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/7634ea65a4e6d9041cfd3f7de18e334a-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/7634ea65a4e6d9041cfd3f7de18e334a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/7634ea65a4e6d9041cfd3f7de18e334a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1654253,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16846312482875659069&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Mathematical Department, University of Rome 'La Sapienza'; School of Electrical Engineering, Purdue University; School of Electrical Engineering, Purdue University",
        "aff_domain": "uniroma1.it;ecn.purdue.edu;drum.ecn.purdue.edu",
        "email": "uniroma1.it;ecn.purdue.edu;drum.ecn.purdue.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Mathematical Department, University of Rome 'La Sapienza';School of Electrical Engineering, Purdue University",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "095d739698",
        "title": "A Silicon Axon",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/aa169b49b583a2b5af89203c2b78c67c-Abstract.html",
        "author": "Bradley A. Minch; Paul E. Hasler; Chris Diorio; Carver Mead",
        "abstract": "We  present  a  silicon  model of an  axon  which  shows  promise  as  a  building  block for  pulse-based  neural  computations involving cor(cid:173) relations of pulses  across  both space  and time.  The  circuit shares  a  number  of features  with  its  biological  counterpart  including  an  excitation  threshold,  a  brief refractory  period  after  pulse  comple(cid:173) tion, pulse amplitude restoration, and pulse width restoration.  We  provide  a simple explanation of circuit operation and present  data  from  a  chip fabricated  in  a standard  2Jlm  CMOS  process  through  the MOS  Implementation Service  (MOSIS).  We  emphasize the ne(cid:173) cessity of the restoration of the width of the pulse in time for stable  propagation in  axons.",
        "bibtex": "@inproceedings{NIPS1994_aa169b49,\n author = {Minch, Bradley and Hasler, Paul and Diorio, Chris and Mead, Carver},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {A Silicon Axon},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/aa169b49b583a2b5af89203c2b78c67c-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/aa169b49b583a2b5af89203c2b78c67c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/aa169b49b583a2b5af89203c2b78c67c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1494496,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2418996395669637548&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Physics of Computation Laboratory, California Institute of Technology; Physics of Computation Laboratory, California Institute of Technology; Physics of Computation Laboratory, California Institute of Technology; Physics of Computation Laboratory, California Institute of Technology",
        "aff_domain": "pcmp.caltech.edu;pcmp.caltech.edu;pcmp.caltech.edu;pcmp.caltech.edu",
        "email": "pcmp.caltech.edu;pcmp.caltech.edu;pcmp.caltech.edu;pcmp.caltech.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Physics of Computation Laboratory, California Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "48339593e0",
        "title": "A Study of Parallel Perturbative Gradient Descent",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/b56a18e0eacdf51aa2a5306b0f533204-Abstract.html",
        "author": "D. Lippe; Joshua Alspector",
        "abstract": "We  have  continued  our  study  of a  parallel  perturbative  learning  method  [Alspector  et al.,  1993]  and implications for  its implemen(cid:173) tation in analog VLSI. Our new results indicate that, in most cases,  a single parallel perturbation (per pattern presentation) of the func(cid:173) tion  parameters  (weights in a  neural network)  is  theoretically  the  best  course.  This  is  not  true,  however,  for  certain  problems  and  may  not  generally  be  true  when  faced  with  issues  of implemen(cid:173) tation  such  as limited  precision.  In  these  cases,  multiple  parallel  perturbations may be best as indicated in our previous results.",
        "bibtex": "@inproceedings{NIPS1994_b56a18e0,\n author = {Lippe, D. and Alspector, Joshua},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {A Study of Parallel Perturbative Gradient Descent},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/b56a18e0eacdf51aa2a5306b0f533204-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/b56a18e0eacdf51aa2a5306b0f533204-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/b56a18e0eacdf51aa2a5306b0f533204-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1522204,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5300734617265213920&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Bellcore; Bellcore + Dept. of EECS, MIT",
        "aff_domain": "mit.edu; ",
        "email": "mit.edu; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "Bellcore;Dept. of EECS, MIT",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "68f0b10e5f",
        "title": "A model of the hippocampus combining self-organization and associative memory function",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/c22abfa379f38b5b0411bc11fa9bf92f-Abstract.html",
        "author": "Michael E. Hasselmo; Eric Schnell; Joshua Berke; Edi Barkai",
        "abstract": "A model of the hippocampus is presented which forms rapid self -orga(cid:173) nized representations of input arriving via the perforant path, performs  recall of previous associations in region CA3, and performs comparison  of this recall with afferent input in region CA 1.  This comparison drives  feedback regulation of cholinergic modulation to set appropriate  dynamics for learning of new representations in region CA3 and CA 1.  The network responds to novel patterns with increased cholinergic mod(cid:173) ulation, allowing storage of new self-organized representations, but  responds to familiar patterns with a decrease in acetylcholine,  allowing  recall based on previous representations.  This requires selectivity of the  cholinergic suppression of synaptic transmission in stratum radiatum of  regions CA3 and CAl, which has been demonstrated experimentally.",
        "bibtex": "@inproceedings{NIPS1994_c22abfa3,\n author = {Hasselmo, Michael and Schnell, Eric and Berke, Joshua and Barkai, Edi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {A model of the hippocampus combining self-organization and associative memory function},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/c22abfa379f38b5b0411bc11fa9bf92f-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/c22abfa379f38b5b0411bc11fa9bf92f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/c22abfa379f38b5b0411bc11fa9bf92f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1713689,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3491824730960497900&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Dept. of Psychology, Harvard University; Dept. of Psychology, Harvard University; ; ",
        "aff_domain": "katla.harvard.edu; ; ; ",
        "email": "katla.harvard.edu; ; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Dept. of Psychology, Harvard University",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "ad5712f056",
        "title": "A solvable connectionist model of immediate recall of ordered lists",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/f47d0ad31c4c49061b9e505593e3db98-Abstract.html",
        "author": "Neil Burgess",
        "abstract": "A  model  of short-term  memory for  serially  ordered  lists  of verbal  stimuli is proposed as an implementation of the 'articulatory loop'  thought  to  mediate  this  type  of memory  (Baddeley,  1986).  The  model predicts the presence  of a  repeatable time-varying 'context'  signal  coding  the  timing  of items'  presentation  in  addition  to  a  store  of phonological information and a  process  of serial rehearsal.  Items are associated with context nodes and phonemes by Hebbian  connections showing both short and long term plasticity.  Items are  activated  by  phonemic input  during  presentation  and reactivated  by context and phonemic feedback  during output.  Serial selection  of items  occurs  via a  winner-take-all  interaction  amongst  items,  with  the  winner  subsequently  receiving  decaying  inhibition.  An  approximate analysis  of error  probabilities due  to  Gaussian  noise  during  output  is  presented.  The  model  provides  an  explanatory  account of the  probability of error as  a  function of serial  position,  list  length,  word  length,  phonemic similarity,  temporal  grouping,  item and list familiarity,  and is proposed  as the starting point for  a  model of rehearsal and vocabulary acquisition.",
        "bibtex": "@inproceedings{NIPS1994_f47d0ad3,\n author = {Burgess, Neil},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {A solvable connectionist model of immediate recall of ordered lists},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/f47d0ad31c4c49061b9e505593e3db98-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/f47d0ad31c4c49061b9e505593e3db98-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/f47d0ad31c4c49061b9e505593e3db98-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1710726,
        "gs_citation": 62,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5939453069736162636&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Anatomy, University College London",
        "aff_domain": "ucl.ac.uk",
        "email": "ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Department of Anatomy, University College London",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "cbc4a8aa83",
        "title": "Active Learning for Function Approximation",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/acf4b89d3d503d8252c9c4ba75ddbf6d-Abstract.html",
        "author": "Kah Kay Sung; Partha Niyogi",
        "abstract": "We develop a principled strategy to sample a function optimally for  function  approximation tasks within a  Bayesian framework.  Using  ideas  from  optimal experiment  design,  we  introduce  an  objective  function (incorporating both bias and variance)  to measure the de(cid:173) gree  of approximation, and the  potential utility of the  data points  towards optimizing this objective.  We show how  the general strat(cid:173) egy  can be used  to  derive  precise  algorithms to select  data for  two  cases:  learning  unit  step  functions  and  polynomial functions.  In  particular, we investigate whether such active algorithms can learn  the  target  with fewer  examples.  We obtain theoretical  and empir(cid:173) ical  results  to suggest  that  this is  the  case.",
        "bibtex": "@inproceedings{NIPS1994_acf4b89d,\n author = {Sung, Kah and Niyogi, Partha},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Active Learning for Function Approximation},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/acf4b89d3d503d8252c9c4ba75ddbf6d-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/acf4b89d3d503d8252c9c4ba75ddbf6d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/acf4b89d3d503d8252c9c4ba75ddbf6d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1646259,
        "gs_citation": 68,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16298890877399999641&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Massachusetts Institute of Technology Artificial Intelligence Laboratory; Massachusetts Institute of Technology Artificial Intelligence Laboratory",
        "aff_domain": "ai.mit.edu;ai.mit.edu",
        "email": "ai.mit.edu;ai.mit.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology Artificial Intelligence Laboratory",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "9b604515e9",
        "title": "Active Learning with Statistical Models",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/7f975a56c761db6506eca0b37ce6ec87-Abstract.html",
        "author": "David A. Cohn; Zoubin Ghahramani; Michael I. Jordan",
        "abstract": "For many types of learners one can compute the statistically \"op(cid:173) timal\" way to select data. We review how these techniques have  been used with feedforward neural networks [MacKay, 1992; Cohn,  1994] . We then show how the same principles may be used to select  data for two alternative, statistically-based learning architectures:  mixtures of Gaussians and locally weighted regression. While the  techniques for neural networks are expensive and approximate, the  techniques for mixtures of Gaussians and locally weighted regres(cid:173) sion are both efficient and accurate.",
        "bibtex": "@inproceedings{NIPS1994_7f975a56,\n author = {Cohn, David and Ghahramani, Zoubin and Jordan, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Active Learning with Statistical Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/7f975a56c761db6506eca0b37ce6ec87-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/7f975a56c761db6506eca0b37ce6ec87-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/7f975a56c761db6506eca0b37ce6ec87-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1393320,
        "gs_citation": 2672,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12149325910758245378&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 31,
        "aff": "Department of Brain and Cognitive Sciences; Department of Brain and Cognitive Sciences; Department of Brain and Cognitive Sciences",
        "aff_domain": "cohnQpsyche.mit.edu; zoubinQpsyche.mit.edu; jordan~syche.mit.edu",
        "email": "cohnQpsyche.mit.edu; zoubinQpsyche.mit.edu; jordan~syche.mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Department of Brain and Cognitive Sciences",
        "aff_unique_url": "https://web.mit.edu/bcs/",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "be39b0cbcc",
        "title": "Adaptive Elastic Input Field for Recognition Improvement",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/e744f91c29ec99f0e662c9177946c627-Abstract.html",
        "author": "Minoru Asogawa",
        "abstract": "For machines to perform classification tasks, such as speech and  character recognition, appropriately handling deformed patterns  is a key to achieving high performance. The authors presents a  new type of classification system, an Adaptive Input Field Neu(cid:173) ral Network (AIFNN), which includes a simple pre-trained neural  network and an elastic input field attached to an input layer. By  using an iterative method, AIFNN can determine an optimal affine  translation for an elastic input field to compensate for the original  deformations. The convergence of the AIFNN algorithm is shown.  AIFNN is applied for handwritten numerals recognition. Conse(cid:173) quently, 10.83% of originally misclassified patterns are correctly  categorized and total performance is improved, without modifying  the neural network.",
        "bibtex": "@inproceedings{NIPS1994_e744f91c,\n author = {Asogawa, Minoru},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Adaptive Elastic Input Field for Recognition Improvement},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/e744f91c29ec99f0e662c9177946c627-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/e744f91c29ec99f0e662c9177946c627-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/e744f91c29ec99f0e662c9177946c627-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1517914,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:pqNaR5PyuC8J:scholar.google.com/&scioq=Adaptive+Elastic+Input+Field+for+Recognition+Improvement&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "C&C Research Laboratories, NEe Miyamae, Miyazaki, Kawasaki Kanagawa 213 Japan",
        "aff_domain": "asogawa~csl.cl.nec.co.jp",
        "email": "asogawa~csl.cl.nec.co.jp",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "C&C Research Laboratories, NEe Miyamae, Miyazaki, Kawasaki Kanagawa 213 Japan",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "ee3f3e5b6b",
        "title": "Advantage Updating Applied to a Differential Game",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/2a9d121cd9c3a1832bb6d2cc6bd7a8a7-Abstract.html",
        "author": "Mance E. Harmon; Leemon C. Baird III; A. Harry Klopf",
        "abstract": "An application of reinforcement learning to a linear-quadratic, differential  game  is  presented.  The reinforcement learning  system  uses  a  recently  developed  algorithm,  the residual gradient form  of advantage updating.  The  game  is a  Markov  Decision  Process  (MDP)  with continuous  time,  states, and actions, linear dynamics, and a quadratic cost function.  The  game consists of two players, a missile and a plane;  the missile pursues  the plane and  the plane evades the  missile.  The reinforcement learning  algorithm for optimal control is modified for differential games in order to  find the minimax point, rather than  the maximum.  Simulation results are  compared  to  the  optimal  solution,  demonstrating  that  the  simulated  reinforcement learning  system  converges  to  the optimal  answer.  The  performance of both the residual gradient and non-residual gradient forms  of advantage updating and Q-learning are compared.  The results show that  advantage  updating  converges faster  than  Q-learning in  all  simulations.  The results also show advantage updating converges regardless of the time  step duration; Q-learning is  unable to converge as the  time step duration  ~rows small.",
        "bibtex": "@inproceedings{NIPS1994_2a9d121c,\n author = {Harmon, Mance E. and Baird, Leemon and Klopf, A. Harry},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Advantage Updating Applied to a Differential Game},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/2a9d121cd9c3a1832bb6d2cc6bd7a8a7-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/2a9d121cd9c3a1832bb6d2cc6bd7a8a7-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/2a9d121cd9c3a1832bb6d2cc6bd7a8a7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1450180,
        "gs_citation": 61,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15687781602590261916&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Wright Laboratory; Wright Laboratory; Wright Laboratory + U.S.A.F. Academy",
        "aff_domain": "aa.wpafb.mil;aa.wpafb.mil;cs.usafa.af.mil",
        "email": "aa.wpafb.mil;aa.wpafb.mil;cs.usafa.af.mil",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0+1",
        "aff_unique_norm": "Wright Laboratory;U.S.A.F. Academy",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "2818a3047c",
        "title": "An Actor/Critic Algorithm that is Equivalent to Q-Learning",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/23ce1851341ec1fa9e0c259de10bf87c-Abstract.html",
        "author": "Robert H. Crites; Andrew G. Barto",
        "abstract": "We prove the convergence of an actor/critic algorithm that is equiv(cid:173) alent  to Q-Iearning by construction.  Its equivalence is  achieved by  encoding Q-values within  the  policy  and value function  of the  ac(cid:173) tor and critic.  The resultant actor/critic algorithm is novel in two  ways:  it  updates the critic  only  when  the  most  probable  action  is  executed from  any given state, and it  rewards  the actor using cri(cid:173) teria that depend on the relative probability of the action that was  executed.",
        "bibtex": "@inproceedings{NIPS1994_23ce1851,\n author = {Crites, Robert and Barto, Andrew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {An Actor/Critic Algorithm that is Equivalent to Q-Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/23ce1851341ec1fa9e0c259de10bf87c-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/23ce1851341ec1fa9e0c259de10bf87c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/23ce1851341ec1fa9e0c259de10bf87c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1505957,
        "gs_citation": 58,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5253559638100374895&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "e6cbb787e6",
        "title": "An Alternative Model for Mixtures of Experts",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/c8fbbc86abe8bd6a5eb6a3b4d0411301-Abstract.html",
        "author": "Lei Xu; Michael I. Jordan; Geoffrey E. Hinton",
        "abstract": "We propose an alternative model for mixtures of experts which uses  a  different  parametric form  for  the gating network.  The  modified  model is  trained by the EM  algorithm.  In comparison with earlier  models-trained by either EM  or gradient ascent-there is no need  to  select  a  learning  stepsize.  We  report  simulation experiments  which  show  that  the  new  architecture  yields  faster  convergence.  We  also  apply the new  model to two  problem domains:  piecewise  nonlinear function approximation and the combination of multiple  previously  trained classifiers.",
        "bibtex": "@inproceedings{NIPS1994_c8fbbc86,\n author = {Xu, Lei and Jordan, Michael and Hinton, Geoffrey E},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {An Alternative Model for Mixtures of Experts},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/c8fbbc86abe8bd6a5eb6a3b4d0411301-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/c8fbbc86abe8bd6a5eb6a3b4d0411301-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/c8fbbc86abe8bd6a5eb6a3b4d0411301-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1521785,
        "gs_citation": 413,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3734915090429669736&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Dept. of Computer Science, The Chinese University of Hong Kong; Dept. of Brain and Cognitive Sciences, MIT; Dept. of Computer Science, University of Toronto",
        "aff_domain": "cs.cuhk.hk; ; ",
        "email": "cs.cuhk.hk; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Dept. of Computer Science, The Chinese University of Hong Kong;Dept. of Brain and Cognitive Sciences, MIT;University of Toronto",
        "aff_unique_dep": ";;Department of Computer Science",
        "aff_unique_url": ";;https://www.utoronto.ca",
        "aff_unique_abbr": ";;U of T",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Toronto",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";Canada"
    },
    {
        "id": "18a6304ce2",
        "title": "An Analog Neural Network Inspired by Fractal Block Coding",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/2ba596643cbbbc20318224181fa46b28-Abstract.html",
        "author": "Fernando J. Pineda; Andreas G. Andreou",
        "abstract": "We consider the problem of decoding block coded data, using a physical  dynamical system. We sketch out a decompression algorithm for fractal  block  codes  and  then  show  how  to  implement  a  recurrent  neural  network  using  physically  simple  but highly-nonlinear,  analog  circuit  models of neurons and synapses. The nonlinear system has many fixed  points, but we have at our disposal a procedure to choose the parameters  in  such a way  that only one solution, the desired solution, is stable. As  a  partial  proof of the  concept,  we  present experimental  data  from  a  small system a 16-neuron analog CMOS chip fabricated in a 2m analog  p-well process. This chip operates in the subthreshold regime and,  for  each choice of parameters, converges to a unique stable state. Each state  exhibits a qualitatively fractal shape.",
        "bibtex": "@inproceedings{NIPS1994_2ba59664,\n author = {Pineda, Fernando and Andreou, Andreas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {An Analog Neural Network Inspired by Fractal Block Coding},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/2ba596643cbbbc20318224181fa46b28-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/2ba596643cbbbc20318224181fa46b28-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/2ba596643cbbbc20318224181fa46b28-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1530013,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12753906765659156088&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "The Applied Physics Laboratory, The Johns Hopkins University; Dept. of Electrical & Computer Engineering, The Johns Hopkins University",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "The Applied Physics Laboratory, The Johns Hopkins University;Dept. of Electrical & Computer Engineering, The Johns Hopkins University",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "cfd749abfe",
        "title": "An Auditory Localization and Coordinate Transform Chip",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/1ce927f875864094e3906a4a0b5ece68-Abstract.html",
        "author": "Timothy K. Horiuchi",
        "abstract": "The  localization  and  orientation  to  various  novel  or  interesting  events  in  the  environment  is  a  critical  sensorimotor  ability in  all  animals,  predator  or  prey.  In  mammals,  the  superior  colliculus  (SC)  plays  a  major  role  in  this  behavior,  the  deeper  layers  ex(cid:173) hibiting topographically mapped responses to visual, auditory, and  somatosensory  stimuli.  Sensory  information arriving from  differ(cid:173) ent modalities should  then be  represented  in  the same coordinate  frame.  Auditory  cues,  in  particular,  are  thought to be  computed  in head-based coordinates which must then be transformed to reti(cid:173) nal coordinates.  In this paper, an analog VLSI implementation for  auditory localization in the azimuthal plane is described  which ex(cid:173) tends  the  architecture  proposed for  the  barn owl to a  primate eye  movement system  where  further  transformation  is  required.  This  transformation is intended to model the projection in primates from  auditory  cortical areas to the deeper layers of the primate superior  colliculus.  This  system  is  interfaced  with  an  analog  VLSI-based  saccadic eye movement system also being constructed  in our labo(cid:173) ratory.",
        "bibtex": "@inproceedings{NIPS1994_1ce927f8,\n author = {Horiuchi, Timothy},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {An Auditory Localization and Coordinate Transform Chip},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/1ce927f875864094e3906a4a0b5ece68-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/1ce927f875864094e3906a4a0b5ece68-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/1ce927f875864094e3906a4a0b5ece68-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1252547,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15835564170633677647&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Computation and Neural Systems Program, California Institute of Technology, Pasadena, CA 91125",
        "aff_domain": "cns.caltech.edu",
        "email": "cns.caltech.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Computation and Neural Systems Program, California Institute of Technology, Pasadena, CA 91125",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "14ee3141f4",
        "title": "An Input Output HMM Architecture",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/8065d07da4a77621450aa84fee5656d9-Abstract.html",
        "author": "Yoshua Bengio; Paolo Frasconi",
        "abstract": "We  introduce  a  recurrent  architecture  having  a  modular structure  and we formulate a training procedure based on the EM  algorithm.  The resulting model has similarities to hidden  Markov models, but  supports  recurrent  networks  processing style and allows  to exploit  the supervised  learning paradigm while using maximum likelihood  estimation.",
        "bibtex": "@inproceedings{NIPS1994_8065d07d,\n author = {Bengio, Yoshua and Frasconi, Paolo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {An Input Output HMM Architecture},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/8065d07da4a77621450aa84fee5656d9-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/8065d07da4a77621450aa84fee5656d9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/8065d07da4a77621450aa84fee5656d9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1962650,
        "gs_citation": 533,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8431752505985660773&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Dept. Informatique et Recherche Operationnelle, Universite de Montreal, Qc H3C-3J7 + AT&T Bell Labs, Holmdel, N J 07733; Dipartimento di Sistemi e Informatica, Universita di Firenze (Italy)",
        "aff_domain": "bengioyOIRO.UMontreal.CA; paoloOmcculloch.ing.unifi.it",
        "email": "bengioyOIRO.UMontreal.CA; paoloOmcculloch.ing.unifi.it",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2",
        "aff_unique_norm": "Dept. Informatique et Recherche Operationnelle, Universite de Montreal, Qc H3C-3J7;AT&T Bell Labs, Holmdel, N J 07733;Dipartimento di Sistemi e Informatica, Universita di Firenze (Italy)",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "e390a778aa",
        "title": "An Integrated Architecture of Adaptive Neural Network Control for Dynamic Systems",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/6cfe0e6127fa25df2a0ef2ae1067d915-Abstract.html",
        "author": "Ke Liu; Robert L. Tokar; Brain D. McVey",
        "abstract": "In  this  study,  an  integrated  neural  network  control  architecture  for  nonlinear  dynamic  systems  is  presented.  Most of the recent emphasis in  the neural network control field has no error feedback as the  control  input,  which  rises  the  lack  of adaptation  problem.  The  integrated  architecture  in  this  paper  combines  feed  forward  control  and error  feedback  adaptive  control  using  neural  networks.  The  paper  reveals the  different  internal  functionality  of these  two  kinds of neural  network  controllers  for  certain  input  styles,  e.g.,  state  feedback  and error feedback.  With  error  feedback,  neural  network  controllers  learn  the  slopes  or  the  gains  with  respect  to  the  error  feedback,  producing  an  error  driven  adaptive  control  systems.  The  results  demonstrate  that  the  two  kinds  of control  scheme  can  be  combined  to  realize  their individual  advantages.  Testing  with disturbances added to  the  plant  shows  good  tracking  and adaptation with the integrated neural control architecture.",
        "bibtex": "@inproceedings{NIPS1994_6cfe0e61,\n author = {Liu, Ke and Tokar, Robert and McVey, Brain},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {An Integrated Architecture of Adaptive Neural Network Control for Dynamic Systems},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/6cfe0e6127fa25df2a0ef2ae1067d915-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/6cfe0e6127fa25df2a0ef2ae1067d915-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/6cfe0e6127fa25df2a0ef2ae1067d915-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1486283,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3523073760206189560&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "c4c8a8fc47",
        "title": "An experimental comparison of recurrent neural networks",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/31b3b31a1c2f8a370206f111127c0dbd-Abstract.html",
        "author": "Bill G. Horne; C. Lee Giles",
        "abstract": "Many  different  discrete-time  recurrent  neural  network  architec(cid:173) tures  have  been  proposed.  However,  there  has  been  virtually  no  effort  to compare these arch:tectures experimentally.  In  this paper  we  review  and categorize many of these architectures and compare  how  they  perform on various classes  of simple problems including  grammatical inference  and nonlinear system identification.",
        "bibtex": "@inproceedings{NIPS1994_31b3b31a,\n author = {Horne, Bill and Giles, C.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {An experimental comparison of recurrent neural networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/31b3b31a1c2f8a370206f111127c0dbd-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/31b3b31a1c2f8a370206f111127c0dbd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/31b3b31a1c2f8a370206f111127c0dbd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1582729,
        "gs_citation": 182,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8570192022259079091&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "NEe Research Institute; NEe Research Institute + UMIACS, University of Maryland, College Park, MD 20742",
        "aff_domain": "research.nj.nec.com;research.nj.nec.com",
        "email": "research.nj.nec.com;research.nj.nec.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "NEe Research Institute;UMIACS, University of Maryland, College Park, MD 20742",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "0f7b6904dd",
        "title": "Analysis of Unstandardized Contributions in Cross Connected Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/7504adad8bb96320eb3afdd4df6e1f60-Abstract.html",
        "author": "Thomas R. Shultz; Yuriko Oshima-Takane; Yoshio Takane",
        "abstract": "Understanding  knowledge  representations  in  neural  nets  has  been  a  difficult  problem.  Principal  components  analysis  (PCA)  of  contributions (products of sending activations and connection weights)  has yielded valuable insights into knowledge representations, but much  of this work has focused on the correlation matrix of contributions. The  present work  shows  that analyzing  the  variance-covariance matrix  of  contributions yields more valid  insights by taking account of weights.",
        "bibtex": "@inproceedings{NIPS1994_7504adad,\n author = {Shultz, Thomas and Oshima-Takane, Yuriko and Takane, Yoshio},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Analysis of Unstandardized Contributions in Cross Connected Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/7504adad8bb96320eb3afdd4df6e1f60-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/7504adad8bb96320eb3afdd4df6e1f60-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/7504adad8bb96320eb3afdd4df6e1f60-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1786910,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15646759672952996992&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Psychology, McGill University; Department of Psychology, McGill University; Department of Psychology, McGill University",
        "aff_domain": "psych.mcgill.ca;psych.mcgill.ca;psych.mcgill.ca",
        "email": "psych.mcgill.ca;psych.mcgill.ca;psych.mcgill.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Department of Psychology, McGill University",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "1ee64f760c",
        "title": "Anatomical origin and computational role of diversity in the response properties of cortical neurons",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/6395ebd0f4b478145ecfbaf939454fa4-Abstract.html",
        "author": "Kalanit Grill Spector; Shimon Edelman; Rafael Malach",
        "abstract": "The  maximization of  diversity  of neuronal  response  properties  has  been  recently  suggested  as  an  organizing  principle  for  the  formation  of such  prominent features of the functional architecture of the brain as the corti(cid:173) cal columns and the associated patchy projection patterns (Malach,  1994).  We show that (1) maximal diversity is attained when the ratio of dendritic  and  axonal  arbor  sizes  is  equal  to  one,  as  found  in  many  cortical  areas  and  across  species  (Lund  et al.,  1993;  Malach,  1994),  and  (2)  that maxi(cid:173) mization of diversity leads  to  better  performance  in  systems of receptive  fields  implementing steerable/shiftable  filters,  and  in  matching spatially  distributed  signals,  a  problem that  arises  in  many high-level  visual tasks.",
        "bibtex": "@inproceedings{NIPS1994_6395ebd0,\n author = {Spector, Kalanit and Edelman, Shimon and Malach, Rafael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Anatomical origin and computational role of diversity in the response properties of cortical neurons},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/6395ebd0f4b478145ecfbaf939454fa4-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/6395ebd0f4b478145ecfbaf939454fa4-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/6395ebd0f4b478145ecfbaf939454fa4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1487240,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11130820970428446703&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "46b98f6de5",
        "title": "Associative Decorrelation Dynamics: A Theory of Self-Organization and Optimization in Feedback Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/766d856ef1a6b02f93d894415e6bfa0e-Abstract.html",
        "author": "Dawei W. Dong",
        "abstract": "This  paper  outlines  a  dynamic  theory  of  development  and  adap(cid:173) tation  in  neural  networks  with  feedback  connections.  Given  in(cid:173) put ensemble,  the  connections  change in strength according  to  an  associative  learning  rule  and  approach  a  stable  state  where  the  neuronal  outputs  are  decorrelated .  We  apply  this  theory  to  pri(cid:173) mary  visual  cortex and  examine  the implications of the  dynamical  decorrelation  of the  activities  of orientation  selective  cells  by  the  intracortical connections.  The theory gives a  unified  and quantita(cid:173) tive explanation of the  psychophysical experiments on  orientation  contrast and orientation adaptation.  Using only one parameter, we  achieve  good  agreements  between  the  theoretical  predictions  and  the experimental data.",
        "bibtex": "@inproceedings{NIPS1994_766d856e,\n author = {Dong, Dawei},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Associative Decorrelation Dynamics: A Theory of Self-Organization and Optimization in Feedback Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/766d856ef1a6b02f93d894415e6bfa0e-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/766d856ef1a6b02f93d894415e6bfa0e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/766d856ef1a6b02f93d894415e6bfa0e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1675005,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15234673988240825114&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "0b18a14a9b",
        "title": "Asymptotics of Gradient-based Neural Network Training Algorithms",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/9f53d83ec0691550f7d2507d57f4f5a2-Abstract.html",
        "author": "Sayandev Mukherjee; Terrence L. Fine",
        "abstract": "We study the asymptotic properties of the sequence of iterates of  weight-vector estimates obtained by training a multilayer feed for(cid:173) ward neural network with a basic gradient-descent method using  a fixed learning constant and no batch-processing. In the one(cid:173) dimensional case, an exact analysis establishes the existence of a  limiting distribution that is not Gaussian in general. For the gen(cid:173) eral case and small learning constant, a linearization approximation  permits the application of results from the theory of random ma(cid:173) trices to again establish the existence of a limiting distribution.  We study the first few moments of this distribution to compare  and contrast the results of our analysis with those of techniques of  stochastic approximation.",
        "bibtex": "@inproceedings{NIPS1994_9f53d83e,\n author = {Mukherjee, Sayandev and Fine, Terrence L.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Asymptotics of Gradient-based Neural Network Training Algorithms},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/9f53d83ec0691550f7d2507d57f4f5a2-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/9f53d83ec0691550f7d2507d57f4f5a2-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/9f53d83ec0691550f7d2507d57f4f5a2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1565290,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2776695877789774067&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "School of Electrical Engineering, Cornell University, Ithaca, NY 14853; School of Electrical Engineering, Cornell University, Ithaca, NY 14853",
        "aff_domain": "ee.comell.edu;ee.comell.edu",
        "email": "ee.comell.edu;ee.comell.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "School of Electrical Engineering, Cornell University, Ithaca, NY 14853",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "f5861e3ea7",
        "title": "Bayesian Query Construction for Neural Network Models",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/a9b7ba70783b617e9998dc4dd82eb3c5-Abstract.html",
        "author": "Gerhard Paass; J\u00f6rg Kindermann",
        "abstract": "If data collection is costly, there is much to be gained by actively se(cid:173) lecting particularly informative data points in a sequential  way.  In  a  Bayesian decision-theoretic framework we  develop  a  query selec(cid:173) tion  criterion  which  explicitly takes  into account  the intended  use  of the model predictions.  By  Markov Chain Monte Carlo methods  the  necessary  quantities  can  be  approximated to  a  desired  preci(cid:173) sion.  As  the  number  of data  points  grows,  the  model  complexity  is  modified  by  a  Bayesian  model  selection  strategy.  The  proper(cid:173) ties of two  versions  of the criterion  ate demonstrated in numerical  experiments.",
        "bibtex": "@inproceedings{NIPS1994_a9b7ba70,\n author = {Paass, Gerhard and Kindermann, J\\\"{o}rg},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Bayesian Query Construction for Neural Network Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/a9b7ba70783b617e9998dc4dd82eb3c5-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/a9b7ba70783b617e9998dc4dd82eb3c5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/a9b7ba70783b617e9998dc4dd82eb3c5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1663384,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4178914578190560082&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "0ee7d82144",
        "title": "Bias, Variance and the Combination of Least Squares Estimators",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/0b8aff0438617c055eb55f0ba5d226fa-Abstract.html",
        "author": "Ronny Meir",
        "abstract": "We consider the effect of combining several least squares estimators  on  the expected  performance of a  regression  problem.  Computing  the exact bias and  variance curves  as  a function of the sample size  we  are able to quantitatively compare the effect of the combination  on the bias and variance separately, and thus on the expected  error  which  is  the sum of the  two.  Our exact  calculations,  demonstrate  that the combination of estimators is particularly useful  in the case  where the data set is small and noisy and the function to be learned  is  unrealizable.  For  large  data sets  the  single  estimator produces  superior  results.  Finally,  we  show  that  by  splitting  the  data set  into  several  independent  parts  and  training  each  estimator  on  a  different subset,  the performance can in some cases  be significantly  improved.",
        "bibtex": "@inproceedings{NIPS1994_0b8aff04,\n author = {Meir, Ronny},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Bias, Variance and the Combination of Least Squares Estimators},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/0b8aff0438617c055eb55f0ba5d226fa-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/0b8aff0438617c055eb55f0ba5d226fa-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/0b8aff0438617c055eb55f0ba5d226fa-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1763982,
        "gs_citation": 65,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18061678170884415133&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Faculty of Electrical Engineering, Technion, Haifa 32000, Israel",
        "aff_domain": "rmeirGee.technion.ac.il",
        "email": "rmeirGee.technion.ac.il",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Faculty of Electrical Engineering, Technion, Haifa 32000, Israel",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "5f58cfd52b",
        "title": "Boltzmann Chains and Hidden Markov Models",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/4e0cb6fb5fb446d1c92ede2ed8780188-Abstract.html",
        "author": "Lawrence K. Saul; Michael I. Jordan",
        "abstract": "We  propose  a  statistical  mechanical  framework  for  the  modeling  of discrete  time series.  Maximum likelihood estimation is  done via  Boltzmann learning in one-dimensional networks with tied weights.  We  call  these  networks  Boltzmann  chains  and  show  that  they  contain  hidden  Markov  models  (HMMs)  as  a  special  case.  Our  framework  also  motivates  new  architectures  that  address  partic(cid:173) ular  shortcomings of HMMs.  We  look  at two  such  architectures:  parallel  chains  that model feature  sets  with  disparate  time scales,  and  looped  networks  that  model long-term  dependencies  between  hidden  states.  For  these  networks,  we  show  how  to  implement  the  Boltzmann learning rule  exactly,  in  polynomial time,  without  resort  to simulated or  mean-field  annealing.  The  necessary  com(cid:173) putations are done by exact decimation procedures from statistical  mechanics.",
        "bibtex": "@inproceedings{NIPS1994_4e0cb6fb,\n author = {Saul, Lawrence and Jordan, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Boltzmann Chains and Hidden Markov Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/4e0cb6fb5fb446d1c92ede2ed8780188-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/4e0cb6fb5fb446d1c92ede2ed8780188-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/4e0cb6fb5fb446d1c92ede2ed8780188-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1494718,
        "gs_citation": 138,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2998265012621164615&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "b44c4297b5",
        "title": "Boosting the Performance of RBF Networks with Dynamic Decay Adjustment",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/c8c41c4a18675a74e01c8a20e8a0f662-Abstract.html",
        "author": "Michael R. Berthold; Jay Diamond",
        "abstract": "Radial  Basis  Function  (RBF)  Networks,  also  known  as  networks  of locally-tuned processing units  (see  [6])  are well  known for  their  ease  of use.  Most  algorithms  used  to  train  these  types  of  net(cid:173) works,  however,  require a  fixed  architecture, in  which  the number  of units  in  the  hidden  layer  must  be  determined  before  training  starts.  The RCE training algorithm, introduced by  Reilly,  Cooper  and Elbaum  (see  [8]),  and its  probabilistic extension,  the P-RCE  algorithm,  take advantage of a  growing structure in  which  hidden  units are only introduced when  necessary.  The nature of these al(cid:173) gorithms allows training to reach stability much faster  than is  the  case  for  gradient-descent  based  methods.  Unfortunately  P-RCE  networks do not adjust the standard deviation of their prototypes  individually, using only one global value for  this parameter.  This  paper introduces the Dynamic Decay Adjustment  (DDA)  al(cid:173) gorithm  which  utilizes  the  constructive  nature  of the  P-RCE  al(cid:173) gorithm together with independent adaptation of each prototype's  decay factor.  In addition, this radial adjustment is  class dependent  and  distinguishes  between  different  neighbours.  It is  shown  that  networks  trained  with  the  presented  algorithm  perform  substan(cid:173) tially better than common RBF networks.",
        "bibtex": "@inproceedings{NIPS1994_c8c41c4a,\n author = {Berthold, Michael and Diamond, Jay},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Boosting the Performance of RBF Networks with Dynamic Decay Adjustment},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/c8c41c4a18675a74e01c8a20e8a0f662-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/c8c41c4a18675a74e01c8a20e8a0f662-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/c8c41c4a18675a74e01c8a20e8a0f662-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1468679,
        "gs_citation": 198,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12542764783857640177&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Intel Corporation; Forschungszentrum Informatik Gruppe ACID (Prof. D. Schmid)",
        "aff_domain": "mipos3.intel.com;fzLde",
        "email": "mipos3.intel.com;fzLde",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Intel Corporation;Forschungszentrum Informatik Gruppe ACID (Prof. D. Schmid)",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.intel.com;",
        "aff_unique_abbr": "Intel;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "27037eb1ff",
        "title": "Capacity and Information Efficiency of a Brain-like Associative Net",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/8f468c873a32bb0619eaeb2050ba45d1-Abstract.html",
        "author": "Bruce Graham; David Willshaw",
        "abstract": "We have determined the capacity and information efficiency of an  associative net configured in a brain-like way with partial connec(cid:173) tivity and noisy input cues. Recall theory was used to calculate  the capacity when pattern recall is achieved using a winners-take(cid:173) all strategy. Transforming the dendritic sum according to input  activity and unit usage can greatly increase the capacity of the  associative net under these conditions. For moderately sparse pat(cid:173) terns, maximum information efficiency is achieved with very low  connectivity levels (~ 10%). This corresponds to the level of con(cid:173) nectivity commonly seen in the brain and invites speculation that  the brain is connected in the most information efficient way.",
        "bibtex": "@inproceedings{NIPS1994_8f468c87,\n author = {Graham, Bruce and Willshaw, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Capacity and Information Efficiency of a Brain-like Associative Net},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/8f468c873a32bb0619eaeb2050ba45d1-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/8f468c873a32bb0619eaeb2050ba45d1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/8f468c873a32bb0619eaeb2050ba45d1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1495336,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8881295199783634082&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Centre for Cognitive Science, University of Edinburgh; Centre for Cognitive Science, University of Edinburgh",
        "aff_domain": "cns.ed.ac.uk;cns.ed.ac.uk",
        "email": "cns.ed.ac.uk;cns.ed.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Centre for Cognitive Science, University of Edinburgh",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "00854a0c44",
        "title": "Catastrophic Interference in Human Motor Learning",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/ca75910166da03ff9d4655a0338e6b09-Abstract.html",
        "author": "Tom Brashers-Krug; Reza Shadmehr; Emanuel Todorov",
        "abstract": "Biological sensorimotor systems are not static maps that transform  input  (sensory  information)  into  output  (motor  behavior).  Evi(cid:173) dence  from  many  lines  of research  suggests  that  their  representa(cid:173) tions are plastic, experience-dependent entities.  While this plastic(cid:173) ity is  essential for  flexible  behavior, it presents  the nervous system  with difficult organizational challenges.  If the sensorimotor system  adapts itself to perform well  under one set of circumstances,  will it  then perform poorly when placed  in an environment with different  demands  (negative  transfer)?  Will  a  later  experience-dependent  change  undo  the benefits  of previous  learning  (catastrophic  inter(cid:173) ference)?  We  explore  the first  question  in  a separate paper in  this  volume  (Shadmehr et  al.  1995).  Here  we  present  psychophysical  and computational results that explore the question of catastrophic  interference  in the context of a  dynamic motor learning task.  Un(cid:173) der  some conditions, subjects show  evidence  of catastrophic  inter(cid:173) ference.  Under  other  conditions,  however,  subjects  appear  to  be  immune to  its  effects.  These  results  suggest  that  motor  learning  can  undergo  a  process  of consolidation.  Modular neural  networks  are  well  suited  for  the  demands of learning multiple input/output  mappings.  By  incorporating the notion of fast- and slow-changing  connections  into  a  modular architecture,  we  were  able  to  account  for  the psychophysical results.",
        "bibtex": "@inproceedings{NIPS1994_ca759101,\n author = {Brashers-Krug, Tom and Shadmehr, Reza and Todorov, Emanuel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Catastrophic Interference in Human Motor Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/ca75910166da03ff9d4655a0338e6b09-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/ca75910166da03ff9d4655a0338e6b09-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/ca75910166da03ff9d4655a0338e6b09-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1760179,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8544305732827517152&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Dept. of Brain and Cognitive Sciences, M. I. T., Cambridge, MA 02139; Dept. of Biomedical Eng., Johns Hopkins Univ., Baltimore, MD 21205 + Dept. of Brain and Cognitive Sciences, M. I. T., Cambridge, MA 02139; Dept. of Brain and Cognitive Sciences, M. I. T., Cambridge, MA 02139",
        "aff_domain": "ai.mit.edu;bme.jhu.edu;almit.edu",
        "email": "ai.mit.edu;bme.jhu.edu;almit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+0;0",
        "aff_unique_norm": "Dept. of Brain and Cognitive Sciences, M. I. T., Cambridge, MA 02139;Dept. of Biomedical Eng., Johns Hopkins Univ., Baltimore, MD 21205",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "6f0497197d",
        "title": "Classifying with Gaussian Mixtures and Clusters",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/621461af90cadfdaf0e8d4cc25129f91-Abstract.html",
        "author": "Nanda Kambhatla; Todd K. Leen",
        "abstract": "In this paper, we derive classifiers which are winner-take-all (WTA)  approximations  to  a  Bayes  classifier  with  Gaussian  mixtures  for  class conditional densities.  The derived classifiers include clustering  based algorithms like LVQ  and k-Means.  We propose a  constrained  rank  Gaussian  mixtures model and derive a WTA algorithm for  it.  Our experiments with two speech classification tasks indicate that  the constrained rank model and the WTA approximations improve  the performance over the unconstrained models.",
        "bibtex": "@inproceedings{NIPS1994_621461af,\n author = {Kambhatla, Nanda and Leen, Todd},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Classifying with Gaussian Mixtures and Clusters},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/621461af90cadfdaf0e8d4cc25129f91-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/621461af90cadfdaf0e8d4cc25129f91-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/621461af90cadfdaf0e8d4cc25129f91-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1412836,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9864289006618574019&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "2ba3d38b6f",
        "title": "Coarse-to-Fine Image Search Using Neural Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/fec8d47d412bcbeece3d9128ae855a7a-Abstract.html",
        "author": "Clay Spence; John C. Pearson; Jim Bergen",
        "abstract": "The  efficiency  of image  search  can  be  greatly  improved  by  using  a  coarse-to-fine search strategy with a  multi-resolution image representa(cid:173) tion. However,  if the resolution is so low that the objects have few  dis(cid:173) tinguishing  features,  search  becomes  difficult.  We  show  that  the  performance of search at such low resolutions can be improved by using  context information, i.e., objects visible at low-resolution which are not  the objects of interest but are associated with them. The networks can be  given explicit context information as inputs, or they can learn to detect  the context objects, in which case the user does not have to be aware of  their existence. We also use Integrated Feature Pyramids, which repre(cid:173) sent high-frequency  information  at low  resolutions.  The use of multi(cid:173) resolution search techniques allows us to combine information about the  appearance of the objects on many scales in an efficient way.  A natural  fOlm  of exemplar selection also arises from these  techniques.  We illus(cid:173) trate these ideas by training hierarchical systems of neural  networks to  find clusters of buildings in aerial photographs of farmland.",
        "bibtex": "@inproceedings{NIPS1994_fec8d47d,\n author = {Spence, Clay and Pearson, John and Bergen, Jim},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Coarse-to-Fine Image Search Using Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/fec8d47d412bcbeece3d9128ae855a7a-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/fec8d47d412bcbeece3d9128ae855a7a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/fec8d47d412bcbeece3d9128ae855a7a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1679647,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5690876169879415969&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Nationallnfonnation Display Laboratory; Nationallnfonnation Display Laboratory; Nationallnfonnation Display Laboratory",
        "aff_domain": "sarnoff.com;maca.sarnoff.com;sarnoff.com",
        "email": "sarnoff.com;maca.sarnoff.com;sarnoff.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Nationallnfonnation Display Laboratory",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "caa9a898ba",
        "title": "Combining Estimators Using Non-Constant Weighting Functions",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/6602294be910b1e3c4571bd98c4d5484-Abstract.html",
        "author": "Volker Tresp; Michiaki Taniguchi",
        "abstract": "This paper  discusses  the  linearly  weighted combination of estima(cid:173) tors  in which  the weighting functions  are  dependent  on the input .  We  show  that  the  weighting  functions  can  be  derived  either  by  evaluating  the  input  dependent  variance  of each  estimator  or  by  estimating how  likely  it is  that a given  estimator has  seen  data in  the  region  of the input space  close  to the input pattern.  The lat(cid:173) ter  solution  is  closely  related  to  the  mixture of experts  approach  and we  show how  learning rules  for  the mixture of experts  can  be  derived from the theory  about learning with missing features.  The  presented  approaches  are  modular  since  the  weighting  functions  can  easily  be  modified  (no  retraining)  if more  estimators  are  ad(cid:173) ded.  Furthermore, it is  easy  to incorporate estimators which  were  not  derived from  data such as  expert systems or  algorithms.",
        "bibtex": "@inproceedings{NIPS1994_6602294b,\n author = {Tresp, Volker and Taniguchi, Michiaki},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Combining Estimators Using Non-Constant Weighting Functions},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/6602294be910b1e3c4571bd98c4d5484-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/6602294be910b1e3c4571bd98c4d5484-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/6602294be910b1e3c4571bd98c4d5484-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1515235,
        "gs_citation": 193,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12976200466298569493&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Siemens AG, Central Research; Siemens AG, Central Research",
        "aff_domain": "zfe.siemens.de; ",
        "email": "zfe.siemens.de; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Siemens AG, Central Research",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "18efd9b7f1",
        "title": "Comparing the prediction accuracy of artificial neural networks and other statistical models for breast cancer survival",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/9908279ebbf1f9b250ba689db6a0222b-Abstract.html",
        "author": "Harry B. Burke; David B. Rosen; Philip H. Goodman",
        "abstract": "The  TNM  staging  system  has  been  used  since  the  early  1960's  to  predict  breast  cancer  patient  outcome.  In  an  attempt  to  in(cid:173) crease  prognostic  accuracy,  many putative prognostic factors  have  been  identified.  Because  the  TNM  stage  model  can  not  accom(cid:173) modate  these  new  factors,  the  proliferation  of factors  in  breast  cancer  has  lead  to  clinical  confusion.  What  is  required  is  a  new  computerized  prognostic system  that  can test  putative prognostic  factors  and  integrate  the  predictive  factors  with  the  TNM  vari(cid:173) ables  in order to increase  prognostic  accuracy.  Using  the area un(cid:173) der  the curve of the receiver  operating characteristic,  we  compare  the  accuracy  of the  following  predictive  models  in  terms  of five  year  breast cancer-specific  survival:  pTNM staging system,  princi(cid:173) pal component analysis,  classification and regression  trees,  logistic  regression,  cascade  correlation neural network,  conjugate gradient  descent  neural,  probabilistic neural network,  and backpropagation  neural network.  Several statistical models are significantly more ac-",
        "bibtex": "@inproceedings{NIPS1994_9908279e,\n author = {Burke, Harry B. and Rosen, David B. and Goodman, Philip H.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Comparing the prediction accuracy of artificial neural networks and other statistical models for breast cancer survival},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/9908279ebbf1f9b250ba689db6a0222b-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/9908279ebbf1f9b250ba689db6a0222b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/9908279ebbf1f9b250ba689db6a0222b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1022052,
        "gs_citation": 60,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1997193744627554855&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "5ac46826f5",
        "title": "Computational Structure of coordinate transformations: A generalization study",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/58e4d44e550d0f7ee0a23d6b02d9b0db-Abstract.html",
        "author": "Zoubin Ghahramani; Daniel M. Wolpert; Michael I. Jordan",
        "abstract": "One  of the fundamental properties  that both neural  networks  and  the  central  nervous  system share is  the  ability to learn  and gener(cid:173) alize  from examples.  While this  property  has  been  studied  exten(cid:173) sively  in  the  neural  network  literature  it  has  not  been  thoroughly  explored in human perceptual and motor learning.  We have chosen  a  coordinate  transformation  system-the  visuomotor  map  which  transforms visual coordinates into motor coordinates-to study the  generalization effects  of learning new  input-output  pairs.  Using  a  paradigm of computer  controlled  altered  visual  feedback,  we  have  studied  the  generalization  of the  visuomotor  map  subsequent  to  both local  and context-dependent  remappings.  A  local  remapping  of one  or  two  input-output  pairs  induced  a  significant  global,  yet  decaying,  change  in  the  visuomotor map, suggesting  a  representa(cid:173) tion for  the  map composed of units  with large functional  receptive  fields.  Our study of context-dependent  remappings indicated that  a  single  point  in  visual  space  can  be  mapped to  two different  fin(cid:173) ger  locations  depending  on  a  context  variable-the starting  point  of  the  movement.  Furthermore,  as  the  context  is  varied  there  is  a  gradual  shift  between  the  two  remappings,  consistent  with  two  visuomotor  modules  being  learned  and  gated  smoothly  with  the  context.",
        "bibtex": "@inproceedings{NIPS1994_58e4d44e,\n author = {Ghahramani, Zoubin and Wolpert, Daniel M and Jordan, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Computational Structure of coordinate transformations: A generalization study},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/58e4d44e550d0f7ee0a23d6b02d9b0db-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/58e4d44e550d0f7ee0a23d6b02d9b0db-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/58e4d44e550d0f7ee0a23d6b02d9b0db-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1645372,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11693499978794623749&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": "Department of Brain & Cognitive Sciences, Massachusetts Institute of Technology, Cambridge, MA 02139; Department of Brain & Cognitive Sciences, Massachusetts Institute of Technology, Cambridge, MA 02139; Department of Brain & Cognitive Sciences, Massachusetts Institute of Technology, Cambridge, MA 02139",
        "aff_domain": "psyche.mit.edu;psyche.mit.edu;psyche.mit.edu",
        "email": "psyche.mit.edu;psyche.mit.edu;psyche.mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Department of Brain & Cognitive Sciences, Massachusetts Institute of Technology, Cambridge, MA 02139",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "24f179d2d9",
        "title": "Connectionist Speaker Normalization with Generalized Resource Allocating Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/08fe2621d8e716b02ec0da35256a998d-Abstract.html",
        "author": "Cesare Furlanello; Diego Giuliani; Edmondo Trentin",
        "abstract": "The paper presents  a  rapid speaker-normalization technique based  on  neural  network  spectral  mapping.  The  neural  network  is  used  as a front-end  of a  continuous speech  recognition system  (speaker(cid:173) dependent,  HMM-based) to normalize the input acoustic data from  a  new  speaker.  The  spectral  difference  between  speakers  can  be  reduced  using  a  limited amount  of new  acoustic  data (40  phonet(cid:173) ically  rich  sentences).  Recognition  error  of phone  units  from  the  acoustic-phonetic  continuous  speech  corpus  APASCI  is  decreased  with an adaptability ratio of 25%.  We used  local basis networks of  elliptical  Gaussian  kernels,  with  recursive  allocation  of units  and  on-line  optimization of parameters  (GRAN model).  For  this  ap(cid:173) plication,  the  model included  a  linear  term.  The results  compare  favorably  with  multivariate linear  mapping based  on  constrained  orthonormal transformations.",
        "bibtex": "@inproceedings{NIPS1994_08fe2621,\n author = {Furlanello, Cesare and Giuliani, Diego and Trentin, Edmondo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Connectionist Speaker Normalization with Generalized Resource Allocating Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/08fe2621d8e716b02ec0da35256a998d-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/08fe2621d8e716b02ec0da35256a998d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/08fe2621d8e716b02ec0da35256a998d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1743029,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17325652341606875085&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Istituto per La Ricerca Scientifica e Tecnologica, Povo (Trento), Italy; Istituto per La Ricerca Scientifica e Tecnologica, Povo (Trento), Italy; Istituto per La Ricerca Scientifica e Tecnologica, Povo (Trento), Italy",
        "aff_domain": "lirst.it;lirst.it;lirst.it",
        "email": "lirst.it;lirst.it;lirst.it",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Istituto per La Ricerca Scientifica e Tecnologica, Povo (Trento), Italy",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "0fa6d6e73a",
        "title": "Convergence Properties of the K-Means Algorithms",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/a1140a3d0df1c81e24ae954d935e8926-Abstract.html",
        "author": "L\u00e9on Bottou; Yoshua Bengio",
        "abstract": "This paper studies the convergence properties of the well known  K-Means clustering algorithm. The K-Means algorithm can be de(cid:173) scribed either as a gradient descent algorithm or by slightly extend(cid:173) ing the mathematics of the EM algorithm to this hard threshold  case. We show that the K-Means algorithm actually minimizes the  quantization error using the very fast Newton algorithm.",
        "bibtex": "@inproceedings{NIPS1994_a1140a3d,\n author = {Bottou, L\\'{e}on and Bengio, Yoshua},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Convergence Properties of the K-Means Algorithms},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/a1140a3d0df1c81e24ae954d935e8926-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/a1140a3d0df1c81e24ae954d935e8926-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/a1140a3d0df1c81e24ae954d935e8926-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1529410,
        "gs_citation": 813,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6558710861252718123&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Neuristique, 28 rue des Petites Ecuries, 75010 Paris, France + AT&T Bell Labs, Holmdel, NJ 07733; Dept. LR.O., Universite de Montreal, Montreal, Qc H3C-3J7, Canada",
        "aff_domain": "neuristique.fr;iro.umontreal.ca",
        "email": "neuristique.fr;iro.umontreal.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2",
        "aff_unique_norm": "Neuristique, 28 rue des Petites Ecuries, 75010 Paris, France;AT&T Bell Labs, Holmdel, NJ 07733;Dept. LR.O., Universite de Montreal, Montreal, Qc H3C-3J7, Canada",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "018d956fc9",
        "title": "Correlation and Interpolation Networks for Real-time Expression Analysis/Synthesis",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/b706835de79a2b4e80506f582af3676a-Abstract.html",
        "author": "Trevor Darrell; Irfan A. Essa; Alex Pentland",
        "abstract": "We  describe  a  framework  for  real-time  tracking  of facial  expressions  that  uses  neurally-inspired  correlation  and  interpolation  methods.  A  distributed view-based representation is used to characterize facial state,  and is  computed using a  replicated correlation network.  The ensemble  response of the set of view correlation scores is input to a network based  interpolation method, which maps perceptual state to motor control states  for  a  simulated  3-D  face  model.  Activation levels  of the  motor  state  correspond to muscle activations in an  anatomically derived model.  By  integrating fast and robust 2-D processing with 3-D models, we obtain a  system that is able to quickly track and interpret complex facial motions  in real-time.",
        "bibtex": "@inproceedings{NIPS1994_b706835d,\n author = {Darrell, Trevor and Essa, Irfan and Pentland, Alex},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Correlation and Interpolation Networks for Real-time Expression Analysis/Synthesis},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/b706835de79a2b4e80506f582af3676a-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/b706835de79a2b4e80506f582af3676a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/b706835de79a2b4e80506f582af3676a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1627847,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1576751117528240836&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "872fb909b7",
        "title": "Deterministic Annealing Variant of the EM Algorithm",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/92262bf907af914b95a0fc33c3f33bf6-Abstract.html",
        "author": "Naonori Ueda; Ryohei Nakano",
        "abstract": "We  present  a  deterministic annealing variant of the EM  algorithm  for  maximum likelihood  parameter  estimation  problems.  In  our  approach,  the  EM  process  is  reformulated  as the  problem of min(cid:173) imizing  the  thermodynamic free  energy  by  using  the  principle  of  maximum  entropy and  statistical mechanics  analogy.  Unlike simu(cid:173) lated  annealing approaches,  this  minimization is  deterministically  performed.  Moreover,  the  derived  algorithm,  unlike  the  conven(cid:173) tional EM  algorithm, can obtain better estimates free of the initial  parameter values.",
        "bibtex": "@inproceedings{NIPS1994_92262bf9,\n author = {Ueda, Naonori and Nakano, Ryohei},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Deterministic Annealing Variant of the EM Algorithm},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/92262bf907af914b95a0fc33c3f33bf6-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/92262bf907af914b95a0fc33c3f33bf6-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/92262bf907af914b95a0fc33c3f33bf6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1483365,
        "gs_citation": 106,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12264193327307328004&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "NTT Communication Science Laboratories; NTT Communication Science Laboratories",
        "aff_domain": "cslab.kecl.ntt.jp;cslab.kecl.ntt.jp",
        "email": "cslab.kecl.ntt.jp;cslab.kecl.ntt.jp",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "NTT Communication Science Laboratories",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ntt-csl.com",
        "aff_unique_abbr": "NTT CSL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2d04063691",
        "title": "Diffusion of Credit in Markovian Models",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/3e89ebdb49f712c7d90d1b39e348bbbf-Abstract.html",
        "author": "Yoshua Bengio; Paolo Frasconi",
        "abstract": "This  paper  studies the  problem of diffusion  in  Markovian  models,  such  as  hidden  Markov  models  (HMMs)  and  how  it  makes  very  difficult the task of learning of long-term dependencies in sequences.  Using results from Markov chain theory,  we show that the problem  of diffusion is reduced if the transition probabilities approach 0 or 1.  Under  this condition, standard HMMs have very  limited modeling  capabilities,  but input/output HMMs can still perform interesting  computations.",
        "bibtex": "@inproceedings{NIPS1994_3e89ebdb,\n author = {Bengio, Yoshua and Frasconi, Paolo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Diffusion of Credit in Markovian Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/3e89ebdb49f712c7d90d1b39e348bbbf-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/3e89ebdb49f712c7d90d1b39e348bbbf-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/3e89ebdb49f712c7d90d1b39e348bbbf-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1760288,
        "gs_citation": 82,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2932104434726414506&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 22,
        "aff": "Dept. I.R.O., Universite de Montreal, Montreal, Qc, Canada H3C-3J7 + AT&T Bell Labs, Holmdel, NJ 07733; Dipartimento di Sistemi e Informatica, Universita di Firenze, Italy",
        "aff_domain": "bengioyCIRO.UMontreal.CA; paoloCmcculloch.ing.unifi.it",
        "email": "bengioyCIRO.UMontreal.CA; paoloCmcculloch.ing.unifi.it",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2",
        "aff_unique_norm": "Dept. I.R.O., Universite de Montreal, Montreal, Qc, Canada H3C-3J7;AT&T Bell Labs, Holmdel, NJ 07733;Dipartimento di Sistemi e Informatica, Universita di Firenze, Italy",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "6612e3bd2f",
        "title": "Direct Multi-Step Time Series Prediction Using TD(\u03bb)",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/8d6dc35e506fc23349dd10ee68dabb64-Abstract.html",
        "author": "Peter T. Kazlas; Andreas S. Weigend",
        "abstract": "Abstract Unavailable",
        "bibtex": "@inproceedings{NIPS1994_8d6dc35e,\n author = {Kazlas, Peter T. and Weigend, Andreas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Direct Multi-Step Time Series Prediction Using TD(\\lambda )},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/8d6dc35e506fc23349dd10ee68dabb64-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/8d6dc35e506fc23349dd10ee68dabb64-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/8d6dc35e506fc23349dd10ee68dabb64-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1097733,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11937701470820391980&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 3,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "https://github.com/ai-summarization",
        "project": "https://ai-summarization-project.org",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "f8b78346ce",
        "title": "Direction Selectivity In Primary Visual Cortex Using Massive Intracortical Connections",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/a64c94baaf368e1840a1324e839230de-Abstract.html",
        "author": "Humbert Suarez; Christof Koch; Rodney Douglas",
        "abstract": "Almost all models of orientation and direction selectivity in visual  cortex are based on feedforward connection schemes, where genicu(cid:173) late input provides all excitation to both pyramidal and inhibitory  neurons. The latter neurons then suppress the response of the for(cid:173) mer for non-optimal stimuli. However, anatomical studies show  that up to 90 % of the excitatory synaptic input onto any corti(cid:173) cal cell is provided by other cortical cells. The massive excitatory  feedback nature of cortical circuits is embedded in the canonical  microcircuit of Douglas &. Martin (1991). We here investigate ana(cid:173) lytically and through biologically realistic simulations the function(cid:173) ing of a detailed model of this circuitry, operating in a hysteretic  mode. In the model, weak geniculate input is dramatically ampli(cid:173) fied by intracortical excitation, while inhibition has a dual role: (i)  to prevent the early geniculate-induced excitation in the null di(cid:173) rection and (ii) to restrain excitation and ensure that the neurons  fire only when the stimulus is in their receptive-field. Among the",
        "bibtex": "@inproceedings{NIPS1994_a64c94ba,\n author = {Suarez, Humbert and Koch, Christof and Douglas, Rodney},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Direction Selectivity In Primary Visual Cortex Using Massive Intracortical Connections},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/a64c94baaf368e1840a1324e839230de-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/a64c94baaf368e1840a1324e839230de-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/a64c94baaf368e1840a1324e839230de-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1402503,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16186030700935790820&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "ab008cc3c9",
        "title": "Dynamic Cell Structures",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/92977ae4d2ba21425a59afb269c2a14e-Abstract.html",
        "author": "J\u00f6rg Bruske; Gerald Sommer",
        "abstract": "Dynamic Cell Structures (DCS) represent a family  of artificial  neural  architectures  suited  both  for  unsupervised  and  supervised  learning.  They belong to the recently [Martinetz94] introduced class of Topology  Representing  Networks  (TRN)  which  build  perlectly topology  pre(cid:173) serving feature maps. DCS empI'oy a modified Kohonen learning rule  in conjunction with competitive Hebbian learning. The Kohonen type  learning rule serves to adjust the synaptic weight vectors while Hebbian  learning  establishes  a  dynamic  lateral  connection  structure  between  the units reflecting the topology of the feature manifold. In case of super(cid:173) vised learning, i.e. function approximation, each neural unit implements  a Radial Basis Function, and an  additional layer of linear output units  adjusts according to a delta-rule. DCS is the first RBF-based approxima(cid:173) tion  scheme attempting to concurrently learn and  utilize a perfectly to(cid:173) pology preserving map for improved performance.  Simulations on a selection of CMU-Benchmarks indicate that the DCS  idea applied to the Growing Cell Structure algorithm [Fritzke93] leads  to an  efficient and elegant algorithm  that can  beat conventional  models  on similar tasks.",
        "bibtex": "@inproceedings{NIPS1994_92977ae4,\n author = {Bruske, J\\\"{o}rg and Sommer, Gerald},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Dynamic Cell Structures},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/92977ae4d2ba21425a59afb269c2a14e-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/92977ae4d2ba21425a59afb269c2a14e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/92977ae4d2ba21425a59afb269c2a14e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1609716,
        "gs_citation": 74,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1838730186731741668&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Cognitive Systems, Christian Albrechts University at Kiel; Department of Cognitive Systems, Christian Albrechts University at Kiel",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Department of Cognitive Systems, Christian Albrechts University at Kiel",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "4fa41c9edd",
        "title": "Dynamic Modelling of Chaotic Time Series with Neural Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/4daa3db355ef2b0e64b472968cb70f0d-Abstract.html",
        "author": "Jose C. Principe; Jyh-Ming Kuo",
        "abstract": "The auditory system of the barn owl contains several spatial maps.  In young barn owls raised with optical prisms over their eyes, these  auditory maps are shifted to stay in register with the visual map,  suggesting that the visual input imposes a frame of reference on  the auditory maps. However, the optic tectum, the first site of  convergence of visual with auditory information, is not the site of  plasticity for the shift of the auditory maps; the plasticity occurs  instead in the inferior colliculus, which contains an auditory map  and projects into the optic tectum. We explored a model of the owl  remapping in which a global reinforcement signal whose delivery is  controlled by visual foveation. A hebb learning rule gated by rein(cid:173) forcement learned to appropriately adjust auditory maps. In addi(cid:173) tion, reinforcement learning preferentially adjusted the weights in  the inferior colliculus, as in the owl brain, even though the weights  were allowed to change throughout the auditory system. This ob(cid:173) servation raises the possibility that the site of learning does not  have to be genetically specified, but could be determined by how  the learning procedure interacts with the network architecture.",
        "bibtex": "@inproceedings{NIPS1994_4daa3db3,\n author = {Principe, Jose C. and Kuo, Jyh-Ming},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Dynamic Modelling of Chaotic Time Series with Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/4daa3db355ef2b0e64b472968cb70f0d-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/4daa3db355ef2b0e64b472968cb70f0d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/4daa3db355ef2b0e64b472968cb70f0d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 3202937,
        "gs_citation": 94,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11716300962324759313&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "9bb6487e9b",
        "title": "Effects of Noise on Convergence and Generalization in Recurrent Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/6c3cf77d52820cd0fe646d38bc2145ca-Abstract.html",
        "author": "Kam Jim; Bill G. Horne; C. Lee Giles",
        "abstract": "We introduce and study methods of inserting synaptic noise into  dynamically-driven recurrent neural networks and show that ap(cid:173) plying a controlled amount of noise during training may improve  convergence and generalization. In addition, we analyze the effects  of each noise parameter (additive vs. multiplicative, cumulative vs.  non-cumulative, per time step vs. per string) and predict that best  overall performance can be achieved by injecting additive noise at  each time step. Extensive simulations on learning the dual parity  grammar from temporal strings substantiate these predictions.",
        "bibtex": "@inproceedings{NIPS1994_6c3cf77d,\n author = {Jim, Kam and Horne, Bill and Giles, C.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Effects of Noise on Convergence and Generalization in Recurrent Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/6c3cf77d52820cd0fe646d38bc2145ca-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/6c3cf77d52820cd0fe646d38bc2145ca-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/6c3cf77d52820cd0fe646d38bc2145ca-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1612900,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5804282329599935804&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "NEC Research Institute, Inc., 4 Independence Way, Princeton, NJ 08540; NEC Research Institute, Inc., 4 Independence Way, Princeton, NJ 08540; NEC Research Institute, Inc., 4 Independence Way, Princeton, NJ 08540 + UMIACS, University of Maryland, College Park, MD 20742",
        "aff_domain": "research.nj.nec.com;research.nj.nec.com;research.nj.nec.com",
        "email": "research.nj.nec.com;research.nj.nec.com;research.nj.nec.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0+1",
        "aff_unique_norm": "NEC Research Institute, Inc., 4 Independence Way, Princeton, NJ 08540;UMIACS, University of Maryland, College Park, MD 20742",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "1ae6acc32d",
        "title": "Efficient Methods for Dealing with Missing Data in Supervised Learning",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/54a367d629152b720749e187b3eaa11b-Abstract.html",
        "author": "Volker Tresp; Ralph Neuneier; Subutai Ahmad",
        "abstract": "We present efficient algorithms for dealing with the problem of mis(cid:173) sing inputs (incomplete feature  vectors) during training and recall.  Our approach is based on the approximation of the input data dis(cid:173) tribution using  Parzen  windows.  For  recall,  we  obtain closed form  solutions for arbitrary feedforward networks.  For training, we show  how  the  backpropagation  step  for  an  incomplete  pattern  can  be  approximated by  a  weighted  averaged backpropagation step.  The  complexity  of the  solutions for  training  and  recall  is  independent  of the number of missing features.  We verify our theoretical results  using one classification and one regression  problem.",
        "bibtex": "@inproceedings{NIPS1994_54a367d6,\n author = {Tresp, Volker and Neuneier, Ralph and Ahmad, Subutai},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Efficient Methods for Dealing with Missing Data in Supervised Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/54a367d629152b720749e187b3eaa11b-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/54a367d629152b720749e187b3eaa11b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/54a367d629152b720749e187b3eaa11b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1346580,
        "gs_citation": 117,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15117460910309017417&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Siemens AG, Central Research; Siemens AG, Central Research; Interval Research Corporation + Center for Biological and Computational Learning, MIT",
        "aff_domain": "zfe.siemens.de; ; ",
        "email": "zfe.siemens.de; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1+2",
        "aff_unique_norm": "Siemens AG, Central Research;Interval Research Corporation;Center for Biological and Computational Learning, MIT",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "bf837a114e",
        "title": "Estimating Conditional Probability Densities for Periodic Variables",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/74bba22728b6185eec06286af6bec36d-Abstract.html",
        "author": "Chris M. Bishop; Claire Legleye",
        "abstract": "Most of the common techniques for estimating conditional prob(cid:173) ability densities are inappropriate for applications involving peri(cid:173) odic variables. In this paper we introduce three novel techniques  for tackling such problems, and investigate their performance us(cid:173) ing synthetic data. We then apply these techniques to the problem  of extracting the distribution of wind vector directions from radar  scatterometer data gathered by a remote-sensing satellite.",
        "bibtex": "@inproceedings{NIPS1994_74bba227,\n author = {Bishop, Chris M. and Legleye, Claire},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Estimating Conditional Probability Densities for Periodic Variables},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/74bba22728b6185eec06286af6bec36d-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/74bba22728b6185eec06286af6bec36d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/74bba22728b6185eec06286af6bec36d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1303889,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=472575278055294294&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "cda701c270",
        "title": "Extracting Rules from Artificial Neural Networks with Distributed Representations",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/bea5955b308361a1b07bc55042e25e54-Abstract.html",
        "author": "Sebastian Thrun",
        "abstract": "Although artificial neural networks have been applied in a variety of real-world scenarios  with remarkable success,  they  have often been criticized for exhibiting a low degree of  human comprehensibility.  Techniques that compile compact sets of symbolic rules out  of artificial  neural  networks  offer  a  promising perspective  to  overcome  this  obvious  deficiency of neural network representations.  This paper  presents  an  approach  to  the  extraction of if-then rules  from  artificial  neu(cid:173) Its  key  mechanism  is  validity  interval  analysis,  which  is  a  generic  ral  networks.  tool  for  extracting  symbolic  knowledge  by  propagating rule-like knowledge through  Backpropagation-style neural  networks.  Empirical studies in a robot arm domain illus(cid:173) trate the appropriateness of the proposed method for extracting rules from networks with  real-valued and distributed representations.",
        "bibtex": "@inproceedings{NIPS1994_bea5955b,\n author = {Thrun, Sebastian},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Extracting Rules from Artificial Neural Networks with Distributed Representations},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/bea5955b308361a1b07bc55042e25e54-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/bea5955b308361a1b07bc55042e25e54-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/bea5955b308361a1b07bc55042e25e54-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1891503,
        "gs_citation": 318,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3187883760654471723&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "University of Bonn Department of Computer Science III Romerstr. 164, D-53117 Bonn, Germany",
        "aff_domain": "carbon.informatik.uni-bonn.de",
        "email": "carbon.informatik.uni-bonn.de",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Bonn Department of Computer Science III Romerstr. 164, D-53117 Bonn, Germany",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "9b0b6dad74",
        "title": "FINANCIAL APPLICATIONS OF LEARNING FROM HINTS",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/1cc3633c579a90cfdd895e64021e2163-Abstract.html",
        "author": "Yaser S. Abu-Mostafa",
        "abstract": "The basic paradigm for learning in neural networks is 'learning from  examples' where a training set of input-output examples is used to  teach the network the target function. Learning from hints is a gen(cid:173) eralization of learning from examples where additional information  about the target function can be incorporated in the same learning  process. Such information can come from common sense rules or  special expertise. In financial market applications where the train(cid:173) ing data is very noisy, the use of such hints can have a decisive  advantage. We demonstrate the use of hints in foreign-exchange  trading of the U.S. Dollar versus the British Pound, the German  Mark, the Japanese Yen, and the Swiss Franc, over a period of 32  months. We explain the general method of learning from hints and  how it can be applied to other markets. The learning model for  this method is not restricted to neural networks.",
        "bibtex": "@inproceedings{NIPS1994_1cc3633c,\n author = {Abu-Mostafa, Yaser},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {FINANCIAL APPLICATIONS OF LEARNING FROM HINTS},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/1cc3633c579a90cfdd895e64021e2163-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/1cc3633c579a90cfdd895e64021e2163-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/1cc3633c579a90cfdd895e64021e2163-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1483525,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12256530813667926299&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "253275d1da",
        "title": "Factorial Learning and the EM Algorithm",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/20aee3a5f4643755a79ee5f6a73050ac-Abstract.html",
        "author": "Zoubin Ghahramani",
        "abstract": "Many real world learning problems are best characterized by an  interaction of multiple independent causes or factors. Discover(cid:173) ing such causal structure from the data is the focus of this paper.  Based on Zemel and Hinton's cooperative vector quantizer (CVQ)  architecture, an unsupervised learning algorithm is derived from  the Expectation-Maximization (EM) framework. Due to the com(cid:173) binatorial nature of the data generation process, the exact E-step  is computationally intractable. Two alternative methods for com(cid:173) puting the E-step are proposed: Gibbs sampling and mean-field  approximation, and some promising empirical results are presented.",
        "bibtex": "@inproceedings{NIPS1994_20aee3a5,\n author = {Ghahramani, Zoubin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Factorial Learning and the EM Algorithm},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/20aee3a5f4643755a79ee5f6a73050ac-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/20aee3a5f4643755a79ee5f6a73050ac-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/20aee3a5f4643755a79ee5f6a73050ac-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1592839,
        "gs_citation": 140,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15992946913303398712&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": "Department of Brain & Cognitive Sciences, Massachusetts Institute of Technology",
        "aff_domain": "psyche.mit.edu",
        "email": "psyche.mit.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Department of Brain & Cognitive Sciences, Massachusetts Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "15a46c47a5",
        "title": "Factorial Learning by Clustering Features",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/ef4e3b775c934dada217712d76f3d51f-Abstract.html",
        "author": "Joshua B. Tenenbaum; Emanuel V. Todorov",
        "abstract": "We introduce a novel algorithm for factorial learning, motivated  by segmentation problems in computational vision, in which the  underlying factors correspond to clusters of highly correlated input  features. The algorithm derives from a new kind of competitive  clustering model, in which the cluster generators compete to ex(cid:173) plain each feature of the data set and cooperate to explain each  input example, rather than competing for examples and cooper(cid:173) ating on features, as in traditional clustering algorithms. A natu(cid:173) ral extension of the algorithm recovers hierarchical models of data  generated from multiple unknown categories, each with a differ(cid:173) ent, multiple causal structure. Several simulations demonstrate  the power of this approach.",
        "bibtex": "@inproceedings{NIPS1994_ef4e3b77,\n author = {Tenenbaum, Joshua and Todorov, Emanuel V.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Factorial Learning by Clustering Features},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/ef4e3b775c934dada217712d76f3d51f-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/ef4e3b775c934dada217712d76f3d51f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/ef4e3b775c934dada217712d76f3d51f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1645844,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5752407757971263433&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology; Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology",
        "aff_domain": "psyche.mit.edu;psyche.mit.edu",
        "email": "psyche.mit.edu;psyche.mit.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Department of Brain and Cognitive Sciences",
        "aff_unique_url": "https://web.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "6e5bea193c",
        "title": "Finding Structure in Reinforcement Learning",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/7ce3284b743aefde80ffd9aec500e085-Abstract.html",
        "author": "Sebastian Thrun; Anton Schwartz",
        "abstract": "Reinforcement learning addresses  the problem of learning to select actions in order to  maximize one's performance in unknown environments.  To scale reinforcement learning  to complex real-world tasks, such as typically studied in AI, one must ultimately be able  to discover the structure in the world, in order to abstract away the myriad of details and  to operate in more tractable problem spaces.  This paper presents the SKILLS algorithm.  SKILLS discovers skills, which are partially  defined action policies that arise in the context of multiple, related tasks.  Skills collapse  whole action sequences into single operators.  They are learned by minimizing the com(cid:173) pactness of action policies, using a description length argument on their representation.  Empirical results  in  simple grid  navigation tasks  illustrate the successful  discovery  of  structure in reinforcement learning.",
        "bibtex": "@inproceedings{NIPS1994_7ce3284b,\n author = {Thrun, Sebastian and Schwartz, Anton},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Finding Structure in Reinforcement Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/7ce3284b743aefde80ffd9aec500e085-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/7ce3284b743aefde80ffd9aec500e085-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/7ce3284b743aefde80ffd9aec500e085-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1867727,
        "gs_citation": 313,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=542634664572998513&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff": "University of Bonn, Department of Computer Science; Stanford University, Dept. of Computer Science",
        "aff_domain": "carbon.informatik.uni-bonn.de;cs.stanford.edu",
        "email": "carbon.informatik.uni-bonn.de;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Bonn, Department of Computer Science;Stanford University, Dept. of Computer Science",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "5a7fb652f7",
        "title": "Forward dynamic models in human motor control: Psychophysical evidence",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/a4300b002bcfb71f291dac175d52df94-Abstract.html",
        "author": "Daniel M. Wolpert; Zoubin Ghahramani; Michael I. Jordan",
        "abstract": "Based  on  computational  principles,  with  as  yet  no  direct  experi(cid:173) mental  validation,  it  has  been  proposed  that  the  central  nervous  system  (CNS)  uses  an internal model to simulate the  dynamic be(cid:173) havior of the motor system in planning, control and learning (Sut(cid:173) ton  and  Barto,  1981;  Ito,  1984;  Kawato  et  aI.,  1987;  Jordan  and  Rumelhart,  1992;  Miall et aI.,  1993).  We  present  experimental re(cid:173) sults  and simulations based on a  novel  approach  that investigates  the temporal propagation of errors  in the sensorimotor integration  process.  Our results  provide  direct  support for  the  existence  of an  internal model.",
        "bibtex": "@inproceedings{NIPS1994_a4300b00,\n author = {Wolpert, Daniel M and Ghahramani, Zoubin and Jordan, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Forward dynamic models in human motor control: Psychophysical evidence},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/a4300b002bcfb71f291dac175d52df94-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/a4300b002bcfb71f291dac175d52df94-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/a4300b002bcfb71f291dac175d52df94-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1572919,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3279107141225112274&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Brain & Cognitive Sciences; Department of Brain & Cognitive Sciences; Department of Brain & Cognitive Sciences",
        "aff_domain": "psyche.mit.edu;psyche.mit.edu;psyche.mit.edu",
        "email": "psyche.mit.edu;psyche.mit.edu;psyche.mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Department of Brain & Cognitive Sciences",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "5e178617d6",
        "title": "From Data Distributions to Regularization in Invariant Learning",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/7fa732b517cbed14a48843d74526c11a-Abstract.html",
        "author": "Todd K. Leen",
        "abstract": "Ideally pattern recognition machines provide constant output when  the inputs are transformed under a group 9 of desired invariances.  These invariances can be achieved  by  enhancing the training data  to include examples of inputs transformed by elements of g,  while  leaving  the  corresponding  targets  unchanged.  Alternatively  the  cost  function  for  training  can  include  a  regularization  term  that  penalizes changes in the output when the input is  transformed un(cid:173) der the group.",
        "bibtex": "@inproceedings{NIPS1994_7fa732b5,\n author = {Leen, Todd},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {From Data Distributions to Regularization in Invariant Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/7fa732b517cbed14a48843d74526c11a-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/7fa732b517cbed14a48843d74526c11a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/7fa732b517cbed14a48843d74526c11a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1441348,
        "gs_citation": 80,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2992896767663578373&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Computer Science and Engineering, Oregon Graduate Institute of Science and Technology",
        "aff_domain": "cse.ogi.edu",
        "email": "cse.ogi.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Department of Computer Science and Engineering, Oregon Graduate Institute of Science and Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "886d273894",
        "title": "Generalisation in Feedforward Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/8b5040a8a5baf3e0e67386c2e3a9b903-Abstract.html",
        "author": "Adam Kowalczyk; Herman L. Ferr\u00e1",
        "abstract": "We  discuss  a  model  of consistent  learning  with  an  additional  re(cid:173) striction  on  the  probability  distribution  of training  samples,  the  target concept  and hypothesis class.  We  show  that the model pro(cid:173) vides  a  significant  improvement  on  the  upper  bounds  of sample  complexity,  i.e.  the  minimal number  of random  training samples  allowing  a  selection  of the  hypothesis  with  a  predefined  accuracy  and  confidence.  Further,  we  show  that  the  model  has  the  poten(cid:173) tial  for  providing  a  finite  sample  complexity  even  in  the  case  of  infinite  VC-dimension  as  well  as  for  a  sample  complexity  below  VC-dimension.  This is  achieved  by  linking sample  complexity  to  an  \"average\"  number  of implement able dichotomies of a  training  sample  rather  than  the  maximal  size  of a  shattered  sample,  i.e.  VC-dimension.",
        "bibtex": "@inproceedings{NIPS1994_8b5040a8,\n author = {Kowalczyk, Adam and Ferr\\'{a}, Herman},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Generalisation in Feedforward Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/8b5040a8a5baf3e0e67386c2e3a9b903-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/8b5040a8a5baf3e0e67386c2e3a9b903-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/8b5040a8a5baf3e0e67386c2e3a9b903-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1520920,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=615625964635821500&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "ceaae2e41a",
        "title": "Generalization in Reinforcement Learning: Safely Approximating the Value Function",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/ef50c335cca9f340bde656363ebd02fd-Abstract.html",
        "author": "Justin A. Boyan; Andrew W. Moore",
        "abstract": "A  straightforward  approach  to  the  curse  of  dimensionality  in  re(cid:173) inforcement  learning  and  dynamic  programming  is  to  replace  the  lookup table with a generalizing function approximator such as a neu(cid:173) ral net.  Although this has been successful in the domain of backgam(cid:173) mon,  there  is  no  guarantee  of convergence.  In  this  paper,  we  show  that the combination of dynamic programming and function approx(cid:173) imation  is  not  robust,  and  in  even  very  benign  cases,  may produce  an  entirely  wrong  policy.  We  then  introduce  Grow-Support,  a  new  algorithm which is safe from divergence yet can still reap the benefits  of successful  generalization .",
        "bibtex": "@inproceedings{NIPS1994_ef50c335,\n author = {Boyan, Justin and Moore, Andrew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Generalization in Reinforcement Learning: Safely Approximating the Value Function},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/ef50c335cca9f340bde656363ebd02fd-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/ef50c335cca9f340bde656363ebd02fd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/ef50c335cca9f340bde656363ebd02fd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1726434,
        "gs_citation": 932,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10225942405074993697&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Computer Science Department, Carnegie Mellon University; Computer Science Department, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "1324829a7e",
        "title": "Glove-TalkII: Mapping Hand Gestures to Speech Using Neural Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/c2aee86157b4a40b78132f1e71a9e6f1-Abstract.html",
        "author": "Sidney Fels; Geoffrey E. Hinton",
        "abstract": "Glove-TaikII is  a  system which  translates  hand gestures  to speech  through  an  adaptive interface.  Hand  gestures  are  mapped  contin(cid:173) uously  to  10  control  parameters  of a  parallel formant  speech  syn(cid:173) thesizer.  The mapping allows the hand to act  as  an  artificial  vocal  tract  that  produces  speech  in  real  time.  This  gives  an  unlimited  vocabulary  in addition to direct  control of fundamental  frequency  and  volume.  Currently,  the  best  version  of Glove-TalkII  uses  sev(cid:173) eral  input  devices  (including  a  CyberGlove,  a  ContactGlove,  a  3- space  tracker,  and a foot-pedal),  a  parallel formant speech  synthe(cid:173) sizer  and  3 neural  networks.  The gesture-to-speech  task  is  divided  into  vowel  and  consonant  production  by  using  a  gating  network  to weight  the outputs of a  vowel  and  a  consonant  neural  network.  The  gating  network  and  the  consonant  network  are  trained  with  examples  from  the  user.  The  vowel  network  implements  a  fixed,  user-defined  relationship  between  hand-position  and  vowel  sound  and does  not require any training examples from the user.  Volume,  fundamental  frequency  and  stop  consonants  are  produced  with  a  fixed  mapping from  the  input devices.  One subject  has  trained  to  speak  intelligibly with Glove-TalkII. He  speaks slowly with speech  quality  similar  to a  text-to-speech  synthesizer  but  with  far  more  natural-sounding pitch  variations.",
        "bibtex": "@inproceedings{NIPS1994_c2aee861,\n author = {Fels, Sidney and Hinton, Geoffrey E},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Glove-TalkII: Mapping Hand Gestures to Speech Using Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/c2aee86157b4a40b78132f1e71a9e6f1-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/c2aee86157b4a40b78132f1e71a9e6f1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/c2aee86157b4a40b78132f1e71a9e6f1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1714762,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11264409356224657862&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science, University of Toronto; Department of Computer Science, University of Toronto",
        "aff_domain": "ai.toronto.edu;ai.toronto.edu",
        "email": "ai.toronto.edu;ai.toronto.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Toronto",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utoronto.ca",
        "aff_unique_abbr": "U of T",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Toronto",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "7fac643526",
        "title": "Grammar Learning by a Self-Organizing Network",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/fe7ee8fc1959cc7214fa21c4840dff0a-Abstract.html",
        "author": "Michiro Negishi",
        "abstract": "This  paper presents the design and simulation results of a  self(cid:173) organizing neural network which induces a grammar from exam(cid:173) ple sentences. Input sentences are generated from a simple phrase  structure grammar including number agreement,  verb  transitiv(cid:173) ity,  and recursive  noun phrase construction rules.  The  network  induces a grammar explicitly in the form of symbol categorization  rules and phrase structure rules.",
        "bibtex": "@inproceedings{NIPS1994_fe7ee8fc,\n author = {Negishi, Michiro},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Grammar Learning by a Self-Organizing Network},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/fe7ee8fc1959cc7214fa21c4840dff0a-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/fe7ee8fc1959cc7214fa21c4840dff0a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/fe7ee8fc1959cc7214fa21c4840dff0a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1387820,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6451978029080337798&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Dept. of Cognitive and Neural Systems, Boston University",
        "aff_domain": "cns.bu.edu",
        "email": "cns.bu.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Dept. of Cognitive and Neural Systems, Boston University",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "014a71fcb8",
        "title": "Grouping Components of Three-Dimensional Moving Objects in Area MST of Visual Cortex",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/860320be12a1c050cd7731794e231bd3-Abstract.html",
        "author": "Richard S. Zemel; Terrence J. Sejnowski",
        "abstract": "Many  cells  in  the  dorsal  part  of  the  medial  superior  temporal  (MST)  area  of  visual  cortex  respond  selectively  to  spiral  flow  patterns-specific  combinations of expansion/ contraction  and  ro(cid:173) tation  motions.  Previous  investigators  have  suggested  that  these  cells  may represent  self-motion.  Spiral patterns  can  also  be  gener(cid:173) ated by the relative motion of the observer  and a particular object.  An  MST  cell  may  then  account  for  some  portion  of the  complex  flow  field,  and  the  set  of active  cells  could  encode  the  entire  flow;  in  this  manner,  MST  effectively  segments  moving  objects.  Such  a  grouping operation  is  essential  in  interpreting  scenes  containing  several  independent  moving objects  and  observer  motion.  We  de(cid:173) scribe  a  model  based  on  the  hypothesis  that  the  selective  tuning  of MST  cells  reflects  the grouping of object  components  undergo(cid:173) ing  coherent  motion.  Inputs  to  the  model  were  generated  from  sequences  of ray-traced  images that simulated realistic  motion sit(cid:173) uations,  combining observer  motion, eye  movements, and indepen(cid:173) dent  object  motion.  The  input  representation  was  modeled  after  response  properties of neurons in area MT,  which provides the pri(cid:173) mary input to area MST.  After  applying an  unsupervised  learning  algorithm,  the  units  became  tuned  to  patterns  signaling  coherent  motion.  The results  match many of the  known  properties  of MST  cells  and  are  consistent  with  recent  studies  indicating  that  these  cells  process  3-D  object  motion information.",
        "bibtex": "@inproceedings{NIPS1994_860320be,\n author = {Zemel, Richard and Sejnowski, Terrence J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Grouping Components of Three-Dimensional Moving Objects in Area MST of Visual Cortex},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/860320be12a1c050cd7731794e231bd3-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/860320be12a1c050cd7731794e231bd3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/860320be12a1c050cd7731794e231bd3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 667292,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15039109588590965208&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Carnegie Mellon University Department of Psychology Pittsburgh, PA 15213; CNL, The Salk Institute P.O. Box 85800 San Diego, CA 92186-5800",
        "aff_domain": "lcmu.edu;lsalk.edu",
        "email": "lcmu.edu;lsalk.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Carnegie Mellon University Department of Psychology Pittsburgh, PA 15213;CNL, The Salk Institute P.O. Box 85800 San Diego, CA 92186-5800",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "c88e79f830",
        "title": "Hierarchical Mixtures of Experts Methodology Applied to Continuous Speech Recognition",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/0d0871f0806eae32d30983b62252da50-Abstract.html",
        "author": "Ying Zhao; Richard M. Schwartz; Jason J. Sroka; John Makhoul",
        "abstract": "In  this  paper,  we  incorporate  the  Hierarchical  Mixtures  of Experts  (HME)  method  of probability  estimation,  developed  by  Jordan  [1],  into  an  HMM(cid:173) based  continuous  speech  recognition  system.  The  resulting  system  can  be  thought of as a continuous-density HMM system, but instead of using gaussian  mixtures,  the HME system employs a large set of hierarchically organized but  relatively  small  neural  networks  to  perform the probability density estimation.  The  hierarchical  structure  is  reminiscent  of  a  decision  tree  except  for  two  important differences:  each  \"expert\" or neural  net  performs a  \"soft\" decision  rather than a hard decision, and,  unlike ordinary decision trees,  the parameters  of all  the  neural  nets  in  the  HME  are  automatically  trainable  using  the  EM  algorithm.  We  report results on  the ARPA  5,OOO-word  and 4O,OOO-word Wall  Street Journal  corpus using  HME  models.",
        "bibtex": "@inproceedings{NIPS1994_0d0871f0,\n author = {Zhao, Ying and Schwartz, Richard and Sroka, Jason and Makhoul, John},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Hierarchical Mixtures of Experts Methodology Applied to Continuous Speech Recognition},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/0d0871f0806eae32d30983b62252da50-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/0d0871f0806eae32d30983b62252da50-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/0d0871f0806eae32d30983b62252da50-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1295955,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15185889175605329735&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "89d074db9f",
        "title": "Higher Order Statistical Decorrelation without Information Loss",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/892c91e0a653ba19df81a90f89d99bcd-Abstract.html",
        "author": "Gustavo Deco; Wilfried Brauer",
        "abstract": "A neural network learning paradigm based on information theory is pro(cid:173) posed as a way to perform in an unsupervised fashion, redundancy  reduction among the elements of the output layer without loss of infor(cid:173) mation from the sensory input. The model developed performs nonlin(cid:173) ear decorrelation up to higher orders of the cumulant tensors and results  in probabilistic ally independent components of the output layer. This  means that we don't need to assume Gaussian distribution neither at the  input nor at the output. The theory presented is related to the unsuper(cid:173) vised-learning theory of Barlow, which proposes redundancy reduction  as the goal of cognition. When nonlinear units are used nonlinear princi(cid:173) pal component analysis is obtained. In this case nonlinear manifolds can  be reduced to minimum dimension manifolds. If such units are used the  network performs a generalized principal component analysis in the  sense that non-Gaussian distributions can be linearly decorrelated and  higher orders of the correlation tensors are also taken into account. The  basic structure of the architecture involves a general transfOlmation that  is volume conserving and therefore the entropy, yielding a map without  loss of infoIIDation. Minimization of the mutual infoIIDation among the  output neurons eliminates the redundancy between the outputs and  results in statistical decorrelation of the extracted features. This is  known as factorialleaming.",
        "bibtex": "@inproceedings{NIPS1994_892c91e0,\n author = {Deco, Gustavo and Brauer, Wilfried},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Higher Order Statistical Decorrelation without Information Loss},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/892c91e0a653ba19df81a90f89d99bcd-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/892c91e0a653ba19df81a90f89d99bcd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/892c91e0a653ba19df81a90f89d99bcd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1483690,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2915608110344566919&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "SiemensAG Central Research; Technische UniversiUit MUnchen Institut fur InfoIIDatik",
        "aff_domain": "; ",
        "email": "; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "SiemensAG Central Research;Technische UniversiUit MUnchen Institut fur InfoIIDatik",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "df31f5b5c2",
        "title": "Hyperparameters Evidence and Generalisation for an Unrealisable Rule",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/e6cb2a3c14431b55aa50c06529eaa21b-Abstract.html",
        "author": "Glenn Marion; David Saad",
        "abstract": "Using a statistical mechanical formalism we  calculate the evidence,  generalisation  error  and  consistency  measure  for  a  linear  percep(cid:173) tron  trained  and  tested  on  a  set  of examples  generated  by  a  non  linear  teacher.  The teacher  is  said  to be  unrealisable  because  the  student  can  never  model it without error.  Our model allows  us  to  interpolate between the known case of a linear teacher,  and an un(cid:173) realisable, nonlinear teacher.  A comparison of the hyperparameters  which  maximise the evidence  with  those  that optimise the perfor(cid:173) mance measures  reveals  that,  in  the  non-linear  case,  the  evidence  procedure is a misleading guide to optimising performance.  Finally,  we explore the extent to which the evidence procedure is  unreliable  and find  that, despite being sub-optimal, in  some circumstances  it  might be  a  useful  method for  fixing  the hyperparameters.",
        "bibtex": "@inproceedings{NIPS1994_e6cb2a3c,\n author = {Marion, Glenn and Saad, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Hyperparameters Evidence and Generalisation for an Unrealisable Rule},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/e6cb2a3c14431b55aa50c06529eaa21b-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/e6cb2a3c14431b55aa50c06529eaa21b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/e6cb2a3c14431b55aa50c06529eaa21b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1576425,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2917124616334855862&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Physics, University of Edinburgh, Edinburgh, EH9 3JZ, U.K.; Department of Physics, University of Edinburgh, Edinburgh, EH9 3JZ, U.K.",
        "aff_domain": "ed.ac.uk;ed.ac.uk",
        "email": "ed.ac.uk;ed.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Department of Physics, University of Edinburgh, Edinburgh, EH9 3JZ, U.K.",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "ee97de4c55",
        "title": "H\u221e Optimal Training Algorithms and their Relation to Backpropagation",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/f57a2f557b098c43f11ab969efe1504b-Abstract.html",
        "author": "Babak Hassibi; Thomas Kailath",
        "abstract": "Abstract Unavailable",
        "bibtex": "@inproceedings{NIPS1994_f57a2f55,\n author = {Hassibi, Babak and Kailath, Thomas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {H\\infty  Optimal Training Algorithms and their Relation to Backpropagation},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/f57a2f557b098c43f11ab969efe1504b-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/f57a2f557b098c43f11ab969efe1504b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/f57a2f557b098c43f11ab969efe1504b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 885197,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15826424683322768103&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "https://github.com/ai-research-paper-summarizer",
        "project": "https://ai-research-project.org",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "e147ded33d",
        "title": "ICEG Morphology Classification using an Analogue VLSI Neural Network",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/fed33392d3a48aa149a87a38b875ba4a-Abstract.html",
        "author": "Richard Coggins; Marwan A. Jabri; Barry Flower; Stephen Pickard",
        "abstract": "An  analogue  VLSI  neural  network  has  been  designed  and  tested  to perform cardiac morphology classification tasks.  Analogue tech(cid:173) niques were chosen to meet the strict power and area requirements  of an Implantable Cardioverter Defibrillator (ICD) system.  The ro(cid:173) bustness  of the neural network  architecture  reduces  the impact of  noise,  drift  and offsets  inherent  in  analogue approaches.  The net(cid:173) work  is a  10:6:3 multi-layer percept ron  with on  chip  digital weight  storage,  a  bucket  brigade input  to feed  the  Intracardiac  Electro(cid:173) gram  (ICEG)  to  the  network  and  has  a  winner  take  all  circuit  at  the  output.  The  network  was  trained  in  loop  and  included  a  commercial ICD in the signal processing path.  The system has suc(cid:173) cessfully distinguished arrhythmia for different patients with better  than 90%  true positive and true negative detections for  dangerous  rhythms which cannot be detected  by present  ICDs.  The chip  was  implemented in 1.2um CMOS and consumes less than 200n W max(cid:173) imum average power in an area of 2.2  x 2.2mm2.",
        "bibtex": "@inproceedings{NIPS1994_fed33392,\n author = {Coggins, Richard and Jabri, Marwan and Flower, Barry and Pickard, Stephen},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {ICEG Morphology Classification using an Analogue VLSI Neural Network},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/fed33392d3a48aa149a87a38b875ba4a-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/fed33392d3a48aa149a87a38b875ba4a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/fed33392d3a48aa149a87a38b875ba4a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1548389,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17075530586302858958&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Systems Engineering and Design Automation Laboratory, Department of Electrical Engineering J03, University of Sydney, 2006, Australia; Systems Engineering and Design Automation Laboratory, Department of Electrical Engineering J03, University of Sydney, 2006, Australia; Systems Engineering and Design Automation Laboratory, Department of Electrical Engineering J03, University of Sydney, 2006, Australia; Systems Engineering and Design Automation Laboratory, Department of Electrical Engineering J03, University of Sydney, 2006, Australia",
        "aff_domain": "sedal.su.oz.au; ; ; ",
        "email": "sedal.su.oz.au; ; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Systems Engineering and Design Automation Laboratory, Department of Electrical Engineering J03, University of Sydney, 2006, Australia",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "75be8e881f",
        "title": "Implementation of Neural Hardware with the Neural VLSI of URAN in Applications with Reduced Representations",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/704afe073992cbe4813cae2f7715336f-Abstract.html",
        "author": "Il Song Han; Ki-Chul Kim; Hwang-Soo Lee",
        "abstract": "implement  Korean",
        "bibtex": "@inproceedings{NIPS1994_704afe07,\n author = {Han, Il and Kim, Ki-Chul and Lee, Hwang-Soo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Implementation of Neural Hardware with the Neural VLSI of URAN in Applications with Reduced Representations},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/704afe073992cbe4813cae2f7715336f-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/704afe073992cbe4813cae2f7715336f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/704afe073992cbe4813cae2f7715336f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 885685,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3434393142524760666&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "de81066b24",
        "title": "Inferring Ground Truth from Subjective Labelling of Venus Images",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/3cef96dcc9b8035d23f69e30bb19218a-Abstract.html",
        "author": "Padhraic Smyth; Usama M. Fayyad; Michael C. Burl; Pietro Perona; Pierre Baldi",
        "abstract": "In  remote sensing  applications  \"ground-truth\"  data is  often  used  as  the  basis for  training  pattern  recognition  algorithms  to  gener(cid:173) ate  thematic  maps  or  to  detect  objects  of  interest.  In  practical  situations, experts may visually examine the images and provide a  subjective noisy  estimate of the  truth.  Calibrating the  reliability  and bias  of expert labellers is  a  non-trivial problem.  In this paper  we  discuss  some  of  our  recent  work  on  this  topic  in  the  context  of  detecting  small  volcanoes  in  Magellan  SAR  images  of  Venus.  Empirical results (using the Expectation-Maximization procedure)  suggest  that  accounting  for  subjective  noise  can  be  quite  signifi(cid:173) cant in terms of quantifying both human and algorithm detection  performance.",
        "bibtex": "@inproceedings{NIPS1994_3cef96dc,\n author = {Smyth, Padhraic and Fayyad, Usama and Burl, Michael and Perona, Pietro and Baldi, Pierre},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Inferring Ground Truth from Subjective Labelling of Venus Images},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/3cef96dcc9b8035d23f69e30bb19218a-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/3cef96dcc9b8035d23f69e30bb19218a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/3cef96dcc9b8035d23f69e30bb19218a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1538741,
        "gs_citation": 414,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2744599364238527018&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Jet Propulsion Laboratory, Caltech; Jet Propulsion Laboratory, Caltech; Department of Electrical Engineering, Caltech; Department of Electrical Engineering, Caltech; Jet Propulsion Laboratory, Caltech + Division of Biology, California Institute of Technology",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;1;0+2",
        "aff_unique_norm": "Jet Propulsion Laboratory, Caltech;Department of Electrical Engineering, Caltech;Division of Biology, California Institute of Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "a8ccfd7fcf",
        "title": "Instance-Based State Identification for Reinforcement Learning",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/d2ed45a52bc0edfa11c2064e9edee8bf-Abstract.html",
        "author": "R. Andrew McCallum",
        "abstract": "This paper presents instance-based state identification, an approach  to reinforcement learning and hidden state that builds disambiguat(cid:173) ing amounts of short-term memory on-line, and also learns with an  order of magnitude fewer training steps than several previous ap(cid:173) proaches. Inspired by a key similarity between learning with hidden  state and learning in continuous geometrical spaces, this approach  uses instance-based (or \"memory-based\") learning, a method that  has worked well in continuous spaces.",
        "bibtex": "@inproceedings{NIPS1994_d2ed45a5,\n author = {McCallum, R. Andrew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Instance-Based State Identification for Reinforcement Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/d2ed45a52bc0edfa11c2064e9edee8bf-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/d2ed45a52bc0edfa11c2064e9edee8bf-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/d2ed45a52bc0edfa11c2064e9edee8bf-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1710724,
        "gs_citation": 110,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2321360877030015412&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science, University of Rochester",
        "aff_domain": "cs.rochester.edu",
        "email": "cs.rochester.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Rochester",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.rochester.edu",
        "aff_unique_abbr": "U of R",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "affdb2aa11",
        "title": "Interference in Learning Internal Models of Inverse Dynamics in Humans",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/1e48c4420b7073bc11916c6c1de226bb-Abstract.html",
        "author": "Reza Shadmehr; Tom Brashers-Krug; Ferdinando A. Mussa-Ivaldi",
        "abstract": "Experiments were  performed  to  reveal  some of the  computational  properties  of the  human  motor  memory  system.  We  show  that  as  humans  practice  reaching  movements while  interacting  with  a  novel  mechanical environment, they learn an  internal model of the  inverse dynamics of that environment.  Subjects show recall of this  model  at testing  sessions  24  hours  after  the  initial practice.  The  representation  of the internal model in  memory is  such  that there  is  interference  when  there  is  an  attempt  to  learn  a  new  inverse  dynamics  map  immediately after  an  anticorrelated  mapping  was  learned.  We  suggest  that  this  interference  is  an  indication  that  the same computational elements  used  to encode  the first  inverse  dynamics  map  are  being  used  to  learn  the  second  mapping.  We  predict  that this leads to a forgetting  of the initially learned skill.",
        "bibtex": "@inproceedings{NIPS1994_1e48c442,\n author = {Shadmehr, Reza and Brashers-Krug, Tom and Mussa-Ivaldi, Ferdinando},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Interference in Learning Internal Models of Inverse Dynamics in Humans},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/1e48c4420b7073bc11916c6c1de226bb-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/1e48c4420b7073bc11916c6c1de226bb-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/1e48c4420b7073bc11916c6c1de226bb-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1750641,
        "gs_citation": 56,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1317145481978381709&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Dept. of Brain and Cognitive Sciences, M. I. T., Cambridge, MA 02139 + Dept. Biomedical Eng, Johns Hopkins Univ, Baltimore, MD 21205; Dept. of Brain and Cognitive Sciences, M. I. T., Cambridge, MA 02139; Dept. of Brain and Cognitive Sciences, M. I. T., Cambridge, MA 02139 + Dept. Physiology, Northwestern Univ Med Sch (M211), Chicago, IL 60611",
        "aff_domain": "bme.jhu.edu;ai.mit.edu;parker.physio.nwu.edu",
        "email": "bme.jhu.edu;ai.mit.edu;parker.physio.nwu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;0+2",
        "aff_unique_norm": "Dept. of Brain and Cognitive Sciences, M. I. T., Cambridge, MA 02139;Dept. Biomedical Eng, Johns Hopkins Univ, Baltimore, MD 21205;Dept. Physiology, Northwestern Univ Med Sch (M211), Chicago, IL 60611",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": ";",
        "aff_country_unique": ""
    },
    {
        "id": "e00bd29e43",
        "title": "Interior Point Implementations of Alternating Minimization Training",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/d79aac075930c83c2f1e369a511148fe-Abstract.html",
        "author": "Michael Lemmon; Peter T. Szymanski",
        "abstract": "This paper presents an alternating minimization (AM) algorithm  used in the training of radial basis function and linear regressor  networks. The algorithm is a modification of a small-step interior  point method used in solving primal linear programs. The algo(cid:173) rithm has a convergence rate of O( fo,L) iterations where n is a  measure of the network size and L is a measure of the resulting  solution's accuracy. Two results are presented that specify how  aggressively the two steps of the AM may be pursued to ensure  convergence of each step of the alternating minimization.",
        "bibtex": "@inproceedings{NIPS1994_d79aac07,\n author = {Lemmon, Michael and Szymanski, Peter},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Interior Point Implementations of Alternating Minimization Training},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/d79aac075930c83c2f1e369a511148fe-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/d79aac075930c83c2f1e369a511148fe-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/d79aac075930c83c2f1e369a511148fe-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1623506,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15439578903572182824&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Dept. of Electrical Engineering, University of Notre Dame; Dept. of Electrical Engineering, University of Notre Dame",
        "aff_domain": "maddog.ee.nd.edu;maddog.ee.nd.edu",
        "email": "maddog.ee.nd.edu;maddog.ee.nd.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Dept. of Electrical Engineering, University of Notre Dame",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "ff26258f46",
        "title": "JPMAX: Learning to Recognize Moving Objects as a Model-fitting Problem",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/4b0250793549726d5c1ea3906726ebfe-Abstract.html",
        "author": "Suzanna Becker",
        "abstract": "Unsupervised learning procedures have been successful at low-level  feature  extraction  and  preprocessing of raw  sensor  data.  So  far,  however,  they  have  had  limited  success  in  learning  higher-order  representations, e.g.,  of objects in  visual images.  A promising ap(cid:173) proach  is  to  maximize  some  measure  of  agreement  between  the  outputs of two groups of units which receive inputs physically sep(cid:173) arated in  space, time or modality,  as  in  (Becker and Hinton,  1992;  Becker, 1993; de Sa,  1993).  Using the same approach, a much sim(cid:173) pler  learning  procedure is  proposed  here  which  discovers  features  in a single-layer network consisting of several populations of units,  and  can  be  applied  to  multi-layer  networks  trained  one  layer  at  a  time.  When  trained with  this  algorithm  on  image  sequences  of  moving geometric objects a two-layer network can learn to perform  accurate position-invariant object classification.",
        "bibtex": "@inproceedings{NIPS1994_4b025079,\n author = {Becker, Suzanna},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {JPMAX: Learning to Recognize Moving Objects as a Model-fitting Problem},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/4b0250793549726d5c1ea3906726ebfe-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/4b0250793549726d5c1ea3906726ebfe-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/4b0250793549726d5c1ea3906726ebfe-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1630885,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1207600887414835167&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Psychology, McMaster University",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Department of Psychology, McMaster University",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "f09d9c808b",
        "title": "Learning Local Error Bars for Nonlinear Regression",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/061412e4a03c02f9902576ec55ebbe77-Abstract.html",
        "author": "David A. Nix; Andreas S. Weigend",
        "abstract": "We  present a  new  method for  obtaining  local  error bars for  nonlinear  regression,  i.e.,  estimates of the confidence in predicted values that de(cid:173) pend on the input.  We approach this problem by applying a maximum(cid:173) likelihood framework to an assumed distribution of errors.  We demon(cid:173) strate our method first on computer-generated data with locally varying,  normally distributed target noise.  We then apply it to laser data from the  Santa Fe  Time  Series Competition where the underlying system noise is  known quantization error and the error bars give local estimates of model  misspecification.  In  both  cases,  the method also provides a  weighted(cid:173) regression effect that improves generalization performance.",
        "bibtex": "@inproceedings{NIPS1994_061412e4,\n author = {Nix, David and Weigend, Andreas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Learning Local Error Bars for Nonlinear Regression},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/061412e4a03c02f9902576ec55ebbe77-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/061412e4a03c02f9902576ec55ebbe77-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/061412e4a03c02f9902576ec55ebbe77-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1837844,
        "gs_citation": 137,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8097362429298782320&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science and Institute of Cognitive Science, University of Colorado, Boulder, CO 80309-0430; Department of Computer Science and Institute of Cognitive Science, University of Colorado, Boulder, CO 80309-0430",
        "aff_domain": "cs.colorado.edu;cs.colorado.edu",
        "email": "cs.colorado.edu;cs.colorado.edu",
        "github": "",
        "project": "http://www.cs.colorado.edu/~andreas/Home.html",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Department of Computer Science and Institute of Cognitive Science, University of Colorado, Boulder, CO 80309-0430",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "8e9723a7b7",
        "title": "Learning Many Related Tasks at the Same Time with Backpropagation",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/0f840be9b8db4d3fbd5ba2ce59211f55-Abstract.html",
        "author": "Rich Caruana",
        "abstract": "Hinton  [6]  proposed  that  generalization  in  artificial  neural  nets  should improve  if nets  learn  to represent the  domain's underlying  regularities.  Abu-Mustafa's  hints work  [1]  shows  that the  outputs  of a  backprop  net  can  be  used  as  inputs  through  which  domain(cid:173) specific  information can be given to the net.  We extend these ideas  by showing that a  backprop net learning many related tasks at the  same time can use these  tasks  as inductive  bias for  each other and  thus learn better.  We  identify five  mechanisms by which multitask  backprop improves generalization and give  empirical evidence that  multi task backprop generalizes  better  in real  domains.",
        "bibtex": "@inproceedings{NIPS1994_0f840be9,\n author = {Caruana, Rich},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Learning Many Related Tasks at the Same Time with Backpropagation},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/0f840be9b8db4d3fbd5ba2ce59211f55-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/0f840be9b8db4d3fbd5ba2ce59211f55-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/0f840be9b8db4d3fbd5ba2ce59211f55-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1846735,
        "gs_citation": 405,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12613718332426445306&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "School of Computer Science, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu",
        "email": "cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3b343bc3b4",
        "title": "Learning Prototype Models for Tangent Distance",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/3df1d4b96d8976ff5986393e8767f5b2-Abstract.html",
        "author": "Trevor Hastie; Patrice Simard",
        "abstract": "Simard,  LeCun  & Denker  (1993)  showed  that the performance of  nearest-neighbor  classification  schemes  for  handwritten  character  recognition  can  be  improved  by  incorporating  invariance  to  spe(cid:173) the  so  cific  transformations in  the  underlying  distance  metric  - called  tangent  distance.  The  resulting  classifier,  however,  can  be  prohibitively slow and memory intensive due to the large amount of  prototypes that need to be stored and used in the distance compar(cid:173) isons.  In this  paper we  develop  rich  models for  representing  large  subsets of the prototypes.  These models are either used singly per  class,  or  as  basic building blocks  in conjunction with the K-means  clustering  algorithm.",
        "bibtex": "@inproceedings{NIPS1994_3df1d4b9,\n author = {Hastie, Trevor and Simard, Patrice},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Learning Prototype Models for Tangent Distance},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/3df1d4b96d8976ff5986393e8767f5b2-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/3df1d4b96d8976ff5986393e8767f5b2-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/3df1d4b96d8976ff5986393e8767f5b2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1554860,
        "gs_citation": 78,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15919355427893283724&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "da443a151c",
        "title": "Learning Saccadic Eye Movements Using Multiscale Spatial Filters",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/c4015b7f368e6b4871809f49debe0579-Abstract.html",
        "author": "Rajesh P. N. Rao; Dana H. Ballard",
        "abstract": "We  describe  a  framework  for  learning  saccadic  eye  movements  using  a  photometric representation of target points  in natural scenes.  The rep(cid:173) resentation takes the form of a high-dimensional vector comprised of the  responses  of spatial filters  at different  orientations and  scales.  We  first  demonstrate the use  of this  response  vector in  the task of locating pre(cid:173) viously foveated  points in a  scene and subsequently use  this property in  a  multisaccade strategy to derive  an adaptive motor map for  delivering  accurate saccades.",
        "bibtex": "@inproceedings{NIPS1994_c4015b7f,\n author = {Rao, Rajesh and Ballard, Dana},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Learning Saccadic Eye Movements Using Multiscale Spatial Filters},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/c4015b7f368e6b4871809f49debe0579-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/c4015b7f368e6b4871809f49debe0579-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/c4015b7f368e6b4871809f49debe0579-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1672532,
        "gs_citation": 47,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14043578898241772725&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, University of Rochester; Department of Computer Science, University of Rochester",
        "aff_domain": "cs.rochester.edu;cs.rochester.edu",
        "email": "cs.rochester.edu;cs.rochester.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Rochester",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.rochester.edu",
        "aff_unique_abbr": "U of R",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "55c78b9f4c",
        "title": "Learning Stochastic Perceptrons Under k-Blocking Distributions",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/dd8eb9f23fbd362da0e3f4e70b878c16-Abstract.html",
        "author": "Mario Marchand; Saeed Hadjifaradji",
        "abstract": "We  present  a  statistical  method  that  PAC  learns  the  class  of  stochastic  perceptrons  with  arbitrary  monotonic  activation  func(cid:173) tion and weights Wi  E  {-I, 0, + I} when the probability distribution  that  generates  the input examples  is  member of a  family  that we  call k-blocking distributions.  Such distributions represent an impor(cid:173) tant step beyond the case where each input variable is statistically  independent  since the 2k-blocking  family  contains  all  the  Markov  distributions of order k.  By stochastic  percept ron we  mean  a  per(cid:173) ceptron which,  upon presentation of input vector x, outputs 1 with  probability  fCLJi WiXi  - B).  Because the same algorithm works  for  any  monotonic  (nondecreasing  or  nonincreasing)  activation  func(cid:173) tion  f  on  Boolean  domain,  it  handles  the  well  studied  cases  of  sigmolds  and the  \"usual\"  radial basis functions.",
        "bibtex": "@inproceedings{NIPS1994_dd8eb9f2,\n author = {Marchand, Mario and Hadjifaradji, Saeed},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Learning Stochastic Perceptrons Under k-Blocking Distributions},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/dd8eb9f23fbd362da0e3f4e70b878c16-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/dd8eb9f23fbd362da0e3f4e70b878c16-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/dd8eb9f23fbd362da0e3f4e70b878c16-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1669848,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:_KoIGfDfyMEJ:scholar.google.com/&scioq=Learning+Stochastic+Perceptrons+Under+k-Blocking+Distributions&hl=en&as_sdt=0,5",
        "gs_version_total": 11,
        "aff": "Ottawa-Carleton Institute for Physics, University of Ottawa, Ottawa, Ont., Canada KIN 6N5; Ottawa-Carleton Institute for Physics, University of Ottawa, Ottawa, Ont., Canada KIN 6N5",
        "aff_domain": "physics.uottawa.ca;physics.uottawa.ca",
        "email": "physics.uottawa.ca;physics.uottawa.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Ottawa-Carleton Institute for Physics, University of Ottawa, Ottawa, Ont., Canada KIN 6N5",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "e5a6fefb93",
        "title": "Learning direction in global motion: two classes of psychophysically-motivated models",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/9ab0d88431732957a618d4a469a0d4c3-Abstract.html",
        "author": "V. Sundareswaran; Lucia M. Vaina",
        "abstract": "Perceptual learning is  defined  as  fast  improvement  in  performance and  retention  of the  learned  ability  over  a  period  of time.  In a  set  of psy(cid:173) chophysical experiments  we  demonstrated  that  perceptual learning oc(cid:173) curs for the discrimination of direction in stochastic motion stimuli.  Here  we  model  this  learning  using  two  approaches:  a clustering  model  that  learns  to  accommodate  the motion  noise,  and an averaging  model  that  learns to  ignore the noise.  Simulations of the models show  performance  similar to the psychophysical results.",
        "bibtex": "@inproceedings{NIPS1994_9ab0d884,\n author = {Sundareswaran, V. and Vaina, Lucia},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Learning direction in global motion: two classes of psychophysically-motivated models},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/9ab0d88431732957a618d4a469a0d4c3-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/9ab0d88431732957a618d4a469a0d4c3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/9ab0d88431732957a618d4a469a0d4c3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1634550,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5888556221985656164&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Intelligent Systems Laboratory, College of Engineering, Boston University; Intelligent Systems Laboratory, College of Engineering, Boston University",
        "aff_domain": ";bu.edu",
        "email": ";bu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Intelligent Systems Laboratory, College of Engineering, Boston University",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "9fe113e7e6",
        "title": "Learning from queries for maximum information gain in imperfectly learnable problems",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/eeb69a3cb92300456b6a5f4162093851-Abstract.html",
        "author": "Peter Sollich; David Saad",
        "abstract": "In  supervised  learning,  learning  from  queries  rather  than  from  random  examples  can  improve  generalization  performance  signif(cid:173) icantly.  We  study  the  performance of query  learning for  problems  where  the  student  cannot learn  the  teacher  perfectly,  which  occur  frequently  in  practice.  As  a  prototypical scenario of this  kind,  we  consider  a  linear  perceptron  student  learning a  binary  perceptron  teacher.  Two kinds of queries for  maximum information gain,  i.e.,  minimum entropy,  are  investigated:  Minimum  student  space  en(cid:173) tropy  (MSSE)  queries,  which  are  appropriate  if the  teacher  space  is  unknown,  and  minimum teacher  space entropy  (MTSE)  queries,  which can be used  if the teacher space is  assumed to be known, but  a student of a simpler form  has deliberately  been  chosen.  We  find  that  for  MSSE  queries,  the  structure  of the  student  space  deter(cid:173) mines  the  efficacy  of query  learning,  whereas  MTSE  queries  lead  to  a  higher  generalization  error  than  random examples,  due  to  a  lack of feedback about the progress of the student in the way queries  are  selected.",
        "bibtex": "@inproceedings{NIPS1994_eeb69a3c,\n author = {Sollich, Peter and Saad, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Learning from queries for maximum information gain in imperfectly learnable problems},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/eeb69a3cb92300456b6a5f4162093851-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/eeb69a3cb92300456b6a5f4162093851-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/eeb69a3cb92300456b6a5f4162093851-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1716347,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5851679499684112567&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Physics, University of Edinburgh; Department of Physics, University of Edinburgh",
        "aff_domain": "ed.ac.uk;ed.ac.uk",
        "email": "ed.ac.uk;ed.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Department of Physics, University of Edinburgh",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "a0f3031bc9",
        "title": "Learning in large linear perceptrons and why the thermodynamic limit is relevant to the real world",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/c32d9bf27a3da7ec8163957080c8628e-Abstract.html",
        "author": "Peter Sollich",
        "abstract": "We  present  a  new  method  for  obtaining  the  response  function  9  and  its  average  G  from  which  most  of the  properties  of learning  and  generalization  in  linear  perceptrons  can  be  derived.  We  first  rederive the  known results for  the 'thermodynamic limit' of infinite  perceptron  size  N  and  show  explicitly  that  9  is  self-averaging  in  this limit.  We  then discuss extensions of our  method to  more gen(cid:173) eral  learning scenarios with  anisotropic teacher  space priors,  input  distributions, and weight  decay  terms.  Finally, we  use our method  to calculate the  finite  N  corrections of order  1/ N  to G  and discuss  the corresponding  finite  size  effects  on generalization  and learning  dynamics.  An  important  spin-off  is  the  observation  that  results  obtained  in  the  thermodynamic limit are often  directly  relevant  to  systems of fairly  modest, 'real-world' sizes.",
        "bibtex": "@inproceedings{NIPS1994_c32d9bf2,\n author = {Sollich, Peter},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Learning in large linear perceptrons and why the thermodynamic limit is relevant to the real world},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/c32d9bf27a3da7ec8163957080c8628e-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/c32d9bf27a3da7ec8163957080c8628e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/c32d9bf27a3da7ec8163957080c8628e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1617094,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5253224709310115009&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Physics, University of Edinburgh",
        "aff_domain": "ed.ac.uk",
        "email": "ed.ac.uk",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Department of Physics, University of Edinburgh",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "7ede86d031",
        "title": "Learning to Play the Game of Chess",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/d7322ed717dedf1eb4e6e52a37ea7bcd-Abstract.html",
        "author": "Sebastian Thrun",
        "abstract": "This paper presents NeuroChess,  a program which learns to play chess from the final  outcome of games.  NeuroChess learns chess  board  evaluation functions,  represented  by artificial neural  networks.  It integrates inductive neural  network learning, temporal  differencing, and a variant of explanation-based learning.  Performance results illustrate  some of the strengths and weaknesses of this approach.",
        "bibtex": "@inproceedings{NIPS1994_d7322ed7,\n author = {Thrun, Sebastian},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Learning to Play the Game of Chess},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/d7322ed717dedf1eb4e6e52a37ea7bcd-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/d7322ed717dedf1eb4e6e52a37ea7bcd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/d7322ed717dedf1eb4e6e52a37ea7bcd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1832089,
        "gs_citation": 301,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10411886069876131220&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "822886c377",
        "title": "Learning with Preknowledge: Clustering with Point and Graph Matching Distance Measures",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/043c3d7e489c69b48737cc0c92d0f3a2-Abstract.html",
        "author": "Steven Gold; Anand Rangarajan; Eric Mjolsness",
        "abstract": "Prior constraints are imposed upon a learning problem in the form  of distance measures. Prototypical 2-D point sets and graphs are  learned by clustering with point matching and graph matching dis(cid:173) tance measures. The point matching distance measure is approx.  invariant under affine transformations - translation, rotation, scale  and shear - and permutations. It operates between noisy images  with missing and spurious points. The graph matching distance  measure operates on weighted graphs and is invariant under per(cid:173) mutations. Learning is formulated as an optimization problem .  Large objectives so formulated ('\" million variables) are efficiently  minimized using a combination of optimization techniques - alge(cid:173) braic transformations, iterative projective scaling, clocked objec(cid:173) tives, and deterministic annealing.",
        "bibtex": "@inproceedings{NIPS1994_043c3d7e,\n author = {Gold, Steven and Rangarajan, Anand and Mjolsness, Eric},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Learning with Preknowledge: Clustering with Point and Graph Matching Distance Measures},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/043c3d7e489c69b48737cc0c92d0f3a2-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/043c3d7e489c69b48737cc0c92d0f3a2-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/043c3d7e489c69b48737cc0c92d0f3a2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1587916,
        "gs_citation": 102,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9717058609154263489&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 20,
        "aff": "Department of Computer Science, Yale University; Department of Computer Science, Yale University; Department of Computer Science and Engineering, University of California at San Diego (UCSD)",
        "aff_domain": "cs.yale.edu;cs.yale.edu;cs.ucsd.edu",
        "email": "cs.yale.edu;cs.yale.edu;cs.ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Yale University;Department of Computer Science and Engineering, University of California at San Diego (UCSD)",
        "aff_unique_dep": "Department of Computer Science;",
        "aff_unique_url": "https://www.yale.edu;",
        "aff_unique_abbr": "Yale;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "c8b8adad06",
        "title": "Learning with Product Units",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/4fac9ba115140ac4f1c22da82aa0bc7f-Abstract.html",
        "author": "Laurens R. Leerink; C. Lee Giles; Bill G. Horne; Marwan A. Jabri",
        "abstract": "The  TNM  staging  system  has  been  used  since  the  early  1960's  to  predict  breast  cancer  patient  outcome.  In  an  attempt  to  in(cid:173) crease  prognostic  accuracy,  many putative prognostic factors  have  been  identified.  Because  the  TNM  stage  model  can  not  accom(cid:173) modate  these  new  factors,  the  proliferation  of factors  in  breast  cancer  has  lead  to  clinical  confusion.  What  is  required  is  a  new  computerized  prognostic system  that  can test  putative prognostic  factors  and  integrate  the  predictive  factors  with  the  TNM  vari(cid:173) ables  in order to increase  prognostic  accuracy.  Using  the area un(cid:173) der  the curve of the receiver  operating characteristic,  we  compare  the  accuracy  of the  following  predictive  models  in  terms  of five  year  breast cancer-specific  survival:  pTNM staging system,  princi(cid:173) pal component analysis,  classification and regression  trees,  logistic  regression,  cascade  correlation neural network,  conjugate gradient  descent  neural,  probabilistic neural network,  and backpropagation  neural network.  Several statistical models are significantly more ac-",
        "bibtex": "@inproceedings{NIPS1994_4fac9ba1,\n author = {Leerink, Laurens and Giles, C. and Horne, Bill and Jabri, Marwan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Learning with Product Units},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/4fac9ba115140ac4f1c22da82aa0bc7f-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/4fac9ba115140ac4f1c22da82aa0bc7f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/4fac9ba115140ac4f1c22da82aa0bc7f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2717194,
        "gs_citation": 105,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10256552021247260493&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "b96a7c9274",
        "title": "Limits on Learning Machine Accuracy Imposed by Data Quality",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/1e056d2b0ebd5c878c550da6ac5d3724-Abstract.html",
        "author": "Corinna Cortes; L. D. Jackel; Wan-Ping Chiang",
        "abstract": "Random  errors  and  insufficiencies  in  databases  limit  the  perfor(cid:173) mance  of any classifier  trained  from  and applied  to the  database.  In this paper we  propose a method to estimate the limiting perfor(cid:173) mance of classifiers imposed by the database.  We demonstrate this  technique  on  the  task  of predicting  failure  in  telecommunication  paths.",
        "bibtex": "@inproceedings{NIPS1994_1e056d2b,\n author = {Cortes, Corinna and Jackel, L. D. and Chiang, Wan-Ping},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Limits on Learning Machine Accuracy Imposed by Data Quality},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/1e056d2b0ebd5c878c550da6ac5d3724-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/1e056d2b0ebd5c878c550da6ac5d3724-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/1e056d2b0ebd5c878c550da6ac5d3724-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1724666,
        "gs_citation": 188,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12183970284290679040&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "98b51aa372",
        "title": "Model of a Biological Neuron as a Temporal Neural Network",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/4311359ed4969e8401880e3c1836fbe1-Abstract.html",
        "author": "Sean D. Murphy; Edward W. Kairiss",
        "abstract": "A biological neuron can be viewed as a device that maps a multidimen(cid:173) sional  temporal  event signal  (dendritic  postsynaptic  activations)  into a  unidimensional  temporal  event  signal  (action  potentials).  We  have  designed  a  network,  the  Spatio-Temporal  Event  Mapping  (STEM)  architecture, which can learn to perform this mapping for arbitrary bio(cid:173) physical  models  of  neurons.  Such  a  network  appropriately  trained,  called a STEM cell, can be used in place of a conventional compartmen(cid:173) tal  model  in  simulations  where only the  transfer function  is  important,  such  as  network  simulations.  The  STEM  cell  offers  advantages  over  compartmental  models in  terms of computational efficiency, analytical  tractabili1ty,  and as a framework  for  VLSI  implementations of biologi(cid:173) cal neurons.",
        "bibtex": "@inproceedings{NIPS1994_4311359e,\n author = {Murphy, Sean D. and Kairiss, Edward W.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Model of a Biological Neuron as a Temporal Neural Network},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/4311359ed4969e8401880e3c1836fbe1-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/4311359ed4969e8401880e3c1836fbe1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/4311359ed4969e8401880e3c1836fbe1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1506888,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6506543118430968140&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Interdepartmental Neuroscience Program, Department of Psychology, and The Center for Theoretical and Applied Neuroscience, Yale University; Interdepartmental Neuroscience Program, Department of Psychology, and The Center for Theoretical and Applied Neuroscience, Yale University",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Interdepartmental Neuroscience Program, Department of Psychology, and The Center for Theoretical and Applied Neuroscience, Yale University",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "d25470b361",
        "title": "Morphogenesis of the Lateral Geniculate Nucleus: How Singularities Affect Global Structure",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/cfbce4c1d7c425baf21d6b6f2babe6be-Abstract.html",
        "author": "Svilen Tzonev; Klaus Schulten; Joseph G. Malpeli",
        "abstract": "The macaque lateral geniculate nucleus (LGN) exhibits an intricate  lamination pattern, which changes midway through the nucleus at a  point coincident with small gaps due to the blind spot in the retina.  We  present a  three-dimensional model of morphogenesis in  which  local cell interactions cause a  wave  of development of neuronal re(cid:173) ceptive fields  to propagate through the nucleus  and establish two  distinct lamination patterns.  We  examine the interactions between  the wave  and the localized singularities due  to the gaps,  and find  that the gaps induce the change in lamination pattern.  We  explore  critical factors which  determine general LGN organization.",
        "bibtex": "@inproceedings{NIPS1994_cfbce4c1,\n author = {Tzonev, Svilen and Schulten, Klaus and Malpeli, Joseph},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Morphogenesis of the Lateral Geniculate Nucleus: How Singularities Affect Global Structure},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/cfbce4c1d7c425baf21d6b6f2babe6be-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/cfbce4c1d7c425baf21d6b6f2babe6be-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/cfbce4c1d7c425baf21d6b6f2babe6be-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1633303,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6796022355882781299&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Beckman Institute, University of Illinois, Urbana, IL 61801; Beckman Institute, University of Illinois, Urbana, IL 61801; Psychology Department, University of Illinois, Champaign, IL 61820",
        "aff_domain": "ks.uiuc.edu;ks.uiuc.edu;uiuc.edu",
        "email": "ks.uiuc.edu;ks.uiuc.edu;uiuc.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Beckman Institute, University of Illinois, Urbana, IL 61801;Psychology Department, University of Illinois, Champaign, IL 61820",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "accdb954a4",
        "title": "Multidimensional Scaling and Data Clustering",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/1587965fb4d4b5afe8428a4a024feb0d-Abstract.html",
        "author": "Thomas Hofmann; Joachim Buhmann",
        "abstract": "Visualizing and structuring pairwise dissimilarity data are difficult combinatorial op(cid:173) timization problems known as multidimensional scaling or pairwise data clustering.  Algorithms for embedding dissimilarity data set in a Euclidian space, for clustering  these data and for actively selecting data to support the clustering process are discussed  in the maximum entropy framework. Active data selection provides a strategy to discover  structure in a data set efficiently with partially unknown data.",
        "bibtex": "@inproceedings{NIPS1994_1587965f,\n author = {Hofmann, Thomas and Buhmann, Joachim},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Multidimensional Scaling and Data Clustering},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/1587965fb4d4b5afe8428a4a024feb0d-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/1587965fb4d4b5afe8428a4a024feb0d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/1587965fb4d4b5afe8428a4a024feb0d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1650954,
        "gs_citation": 74,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16381149265602374605&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Rheinische Friedrich-Wilhelms-Universitat Institut fur Informatik ill, Romerstra6e 164 D-53117 Bonn, Germany; Rheinische Friedrich-Wilhelms-Universitat Institut fur Informatik ill, Romerstra6e 164 D-53117 Bonn, Germany",
        "aff_domain": "cs.uni-bonn.de;cs.uni-bonn.de",
        "email": "cs.uni-bonn.de;cs.uni-bonn.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Rheinische Friedrich-Wilhelms-Universitat Institut fur Informatik ill, Romerstra6e 164 D-53117 Bonn, Germany",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "6e2bab7050",
        "title": "Neural Network Ensembles, Cross Validation, and Active Learning",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/b8c37e33defde51cf91e1e03e51657da-Abstract.html",
        "author": "Anders Krogh; Jesper Vedelsby",
        "abstract": "Learning  of continuous  valued  functions  using  neural  network  en(cid:173) sembles  (committees) can give  improved accuracy,  reliable estima(cid:173) tion of the generalization error,  and active learning.  The  ambiguity  is  defined as the variation of the output of ensemble members aver(cid:173) aged  over  unlabeled  data, so  it quantifies the  disagreement  among  the networks.  It is  discussed  how  to use the ambiguity in combina(cid:173) tion with cross-validation to give a reliable estimate of the ensemble  generalization error, and how this type of ensemble cross-validation  can sometimes improve performance.  It is  shown  how  to estimate  the optimal weights of the ensemble members using unlabeled  data.  By a generalization of query  by  committee, it is finally shown how  the ambiguity can be used to select new training data to be labeled  in an active learning scheme.",
        "bibtex": "@inproceedings{NIPS1994_b8c37e33,\n author = {Krogh, Anders and Vedelsby, Jesper},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Neural Network Ensembles, Cross Validation, and Active Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/b8c37e33defde51cf91e1e03e51657da-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/b8c37e33defde51cf91e1e03e51657da-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/b8c37e33defde51cf91e1e03e51657da-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1604772,
        "gs_citation": 3155,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18178760391054521496&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Nordita, Blegdamsvej 17, 2100 Copenhagen, Denmark; Electronics Institute, Building 349, Technical University of Denmark, 2800 Lyngby, Denmark",
        "aff_domain": "kroghlnordita.elk; ",
        "email": "kroghlnordita.elk; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Nordita, Blegdamsvej 17, 2100 Copenhagen, Denmark;Electronics Institute, Building 349, Technical University of Denmark, 2800 Lyngby, Denmark",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "e1f33182f6",
        "title": "New Algorithms for 2D and 3D Point Matching: Pose Estimation and Correspondence",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/cc1aa436277138f61cda703991069eaf-Abstract.html",
        "author": "Steven Gold; Chien-Ping Lu; Anand Rangarajan; Suguna Pappu; Eric Mjolsness",
        "abstract": "A  fundamental  open  problem  in  computer  vision-determining  pose  and  correspondence  between  two  sets  of  points  in  space(cid:173) is  solved with a novel, robust  and easily implementable algorithm.  The  technique  works  on  noisy  point sets  that  may be  of unequal  sizes  and  may  differ  by  non-rigid  transformations.  A  2D  varia(cid:173) tion  calculates  the  pose  between  point  sets  related  by  an  affine  transformation-translation, rotation, scale and shear.  A 3D to 3D  variation calculates translation and rotation.  An objective describ(cid:173) ing  the  problem is  derived  from  Mean field  theory.  The objective  is  minimized with clocked  (EM-like)  dynamics.  Experiments with  both  handwritten  and  synthetic  data provide  empirical  evidence  for  the method.",
        "bibtex": "@inproceedings{NIPS1994_cc1aa436,\n author = {Gold, Steven and Lu, Chien-Ping and Rangarajan, Anand and Pappu, Suguna and Mjolsness, Eric},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {New Algorithms for 2D and 3D Point Matching: Pose Estimation and Correspondence},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/cc1aa436277138f61cda703991069eaf-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/cc1aa436277138f61cda703991069eaf-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/cc1aa436277138f61cda703991069eaf-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1483338,
        "gs_citation": 763,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11197008897888276205&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, Yale University; Department of Computer Science, Yale University; Department of Computer Science, Yale University; Department of Computer Science, Yale University; Department of Computer Science and Engineering, University of California at San Diego (UCSD)",
        "aff_domain": "cs.yale.edu;cs.yale.edu;cs.yale.edu;cs.yale.edu;cs.ucsd.edu",
        "email": "cs.yale.edu;cs.yale.edu;cs.yale.edu;cs.yale.edu;cs.ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;1",
        "aff_unique_norm": "Yale University;Department of Computer Science and Engineering, University of California at San Diego (UCSD)",
        "aff_unique_dep": "Department of Computer Science;",
        "aff_unique_url": "https://www.yale.edu;",
        "aff_unique_abbr": "Yale;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "3a1f0c6b29",
        "title": "Non-linear Prediction of Acoustic Vectors Using Hierarchical Mixtures of Experts",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/437d7d1d97917cd627a34a6a0fb41136-Abstract.html",
        "author": "Steve R. Waterhouse; Anthony J. Robinson",
        "abstract": "In this paper we consider speech coding as a problem of speech  modelling. In particular, prediction of parameterised speech over  short time segments is performed using the Hierarchical Mixture of  Experts (HME) (Jordan & Jacobs 1994). The HME gives two ad(cid:173) vantages over traditional non-linear function approximators such  as the Multi-Layer Percept ron (MLP); a statistical understand(cid:173) ing of the operation of the predictor and provision of information  about the performance of the predictor in the form of likelihood  information and local error bars. These two issues are examined  on both toy and real world problems of regression and time series  prediction. In the speech coding context, we extend the principle  of combining local predictions via the HME to a Vector Quantiza(cid:173) tion scheme in which fixed local codebooks are combined on-line  for each observation.",
        "bibtex": "@inproceedings{NIPS1994_437d7d1d,\n author = {Waterhouse, Steve and Robinson, Anthony},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Non-linear Prediction of Acoustic Vectors Using Hierarchical Mixtures of Experts},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/437d7d1d97917cd627a34a6a0fb41136-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/437d7d1d97917cd627a34a6a0fb41136-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/437d7d1d97917cd627a34a6a0fb41136-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1532641,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1959611182740957844&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "4f5db8f7b2",
        "title": "Nonlinear Image Interpolation using Manifold Learning",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/d516b13671a4179d9b7b458a6ebdeb92-Abstract.html",
        "author": "Christoph Bregler; Stephen M. Omohundro",
        "abstract": "The problem of interpolating between specified images in an image  sequence  is  a  simple,  but  important  task  in  model-based  vision.  We  describe  an  approach based  on  the abstract  task of  \"manifold  learning\"  and present  results  on  both synthetic and real image se(cid:173) quences.  This  problem  arose  in  the  development  of a  combined  lip-reading and speech  recognition system.",
        "bibtex": "@inproceedings{NIPS1994_d516b136,\n author = {Bregler, Christoph and Omohundro, Stephen},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Nonlinear Image Interpolation using Manifold Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/d516b13671a4179d9b7b458a6ebdeb92-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/d516b13671a4179d9b7b458a6ebdeb92-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/d516b13671a4179d9b7b458a6ebdeb92-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1292140,
        "gs_citation": 148,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13292105594293422359&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Computer Science Division, University of California, Berkeley, CA 94720; Int. Computer Science Institute, 1947 Center Street Suite 600, Berkeley, CA 94704 + NEe Research Institute, Inc., 4 Independence Way, Princeton, NJ 08540",
        "aff_domain": "cs.berkeley.edu;research.nj.nec.com",
        "email": "cs.berkeley.edu;research.nj.nec.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+2",
        "aff_unique_norm": "University of California, Berkeley;Int. Computer Science Institute, 1947 Center Street Suite 600, Berkeley, CA 94704;NEe Research Institute, Inc., 4 Independence Way, Princeton, NJ 08540",
        "aff_unique_dep": "Computer Science Division;;",
        "aff_unique_url": "https://www.berkeley.edu;;",
        "aff_unique_abbr": "UC Berkeley;;",
        "aff_campus_unique_index": "0;",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;",
        "aff_country_unique": "United States;"
    },
    {
        "id": "c4e6738bcb",
        "title": "Ocular Dominance and Patterned Lateral Connections in a Self-Organizing Model of the Primary Visual Cortex",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/6b180037abbebea991d8b1232f8a8ca9-Abstract.html",
        "author": "Joseph Sirosh; Risto Miikkulainen",
        "abstract": "A neural network model for the self-organization of ocular dominance and  lateral connections from binocular input is presented.  The self-organizing  process results in a network where (1) afferent weights of each neuron or(cid:173) ganize into smooth hill-shaped receptive fields primarily on one of the reti(cid:173) nas, (2) neurons with common eye preference form connected, intertwined  patches, and (3) lateral connections primarily link regions of the same eye  preference.  Similar self-organization of cortical  structures has  been  ob(cid:173) served  experimentally in strabismic kittens.  The model  shows how  pat(cid:173) terned lateral connections in  the cortex may  develop based on correlated  activity and explains why lateral connection patterns follow receptive field  properties such as ocular dominance.",
        "bibtex": "@inproceedings{NIPS1994_6b180037,\n author = {Sirosh, Joseph and Miikkulainen, Risto},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Ocular Dominance and Patterned Lateral Connections in a Self-Organizing Model of the Primary Visual Cortex},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/6b180037abbebea991d8b1232f8a8ca9-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/6b180037abbebea991d8b1232f8a8ca9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/6b180037abbebea991d8b1232f8a8ca9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1780100,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5339388028598749584&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Sciences, University of Texas at Austin; Department of Computer Sciences, University of Texas at Austin",
        "aff_domain": "cs.utexas.edu;cs.utexas.edu",
        "email": "cs.utexas.edu;cs.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "Department of Computer Sciences",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "78784ca6ab",
        "title": "On the Computational Complexity of Networks of Spiking Neurons",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/cbb6a3b884f4f88b3a8e3d44c636cbd8-Abstract.html",
        "author": "Wolfgang Maass",
        "abstract": "We investigate the computational power of a formal model for net(cid:173) works of spiking neurons,  both for  the assumption of an unlimited  timing precision, and for  the case of a limited timing precision.  We  also prove upper and lower bounds for the number of examples that  are  needed  to train such networks.",
        "bibtex": "@inproceedings{NIPS1994_cbb6a3b8,\n author = {Maass, Wolfgang},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {On the Computational Complexity of Networks of Spiking Neurons},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/cbb6a3b884f4f88b3a8e3d44c636cbd8-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/cbb6a3b884f4f88b3a8e3d44c636cbd8-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/cbb6a3b884f4f88b3a8e3d44c636cbd8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1825080,
        "gs_citation": 89,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6863995403917948932&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Institute for Theoretical Computer Science, Technische Universitaet Graz",
        "aff_domain": "igi.tu-graz.ac.at",
        "email": "igi.tu-graz.ac.at",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Institute for Theoretical Computer Science, Technische Universitaet Graz",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "c268375c2a",
        "title": "On the Computational Utility of Consciousness",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/6aab1270668d8cac7cef2566a1c5f569-Abstract.html",
        "author": "Donald W. Mathis; Michael Mozer",
        "abstract": "We  propose  a  computational  framework  for  understanding  and  modeling  human consciousness.  This  framework  integrates  many  existing theoretical perspectives,  yet is sufficiently concrete to allow  simulation experiments.  We do not attempt to explain qualia (sub(cid:173) jective  experience),  but  instead  ask  what  differences  exist  within  the cognitive information processing  system  when a  person is  con(cid:173) scious  of mentally-represented  information versus  when that infor(cid:173) mation is  unconscious.  The central idea we explore is  that the con(cid:173) tents  of consciousness  correspond  to  temporally  persistent  states  in a  network of computational modules.  Three simulations are de(cid:173) scribed  illustrating  that  the  behavior  of  persistent  states  in  the  models  corresponds  roughly  to  the  behavior  of  conscious  states  people  experience when  performing similar tasks.  Our simulations  show that periodic  settling to persistent  (i.e.,  conscious)  states im(cid:173) proves  performance  by cleaning  up inaccuracies  and noise,  forcing  decisions,  and helping  keep the system on track  toward a  solution.",
        "bibtex": "@inproceedings{NIPS1994_6aab1270,\n author = {Mathis, Donald and Mozer, Michael C},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {On the Computational Utility of Consciousness},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/6aab1270668d8cac7cef2566a1c5f569-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/6aab1270668d8cac7cef2566a1c5f569-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/6aab1270668d8cac7cef2566a1c5f569-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1586407,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12793055810578300068&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 8,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "2f7a01279f",
        "title": "On-line Learning of Dichotomies",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/9c01802ddb981e6bcfbec0f0516b8e35-Abstract.html",
        "author": "N. Barkai; H. S. Seung; H. Sompolinsky",
        "abstract": "The performance of on-line algorithms for learning dichotomies is studied. In on-line learn(cid:173) ing, the number of examples P is equivalent to the learning time, since each example is  presented only once. The learning curve, or generalization error as a function of P, depends  on the schedule at which the learning rate is lowered. For a target that is a perceptron rule,  the learning curve of the perceptron algorithm can decrease as fast as p- 1 , if the sched(cid:173) ule is optimized. If the target is not realizable by a perceptron, the perceptron algorithm  does not generally converge to the solution with lowest generalization error. For the case  of unrealizability due to a simple output noise, we propose a new on-line algorithm for a  perceptron yielding a learning curve that can approach the optimal generalization error as  fast as p-l/2. We then generalize the perceptron algorithm to any class of thresholded  smooth functions learning a target from that class. For \"well-behaved\" input distributions,  if this algorithm converges to the optimal solution, its learning curve can decrease as fast  as p-l.",
        "bibtex": "@inproceedings{NIPS1994_9c01802d,\n author = {Barkai, N. and Seung, H. and Sompolinsky, H.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {On-line Learning of Dichotomies},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/9c01802ddb981e6bcfbec0f0516b8e35-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/9c01802ddb981e6bcfbec0f0516b8e35-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/9c01802ddb981e6bcfbec0f0516b8e35-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1423634,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10116468430792685912&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Racah Institute of Physics, The Hebrew University, Jerusalem, Israel 91904; AT&T Bell Laboratories, Murray Hill, NJ 07974; Racah Institute of Physics, The Hebrew University, Jerusalem, Israel 91904 + AT&T Bell Laboratories",
        "aff_domain": "naamaCfiz.huji.ac.il; seungCphysics.att.com; haimCfiz.huji.ac.il",
        "email": "naamaCfiz.huji.ac.il; seungCphysics.att.com; haimCfiz.huji.ac.il",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0+2",
        "aff_unique_norm": "Racah Institute of Physics, The Hebrew University, Jerusalem, Israel 91904;AT&T Bell Laboratories, Murray Hill, NJ 07974;AT&T Bell Laboratories",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;https://www.att.com/labs",
        "aff_unique_abbr": ";;AT&T Bell Labs",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "4e65ad784a",
        "title": "Optimal Movement Primitives",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/1f4477bad7af3616c1f933a02bfabe4e-Abstract.html",
        "author": "Terence D. Sanger",
        "abstract": "The  theory  of Optimal  Unsupervised  Motor  Learning  shows  how  a  network  can  discover  a  reduced-order  controller for  an  unknown  nonlinear system by  representing only  the most significant  modes.  Here,  I extend the theory  to apply to command sequences,  so that  the most significant  components discovered  by  the  network  corre(cid:173) spond  to  motion  \"primitives\".  Combinations of these  primitives  can  be  used  to  produce  a  wide  variety  of  different  movements.  I  demonstrate  applications  to  human  handwriting  decomposition  and  synthesis,  as  well  as  to  the  analysis  of  electrophysiological  experiments  on  movements resulting  from  stimulation of the  frog  spinal cord.",
        "bibtex": "@inproceedings{NIPS1994_1f4477ba,\n author = {Sanger, Terence},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Optimal Movement Primitives},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/1f4477bad7af3616c1f933a02bfabe4e-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/1f4477bad7af3616c1f933a02bfabe4e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/1f4477bad7af3616c1f933a02bfabe4e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1246237,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16664465341994526272&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Jet Propulsion Laboratory",
        "aff_domain": "ai.mit.edu",
        "email": "ai.mit.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Jet Propulsion Laboratory",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.jpl.nasa.gov",
        "aff_unique_abbr": "JPL",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Pasadena",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e57b8a7e3b",
        "title": "PCA-Pyramids for Image Compression",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/ccc0aa1b81bf81e16c676ddb977c5881-Abstract.html",
        "author": "Horst Bischof; Kurt Hornik",
        "abstract": "This paper presents a new  method for image compression by neural  networks.  First, we  show  that we  can use neural networks in  a  py(cid:173) ramidal framework, yielding the so-called PCA pyramids.  Then we  present an image compression method based on the PCA pyramid,  which  is  similar  to  the  Laplace  pyramid  and  wavelet  transform.  Some experimental results  with  real images are reported.  Finally,  we  present  a  method  to  combine  the  quantization  step  with  the  learning of the PCA pyramid.",
        "bibtex": "@inproceedings{NIPS1994_ccc0aa1b,\n author = {Bischof, Horst and Hornik, Kurt},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {PCA-Pyramids for Image Compression},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/ccc0aa1b81bf81e16c676ddb977c5881-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/ccc0aa1b81bf81e16c676ddb977c5881-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/ccc0aa1b81bf81e16c676ddb977c5881-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1542396,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4333557942644710745&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Department for Pattern Recognition and Image Processing, Technical University Vienna; Institut fur Statistik und Wahrscheinlichkeitstheorie, Technische UniversiUit Wien",
        "aff_domain": "prip.tuwien.ac.at;ci.tuwien.ac.at",
        "email": "prip.tuwien.ac.at;ci.tuwien.ac.at",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Department for Pattern Recognition and Image Processing, Technical University Vienna;Institut fur Statistik und Wahrscheinlichkeitstheorie, Technische UniversiUit Wien",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "67255aae5c",
        "title": "Pairwise Neural Network Classifiers with Probabilistic Outputs",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/210f760a89db30aa72ca258a3483cc7f-Abstract.html",
        "author": "David Price; Stefan Knerr; L\u00e9on Personnaz; G\u00e9rard Dreyfus",
        "abstract": "Multi-class  classification  problems  can  be  efficiently  solved  by  partitioning the  original problem into  sub-problems  involving  only  two  classes:  for each pair of classes,  a (potentially small)  neural  network is  trained  using  only  the  data  of  these  two  classes.  We  show  how  to  combine the outputs of the  two-class  neural  networks in order to  obtain  posterior probabilities for  the class decisions.  The resulting probabilistic  pairwise classifier is part of a handwriting recognition  system which  is  currently applied to check reading. We present results on real world data  bases and show that, from a practical point of view, these results compare  favorably to other neural network approaches.",
        "bibtex": "@inproceedings{NIPS1994_210f760a,\n author = {Price, David and Knerr, Stefan and Personnaz, L\\'{e}on and Dreyfus, G\\'{e}rard},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Pairwise Neural Network Classifiers with Probabilistic Outputs},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/210f760a89db30aa72ca258a3483cc7f-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/210f760a89db30aa72ca258a3483cc7f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/210f760a89db30aa72ca258a3483cc7f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1519381,
        "gs_citation": 197,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6334218667334118168&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "A2iA and ESPCI; ESPCI and CNRS (UPR AOOO5); ESPeI, Laboratoire d'Electronique; ESPeI, Laboratoire d'Electronique",
        "aff_domain": "dialup.francenet.fr;neurones.espci.fr;neurones.espci.fr;neurones.espci.fr",
        "email": "dialup.francenet.fr;neurones.espci.fr;neurones.espci.fr;neurones.espci.fr",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;2",
        "aff_unique_norm": "A2iA and ESPCI;ESPCI and CNRS (UPR AOOO5);ESPeI, Laboratoire d'Electronique",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "277d80f2c0",
        "title": "Pattern Playback in the 90s",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/2ab56412b1163ee131e1246da0955bd1-Abstract.html",
        "author": "Malcolm Slaney",
        "abstract": "Deciding  the  appropriate  representation  to  use  for  modeling  human  auditory processing  is  a critical issue  in  auditory  science. While  engi(cid:173) neers have successfully performed many single-speaker tasks with LPC  and  spectrogram  methods,  more  difficult  problems  will  need  a  richer  representation. This paper describes a powerful auditory representation  known as the correlogram and shows how this non-linear representation  can be converted back into  sound, with no  loss  of perceptually  impor(cid:173) tant information.  The correlogram  is  interesting because  it is  a  neuro(cid:173) physiologically  plausible  representation  of  sound.  This  paper  shows  improved  methods  for  spectrogram  inversion  (conventional  pattern  playback), inversion of a cochlear model, and inversion of the  correlo(cid:173) gram representation.",
        "bibtex": "@inproceedings{NIPS1994_2ab56412,\n author = {Slaney, Malcolm},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Pattern Playback in the 90s},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/2ab56412b1163ee131e1246da0955bd1-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/2ab56412b1163ee131e1246da0955bd1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/2ab56412b1163ee131e1246da0955bd1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1804436,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10910604531481153009&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "430d5512b9",
        "title": "Patterns of damage in neural networks: The effects of lesion area, shape and number",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/b55ec28c52d5f6205684a473a2193564-Abstract.html",
        "author": "Eytan Ruppin; James A. Reggia",
        "abstract": "Current understanding of the effects of damage on neural networks  is  rudimentary, even  though such  understanding  could lead to im(cid:173) portant insights concerning neurological and psychiatric disorders.  Motivated  by  this  consideration,  we  present  a  simple  analytical  framework for  estimating the functional  damage resulting from fo(cid:173) cal  structural  lesions  to  a  neural  network.  The  effects  of focal  le(cid:173) sions of varying area,  shape and number on the retrieval capacities  of a spatially-organized associative memory.  Although our analyti(cid:173) cal results are based on some approximations, they correspond well  with simulation results.  This study sheds light on  some important  features  characterizing  the  clinical  manifestations of multi-infarct  dementia, including the strong  association between  the number of  infarcts and the prevalence of dementia after stroke,  and the 'mul(cid:173) tiplicative' interaction that has been  postulated  to occur  between  Alzheimer's disease  and multi-infarct dementia.",
        "bibtex": "@inproceedings{NIPS1994_b55ec28c,\n author = {Ruppin, Eytan and Reggia, James},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Patterns of damage in neural networks: The effects of lesion area, shape and number},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/b55ec28c52d5f6205684a473a2193564-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/b55ec28c52d5f6205684a473a2193564-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/b55ec28c52d5f6205684a473a2193564-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1540397,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:3ZurKPp1ploJ:scholar.google.com/&scioq=Patterns+of+damage+in+neural+networks:+The+effects+of+lesion+area,+shape+and+number&hl=en&as_sdt=0,33",
        "gs_version_total": 4,
        "aff": "Department of Computer Science, University of Maryland; Department of Computer Science, University of Maryland + Department of Neurology, University of Maryland + Institute of Advanced Computer Studies, University of Maryland",
        "aff_domain": "cs.umd.edu;cs.umd.edu",
        "email": "cs.umd.edu;cs.umd.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1+2",
        "aff_unique_norm": "University of Maryland;Department of Neurology, University of Maryland;Institute of Advanced Computer Studies, University of Maryland",
        "aff_unique_dep": "Department of Computer Science;;",
        "aff_unique_url": "https://www/umd.edu;;",
        "aff_unique_abbr": "UMD;;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "eddad4eb98",
        "title": "Phase-Space Learning",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/2f885d0fbe2e131bfc9d98363e55d1d4-Abstract.html",
        "author": "Fu-Sheng Tsung; Garrison W. Cottrell",
        "abstract": "Existing recurrent  net learning algorithms are inadequate.  We  in(cid:173) troduce the conceptual framework of viewing recurrent  training as  matching vector fields of dynamical systems in phase space.  Phase(cid:173) space  reconstruction  techniques  make  the  hidden  states  explicit,  reducing  temporal  learning  to  a  feed-forward  problem.  In  short,  we  propose  viewing  iterated  prediction  [LF88]  as  the  best  way  of  training  recurrent  networks  on  deterministic  signals.  Using  this  framework,  we  can  train  multiple trajectories,  insure  their  stabil(cid:173) ity,  and  design  arbitrary dynamical systems.",
        "bibtex": "@inproceedings{NIPS1994_2f885d0f,\n author = {Tsung, Fu-Sheng and Cottrell, Garrison},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Phase-Space Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/2f885d0fbe2e131bfc9d98363e55d1d4-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/2f885d0fbe2e131bfc9d98363e55d1d4-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/2f885d0fbe2e131bfc9d98363e55d1d4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1669307,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12204592375646669265&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 10,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "4196ba8e3c",
        "title": "Plasticity-Mediated Competitive Learning",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/aa68c75c4a77c87f97fb686b2f068676-Abstract.html",
        "author": "Nicol N. Schraudolph; Terrence J. Sejnowski",
        "abstract": "Differentiation between the nodes of a competitive learning net(cid:173) work is conventionally achieved through competition on the ba(cid:173) sis of neural activity.  Simple inhibitory mechanisms are  limited  to  sparse  representations,  while  decorrelation  and  factorization  schemes that support distributed representations are computation(cid:173) ally unattractive.  By letting neural plasticity mediate the compet(cid:173) itive interaction instead, we obtain diffuse, nonadaptive alterna(cid:173) tives for fully distributed representations.  We  use this technique  to  Simplify and improve our binary information gain optimiza(cid:173) tion algorithm for feature extraction (Schraudolph and Sejnowski,  1993); the same approach could be used to improve other learning  algorithms.",
        "bibtex": "@inproceedings{NIPS1994_aa68c75c,\n author = {Schraudolph, Nicol and Sejnowski, Terrence J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Plasticity-Mediated Competitive Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/aa68c75c4a77c87f97fb686b2f068676-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/aa68c75c4a77c87f97fb686b2f068676-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/aa68c75c4a77c87f97fb686b2f068676-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1105711,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1442546921829913572&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "Computational Neurobiology Laboratory, The Salk Institute for Biological Studies, San Diego, CA 92186-5800 + Computer Science & Engineering Department, University of California, San Diego, La Jolla, CA 92093-0114; Computational Neurobiology Laboratory, The Salk Institute for Biological Studies, San Diego, CA 92186-5800 + Computer Science & Engineering Department, University of California, San Diego, La Jolla, CA 92093-0114",
        "aff_domain": "salk.edu;salk.edu",
        "email": "salk.edu;salk.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "Computational Neurobiology Laboratory, The Salk Institute for Biological Studies, San Diego, CA 92186-5800;Computer Science & Engineering Department, University of California, San Diego, La Jolla, CA 92093-0114",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": ";",
        "aff_country_unique": ""
    },
    {
        "id": "f4d955aa8c",
        "title": "Predicting the Risk of Complications in Coronary Artery Bypass Operations using Neural Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/168908dd3227b8358eababa07fcaf091-Abstract.html",
        "author": "Richard P Lippmann; Linda Kukolich; David Shahian",
        "abstract": "Experiments  demonstrated  that  sigmoid  multilayer  perceptron  (MLP)  networks  provide  slightly  better  risk  prediction  than  conventional  logistic  regression  when  used  to  predict  the  risk  of death,  stroke,  and  renal  failure  on  1257  patients  who  underwent  coronary  artery  bypass  operations at the Lahey Clinic. MLP networks with no hidden layer and  networks  with one hidden  layer were trained  using  stochastic  gradient  descent with early stopping. MLP networks and logistic regression used  the  same  input  features  and  were  evaluated  using  bootstrap  sampling  with  50  replications.  ROC  areas  for  predicting  mortality  using  preoperative  input  features  were  70.5%  for  logistic  regression  and  76.0%  for  MLP  networks.  Regularization  provided  by  early  stopping  was  an  important  component  of improved  perfonnance.  A  simplified  approach  to  generating  confidence  intervals  for  MLP risk  predictions  using  an  auxiliary  \"confidence  MLP\"  was  developed.  The  confidence  MLP  is  trained  to  reproduce  confidence  intervals  that  were  generated  during  training  using  the  outputs  of  50  MLP  networks  trained  with  different bootstrap samples.",
        "bibtex": "@inproceedings{NIPS1994_168908dd,\n author = {Lippmann, Richard P and Kukolich, Linda and Shahian, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Predicting the Risk of Complications in Coronary Artery Bypass Operations using Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/168908dd3227b8358eababa07fcaf091-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/168908dd3227b8358eababa07fcaf091-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/168908dd3227b8358eababa07fcaf091-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1692171,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12478575371201586537&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "3933ef74f4",
        "title": "Predictive Coding with Neural Nets: Application to Text Compression",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/5705e1164a8394aace6018e27d20d237-Abstract.html",
        "author": "J\u00fcrgen Schmidhuber; Stefan Heil",
        "abstract": "To compress text files,  a neural predictor network  P  is used to ap(cid:173) proximate the conditional probability distribution of possible  \"next  characters\",  given  n  previous  characters.  P's outputs are fed  into  standard coding algorithms that generate short codes for characters  with  high  predicted  probability and  long  codes  for  highly  unpre(cid:173) dictable  characters.  Tested  on  short  German  newspaper  articles,  our method outperforms widely used  Lempel-Ziv algorithms (used  in  UNIX  functions such  as  \"compress\"  and  \"gzip\").",
        "bibtex": "@inproceedings{NIPS1994_5705e116,\n author = {Schmidhuber, J\\\"{u}rgen and Heil, Stefan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Predictive Coding with Neural Nets: Application to Text Compression},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/5705e1164a8394aace6018e27d20d237-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/5705e1164a8394aace6018e27d20d237-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/5705e1164a8394aace6018e27d20d237-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1436389,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7652037535433467385&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Fakultat fiir Informatik, Technische Universitat Miinchen; ",
        "aff_domain": "; ",
        "email": "; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Fakultat fiir Informatik, Technische Universitat Miinchen",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "ed116edd24",
        "title": "Pulsestream Synapses with Non-Volatile Analogue Amorphous-Silicon Memories",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/9246444d94f081e3549803b928260f56-Abstract.html",
        "author": "A. J. Holmes; Alan F. Murray; Stephen Churcher; J. Hajto; M. J. Rose",
        "abstract": "This  paper  presents  results  from  the  first  use  of neural  networks  for  the real-time feedback  control  of high  temperature plasmas in  a  tokamak fusion  experiment.  The tokamak is  currently  the  prin(cid:173) cipal  experimental  device  for  research  into  the  magnetic  confine(cid:173) ment  approach  to  controlled  fusion.  In  the  tokamak,  hydrogen  plasmas,  at  temperatures  of  up  to  100  Million  K,  are  confined  by  strong  magnetic  fields.  Accurate  control  of the  position  and  shape  of the plasma boundary  requires  real-time feedback  control  of the magnetic field  structure on  a  time-scale of a  few  tens of mi(cid:173) croseconds.  Software simulations have demonstrated that a  neural  network  approach  can  give  significantly  better  performance  than  the linear technique currently  used  on  most tokamak experiments.  The practical  application of the neural  network  approach  requires  high-speed  hardware,  for  which  a  fully  parallel  implementation of  the  multilayer perceptron,  using  a  hybrid  of digital and  analogue  technology,  has been  developed.",
        "bibtex": "@inproceedings{NIPS1994_9246444d,\n author = {Holmes, A. and Murray, Alan and Churcher, Stephen and Hajto, J. and Rose, M.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Pulsestream Synapses with Non-Volatile Analogue Amorphous-Silicon Memories},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/9246444d94f081e3549803b928260f56-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/9246444d94f081e3549803b928260f56-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/9246444d94f081e3549803b928260f56-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2693817,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11671564541130076835&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "83063b89a7",
        "title": "Real-Time Control of a Tokamak Plasma Using Neural Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/2387337ba1e0b0249ba90f55b2ba2521-Abstract.html",
        "author": "Chris M. Bishop; Paul S. Haynes; Mike E U Smith; Tom N. Todd; David L. Trotman; Colin G. Windsor",
        "abstract": "This  paper  presents  results  from  the  first  use  of neural  networks  for  the real-time feedback  control  of high  temperature plasmas in  a  tokamak fusion  experiment.  The tokamak is  currently  the  prin(cid:173) cipal  experimental  device  for  research  into  the  magnetic  confine(cid:173) ment  approach  to  controlled  fusion.  In  the  tokamak,  hydrogen  plasmas,  at  temperatures  of  up  to  100  Million  K,  are  confined  by  strong  magnetic  fields.  Accurate  control  of the  position  and  shape  of the plasma boundary  requires  real-time feedback  control  of the magnetic field  structure on  a  time-scale of a  few  tens of mi(cid:173) croseconds.  Software simulations have demonstrated that a  neural  network  approach  can  give  significantly  better  performance  than  the linear technique currently  used  on  most tokamak experiments.  The practical  application of the neural  network  approach  requires  high-speed  hardware,  for  which  a  fully  parallel  implementation of  the  multilayer perceptron,  using  a  hybrid  of digital and  analogue  technology,  has been  developed.",
        "bibtex": "@inproceedings{NIPS1994_2387337b,\n author = {Bishop, Chris M. and Haynes, Paul S. and Smith, Mike E U and Todd, Tom N. and Trotman, David L. and Windsor, Colin G.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Real-Time Control of a Tokamak Plasma Using Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/2387337ba1e0b0249ba90f55b2ba2521-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/2387337ba1e0b0249ba90f55b2ba2521-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/2387337ba1e0b0249ba90f55b2ba2521-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1498904,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4240909116395585691&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Neural Computing Research Group, Department of Computer Science, Aston University, Birmingham, B4 7ET, U.K.; AEA Technology, Culham Laboratory, Oxfordshire OX14 3DB (Euratom/UKAEA Fusion Association); AEA Technology, Culham Laboratory, Oxfordshire OX14 3DB (Euratom/UKAEA Fusion Association); AEA Technology, Culham Laboratory, Oxfordshire OX14 3DB (Euratom/UKAEA Fusion Association); AEA Technology, Culham Laboratory, Oxfordshire OX14 3DB (Euratom/UKAEA Fusion Association); AEA Technology, Culham Laboratory, Oxfordshire OX14 3DB (Euratom/UKAEA Fusion Association)",
        "aff_domain": "aston.ac.uk; ; ; ; ; ",
        "email": "aston.ac.uk; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1;1;1",
        "aff_unique_norm": "Neural Computing Research Group, Department of Computer Science, Aston University, Birmingham, B4 7ET, U.K.;AEA Technology, Culham Laboratory, Oxfordshire OX14 3DB (Euratom/UKAEA Fusion Association)",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "79b8c6ab49",
        "title": "Recognizing Handwritten Digits Using Mixtures of Linear Models",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/5c936263f3428a40227908d5a3847c0b-Abstract.html",
        "author": "Geoffrey E. Hinton; Michael Revow; Peter Dayan",
        "abstract": "We construct a mixture of locally linear generative models of a col(cid:173) lection of pixel-based images of digits, and use them for  recogni(cid:173) tion. Different models of a given digit are used to capture different  styles of writing, and new images are classified by evaluating their  log-likelihoods under each model. We use an EM-based algorithm  in which the M-step is computationally straightforward principal  components analysis (PCA).  Incorporating tangent-plane informa(cid:173) tion [12]  about expected local deformations only requires adding  tangent vectors into the sample covariance matrices for  the PCA,  and it demonstrably improves performance.",
        "bibtex": "@inproceedings{NIPS1994_5c936263,\n author = {Hinton, Geoffrey E and Revow, Michael and Dayan, Peter},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Recognizing Handwritten Digits Using Mixtures of Linear Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/5c936263f3428a40227908d5a3847c0b-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/5c936263f3428a40227908d5a3847c0b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/5c936263f3428a40227908d5a3847c0b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1856213,
        "gs_citation": 159,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11359175056511972184&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "44c16601d5",
        "title": "Recurrent Networks: Second Order Properties and Pruning",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/df6d2338b2b8fce1ec2f6dda0a630eb0-Abstract.html",
        "author": "Morten With Pedersen; Lars Kai Hansen",
        "abstract": "Second  order  properties  of cost  functions  for  recurrent  networks  are investigated.  We  analyze a layered fully recurrent  architecture,  the  virtue  of this  architecture  is  that  it features  the  conventional  feedforward architecture as a special case.  A detailed description of  recursive  computation of the full  Hessian of the network cost func(cid:173) tion is  provided.  We  discuss  the possibility of invoking simplifying  approximations of the Hessian and show how weight decays iron the  cost function and thereby greatly assist training. We present tenta(cid:173) tive  pruning results,  using  Hassibi  et  al.'s  Optimal Brain  Surgeon,  demonstrating  that  recurrent  networks  can  construct  an  efficient  internal memory.",
        "bibtex": "@inproceedings{NIPS1994_df6d2338,\n author = {Pedersen, Morten and Hansen, Lars},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Recurrent Networks: Second Order Properties and Pruning},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/df6d2338b2b8fce1ec2f6dda0a630eb0-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/df6d2338b2b8fce1ec2f6dda0a630eb0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/df6d2338b2b8fce1ec2f6dda0a630eb0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1449652,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8834227265788018600&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "CONNECT, Electronics Institute, Technical University of Denmark B349, DK-2800 Lyngby, DENMARK; CONNECT, Electronics Institute, Technical University of Denmark B349, DK-2800 Lyngby, DENMARK",
        "aff_domain": "ei.dtu.dk;ei.dtu.dk",
        "email": "ei.dtu.dk;ei.dtu.dk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "CONNECT, Electronics Institute, Technical University of Denmark B349, DK-2800 Lyngby, DENMARK",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "353f8a3d10",
        "title": "Reinforcement Learning Algorithm for Partially Observable Markov Decision Problems",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/1c1d4df596d01da60385f0bb17a4a9e0-Abstract.html",
        "author": "Tommi Jaakkola; Satinder P. Singh; Michael I. Jordan",
        "abstract": "Increasing  attention has  been  paid to reinforcement  learning  algo(cid:173) rithms  in  recent  years,  partly  due  to  successes  in  the  theoretical  analysis  of their  behavior in  Markov  environments.  If the  Markov  assumption  is  removed,  however,  neither  generally  the  algorithms  nor  the  analyses  continue  to  be  usable.  We  propose  and  analyze  a  new  learning  algorithm  to  solve  a  certain  class  of  non-Markov  decision  problems.  Our  algorithm  applies  to  problems  in  which  the  environment  is  Markov,  but  the  learner  has  restricted  access  to  state  information.  The  algorithm involves  a  Monte-Carlo  pol(cid:173) icy evaluation combined with a  policy improvement method that is  similar to  that  of Markov  decision  problems  and  is  guaranteed  to  converge to a local maximum.  The algorithm operates in the space  of stochastic  policies,  a  space  which  can  yield  a  policy  that  per(cid:173) forms  considerably  better  than any deterministic policy.  Although  the  space  of stochastic  policies  is  continuous-even  for  a  discrete  action space-our algorithm is computationally tractable.",
        "bibtex": "@inproceedings{NIPS1994_1c1d4df5,\n author = {Jaakkola, Tommi and Singh, Satinder and Jordan, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Reinforcement Learning Algorithm for Partially Observable Markov Decision Problems},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/1c1d4df596d01da60385f0bb17a4a9e0-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/1c1d4df596d01da60385f0bb17a4a9e0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/1c1d4df596d01da60385f0bb17a4a9e0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1584184,
        "gs_citation": 661,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=385363240349174595&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Department of Brain and Cognitive Sciences, BId. E10 Massachusetts Institute of Technology Cambridge, MA 02139; Department of Brain and Cognitive Sciences, BId. E10 Massachusetts Institute of Technology Cambridge, MA 02139; Department of Brain and Cognitive Sciences, BId. E10 Massachusetts Institute of Technology Cambridge, MA 02139",
        "aff_domain": "psyche.mit.edu;psyche.mit.edu;psyche.mit.edu",
        "email": "psyche.mit.edu;psyche.mit.edu;psyche.mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Department of Brain and Cognitive Sciences, BId. E10 Massachusetts Institute of Technology Cambridge, MA 02139",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "bd6286b8fc",
        "title": "Reinforcement Learning Methods for Continuous-Time Markov Decision Problems",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/07871915a8107172b3b5dc15a6574ad3-Abstract.html",
        "author": "Steven J. Bradtke; Michael O. Duff",
        "abstract": "Semi-Markov  Decision  Problems  are  continuous  time  generaliza(cid:173) tions  of discrete  time  Markov  Decision  Problems.  A  number  of  reinforcement  learning  algorithms  have  been  developed  recently  for  the  solution  of Markov  Decision  Problems,  based on  the ideas  of asynchronous dynamic programming and stochastic approxima(cid:173) tion.  Among these  are TD(,x), Q-Iearning,  and Real-time Dynamic  Programming.  After  reviewing  semi-Markov  Decision  Problems  and Bellman's optimality equation in  that context,  we  propose al(cid:173) gorithms similar  to those  named above,  adapted to the solution  of  semi-Markov Decision  Problems.  We demonstrate these algorithms  by  applying  them to the  problem of determining  the  optimal con(cid:173) trol for  a  simple  queueing  system.  We  conclude  with  a  discussion  of circumstances  under which these  algorithms may be usefully  ap(cid:173) plied.",
        "bibtex": "@inproceedings{NIPS1994_07871915,\n author = {Bradtke, Steven and Duff, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Reinforcement Learning Methods for Continuous-Time Markov Decision Problems},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/07871915a8107172b3b5dc15a6574ad3-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/07871915a8107172b3b5dc15a6574ad3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/07871915a8107172b3b5dc15a6574ad3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1384239,
        "gs_citation": 528,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6197484877413282156&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 10,
        "aff": "Computer Science Department, University of Massachusetts; Computer Science Department, University of Massachusetts",
        "aff_domain": "bradtkeGcs.umass.edu; duffGcs.umass.edu",
        "email": "bradtkeGcs.umass.edu; duffGcs.umass.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Computer Science Department, University of Massachusetts",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "c616cf66c4",
        "title": "Reinforcement Learning Predicts the Site of Plasticity for Auditory Remapping in the Barn Owl",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/d045c59a90d7587d8d671b5f5aec4e7c-Abstract.html",
        "author": "Alexandre Pouget; Cedric Deffayet; Terrence J. Sejnowski",
        "abstract": "The auditory system of the barn owl  contains several spatial maps.  In young barn owls raised with optical prisms over their eyes,  these  auditory  maps are  shifted  to stay  in  register  with the  visual  map,  suggesting  that  the  visual  input  imposes  a  frame  of reference  on  the  auditory  maps.  However,  the  optic  tectum,  the  first  site  of  convergence  of visual  with  auditory information, is  not the  site  of  plasticity for  the  shift  of the  auditory  maps;  the  plasticity occurs  instead  in  the  inferior  colliculus, which  contains  an  auditory  map  and projects into the optic tectum.  We explored a model of the owl  remapping in which  a global reinforcement signal whose  delivery  is  controlled  by  visual foveation.  A hebb  learning rule gated by  rein(cid:173) forcement  learned  to appropriately adjust auditory  maps.  In addi(cid:173) tion,  reinforcement  learning  preferentially  adjusted  the  weights  in  the inferior colliculus,  as in the owl brain, even  though the weights  were  allowed  to  change  throughout  the auditory system.  This ob(cid:173) servation  raises  the  possibility  that  the  site  of learning  does  not  have  to  be  genetically specified,  but  could  be  determined  by  how  the learning procedure  interacts with the network  architecture.",
        "bibtex": "@inproceedings{NIPS1994_d045c59a,\n author = {Pouget, Alexandre and Deffayet, Cedric and Sejnowski, Terrence J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Reinforcement Learning Predicts the Site of Plasticity for Auditory Remapping in the Barn Owl},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/d045c59a90d7587d8d671b5f5aec4e7c-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/d045c59a90d7587d8d671b5f5aec4e7c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/d045c59a90d7587d8d671b5f5aec4e7c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1528134,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14676133547458404983&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "The Salk Institute; The Salk Institute; The Salk Institute + Ecole Normale Superieure",
        "aff_domain": "salk.edu;salk.edu;salk.edu",
        "email": "salk.edu;salk.edu;salk.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0+1",
        "aff_unique_norm": "The Salk Institute;Ecole Normale Superieure",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://www.ens.fr",
        "aff_unique_abbr": ";ENS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";France"
    },
    {
        "id": "2649d5dfca",
        "title": "Reinforcement Learning with Soft State Aggregation",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/287e03db1d99e0ec2edb90d079e142f3-Abstract.html",
        "author": "Satinder P. Singh; Tommi Jaakkola; Michael I. Jordan",
        "abstract": "It is  widely  accepted  that the use  of more compact representations  than lookup tables is crucial to scaling reinforcement learning (RL)  algorithms to real-world problems.  Unfortunately almost all of the  theory  of reinforcement  learning assumes  lookup table representa(cid:173) tions.  In  this  paper  we  address  the  pressing  issue  of  combining  function  approximation and RL,  and present  1)  a function approx(cid:173) imator  based  on  a  simple extension  to  state  aggregation  (a  com(cid:173) monly  used  form  of  compact  representation),  namely  soft  state  aggregation,  2)  a  theory  of convergence for  RL  with arbitrary, but  fixed,  soft  state  aggregation,  3)  a  novel  intuitive  understanding of  the effect  of state aggregation on online RL, and 4)  a new heuristic  adaptive  state aggregation algorithm that finds  improved compact  representations  by  exploiting the  non-discrete  nature  of soft  state  aggregation.  Preliminary empirical results  are  also  presented.",
        "bibtex": "@inproceedings{NIPS1994_287e03db,\n author = {Singh, Satinder and Jaakkola, Tommi and Jordan, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Reinforcement Learning with Soft State Aggregation},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/287e03db1d99e0ec2edb90d079e142f3-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/287e03db1d99e0ec2edb90d079e142f3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/287e03db1d99e0ec2edb90d079e142f3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1568861,
        "gs_citation": 485,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4252365998104730268&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 19,
        "aff": "Dept. of Brain & Cognitive Sciences (E-lO), M.I.T., Cambridge, MA 02139; Dept. of Brain & Cognitive Sciences (E-lO), M.I.T., Cambridge, MA 02139; Dept. of Brain & Cognitive Sciences (E-lO), M.I.T., Cambridge, MA 02139",
        "aff_domain": "psyche.mit.edu;psyche.mit.edu;psyche.mit.edu",
        "email": "psyche.mit.edu;psyche.mit.edu;psyche.mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Dept. of Brain & Cognitive Sciences (E-lO), M.I.T., Cambridge, MA 02139",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "4da6cf46e7",
        "title": "SARDNET: A Self-Organizing Feature Map for Sequences",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/90794e3b050f815354e3e29e977a88ab-Abstract.html",
        "author": "Daniel L. James; Risto Miikkulainen",
        "abstract": "A  self-organizing  neural  network  for  sequence  classification  called  SARDNET is described  and analyzed experimentally.  SARDNET  extends the Kohonen  Feature Map architecture  with activation re(cid:173) tention  and  decay  in  order  to  create  unique  distributed  response  patterns for different sequences.  SARDNET yields extremely dense  yet descriptive representations of sequential input in very few  train(cid:173) ing  iterations.  The  network  has  proven  successful  on  mapping ar(cid:173) bitrary sequences  of binary and real numbers,  as well  as  phonemic  representations  of  English  words.  Potential  applications  include  isolated  spoken  word  recognition  and  cognitive  science  models  of  sequence  processing.",
        "bibtex": "@inproceedings{NIPS1994_90794e3b,\n author = {James, Daniel L. and Miikkulainen, Risto},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {SARDNET: A Self-Organizing Feature Map for Sequences},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/90794e3b050f815354e3e29e977a88ab-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/90794e3b050f815354e3e29e977a88ab-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/90794e3b050f815354e3e29e977a88ab-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1638102,
        "gs_citation": 143,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3857732718662372876&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "cc6ffcda68",
        "title": "SIMPLIFYING NEURAL NETS BY DISCOVERING FLAT MINIMA",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/01882513d5fa7c329e940dda99b12147-Abstract.html",
        "author": "Sepp Hochreiter; J\u00fcrgen Schmidhuber",
        "abstract": "We present a new algorithm for finding low complexity networks  with high generalization capability. The algorithm searches for  large connected regions of so-called ''fiat'' minima of the error func(cid:173) tion. In the weight-space environment of a \"flat\" minimum, the  error remains approximately constant. Using an MDL-based ar(cid:173) gument, flat minima can be shown to correspond to low expected  overfitting. Although our algorithm requires the computation of  second order derivatives, it has backprop's order of complexity.  Experiments with feedforward and recurrent nets are described. In  an application to stock market prediction, the method outperforms  conventional backprop, weight decay, and \"optimal brain surgeon\" .",
        "bibtex": "@inproceedings{NIPS1994_01882513,\n author = {Hochreiter, Sepp and Schmidhuber, J\\\"{u}rgen},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {SIMPLIFYING NEURAL NETS BY DISCOVERING FLAT MINIMA},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/01882513d5fa7c329e940dda99b12147-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/01882513d5fa7c329e940dda99b12147-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/01882513d5fa7c329e940dda99b12147-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1827614,
        "gs_citation": 270,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13476347956997566808&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Fakultat fiir Informatik, H2 Technische Universitat Miinchen; Fakultat fiir Informatik, H2 Technische Universitat Miinchen",
        "aff_domain": "informatik.tu-muenchen.de;informatik.tu-muenchen.de",
        "email": "informatik.tu-muenchen.de;informatik.tu-muenchen.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Fakultat fiir Informatik, H2 Technische Universitat Miinchen",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "b58fd9bc0e",
        "title": "Sample Size Requirements for Feedforward Neural Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/89fcd07f20b6785b92134bd6c1d0fa42-Abstract.html",
        "author": "Michael J. Turmon; Terrence L. Fine",
        "abstract": "We estimate the number of training samples required to ensure that  the performance of a neural network on its training data matches  that obtained when fresh data is applied to the network. Existing  estimates are higher by orders of magnitude than practice indicates.  This work seeks to narrow the gap between theory and practice by  transforming the problem into determining the distribution of the  supremum of a random field in the space of weight vectors, which  in turn is attacked by application of a recent technique called the  Poisson clumping heuristic.",
        "bibtex": "@inproceedings{NIPS1994_89fcd07f,\n author = {Turmon, Michael and Fine, Terrence L.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Sample Size Requirements for Feedforward Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/89fcd07f20b6785b92134bd6c1d0fa42-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/89fcd07f20b6785b92134bd6c1d0fa42-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/89fcd07f20b6785b92134bd6c1d0fa42-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1604578,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10212733815117386105&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Cornell Univ. Electrical Engineering; Cornell Univ. Electrical Engineering",
        "aff_domain": "ee.comell.edu;ee.comell.edu",
        "email": "ee.comell.edu;ee.comell.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Cornell Univ. Electrical Engineering",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "1cbd531332",
        "title": "Single Transistor Learning Synapses",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/934815ad542a4a7c5e8a2dfa04fea9f5-Abstract.html",
        "author": "Paul E. Hasler; Chris Diorio; Bradley A. Minch; Carver Mead",
        "abstract": "We  describe single-transistor silicon synapses that compute, learn,  and  provide  non-volatile  memory retention.  The single  transistor  synapses  simultaneously  perform long  term  weight  storage,  com(cid:173) pute the product of the input and the weight value, and update the  weight value according to a Hebbian or a backpropagation learning  rule.  Memory  is  accomplished  via  charge  storage  on  polysilicon  floating  gates, providing long-term  retention without refresh.  The  synapses efficiently use the physics of silicon to perform weight  up(cid:173) dates; the weight value is  increased using tunneling and the weight  value  decreases  using  hot  electron  injection.  The  small  size  and  low  power operation of single transistor synapses allows  the devel(cid:173) opment  of  dense  synaptic  arrays.  We  describe  the  design,  fabri(cid:173) cation,  characterization,  and  modeling  of an  array  of single  tran(cid:173) sistor  synapses.  When  the steady state source  current  is  used  as  the representation of the weight  value,  both the incrementing and  decrementing functions  are  proportional to a  power of the source  current.  The synaptic  array  was  fabricated  in  the standard  21'm  double - poly,  analog process available from  MOSIS.",
        "bibtex": "@inproceedings{NIPS1994_934815ad,\n author = {Hasler, Paul and Diorio, Chris and Minch, Bradley and Mead, Carver},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Single Transistor Learning Synapses},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/934815ad542a4a7c5e8a2dfa04fea9f5-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/934815ad542a4a7c5e8a2dfa04fea9f5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/934815ad542a4a7c5e8a2dfa04fea9f5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1571749,
        "gs_citation": 141,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5156346147622068932&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "California Institute of Technology; California Institute of Technology; California Institute of Technology; California Institute of Technology",
        "aff_domain": "hobiecat.pcmp.caltech.edu; ; ; ",
        "email": "hobiecat.pcmp.caltech.edu; ; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "California Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.caltech.edu",
        "aff_unique_abbr": "Caltech",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Pasadena",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d62ef1f5da",
        "title": "Spatial Representations in the Parietal Cortex May Use Basis Functions",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/170c944978496731ba71f34c25826a34-Abstract.html",
        "author": "Alexandre Pouget; Terrence J. Sejnowski",
        "abstract": "The  parietal  cortex  is  thought  to  represent  the  egocentric  posi(cid:173) tions  of objects  in  particular  coordinate systems.  We  propose  an  alternative  approach  to  spatial  perception  of objects  in  the  pari(cid:173) etal  cortex from  the  perspective  of sensorimotor  transformations.  The responses of single parietal neurons can be modeled as  a gaus(cid:173) sian  function  of retinal  position  multiplied by  a  sigmoid function  of eye  position,  which  form  a  set of basis functions.  We  show  here  how  these  basis functions  can  be  used  to generate  receptive  fields  in  either  retinotopic or head-centered  coordinates by simple linear  transformations.  This raises the possibility that the parietal cortex  does  not  attempt to compute the positions of objects  in  a  partic(cid:173) ular  frame  of  reference  but  instead  computes  a  general  purpose  representation  of the retinal  location and eye  position from  which  any  transformation can  be  synthesized  by  direct  projection.  This  representation  predicts that hemineglect,  a  neurological syndrome  produced  by  parietal lesions,  should  not  be  confined  to egocentric  coordinates,  but should be observed in multiple frames of reference  in  single patients,  a  prediction supported  by several experiments.",
        "bibtex": "@inproceedings{NIPS1994_170c9449,\n author = {Pouget, Alexandre and Sejnowski, Terrence J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Spatial Representations in the Parietal Cortex May Use Basis Functions},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/170c944978496731ba71f34c25826a34-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/170c944978496731ba71f34c25826a34-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/170c944978496731ba71f34c25826a34-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1535416,
        "gs_citation": 67,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16593540471450849575&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Howard Hughes Medical Institute+The Salk Institute; Department of Biology, University of California, San Diego",
        "aff_domain": "salk.edu;salk.edu",
        "email": "salk.edu;salk.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2",
        "aff_unique_norm": "Howard Hughes Medical Institute;The Salk Institute;Department of Biology, University of California, San Diego",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.hhmi.org;;",
        "aff_unique_abbr": "HHMI;;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "a11eb7b9ee",
        "title": "Stochastic Dynamics of Three-State Neural Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/692f93be8c7a41525c0baf2076aecfb4-Abstract.html",
        "author": "Toru Ohira; Jack D. Cowan",
        "abstract": "We  present  here  an  analysis  of  the  stochastic  neurodynamics  of  a  neural  network  composed  of  three-state  neurons  described  by  a  master equation.  An  outer-product representation of the mas(cid:173) ter  equation  is  employed.  In  this  representation,  an  extension  of  the  analysis  from  two  to  three-state neurons is  easily  performed.  We  apply  this  formalism  with  approximation  schemes  to  a  sim(cid:173) ple three-state network and compare the results with Monte Carlo  simulations.",
        "bibtex": "@inproceedings{NIPS1994_692f93be,\n author = {Ohira, Toru and Cowan, Jack},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Stochastic Dynamics of Three-State Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/692f93be8c7a41525c0baf2076aecfb4-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/692f93be8c7a41525c0baf2076aecfb4-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/692f93be8c7a41525c0baf2076aecfb4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1151082,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3519205390215578086&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Sony Computer Science Laboratory; Depts. of Mathematics and Neurology, University of Chicago",
        "aff_domain": "csl.sony.co.jp;synapse.uchicago.edu",
        "email": "csl.sony.co.jp;synapse.uchicago.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Sony Computer Science Laboratory;Depts. of Mathematics and Neurology, University of Chicago",
        "aff_unique_dep": "Computer Science;",
        "aff_unique_url": "https://www.sony.net/ssl/;",
        "aff_unique_abbr": "Sony CSL;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "61e02d796d",
        "title": "Synchrony and Desynchrony in Neural Oscillator Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/64223ccf70bbb65a3a4aceac37e21016-Abstract.html",
        "author": "Deliang Wang; David Terman",
        "abstract": "An novel class of locally excitatory, globally inhibitory oscillator  networks is proposed. The model of each oscillator corresponds to a  standard relaxation oscillator with two time scales. The network  exhibits a mechanism of selective gating, whereby an oscillator  jumping up to its active phase rapidly recruits the oscillators stimulated  by the same pattern, while preventing others from jumping up. We  show analytically that with the selective gating mechanism the network  rapidly achieves both synchronization within blocks of oscillators that  are stimulated by connected regions and desynchronization between  different blocks. Computer simulations demonstrate the network's  promising ability for segmenting multiple input patterns in real time.  This model lays a physical foundation for the oscillatory correlation  theory of feature binding, and may provide an effective computational  framework for scene segmentation and figure/ground segregation.",
        "bibtex": "@inproceedings{NIPS1994_64223ccf,\n author = {Wang, Deliang and Terman, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Synchrony and Desynchrony in Neural Oscillator Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/64223ccf70bbb65a3a4aceac37e21016-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/64223ccf70bbb65a3a4aceac37e21016-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/64223ccf70bbb65a3a4aceac37e21016-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1714945,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2125864742510503391&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer and Information Science and Center for Cognitive Science, The Ohio State University; Department of Mathematics, The Ohio State University",
        "aff_domain": "cis.ohio-state.edu;math.ohio-state.edu",
        "email": "cis.ohio-state.edu;math.ohio-state.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Department of Computer and Information Science and Center for Cognitive Science, The Ohio State University;Department of Mathematics, The Ohio State University",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "a2482e8f5a",
        "title": "Template-Based Algorithms for Connectionist Rule Extraction",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/24896ee4c6526356cc127852413ea3b4-Abstract.html",
        "author": "Jay A. Alexander; Michael Mozer",
        "abstract": "Casting  neural  network  weights  in  symbolic  terms  is  crucial  for  interpreting  and explaining  the  behavior of a network.  Additionally,  in  some  domains,  a  symbolic  description  may  lead  to  more  robust  generalization.  We  present  a  principled  approach  to  symbolic  rule  extraction  based  on  the  notion  of  weight  templates,  parameterized  regions of weight space corresponding to specific symbolic expressions.  With  an  appropriate  choice  of representation,  we  show  how  template  parameters  may  be  efficiently  identified  and  instantiated  to  yield  the  optimal match to a unit's actual weights. Depending on the requirements  of  the  application  domain,  our  method  can  accommodate  arbitrary  disjunctions  and  conjunctions  with  O(k)  complexity,  simple  n-of-m  expressions with O( k!) complexity, or a more general class of recursive  n-of-m  expressions  with  O(k!)  complexity,  where  k  is  the  number  of  inputs  to  a  unit.  Our method  of rule  extraction  offers  several  benefits  over alternative approaches in the literature, and simulation results on  a  variety of problems demonstrate its effectiveness.",
        "bibtex": "@inproceedings{NIPS1994_24896ee4,\n author = {Alexander, Jay and Mozer, Michael C},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Template-Based Algorithms for Connectionist Rule Extraction},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/24896ee4c6526356cc127852413ea3b4-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/24896ee4c6526356cc127852413ea3b4-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/24896ee4c6526356cc127852413ea3b4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1606144,
        "gs_citation": 78,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2304601344843772206&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science and Institute for Cognitive Science, University of Colorado; Department of Computer Science and Institute for Cognitive Science, University of Colorado",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Department of Computer Science and Institute for Cognitive Science, University of Colorado",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "757bb0944e",
        "title": "Temporal Dynamics of Generalization in Neural Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/da0d1111d2dc5d489242e60ebcbaf988-Abstract.html",
        "author": "Changfeng Wang; Santosh S. Venkatesh",
        "abstract": "This  paper  presents  a  rigorous  characterization  of how  a  general  nonlinear learning machine generalizes  during  the training process  when  it  is  trained  on  a  random  sample  using  a  gradient  descent  algorithm  based  on  reduction  of  training  error.  It is  shown,  in  particular, that best generalization performance occurs,  in general,  before  the global  minimum of the  training error  is  achieved.  The  different  roles  played  by  the  complexity of the  machine  class  and  the  complexity of the  specific  machine in the class  during learning  are  also precisely  demarcated.",
        "bibtex": "@inproceedings{NIPS1994_da0d1111,\n author = {Wang, Changfeng and Venkatesh, Santosh},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Temporal Dynamics of Generalization in Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/da0d1111d2dc5d489242e60ebcbaf988-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/da0d1111d2dc5d489242e60ebcbaf988-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/da0d1111d2dc5d489242e60ebcbaf988-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1594623,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17312843185538102120&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Systems Engineering, University Of Pennsylvania; Department of Electrical Engineering, University Of Pennsylvania",
        "aff_domain": "ender.ee.upenn.edu;ee.upenn.edu",
        "email": "ender.ee.upenn.edu;ee.upenn.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Department of Systems Engineering, University Of Pennsylvania;Department of Electrical Engineering, University Of Pennsylvania",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "75e7d938d3",
        "title": "The Electrotonic Transformation: a Tool for Relating Neuronal Form to Function",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/4b6538a44a1dfdc2b83477cd76dee98e-Abstract.html",
        "author": "Nicholas T Carnevale; Kenneth Y. Tsai; Brenda J. Claiborne; Thomas H. Brown",
        "abstract": "The spatial distribution and time course of electrical signals in neurons  have important theoretical and practical consequences. Because it is  difficult to infer how neuronal form affects electrical signaling, we  have developed a quantitative yet intuitive approach to the analysis of  electrotonus. This approach transforms the architecture of the cell  from anatomical to electrotonic space, using the logarithm of voltage  attenuation as the distance metric. We describe the theory behind this  approach and illustrate its use.",
        "bibtex": "@inproceedings{NIPS1994_4b6538a4,\n author = {Carnevale, Nicholas T and Tsai, Kenneth Y. and Claiborne, Brenda and Brown, Thomas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {The Electrotonic Transformation: a Tool for Relating Neuronal Form to Function},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/4b6538a44a1dfdc2b83477cd76dee98e-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/4b6538a44a1dfdc2b83477cd76dee98e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/4b6538a44a1dfdc2b83477cd76dee98e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1596423,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1331066795094729447&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "f3c5ca19c1",
        "title": "The Ni1000: High Speed Parallel VLSI for Implementing Multilayer Perceptrons",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/430c3626b879b4005d41b8a46172e0c0-Abstract.html",
        "author": "Michael P. Perrone; Leon N. Cooper",
        "abstract": "In  this paper  we  present  a  new  version  of the standard  multilayer  perceptron  (MLP) algorithm for  the state-of-the-art in neural net(cid:173) work VLSI implementations:  the Intel Ni1000.  This new version of  the  MLP  uses  a fundamental property of high dimensional spaces  which  allows  the  12-norm  to  be  accurately  approximated  by  the  It -norm.  This  approach  enables  the  standard  MLP  to  utilize  the  parallel architecture of the Ni1000 to achieve on the order of 40000,  256-dimensional classifications per second.",
        "bibtex": "@inproceedings{NIPS1994_430c3626,\n author = {Perrone, Michael and Cooper, Leon},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {The Ni1000: High Speed Parallel VLSI for Implementing Multilayer Perceptrons},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/430c3626b879b4005d41b8a46172e0c0-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/430c3626b879b4005d41b8a46172e0c0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/430c3626b879b4005d41b8a46172e0c0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1499413,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14656667582565325544&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Thomas J. Watson Research Center; Thomas J. Watson Research Center + Institute for Brain and Neural Systems",
        "aff_domain": "mppGwatson.ibm.com; IncGcns.brown.edu",
        "email": "mppGwatson.ibm.com; IncGcns.brown.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "Thomas J. Watson Research Center;Institute for Brain and Neural Systems",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "68a54fdbac",
        "title": "The Use of Dynamic Writing Information in a Connectionist On-Line Cursive Handwriting Recognition System",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/8b6dd7db9af49e67306feb59a8bdc52c-Abstract.html",
        "author": "Stefan Manke; Michael Finke; Alex Waibel",
        "abstract": "In this paper we present NPen ++, a connectionist system for  writer independent, large vocabulary on-line cursive handwriting  recognition. This system combines a robust input representation,  which preserves the dynamic writing information, with a neural  network architecture, a so called Multi-State Time Delay Neural  Network (MS-TDNN), which integrates rec.ognition and segmen(cid:173) tation in a single framework. Our preprocessing transforms the  original coordinate sequence into a (still temporal) sequence offea(cid:173) ture vectors, which combine strictly local features, like curvature  or writing direction, with a bitmap-like representation of the co(cid:173) ordinate's proximity. The MS-TDNN architecture is well suited  for handling temporal sequences as provided by this input rep(cid:173) resentation. Our system is tested both on writer dependent and  writer independent tasks with vocabulary sizes ranging from 400  up to 20,000 words. For example, on a 20,000 word vocabulary we  achieve word recognition rates up to 88.9% (writer dependent) and  84.1 % (writer independent) without using any language models.",
        "bibtex": "@inproceedings{NIPS1994_8b6dd7db,\n author = {Manke, Stefan and Finke, Michael and Waibel, Alex},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {The Use of Dynamic Writing Information in a Connectionist On-Line Cursive Handwriting Recognition System},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/8b6dd7db9af49e67306feb59a8bdc52c-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/8b6dd7db9af49e67306feb59a8bdc52c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/8b6dd7db9af49e67306feb59a8bdc52c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1585135,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6158146261288923265&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "University of Karlsruhe, Computer Science Department, D-76128 Karlsruhe, Germany; University of Karlsruhe, Computer Science Department, D-76128 Karlsruhe, Germany; Carnegie Mellon University, School of Computer Science, Pittsburgh, PA 15213-3890, U.S.A.",
        "aff_domain": "ira.uka.de;ira.uka.de;cs.cmu.edu",
        "email": "ira.uka.de;ira.uka.de;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of Karlsruhe, Computer Science Department, D-76128 Karlsruhe, Germany;Carnegie Mellon University, School of Computer Science, Pittsburgh, PA 15213-3890, U.S.A.",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "4bf82cb606",
        "title": "Transformation Invariant Autoassociation with Application to Handwritten Character Recognition",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/d707329bece455a462b58ce00d1194c9-Abstract.html",
        "author": "Holger Schwenk; Maurice Milgram",
        "abstract": "When  training  neural  networks  by  the classical  backpropagation algo(cid:173) rithm the whole problem to learn must be expressed by a set of inputs and  desired  outputs.  However,  we often  have  high-level knowledge  about  the  learning problem.  In  optical character recognition  (OCR),  for  in(cid:173) stance, we know that the classification should be invariant under a set of  transformations like rotation or translation.  We propose a new modular  classification system based on several autoassociative multilayer percep(cid:173) trons which allows the efficient incorporation of such knowledge.  Results  are reported on the NIST database of upper case handwritten letters and  compared to other approaches to the invariance problem.",
        "bibtex": "@inproceedings{NIPS1994_d707329b,\n author = {Schwenk, Holger and Milgram, Maurice},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Transformation Invariant Autoassociation with Application to Handwritten Character Recognition},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/d707329bece455a462b58ce00d1194c9-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/d707329bece455a462b58ce00d1194c9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/d707329bece455a462b58ce00d1194c9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1552213,
        "gs_citation": 91,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13784306113601612399&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "PARC; Universite Pierre et Marie Curie",
        "aff_domain": "robo.jussieu.fr; ",
        "email": "robo.jussieu.fr; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "PARC;Universite Pierre et Marie Curie",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "6c96531451",
        "title": "Unsupervised Classification of 3D Objects from 2D Views",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/e205ee2a5de471a70c1fd1b46033a75f-Abstract.html",
        "author": "Satoshi Suzuki; Hiroshi Ando",
        "abstract": "This paper presents an unsupervised learning scheme for categorizing  3D  objects  from  their  2D  projected images.  The  scheme  exploits  an  auto-associative network's ability to encode each view of a single object  into a representation that indicates its view direction.  We propose two  models that employ different classification mechanisms; the first model  selects an auto-associative network whose recovered view best matches  the input view, and the second model is based on a modular architecture  whose  additional  network classifies  the  views  by  splitting  the  input  space  nonlinearly.  We demonstrate  the  effectiveness  of the  proposed  classification models through simulations using 3D wire-frame objects.",
        "bibtex": "@inproceedings{NIPS1994_e205ee2a,\n author = {Suzuki, Satoshi and Ando, Hiroshi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Unsupervised Classification of 3D Objects from 2D Views},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/e205ee2a5de471a70c1fd1b46033a75f-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/e205ee2a5de471a70c1fd1b46033a75f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/e205ee2a5de471a70c1fd1b46033a75f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1595586,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8983629183014935374&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "e4385c1aef",
        "title": "Using Voice Transformations to Create Additional Training Talkers for Word Spotting",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/a3d68b461bd9d3533ee1dd3ce4628ed4-Abstract.html",
        "author": "Eric I. Chang; Richard P Lippmann",
        "abstract": "Speech  recognizers  provide  good  performance  for  most  users  but  the  error rate often  increases dramatically  for a small  percentage of talkers  who are \"different\" from  those talkers used for training.  One expensive  solution to  this problem is  to  gather more training data in an attempt to  sample these outlier users. A second solution, explored in this paper,  is  to artificially enlarge the number of training talkers by transforming the  speech of existing training talkers. This approach is similar to enlarging  the training set for  OCR digit recognition  by  warping the training digit  images,  but  is  more  difficult  because  continuous  speech  has  a  much  larger number of dimensions  (e.g.  linguistic,  phonetic,  style,  temporal,  spectral) that differ across talkers. We explored the use of simple linear  spectral warping to enlarge a 48-talker training data base used for  word  spotting.  The  average  detection  rate  overall  was  increased  by  2.9  percentage  points  (from  68.3%  to  71.2%)  for  male  speakers  and  2.5  percentage  points  (from  64.8%  to  67.3%)  for  female  speakers.  This  increase is  small but similar to  that obtained by  doubling the amount of  training data.",
        "bibtex": "@inproceedings{NIPS1994_a3d68b46,\n author = {Chang, Eric and Lippmann, Richard P},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Using Voice Transformations to Create Additional Training Talkers for Word Spotting},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/a3d68b461bd9d3533ee1dd3ce4628ed4-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/a3d68b461bd9d3533ee1dd3ce4628ed4-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/a3d68b461bd9d3533ee1dd3ce4628ed4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1483571,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11041436413604981657&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "a4ba09fbae",
        "title": "Using a Saliency Map for Active Spatial Selective Attention: Implementation & Initial Results",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/1f50893f80d6830d62765ffad7721742-Abstract.html",
        "author": "Shumeet Baluja; Dean A. Pomerleau",
        "abstract": "In many vision based tasks, the ability to focus attention on the important  portions of a scene is crucial for good performance on the tasks. In this paper  we present a simple method of achieving spatial selective attention through  the use of a saliency map. The saliency map indicates which regions of the  input retina are important for performing the task. The saliency map is cre(cid:173) ated through predictive auto-encoding. The performance of this method is  demonstrated on two simple tasks which have multiple very strong distract(cid:173) ing features in the input retina. Architectural extensions and application  directions for this model are presented.",
        "bibtex": "@inproceedings{NIPS1994_1f50893f,\n author = {Baluja, Shumeet and Pomerleau, Dean A.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Using a Saliency Map for Active Spatial Selective Attention: Implementation \\&amp; Initial Results},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/1f50893f80d6830d62765ffad7721742-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/1f50893f80d6830d62765ffad7721742-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/1f50893f80d6830d62765ffad7721742-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1967375,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17168548052206543329&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "School of Computer Science, Carnegie Mellon University; School of Computer Science, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a8ff520b9b",
        "title": "Using a neural net to instantiate a deformable model",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/fba9d88164f3e2d9109ee770223212a0-Abstract.html",
        "author": "Christopher K. I. Williams; Michael Revow; Geoffrey E. Hinton",
        "abstract": "Deformable models are  an attractive approach to recognizing  non(cid:173) rigid objects which have considerable within class variability.  How(cid:173) ever,  there  are  severe  search  problems  associated  with  fitting  the  models to data.  We show that by using neural networks to provide  better starting points, the search time can be significantly reduced.  The method is  demonstrated on a  character  recognition task.",
        "bibtex": "@inproceedings{NIPS1994_fba9d881,\n author = {Williams, Christopher and Revow, Michael and Hinton, Geoffrey E},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Using a neural net to instantiate a deformable model},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/fba9d88164f3e2d9109ee770223212a0-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/fba9d88164f3e2d9109ee770223212a0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/fba9d88164f3e2d9109ee770223212a0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1761253,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4652939310268806708&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "c788909a9a",
        "title": "Visual Speech Recognition with Stochastic Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1994/hash/7b13b2203029ed80337f27127a9f1d28-Abstract.html",
        "author": "Javier R. Movellan",
        "abstract": "This  paper presents  ongoing  work  on  a  speaker  independent  visual  speech recognition system. The work presented here builds on  previous  research  efforts  in  this  area  and  explores  the  potential  use  of simple  hidden  Markov  models  for  limited  vocabulary,  speaker  independent  visual  speech recognition.  The  task  at  hand  is  recognition  of the  first  four  English  digits,  a  task  with  possible  applications  in  car-phone  images  were  modeled  as  mixtures  of  independent  dialing.  The  Gaussian  distributions,  and  the  temporal  dependencies  were  captured  with standard left-to-right hidden Markov  models.  The  results  indicate  that  simple  hidden  Markov  models  may  be  used  to  successfully  recognize relatively unprocessed image sequences. The system  achieved  performance  levels  equivalent  to  untrained  humans  when  asked  to  recognize the fIrst four English digits.",
        "bibtex": "@inproceedings{NIPS1994_7b13b220,\n author = {Movellan, Javier},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {G. Tesauro and D. Touretzky and T. Leen},\n pages = {},\n publisher = {MIT Press},\n title = {Visual Speech Recognition with Stochastic Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1994/file/7b13b2203029ed80337f27127a9f1d28-Paper.pdf},\n volume = {7},\n year = {1994}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1994/file/7b13b2203029ed80337f27127a9f1d28-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1994/file/7b13b2203029ed80337f27127a9f1d28-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1432531,
        "gs_citation": 244,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16748635847765113579&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Cognitive Science, University of California San Diego",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of California San Diego",
        "aff_unique_dep": "Department of Cognitive Science",
        "aff_unique_url": "https://ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    }
]