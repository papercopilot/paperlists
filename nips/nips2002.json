[
    {
        "id": "69b4e05926",
        "title": "\"Name That Song!\" A Probabilistic Approach to Querying on Music and Text",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/0233f3bb964cf325a30f8b1c2ed2da93-Abstract.html",
        "author": "Brochu Eric; Nando de Freitas",
        "abstract": "We present a novel, \ufb02exible statistical approach for modelling music and text jointly. The approach is based on multi-modal mixture models and maximum a posteriori estimation using EM. The learned models can be used to browse databases with documents containing music and text, to search for music using queries consisting of music and text (lyrics and other contextual information), to annotate text documents with music, and to automatically recommend or identify similar songs.",
        "bibtex": "@inproceedings{NIPS2002_0233f3bb,\n author = {Eric, Brochu and de Freitas, Nando},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {\"Name That Song!\" A Probabilistic Approach to Querying on Music and Text},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/0233f3bb964cf325a30f8b1c2ed2da93-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/0233f3bb964cf325a30f8b1c2ed2da93-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/0233f3bb964cf325a30f8b1c2ed2da93-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 101767,
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10398350246562098480&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Computer Science, University of British Columbia, Vancouver, BC, Canada; Department of Computer Science, University of British Columbia, Vancouver, BC, Canada",
        "aff_domain": "cs.ubc.ca;cs.ubc.ca",
        "email": "cs.ubc.ca;cs.ubc.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of British Columbia",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ubc.ca",
        "aff_unique_abbr": "UBC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Vancouver",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "e4b9383281",
        "title": "A Bilinear Model for Sparse Coding",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/5249ee8e0cff02ad6b4cc0ee0e50b7d1-Abstract.html",
        "author": "David B. Grimes; Rajesh P. N. Rao",
        "abstract": "Recent algorithms for sparse coding and independent component analy- sis (ICA) have demonstrated how localized features can be learned from natural images. However, these approaches do not take image transfor- mations into account. As a result, they produce image codes that are redundant because the same feature is learned at multiple locations. We describe an algorithm for sparse coding based on a bilinear generative model of images. By explicitly modeling the interaction between im- age features and their transformations, the bilinear approach helps reduce redundancy in the image code and provides a basis for transformation- invariant vision. We present results demonstrating bilinear sparse coding of natural images. We also explore an extension of the model that can capture spatial relationships between the independent features of an ob- ject, thereby providing a new framework for parts-based object recogni- tion.",
        "bibtex": "@inproceedings{NIPS2002_5249ee8e,\n author = {Grimes, David and Rao, Rajesh P. N.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {A Bilinear Model for Sparse Coding},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/5249ee8e0cff02ad6b4cc0ee0e50b7d1-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/5249ee8e0cff02ad6b4cc0ee0e50b7d1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/5249ee8e0cff02ad6b4cc0ee0e50b7d1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 137471,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12191015660061767712&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science and Engineering, University of Washington; Department of Computer Science and Engineering, University of Washington",
        "aff_domain": "cs.washington.edu;cs.washington.edu",
        "email": "cs.washington.edu;cs.washington.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Washington",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.washington.edu",
        "aff_unique_abbr": "UW",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Seattle",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f5e0b3c658",
        "title": "A Convergent Form of Approximate Policy Iteration",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/9f44e956e3a2b7b5598c625fcc802c36-Abstract.html",
        "author": "Theodore J. Perkins; Doina Precup",
        "abstract": "We study a new, model-free form of approximate policy iteration which uses Sarsa updates with linear state-action value function approximation for policy evaluation, and a \u201cpolicy improvement operator\u201d to generate a new policy based on the learned state-action values. We prove that if the policy improvement operator produces  -soft policies and is Lipschitz continuous in the action values, with a constant that is not too large, then the approximate policy iteration algorithm converges to a unique solu- tion from any initial policy. To our knowledge, this is the \ufb01rst conver- gence result for any form of approximate policy iteration under similar computational-resource assumptions.",
        "bibtex": "@inproceedings{NIPS2002_9f44e956,\n author = {Perkins, Theodore and Precup, Doina},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {A Convergent Form of Approximate Policy Iteration},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/9f44e956e3a2b7b5598c625fcc802c36-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/9f44e956e3a2b7b5598c625fcc802c36-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/9f44e956e3a2b7b5598c625fcc802c36-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 101516,
        "gs_citation": 148,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11427734304422472411&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science, University of Massachusetts Amherst; School of Computer Science, McGill University",
        "aff_domain": "cs.umass.edu;cs.mcgill.ca",
        "email": "cs.umass.edu;cs.mcgill.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Department of Computer Science, University of Massachusetts Amherst;McGill University",
        "aff_unique_dep": ";School of Computer Science",
        "aff_unique_url": ";https://www.mcgill.ca",
        "aff_unique_abbr": ";McGill",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Montreal",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";Canada"
    },
    {
        "id": "be30819948",
        "title": "A Differential Semantics for Jointree Algorithms",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/01eee509ee2f68dc6014898c309e86bf-Abstract.html",
        "author": "James D. Park; Adnan Darwiche",
        "abstract": "A new approach to inference in belief networks has been recently proposed, which is based on an algebraic representation of belief networks using multi{linear functions. According to this approach, the key computational question is that of representing multi{linear functions compactly, since inference reduces to a simple process of ev aluating and di\ufb01erentiating such functions. W e show here that mainstream inference algorithms based on jointrees are a special case of this approach in a v ery precise sense. W e use this result to prov e new properties of jointree algorithms, and then discuss some of its practical and theoretical implications.",
        "bibtex": "@inproceedings{NIPS2002_01eee509,\n author = {Park, James and Darwiche, Adnan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {A Differential Semantics for Jointree Algorithms},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/01eee509ee2f68dc6014898c309e86bf-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/01eee509ee2f68dc6014898c309e86bf-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/01eee509ee2f68dc6014898c309e86bf-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 347953,
        "gs_citation": 78,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=354762986982477011&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Computer Science Department, University of California, Los Angeles, CA 90095; Computer Science Department, University of California, Los Angeles, CA 90095",
        "aff_domain": "cs.ucla.edu;cs.ucla.edu",
        "email": "cs.ucla.edu;cs.ucla.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Computer Science Department, University of California, Los Angeles, CA 90095",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "e972802fd7",
        "title": "A Digital Antennal Lobe for Pattern Equalization: Analysis and Design",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/90aef91f0d9e7c3be322bd7bae41617d-Abstract.html",
        "author": "Alex Holub; Gilles Laurent; Pietro Perona",
        "abstract": "Re-mapping  patterns  in  order  to  equalize  their  distribution  may  greatly  simplify  both the  structure  and  the  training of classifiers.  Here,  the  properties  of one  such  map  obtained  by  running  a  few  steps of discrete-time dynamical system are explored.  The system  is  called  'Digital  Antennal  Lobe'  (DAL)  because  it  is  inspired  by  recent studies of the antennallobe, a structure in the olfactory sys(cid:173) tem  of the  grasshopper.  The  pattern-spreading  properties  of the  DAL  as  well  as  its  average behavior  as  a  function  of its  (few)  de(cid:173) sign  parameters are analyzed by extending previous results of Van  Vreeswijk and Sompolinsky.  Furthermore, a technique for  adapting  the  parameters of the  initial  design  in  order  to  obtain  opportune  noise-rejection behavior is suggested.  Our results are demonstrated  with a  number of simulations.",
        "bibtex": "@inproceedings{NIPS2002_90aef91f,\n author = {Holub, Alex and Laurent, Gilles and Perona, Pietro},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {A Digital Antennal Lobe for Pattern Equalization: Analysis and Design},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/90aef91f0d9e7c3be322bd7bae41617d-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/90aef91f0d9e7c3be322bd7bae41617d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/90aef91f0d9e7c3be322bd7bae41617d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1460132,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9215175787270948302&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Computation and Neural Systems, California Institute of Technology; Computation and Neural Systems, California Institute of Technology; Computation and Neural Systems, California Institute of Technology",
        "aff_domain": "caltech.edu;caltech.edu;caltech.edu",
        "email": "caltech.edu;caltech.edu;caltech.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Computation and Neural Systems, California Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "f0fc56bf85",
        "title": "A Formulation for Minimax Probability Machine Regression",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/95f8d9901ca8878e291552f001f67692-Abstract.html",
        "author": "Thomas Strohmann; Gregory Z. Grudic",
        "abstract": "We formulate the regression problem as one of maximizing the mini- mum probability, symbolized by (cid:10), that future predicted outputs of the regression model will be within some (cid:6)\" bound of the true regression function. Our formulation is unique in that we obtain a direct estimate of this lower probability bound (cid:10). The proposed framework, minimax probability machine regression (MPMR), is based on the recently de- scribed minimax probability machine classi\ufb01cation algorithm [Lanckriet et al.] and uses Mercer Kernels to obtain nonlinear regression models. MPMR is tested on both toy and real world data, verifying the accuracy of the (cid:10) bound, and the ef\ufb01cacy of the regression models.",
        "bibtex": "@inproceedings{NIPS2002_95f8d990,\n author = {Strohmann, Thomas and Grudic, Gregory},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {A Formulation for Minimax Probability Machine Regression},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/95f8d9901ca8878e291552f001f67692-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/95f8d9901ca8878e291552f001f67692-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/95f8d9901ca8878e291552f001f67692-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 100037,
        "gs_citation": 118,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5963453691311366301&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, University of Colorado, Boulder; Department of Computer Science, University of Colorado, Boulder",
        "aff_domain": "cs.colorado.edu;cs.colorado.edu",
        "email": "cs.colorado.edu;cs.colorado.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Colorado",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.colorado.edu",
        "aff_unique_abbr": "CU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Boulder",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "98ffed2b0f",
        "title": "A Hierarchical Bayesian Markovian Model for Motifs in Biopolymer Sequences",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/d6bcb486f72ae7b5dc68b5b7df7ec887-Abstract.html",
        "author": "Eric P. Xing; Michael I. Jordan; Richard M. Karp; Stuart Russell",
        "abstract": "We propose a dynamic Bayesian model for motifs in biopolymer se- quences which captures rich biological prior knowledge and positional dependencies in motif structure in a principled way. Our model posits that the position-speci\ufb01c multinomial parameters for monomer distribu- tion are distributed as a latent Dirichlet-mixture random variable, and the position-speci\ufb01c Dirichlet component is determined by a hidden Markov process. Model parameters can be \ufb01t on training motifs using a vari- ational EM algorithm within an empirical Bayesian framework. Varia- tional inference is also used for detecting hidden motifs. Our model im- proves over previous models that ignore biological priors and positional dependence. It has much higher sensitivity to motifs during detection and a notable ability to distinguish genuine motifs from false recurring patterns.",
        "bibtex": "@inproceedings{NIPS2002_d6bcb486,\n author = {Xing, Eric and Jordan, Michael and Karp, Richard and Russell, Stuart J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {A Hierarchical Bayesian Markovian Model for Motifs in Biopolymer Sequences},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/d6bcb486f72ae7b5dc68b5b7df7ec887-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/d6bcb486f72ae7b5dc68b5b7df7ec887-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/d6bcb486f72ae7b5dc68b5b7df7ec887-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 150493,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16340437424691375865&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 22,
        "aff": "Computer Science Division, University of California, Berkeley; Computer Science Division, University of California, Berkeley; Computer Science Division, University of California, Berkeley; Computer Science Division, University of California, Berkeley",
        "aff_domain": "cs.berkeley.edu;cs.berkeley.edu;cs.berkeley.edu;cs.berkeley.edu",
        "email": "cs.berkeley.edu;cs.berkeley.edu;cs.berkeley.edu;cs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "Computer Science Division",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "fa1e0ff2fd",
        "title": "A Maximum Entropy Approach to Collaborative Filtering in Dynamic, Sparse, High-Dimensional Domains",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/cc7e2b878868cbae992d1fb743995d8f-Abstract.html",
        "author": "Dmitry Y. Pavlov; David M. Pennock",
        "abstract": "We develop a maximum entropy (maxent) approach to generating recom- mendations in the context of a user\u2019s current navigation stream, suitable for environments where data is sparse, high-dimensional, and dynamic\u2014 conditions typical of many recommendation applications. We address sparsity and dimensionality reduction by \ufb01rst clustering items based on user access patterns so as to attempt to minimize the apriori probabil- ity that recommendations will cross cluster boundaries and then recom- mending only within clusters. We address the inherent dynamic nature of the problem by explicitly modeling the data as a time series; we show how this representational expressivity \ufb01ts naturally into a maxent frame- work. We conduct experiments on data from ResearchIndex, a popu- lar online repository of over 470,000 computer science documents. We show that our maxent formulation outperforms several competing algo- rithms in of\ufb02ine tests simulating the recommendation of documents to ResearchIndex users.",
        "bibtex": "@inproceedings{NIPS2002_cc7e2b87,\n author = {Pavlov, Dmitry and Pennock, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {A Maximum Entropy Approach to Collaborative Filtering in Dynamic, Sparse, High-Dimensional Domains},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/cc7e2b878868cbae992d1fb743995d8f-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/cc7e2b878868cbae992d1fb743995d8f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/cc7e2b878868cbae992d1fb743995d8f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 78184,
        "gs_citation": 213,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4698770789948243009&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "NEC Laboratories America; Overture Services, Inc.",
        "aff_domain": "nec-labs.com;overture.com",
        "email": "nec-labs.com;overture.com",
        "github": "",
        "project": "http://www.researchindex.com",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "NEC Laboratories America;Overture Services, Inc.",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.nec-labs.com;",
        "aff_unique_abbr": "NEC Labs America;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "68d45b9f6b",
        "title": "A Minimal Intervention Principle for Coordinated Movement",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/8c5f6ecd29a0eb234459190ca51c16dd-Abstract.html",
        "author": "Emanuel Todorov; Michael I. Jordan",
        "abstract": "Behavioral goals are achieved reliably and repeatedly with movements rarely reproducible in their detail. Here we offer an explanation: we show that not only are variability and goal achievement compatible, but indeed that allowing variability in redundant dimensions is the optimal control strategy in the face of uncertainty. The optimal feedback control laws for typical motor tasks obey a \u201cminimal intervention\u201d principle: deviations from the average trajectory are only corrected when they interfere with the task goals. The resulting behavior exhibits task-constrained variabil- ity, as well as synergetic coupling among actuators\u2014which is another unexplained empirical phenomenon.",
        "bibtex": "@inproceedings{NIPS2002_8c5f6ecd,\n author = {Todorov, Emanuel and Jordan, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {A Minimal Intervention Principle for Coordinated Movement},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/8c5f6ecd29a0eb234459190ca51c16dd-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/8c5f6ecd29a0eb234459190ca51c16dd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/8c5f6ecd29a0eb234459190ca51c16dd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 153913,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "Department of Cognitive Science, University of California, San Diego; Computer Science and Statistics, University of California, Berkeley",
        "aff_domain": "cogsci.ucsd.edu;cs.berkeley.edu",
        "email": "cogsci.ucsd.edu;cs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of California, San Diego;Computer Science and Statistics, University of California, Berkeley",
        "aff_unique_dep": "Department of Cognitive Science;",
        "aff_unique_url": "https://ucsd.edu;",
        "aff_unique_abbr": "UCSD;",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "San Diego;",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "4a07f9f6a0",
        "title": "A Model for Learning Variance Components of Natural Images",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/2596a54cdbb555cfd09cd5d991da0f55-Abstract.html",
        "author": "Yan Karklin; Michael S. Lewicki",
        "abstract": "We present a hierarchical Bayesian model for learning ef\ufb01cient codes of higher-order structure in natural images. The model, a non-linear gen- eralization of independent component analysis, replaces the standard as- sumption of independence for the joint distribution of coef\ufb01cients with a distribution that is adapted to the variance structure of the coef\ufb01cients of an ef\ufb01cient image basis. This offers a novel description of higher- order image structure and provides a way to learn coarse-coded, sparse- distributed representations of abstract image properties such as object location, scale, and texture.",
        "bibtex": "@inproceedings{NIPS2002_2596a54c,\n author = {Karklin, Yan and Lewicki, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {A Model for Learning Variance Components of Natural Images},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/2596a54cdbb555cfd09cd5d991da0f55-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/2596a54cdbb555cfd09cd5d991da0f55-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/2596a54cdbb555cfd09cd5d991da0f55-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 730924,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1321346715770106992&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Computer Science Department & Center for the Neural Basis of Cognition, Carnegie Mellon University; Computer Science Department & Center for the Neural Basis of Cognition, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cnbc.cmu.edu",
        "email": "cs.cmu.edu;cnbc.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Computer Science Department & Center for the Neural Basis of Cognition, Carnegie Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "abb3692652",
        "title": "A Model for Real-Time Computation in Generic Neural Microcircuits",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/6211080fa89981f66b1a0c9d55c61d0f-Abstract.html",
        "author": "Wolfgang Maass; Thomas Natschl\u00e4ger; Henry Markram",
        "abstract": "Henry Markram Brain Mind Institute",
        "bibtex": "@inproceedings{NIPS2002_6211080f,\n author = {Maass, Wolfgang and Natschl\\\"{a}ger, Thomas and Markram, Henry},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {A Model for Real-Time Computation in Generic Neural Microcircuits},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/6211080fa89981f66b1a0c9d55c61d0f-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/6211080fa89981f66b1a0c9d55c61d0f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/6211080fa89981f66b1a0c9d55c61d0f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 131465,
        "gs_citation": 141,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13986033830933807715&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Institute for Theoretical Computer Science, Technische Universitaet Graz, Austria; Institute for Theoretical Computer Science, Technische Universitaet Graz, Austria; Brain Mind Institute, EPFL, Lausanne, Switzerland",
        "aff_domain": "igi.tu-graz.ac.at;igi.tu-graz.ac.at;epfl.ch",
        "email": "igi.tu-graz.ac.at;igi.tu-graz.ac.at;epfl.ch",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Institute for Theoretical Computer Science, Technische Universitaet Graz, Austria;Brain Mind Institute, EPFL, Lausanne, Switzerland",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "f7dfba817d",
        "title": "A Neural Edge-Detection Model for Enhanced Auditory Sensitivity in Modulated Noise",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/be53d253d6bc3258a8160556dda3e9b2-Abstract.html",
        "author": "Alon Fishbach; Bradford J. May",
        "abstract": "Psychophysical data suggest that temporal modulations of stimulus  amplitude envelopes play a prominent role in the perceptual  segregation of concurrent sounds. In particular, the detection of an  unmodulated signal can be significantly improved by adding  amplitude modulation to the spectral envelope of a competing  masking noise. This perceptual phenomenon  is known as  \u201cComodulation Masking Release\u201d (CMR). Despite the obvious  influence of temporal structure on the perception of complex  auditory scenes, the physiological mechanisms that contribute to  CMR and auditory streaming are not well known. A recent  physiological study by Nelken and colleagues has demonstrated an  enhanced cortical representation of auditory signals in modulated  noise. Our study evaluates these CMR-like response patterns from  the perspective of a hypothetical auditory edge-detection neuron. It  is shown that this simple neural model for the detection of  amplitude transients can reproduce not only the physiological data  of Nelken et al., but also, in light of previous results, a variety of  physiological and psychoacoustical phenomena that are related to  the perceptual segregation of concurrent sounds.",
        "bibtex": "@inproceedings{NIPS2002_be53d253,\n author = {Fishbach, Alon and May, Bradford},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {A Neural Edge-Detection Model for Enhanced Auditory Sensitivity in Modulated Noise},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/be53d253d6bc3258a8160556dda3e9b2-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/be53d253d6bc3258a8160556dda3e9b2-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/be53d253d6bc3258a8160556dda3e9b2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 134708,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4269092331411929463&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "c0dcdfbba6",
        "title": "A Note on the Representational Incompatibility of Function Approximation and Factored Dynamics",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/fd69dbe29f156a7ef876a40a94f65599-Abstract.html",
        "author": "Eric Allender; Sanjeev Arora; Michael Kearns; Cristopher Moore; Alexander Russell",
        "abstract": "We establish a new hardness result that shows that the dif\ufb01culty of plan- ning in factored Markov decision processes is representational rather than just computational. More precisely, we give a \ufb01xed family of fac- tored MDPs with linear rewards whose optimal policies and value func- tions simply cannot be represented succinctly in any standard parametric form. Previous hardness results indicated that computing good policies from the MDP parameters was dif\ufb01cult, but left open the possibility of succinct function approximation for any \ufb01xed factored MDP. Our result applies even to policies which yield a polynomially poor approximation to the optimal value, and highlights interesting connectionswith the com- plexity class of Arthur-Merlin games.",
        "bibtex": "@inproceedings{NIPS2002_fd69dbe2,\n author = {Allender, Eric and Arora, Sanjeev and Kearns, Michael and Moore, Cristopher and Russell, Alexander},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {A Note on the Representational Incompatibility of Function Approximation and Factored Dynamics},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/fd69dbe29f156a7ef876a40a94f65599-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/fd69dbe29f156a7ef876a40a94f65599-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/fd69dbe29f156a7ef876a40a94f65599-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 147175,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7011101432923453682&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 22,
        "aff": "Computer Science Department, Rutgers University; Computer Science Department, Princeton University; Department of Computer and Information Science, University of Pennsylvania; Department of Computer Science, University of New Mexico; Department of Computer Science and Engineering, University of Connecticut",
        "aff_domain": "cs.rutgers.edu;cs.princeton.edu;cis.upenn.edu;santafe.edu;cse.uconn.edu",
        "email": "cs.rutgers.edu;cs.princeton.edu;cis.upenn.edu;santafe.edu;cse.uconn.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3;4",
        "aff_unique_norm": "Rutgers University;Princeton University;University of Pennsylvania;University of New Mexico;Department of Computer Science and Engineering, University of Connecticut",
        "aff_unique_dep": "Computer Science Department;Computer Science Department;Department of Computer and Information Science;Department of Computer Science;",
        "aff_unique_url": "https://www.rutgers.edu;https://www.princeton.edu;https://www.upenn.edu;https://www.unm.edu;",
        "aff_unique_abbr": "Rutgers;Princeton;UPenn;UNM;",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Princeton",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "929adf4ece",
        "title": "A Probabilistic Approach to Single Channel Blind Signal Separation",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/8ca8da41fe1ebc8d3ca31dc14f5fc56c-Abstract.html",
        "author": "Gil-jin Jang; Te-Won Lee",
        "abstract": "We present a new technique for achieving source separation when given only a single channel recording. The main idea is based on exploiting the inherent time structure of sound sources by learning a priori sets of basis \ufb01lters in time domain that encode the sources in a statistically ef\ufb01cient manner. We derive a learning algorithm using a maximum likelihood approach given the observed single channel data and sets of basis \ufb01lters. For each time point we infer the source signals and their contribution factors. This inference is possible due to the prior knowledge of the basis \ufb01lters and the associated coef\ufb01cient densities. A \ufb02exible model for density estimation allows accurate modeling of the observation and our experimental results exhibit a high level of separation performance for mixtures of two music signals as well as the separation of two voice signals.",
        "bibtex": "@inproceedings{NIPS2002_8ca8da41,\n author = {Jang, Gil-jin and Lee, Te-Won},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {A Probabilistic Approach to Single Channel Blind Signal Separation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/8ca8da41fe1ebc8d3ca31dc14f5fc56c-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/8ca8da41fe1ebc8d3ca31dc14f5fc56c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/8ca8da41fe1ebc8d3ca31dc14f5fc56c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 444373,
        "gs_citation": 90,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13879134581299132064&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Spoken Language Laboratory, KAIST, Daejon 305-701, South Korea; Institute for Neural Computation, University of California, San Diego, La Jolla, CA 92093, U.S.A.",
        "aff_domain": "bawi.org;inc.ucsd.edu",
        "email": "bawi.org;inc.ucsd.edu",
        "github": "",
        "project": "http://speech.kaist.ac.kr/~jangbal",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Spoken Language Laboratory, KAIST, Daejon 305-701, South Korea;Institute for Neural Computation, University of California, San Diego, La Jolla, CA 92093, U.S.A.",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "6525964175",
        "title": "A Probabilistic Model for Learning Concatenative Morphology",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/ae1eaa32d10b6c886981755d579fb4d8-Abstract.html",
        "author": "Matthew G. Snover; Michael R. Brent",
        "abstract": "This paper describes a system for the unsupervised learning of morpho- logical suf\ufb01xes and stems from word lists. The system is composed of a generative probability model and hill-climbing and directed search algo- rithms. By extracting and examining morphologically rich subsets of an input lexicon, the directed search identi\ufb01es highly productive paradigms. The hill-climbing algorithm then further maximizes the probability of the hypothesis. Quantitative results are shown by measuring the accuracy of the morphological relations identi\ufb01ed. Experiments in English and Pol- ish, as well as comparisons with another recent unsupervised morphol- ogy learning algorithm demonstrate the effectiveness of this technique.",
        "bibtex": "@inproceedings{NIPS2002_ae1eaa32,\n author = {Snover, Matthew and Brent, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {A Probabilistic Model for Learning Concatenative Morphology},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/ae1eaa32d10b6c886981755d579fb4d8-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/ae1eaa32d10b6c886981755d579fb4d8-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/ae1eaa32d10b6c886981755d579fb4d8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 66928,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4625835502645841619&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "Department of Computer Science, Washington University, St Louis, MO, USA, 63130-4809; Department of Computer Science, Washington University, St Louis, MO, USA, 63130-4809",
        "aff_domain": "cs.wustl.edu;cs.wustl.edu",
        "email": "cs.wustl.edu;cs.wustl.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Department of Computer Science, Washington University, St Louis, MO, USA, 63130-4809",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "8d9f56b4e5",
        "title": "A Prototype for Automatic Recognition of Spontaneous Facial Actions",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/fe40fb944ee700392ed51bfe84dd4e3d-Abstract.html",
        "author": "M.S. Bartlett; G.C. Littlewort; T.J. Sejnowski; J.R. Movellan",
        "abstract": "We present ongoing work on a project for automatic recognition of spon- taneous facial actions. Spontaneous facial expressions differ substan- tially from posed expressions, similar to how continuous, spontaneous speech differs from isolated words produced on command. Previous methods for automatic facial expression recognition assumed images were collected in controlled environments in which the subjects delib- erately faced the camera. Since people often nod or turn their heads, automatic recognition of spontaneous facial behavior requires methods for handling out-of-image-plane head rotations. Here we explore an ap- proach based on 3-D warping of images into canonical views. We eval- uated the performance of the approach as a front-end for a spontaneous expression recognition system using support vector machines and hidden Markov models. This system employed general purpose learning mech- anisms that can be applied to recognition of any facial movement. The system was tested for recognition of a set of facial actions de\ufb01ned by the Facial Action Coding System (FACS). We showed that 3D tracking and warping followed by machine learning techniques directly applied to the warped images, is a viable and promising technology for automatic facial expression recognition. One exciting aspect of the approach pre- sented here is that information about movement dynamics emerged out of \ufb01lters which were derived from the statistics of images.",
        "bibtex": "@inproceedings{NIPS2002_fe40fb94,\n author = {Bartlett, M.S. and Littlewort, G.C. and Sejnowski, T.J. and Movellan, J.R.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {A Prototype for Automatic Recognition of Spontaneous Facial Actions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/fe40fb944ee700392ed51bfe84dd4e3d-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/fe40fb944ee700392ed51bfe84dd4e3d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/fe40fb944ee700392ed51bfe84dd4e3d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 131354,
        "gs_citation": 126,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18040308663993893550&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "152a83b3ea",
        "title": "A Statistical Mechanics Approach to Approximate Analytical Bootstrap Averages",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/f12ee9734e1edf70ed02d9829018b3d9-Abstract.html",
        "author": "D\u00f6rthe Malzahn; Manfred Opper",
        "abstract": "We apply the replica method of Statistical Physics combined with a vari- ational method to the approximate analytical computation of bootstrap averages for estimating the generalization error. We demonstrate our ap- proach on regression with Gaussian processes and compare our results with averages obtained by Monte-Carlo sampling.",
        "bibtex": "@inproceedings{NIPS2002_f12ee973,\n author = {Malzahn, D\\\"{o}rthe and Opper, Manfred},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {A Statistical Mechanics Approach to Approximate Analytical Bootstrap Averages},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/f12ee9734e1edf70ed02d9829018b3d9-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/f12ee9734e1edf70ed02d9829018b3d9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/f12ee9734e1edf70ed02d9829018b3d9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 101235,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12591788025339029705&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Informatics and Mathematical Modelling, Technical University of Denmark, R.-Petersens-Plads Building 321, DK-2800 Lyngby, Denmark; Neural Computing Research Group, School of Engineering and Applied Science, Aston University, Birmingham B4 7ET, United Kingdom",
        "aff_domain": "imm.dtu.dk;aston.ac.uk",
        "email": "imm.dtu.dk;aston.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Informatics and Mathematical Modelling, Technical University of Denmark, R.-Petersens-Plads Building 321, DK-2800 Lyngby, Denmark;Neural Computing Research Group, School of Engineering and Applied Science, Aston University, Birmingham B4 7ET, United Kingdom",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "4aa74a1dba",
        "title": "Adaptation and Unsupervised Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/49c0b9d84c2a16fcaf9d25694fda75e1-Abstract.html",
        "author": "Peter Dayan; Maneesh Sahani; Gregoire Deback",
        "abstract": "Adaptation is a ubiquitous neural and psychological phenomenon, with a wealth of instantiations and implications. Although a basic form of plasticity, it has, bar some notable exceptions, attracted computational theory of only one main variety. In this paper, we study adaptation from the perspective of factor analysis, a paradigmatic technique of unsuper- vised learning. We use factor analysis to re-interpret a standard view of adaptation, and apply our new model to some recent data on adaptation in the domain of face discrimination.",
        "bibtex": "@inproceedings{NIPS2002_49c0b9d8,\n author = {Dayan, Peter and Sahani, Maneesh and Deback, Gregoire},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Adaptation and Unsupervised Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/49c0b9d84c2a16fcaf9d25694fda75e1-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/49c0b9d84c2a16fcaf9d25694fda75e1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/49c0b9d84c2a16fcaf9d25694fda75e1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 138125,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9650291184241839658&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Gatsby Computational Neuroscience Unit; Gatsby Computational Neuroscience Unit; Gatsby Computational Neuroscience Unit",
        "aff_domain": "gatsby.ucl.ac.uk;gatsby.ucl.ac.uk;ens-lyon.fr",
        "email": "gatsby.ucl.ac.uk;gatsby.ucl.ac.uk;ens-lyon.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University College London",
        "aff_unique_dep": "Gatsby Computational Neuroscience Unit",
        "aff_unique_url": "https://www.ucl.ac.uk",
        "aff_unique_abbr": "UCL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "eec5e4d713",
        "title": "Adapting Codes and Embeddings for Polychotomies",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/32e05616c8ed659463f9af00b142dd6f-Abstract.html",
        "author": "Gunnar R\u00e4tsch; Sebastian Mika; Alex J. Smola",
        "abstract": "In this paper we consider formulations of multi-class problems based on a generalized notion of a margin and using output coding. This includes, but is not restricted to, standard multi-class SVM formulations. Differ- ently from many previous approaches we learn the code as well as the embedding function. We illustrate how this can lead to a formulation that allows for solving a wider range of problems with for instance many classes or even \u201cmissing classes\u201d. To keep our optimization problems tractable we propose an algorithm capable of solving them using two- class classi\ufb01ers, similar in spirit to Boosting.",
        "bibtex": "@inproceedings{NIPS2002_32e05616,\n author = {R\\\"{a}tsch, Gunnar and Mika, Sebastian and Smola, Alex},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Adapting Codes and Embeddings for Polychotomies},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/32e05616c8ed659463f9af00b142dd6f-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/32e05616c8ed659463f9af00b142dd6f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/32e05616c8ed659463f9af00b142dd6f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 313123,
        "gs_citation": 51,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11778735322991586993&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "RSISE, CSL, Machine Learning Group, The Australian National University, Canberra, 0200 ACT, Australia; RSISE, CSL, Machine Learning Group, The Australian National University, Canberra, 0200 ACT, Australia; Fraunhofer FIRST, Kekulestr. 7, 12489 Berlin, Germany",
        "aff_domain": "anu.edu.au;anu.edu.au;first.fhg.de",
        "email": "anu.edu.au;anu.edu.au;first.fhg.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "RSISE, CSL, Machine Learning Group, The Australian National University, Canberra, 0200 ACT, Australia;Fraunhofer FIRST, Kekulestr. 7, 12489 Berlin, Germany",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "d7db707222",
        "title": "Adaptive Caching by Refetching",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/130f1a8e9e102707f3f91b010f151b0b-Abstract.html",
        "author": "Robert B. Gramacy; Manfred K. Warmuth; Scott A. Brandt; Ismail Ari",
        "abstract": "We are constructing caching policies that have 13-20% lower miss rates than the best of twelve baseline policies over a large variety of request streams. This represents an improvement of 49\u201363% over Least Recently Used, the most commonly implemented policy. We achieve this not by designing a speci\ufb01c new policy but by using on-line Machine Learning algorithms to dynamically shift between the standard policies based on their observed miss rates. A thorough experimental evaluation of our techniques is given, as well as a discussion of what makes caching an interesting on-line learning problem.",
        "bibtex": "@inproceedings{NIPS2002_130f1a8e,\n author = {Gramacy, Robert B and Warmuth, Manfred K. K and Brandt, Scott and Ari, Ismail},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Adaptive Caching by Refetching},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/130f1a8e9e102707f3f91b010f151b0b-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/130f1a8e9e102707f3f91b010f151b0b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/130f1a8e9e102707f3f91b010f151b0b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 863792,
        "gs_citation": 99,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14919021991240526627&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "Department of Computer Science, UCSC; Department of Computer Science, UCSC; Department of Computer Science, UCSC; Department of Computer Science, UCSC",
        "aff_domain": "cs.ucsc.edu;cs.ucsc.edu;cs.ucsc.edu;cs.ucsc.edu",
        "email": "cs.ucsc.edu;cs.ucsc.edu;cs.ucsc.edu;cs.ucsc.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Department of Computer Science, UCSC",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "2816ee3c54",
        "title": "Adaptive Classification by Variational Kalman Filtering",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/efdf562ce2fb0ad460fd8e9d33e57f57-Abstract.html",
        "author": "Peter Sykacek; Stephen J. Roberts",
        "abstract": "We propose in this paper a probabilistic approach for adaptive inference of generalized nonlinear classi\ufb01cation that combines the computational advantage of a parametric solution with the \ufb02exibility of sequential sam- pling techniques. We regard the parameters of the classi\ufb01er as latent states in a \ufb01rst order Markov process and propose an algorithm which can be regarded as variational generalization of standard Kalman \ufb01lter- ing. The variational Kalman \ufb01lter is based on two novel lower bounds that enable us to use a non-degenerate distribution over the adaptation rate. An extensive empirical evaluation demonstrates that the proposed method is capable of infering competitive classi\ufb01ers both in stationary and non-stationary environments. Although we focus on classi\ufb01cation, the algorithm is easily extended to other generalized nonlinear models.",
        "bibtex": "@inproceedings{NIPS2002_efdf562c,\n author = {Sykacek, Peter and Roberts, Stephen J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Adaptive Classification by Variational Kalman Filtering},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/efdf562ce2fb0ad460fd8e9d33e57f57-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/efdf562ce2fb0ad460fd8e9d33e57f57-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/efdf562ce2fb0ad460fd8e9d33e57f57-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 131947,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6926365368135602263&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Department of Engineering Science, University of Oxford; Department of Engineering Science, University of Oxford",
        "aff_domain": "robots.ox.ac.uk;robots.ox.ac.uk",
        "email": "robots.ox.ac.uk;robots.ox.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Oxford",
        "aff_unique_dep": "Department of Engineering Science",
        "aff_unique_url": "https://www.ox.ac.uk",
        "aff_unique_abbr": "Oxford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Oxford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "db8720267c",
        "title": "Adaptive Nonlinear System Identification with Echo State Networks",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/426f990b332ef8193a61cc90516c1245-Abstract.html",
        "author": "Herbert Jaeger",
        "abstract": "Echo state networks  (ESN)  are a  novel approach to recurrent neu(cid:173) ral  network training.  An  ESN  consists  of a  large,  fixed,  recurrent  \"reservoir\"  network, from  which the desired output is  obtained by  training suitable output connection weights.  Determination of op(cid:173) timal  output  weights  becomes  a  linear,  uniquely  solvable  task  of  MSE  minimization.  This  article  reviews  the  basic  ideas  and  de(cid:173) scribes  an  online  adaptation scheme  based  on the  RLS  algorithm  known  from  adaptive  linear  systems.  As  an  example,  a  10-th or(cid:173) der  NARMA  system  is  adaptively identified.  The known  benefits  of the RLS  algorithms carryover from  linear systems to nonlinear  ones;  specifically,  the  convergence rate and  misadjustment  can be  determined at design  time.",
        "bibtex": "@inproceedings{NIPS2002_426f990b,\n author = {Jaeger, Herbert},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Adaptive Nonlinear System Identification with Echo State Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/426f990b332ef8193a61cc90516c1245-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/426f990b332ef8193a61cc90516c1245-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/426f990b332ef8193a61cc90516c1245-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1581642,
        "gs_citation": 900,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3131674030746319826&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "International University Bremen",
        "aff_domain": "iu-bremen.de",
        "email": "iu-bremen.de",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "International University Bremen",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "2d0b16c227",
        "title": "Adaptive Quantization and Density Estimation in Silicon",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/f52854cc99ae1c1966b0a21d0127975b-Abstract.html",
        "author": "David Hsu; Seth Bridges; Miguel Figueroa; Chris Diorio",
        "abstract": "We present the bump mixture model, a statistical model for analog  data where the probabilistic semantics, inference, and learning  rules derive from low-level transistor behavior. The bump mixture  model relies on translinear circuits to perform probabilistic infer- ence, and floating-gate devices to perform adaptation. This system  is low power, asynchronous, and fully parallel, and supports vari- ous on-chip learning algorithms. In addition, the mixture model can  perform several tasks such as probability estimation, vector quanti- zation, classification, and clustering. We tested a fabricated system  on clustering, quantization, and classification of handwritten digits  and show performance comparable to the E-M algorithm on mix- tures of Gaussians.",
        "bibtex": "@inproceedings{NIPS2002_f52854cc,\n author = {Hsu, David and Bridges, Seth and Figueroa, Miguel and Diorio, Chris},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Adaptive Quantization and Density Estimation in Silicon},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/f52854cc99ae1c1966b0a21d0127975b-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/f52854cc99ae1c1966b0a21d0127975b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/f52854cc99ae1c1966b0a21d0127975b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 85667,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3161433774214063202&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "9ab63285d2",
        "title": "Adaptive Scaling for Feature Selection in SVMs",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/ee26fc66b1369c7625333bedafbfcaf6-Abstract.html",
        "author": "Yves Grandvalet; St\u00e9phane Canu",
        "abstract": "This paper introduces an algorithm for the automatic relevance determi- nation of input variables in kernelized Support Vector Machines. Rele- vance is measured by scale factors de\ufb01ning the input space metric, and feature selection is performed by assigning zero weights to irrelevant variables. The metric is automatically tuned by the minimization of the standard SVM empirical risk, where scale factors are added to the usual set of parameters de\ufb01ning the classi\ufb01er. Feature selection is achieved by constraints encouraging the sparsity of scale factors. The resulting algorithm compares favorably to state-of-the-art feature selection proce- dures and demonstrates its effectiveness on a demanding facial expres- sion recognition problem.",
        "bibtex": "@inproceedings{NIPS2002_ee26fc66,\n author = {Grandvalet, Yves and Canu, St\\'{e}phane},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Adaptive Scaling for Feature Selection in SVMs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/ee26fc66b1369c7625333bedafbfcaf6-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/ee26fc66b1369c7625333bedafbfcaf6-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/ee26fc66b1369c7625333bedafbfcaf6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 85823,
        "gs_citation": 259,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6145143550508338821&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Heudiasyc, UMR CNRS 6599, Universit\u00b4e de Technologie de Compi`egne, Compi`egne, France; PSI, INSA de Rouen, St Etienne du Rouvray, France",
        "aff_domain": "utc.fr;insa-rouen.fr",
        "email": "utc.fr;insa-rouen.fr",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Heudiasyc, UMR CNRS 6599, Universit\u00b4e de Technologie de Compi`egne, Compi`egne, France;PSI, INSA de Rouen, St Etienne du Rouvray, France",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "9de4f787f8",
        "title": "An Asynchronous Hidden Markov Model for Audio-Visual Speech Recognition",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/2b45c629e577731c4df84fc34f936a89-Abstract.html",
        "author": "Samy Bengio",
        "abstract": "This paper presents a  novel  Hidden  Markov Model architecture to  model the joint probability of pairs of asynchronous sequences de(cid:173) scribing the same event.  It is based on two other Markovian models,  namely  Asynchronous  Input/ Output  Hidden  Markov  Models  and  Pair Hidden Markov Models.  An EM algorithm to train the model  is  presented,  as  well  as  a  Viterbi  decoder  that can  be  used  to  ob(cid:173) tain  the  optimal  state sequence  as  well  as  the  alignment  between  the two  sequences.  The model has  been tested on an audio-visual  speech  recognition  task  using  the  M2VTS  database  and  yielded  robust performances under various noise  conditions.",
        "bibtex": "@inproceedings{NIPS2002_2b45c629,\n author = {Bengio, Samy},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {An Asynchronous Hidden Markov Model for Audio-Visual Speech Recognition},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/2b45c629e577731c4df84fc34f936a89-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/2b45c629e577731c4df84fc34f936a89-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/2b45c629e577731c4df84fc34f936a89-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1402344,
        "gs_citation": 112,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2491148321731859119&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff": "Dalle Molle Institute for Perceptual Artificial Intelligence (IDIAP)",
        "aff_domain": "idiap.ch",
        "email": "idiap.ch",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Dalle Molle Institute for Perceptual Artificial Intelligence (IDIAP)",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "91253c3779",
        "title": "An Estimation-Theoretic Framework for the Presentation of Multiple Stimuli",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/cd0f74b5955dc87fd0605745c4b49ee8-Abstract.html",
        "author": "Christian W. Eurich",
        "abstract": "A framework is introduced for assessing the encoding accuracy and the discriminational ability of a population of neurons upon simul- taneous presentation of multiple stimuli. Minimal square estima- tion errors are obtained from a Fisher information analysis in an abstract compound space comprising the features of all stimuli. Even for the simplest case of linear superposition of responses and Gaussian tuning, the symmetries in the compound space are very di(cid:11)erent from those in the case of a single stimulus. The analysis allows for a quantitative description of attentional e(cid:11)ects and can be extended to include neural nonlinearities such as nonclassical receptive (cid:12)elds.",
        "bibtex": "@inproceedings{NIPS2002_cd0f74b5,\n author = {Eurich, Christian},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {An Estimation-Theoretic Framework for the Presentation of Multiple Stimuli},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/cd0f74b5955dc87fd0605745c4b49ee8-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/cd0f74b5955dc87fd0605745c4b49ee8-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/cd0f74b5955dc87fd0605745c4b49ee8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 744064,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10358739413514839768&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Institute for Theoretical Neurophysics, University of Bremen",
        "aff_domain": "physik.uni-bremen.de",
        "email": "physik.uni-bremen.de",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Institute for Theoretical Neurophysics, University of Bremen",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "88be98974d",
        "title": "An Impossibility Theorem for Clustering",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/43e4e6a6f341e00671e123714de019a8-Abstract.html",
        "author": "Jon M. Kleinberg",
        "abstract": "Although the study of clustering is centered around an intuitively compelling goal, it has been very di(cid:14)cult to develop a uni(cid:12)ed framework for reasoning about it at a technical level, and pro- foundly diverse approaches to clustering abound in the research community. Here we suggest a formal perspective on the di(cid:14)culty in (cid:12)nding such a uni(cid:12)cation, in the form of an impossibility theo- rem: for a set of three simple properties, we show that there is no clustering function satisfying all three. Relaxations of these prop- erties expose some of the interesting (and unavoidable) trade-o(cid:11)s at work in well-studied clustering techniques such as single-linkage, sum-of-pairs, k-means, and k-median.",
        "bibtex": "@inproceedings{NIPS2002_43e4e6a6,\n author = {Kleinberg, Jon},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {An Impossibility Theorem for Clustering},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/43e4e6a6f341e00671e123714de019a8-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/43e4e6a6f341e00671e123714de019a8-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/43e4e6a6f341e00671e123714de019a8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 93960,
        "gs_citation": 1023,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1489817895906738251&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 31,
        "aff": "Department of Computer Science, Cornell University, Ithaca NY 14853",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Department of Computer Science, Cornell University, Ithaca NY 14853",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "a56663d34a",
        "title": "An Information Theoretic Approach to the Functional Classification of Neurons",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/2a8a812400df8963b2e2ac0ed01b07b8-Abstract.html",
        "author": "Elad Schneidman; William Bialek; Michael Ii",
        "abstract": "A population of neurons typically exhibits a broad diversity of responses to sensory inputs. The intuitive notion of functional classi\ufb01cation is that cells can be clustered so that most of the diversity is captured by the iden- tity of the clusters rather than by individuals within clusters. We show how this intuition can be made precise using information theory, with- out any need to introduce a metric on the space of stimuli or responses. Applied to the retinal ganglion cells of the salamander, this approach re- covers classical results, but also provides clear evidence for subclasses beyond those identi\ufb01ed previously. Further, we \ufb01nd that each of the gan- glion cells is functionally unique, and that even within the same subclass only a few spikes are needed to reliably distinguish between cells.",
        "bibtex": "@inproceedings{NIPS2002_2a8a8124,\n author = {Schneidman, Elad and Bialek, William and Ii, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {An Information Theoretic Approach to the Functional Classification of Neurons},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/2a8a812400df8963b2e2ac0ed01b07b8-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/2a8a812400df8963b2e2ac0ed01b07b8-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/2a8a812400df8963b2e2ac0ed01b07b8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 289955,
        "gs_citation": 50,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12208753257372928339&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 21,
        "aff": "Department of Physics; Department of Physics + Department of Molecular Biology; Department of Molecular Biology",
        "aff_domain": "princeton.edu;princeton.edu;princeton.edu",
        "email": "princeton.edu;princeton.edu;princeton.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1;1",
        "aff_unique_norm": "Institution not specified;Department of Molecular Biology",
        "aff_unique_dep": "Department of Physics;",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "af43530585",
        "title": "Analysis of Information in Speech Based on MANOVA",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/333ac5d90817d69113471fbb6e531bee-Abstract.html",
        "author": "Sachin S. Kajarekar; Hynek Hermansky",
        "abstract": "We  propose  analysis  of information  in  speech  using  three  sources  - language  (phone),  speaker and channeL  Information in speech is  measured as mutual information between the source and the set of  features  extracted  from  speech  signaL  We  assume  that  distribu(cid:173) tion of features  can be modeled  using  Gaussian distribution.  The  mutual  information  is  computed  using  the  results  of  analysis  of  variability in speech.  We  observe similarity in the results of phone  variability and phone information, and show that the results of the  proposed  analysis  have  more  meaningful  interpretations  than  the  analysis of variability.",
        "bibtex": "@inproceedings{NIPS2002_333ac5d9,\n author = {Kajarekar, Sachin and Hermansky, Hynek},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Analysis of Information in Speech Based on MANOVA},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/333ac5d90817d69113471fbb6e531bee-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/333ac5d90817d69113471fbb6e531bee-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/333ac5d90817d69113471fbb6e531bee-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1448011,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18141603728916826404&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Electrical and Computer Engineering OGI School of Science and Engineering at OHSU Beaverton, OR; International Computer Science Institute Berkeley, CA",
        "aff_domain": "asp.ogi.edu;asp.ogi.edu",
        "email": "asp.ogi.edu;asp.ogi.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Department of Electrical and Computer Engineering OGI School of Science and Engineering at OHSU Beaverton, OR;International Computer Science Institute Berkeley, CA",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "40ae031374",
        "title": "Annealing and the Rate Distortion Problem",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/ccbd8ca962b80445df1f7f38c57759f0-Abstract.html",
        "author": "Albert E. Parker; Tom\u00e1\\v S. Gedeon; Alexander G. Dimitrov",
        "abstract": "In this paper we introduce methodology to determine the bifurcation structure of optima for a class of similar cost functions from Rate Distortion Theory, Determin- istic Annealing, Information Distortion and the Information Bottleneck Method. We also introduce a numerical algorithm which uses the explicit form of the bifur- cating branches to \ufb01nd optima at a bifurcation point.",
        "bibtex": "@inproceedings{NIPS2002_ccbd8ca9,\n author = {Parker, Albert and Gedeon, Tom\\'{a}\\textbackslash v and Dimitrov, Alexander},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Annealing and the Rate Distortion Problem},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/ccbd8ca962b80445df1f7f38c57759f0-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/ccbd8ca962b80445df1f7f38c57759f0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/ccbd8ca962b80445df1f7f38c57759f0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 95957,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17274095993119633938&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Mathematical Sciences, Montana State University; Department of Mathematical Sciences, Montana State University; Center for Computational Biology, Montana State University",
        "aff_domain": "math.montana.edu;math.montana.edu;nervana.montana.edu",
        "email": "math.montana.edu;math.montana.edu;nervana.montana.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Department of Mathematical Sciences, Montana State University;Center for Computational Biology, Montana State University",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "f2a8f94edb",
        "title": "Application of Variational Bayesian Approach to Speech Recognition",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/d1a21da7bca4abff8b0b61b87597de73-Abstract.html",
        "author": "Shinji Watanabe; Yasuhiro Minami; Atsushi Nakamura; Naonori Ueda",
        "abstract": "In this paper, we propose a Bayesian framework, which constructs shared-state triphone HMMs based on a variational Bayesian approach, and recognizes speech based on the Bayesian prediction classi(cid:2)cation; variational Bayesian estimation and clustering for speech recognition (VBEC). An appropriate model structure with high recognition perfor- mance can be found within a VBEC framework. Unlike conventional methods, including BIC or MDL criterion based on the maximum likeli- hood approach, the proposed model selection is valid in principle, even when there are insuf(cid:2)cient amounts of data, because it does not use an asymptotic assumption. In isolated word recognition experiments, we show the advantage of VBEC over conventional methods, especially when dealing with small amounts of data.",
        "bibtex": "@inproceedings{NIPS2002_d1a21da7,\n author = {Watanabe, Shinji and Minami, Yasuhiro and Nakamura, Atsushi and Ueda, Naonori},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Application of Variational Bayesian Approach to Speech Recognition},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/d1a21da7bca4abff8b0b61b87597de73-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/d1a21da7bca4abff8b0b61b87597de73-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/d1a21da7bca4abff8b0b61b87597de73-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 283949,
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10386154450535384307&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "0a3d39e317",
        "title": "Approximate Inference and Protein-Folding",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/100d9f30ca54b18d14821dc88fea0631-Abstract.html",
        "author": "Chen Yanover; Yair Weiss",
        "abstract": "Side-chain prediction is an important subtask in the protein-folding  problem.  We  show  that  finding  a  minimal  energy  side-chain  con(cid:173) figuration  is  equivalent  to  performing  inference  in  an  undirected  graphical model.  The graphical model  is  relatively  sparse yet  has  many cycles.  We used this equivalence to assess the performance of  approximate inference  algorithms  in  a  real-world  setting.  Specifi(cid:173) cally we  compared belief propagation (BP), generalized BP (GBP)  and naive mean field  (MF).  In  cases  where  exact  inference  was  possible,  max-product  BP  al(cid:173) ways found the global minimum of the energy  (except  in few  cases  where it failed  to converge), while other approximation algorithms  of similar  complexity  did  not.  In  the  full  protein  data set,  max(cid:173) product  BP  always  found  a  lower  energy  configuration  than  the  other algorithms, including  a  widely  used  protein-folding software  (SCWRL).",
        "bibtex": "@inproceedings{NIPS2002_100d9f30,\n author = {Yanover, Chen and Weiss, Yair},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Approximate Inference and Protein-Folding},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/100d9f30ca54b18d14821dc88fea0631-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/100d9f30ca54b18d14821dc88fea0631-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/100d9f30ca54b18d14821dc88fea0631-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1476398,
        "gs_citation": 143,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15404776311088845972&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "96209a7b34",
        "title": "Approximate Linear Programming for Average-Cost Dynamic Programming",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/b6e32320fa6bc5a588b90183b95dc028-Abstract.html",
        "author": "Benjamin V. Roy; Daniela D. Farias",
        "abstract": "This paper extends our earlier analysis on approximate linear program- ming as an approach to approximating the cost-to-go function in a discounted-cost dynamic program [6]. In this paper, we consider the average-cost criterion and a version of approximate linear programming that generates approximations to the optimal average cost and differential cost function. We demonstrate that a naive version of approximate linear programming prioritizes approximation of the optimal average cost and that this may not be well-aligned with the objective of deriving a policy with low average cost. For that, the algorithm should aim at producing a good approximation of the differential cost function. We propose a two- phase variant of approximate linear programming that allows for external control of the relative accuracy of the approximation of the differential cost function over different portions of the state space via state-relevance weights. Performance bounds suggest that the new algorithm is compat- ible with the objective of optimizing performance and provide guidance on appropriate choices for state-relevance weights.",
        "bibtex": "@inproceedings{NIPS2002_b6e32320,\n author = {Roy, Benjamin and Farias, Daniela},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Approximate Linear Programming for Average-Cost Dynamic Programming},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/b6e32320fa6bc5a588b90183b95dc028-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/b6e32320fa6bc5a588b90183b95dc028-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/b6e32320fa6bc5a588b90183b95dc028-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 96212,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13725123898898388103&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "IBM Almaden Research Center; Department of Management Science and Engineering, Stanford University",
        "aff_domain": "mit.edu;stanford.edu",
        "email": "mit.edu;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "IBM;Stanford University",
        "aff_unique_dep": "Research Center;Department of Management Science and Engineering",
        "aff_unique_url": "https://www.ibm.com/research;https://www.stanford.edu",
        "aff_unique_abbr": "IBM;Stanford",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Almaden;Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "9b17cd7043",
        "title": "Artefactual Structure from Least-Squares Multidimensional Scaling",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/487d4c6a324446b3fa45b30cfcee5337-Abstract.html",
        "author": "Nicholas P. Hughes; David Lowe",
        "abstract": "We consider the problem of illusory or artefactual structure from the vi- sualisation of high-dimensional structureless data. In particular we ex- amine the role of the distance metric in the use of topographic mappings based on the statistical \ufb01eld of multidimensional scaling. We show that the use of a squared Euclidean metric (i.e. the SS TRESS measure) gives rise to an annular structure when the input data is drawn from a high- dimensional isotropic distribution, and we provide a theoretical justi\ufb01ca- tion for this observation.",
        "bibtex": "@inproceedings{NIPS2002_487d4c6a,\n author = {Hughes, Nicholas and Lowe, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Artefactual Structure from Least-Squares Multidimensional Scaling},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/487d4c6a324446b3fa45b30cfcee5337-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/487d4c6a324446b3fa45b30cfcee5337-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/487d4c6a324446b3fa45b30cfcee5337-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 358527,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2874439410087615410&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Engineering Science, University of Oxford, Oxford, 0X1 3PJ, UK; Neural Computing Research Group, Aston University, Birmingham, B4 7ET, UK",
        "aff_domain": "robots.ox.ac.uk;aston.ac.uk",
        "email": "robots.ox.ac.uk;aston.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Department of Engineering Science, University of Oxford, Oxford, 0X1 3PJ, UK;Neural Computing Research Group, Aston University, Birmingham, B4 7ET, UK",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "afaf90e316",
        "title": "Automatic Acquisition and Efficient Representation of Syntactic Structures",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/ce6c92303f38d297e263c7180f03d402-Abstract.html",
        "author": "Zach Solan; Eytan Ruppin; David Horn; Shimon Edelman",
        "abstract": "The distributional principle according to which morphemes that occur in identical contexts belong, in some sense, to the same category [1] has been advanced as a means for extracting syntactic structures from corpus data. We extend this principle by applying it recursively, and by us- ing mutual information for estimating category coherence. The resulting model learns, in an unsupervised fashion, highly structured, distributed representations of syntactic knowledge from corpora. It also exhibits promising behavior in tasks usually thought to require representations anchored in a grammar, such as systematicity.",
        "bibtex": "@inproceedings{NIPS2002_ce6c9230,\n author = {Solan, Zach and Ruppin, Eytan and Horn, David and Edelman, Shimon},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Automatic Acquisition and Efficient Representation of Syntactic Structures},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/ce6c92303f38d297e263c7180f03d402-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/ce6c92303f38d297e263c7180f03d402-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/ce6c92303f38d297e263c7180f03d402-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 272184,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4458730850391337832&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff": "Faculty of Exact Sciences, Tel Aviv University; Faculty of Exact Sciences, Tel Aviv University; Faculty of Exact Sciences, Tel Aviv University; Department of Psychology, Cornell University",
        "aff_domain": "post.tau.ac.il;post.tau.ac.il;post.tau.ac.il;cornell.edu",
        "email": "post.tau.ac.il;post.tau.ac.il;post.tau.ac.il;cornell.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Faculty of Exact Sciences, Tel Aviv University;Department of Psychology, Cornell University",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "3013581335",
        "title": "Automatic Alignment of Local Representations",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/3a1dd98341fafc1dfe9bcf36360e6b84-Abstract.html",
        "author": "Yee W. Teh; Sam T. Roweis",
        "abstract": "We present an automatic alignment procedure which maps the disparate internal representations learned by several local dimensionality reduction experts into a single, coherent global coordinate system for the original data space. Our algorithm can be applied to any set of experts, each of which produces a low-dimensional local representation of a high- dimensional input. Unlike recent efforts to coordinate such models by modifying their objective functions [1, 2], our algorithm is invoked after training and applies an ef\ufb01cient eigensolver to post-process the trained models. The post-processing has no local optima and the size of the sys- tem it must solve scales with the number of local models rather than the number of original data points, making it more ef\ufb01cient than model-free algorithms such as Isomap [3] or LLE [4].\n\n1 Introduction: Local vs. Global Dimensionality Reduction Beyond density modelling, an important goal of unsupervised learning is to discover com- pact, informative representations of high-dimensional data. If the data lie on a smooth low dimensional manifold, then an excellent encoding is the coordinates internal to that man- ifold. The process of determining such coordinates is dimensionality reduction. Linear dimensionality reduction methods such as principal component analysis and factor analy- sis are easy to train but cannot capture the structure of curved manifolds. Mixtures of these simple unsupervised models [5, 6, 7, 8] have been used to perform local dimensionality reduction, and can provide good density models for curved manifolds, but unfortunately such mixtures cannot do dimensionality reduction. They do not describe a single, coher- ent low-dimensional coordinate system for the data since there is no pressure for the local coordinates of each component to agree.\n\nRoweis et al [1] recently proposed a model which performs global coordination of local coordinate systems in a mixture of factor analyzers (MFA). Their model is trained by max- imizing the likelihood of the data, with an additional variational penalty term to encourage the internal coordinates of the factor analyzers to agree. While their model can trade off modelling the data and having consistent local coordinate systems, it requires a user given trade-off parameter, training is quite inef\ufb01cient (although [2] describes an improved train- ing algorithm for a more constrained model), and it has quite serious local minima problems (methods like LLE [4] or Isomap [3] have to be used for initialization).\n\nIn this paper we describe a novel, automatic way to align the hidden representations used by each component of a mixture of dimensionality reducers into a single global representation of the data throughout space. Given an already trained mixture, the alignment is achieved by applying an eigensolver to a matrix constructed from the internal representations of the mixture components. Our method is ef\ufb01cient, simple to implement, and has no local optima in its optimization nor any learning rates or annealing schedules.\n\n\n\n2 The Locally Linear Coordination Algorithm\n\nSuppose we have a set of data points given by the rows of  \t \n\n\n  a -dimensional space, which we assume are sampled from a    fold. We approximate the manifold coordinates using images \n\n !   dimensional embedding space. Suppose also that we have already trained, or have been th reducer produces a%$ dimen- given, a mixture of\" sional internal representation&(' $ for data point )' as well as a \u201cresponsibility\u201d*\t' $,+.- ' $ describing how reliable the# 10 is. These satisfy/\n\nlocal dimensionality reducers. The# th reducer\u2019s representation of\n\nand can be obtained, for example, using a gating network in a mixture of experts, or the posterior probabilities in a probabilistic network. Notice that the manifold coordinates and internal representations need not have the same number of dimensions.\n\nfrom dimensional mani- in a\n\n  \n\nGiven the data, internal representations, and responsibilities, our algorithm automatically aligns the various hidden representations into a single global coordinate system. Two key ideas motivate the method. First, to use a convex cost function whose unique minimum is to\n\nattained at the desired global coordinates. Second, to restrict the global coordinates  ' $ and responsibilities* depend on the data\n\nthereby leveraging the structure of the mixture model to regularize and reduce the effective size of the optimization problem. In effect, rather than working with individual data points, we work with large groups of points belonging to particular submodels.\n\n' only through the local representations &",
        "bibtex": "@inproceedings{NIPS2002_3a1dd983,\n author = {Teh, Yee and Roweis, Sam},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Automatic Alignment of Local Representations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/3a1dd98341fafc1dfe9bcf36360e6b84-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/3a1dd98341fafc1dfe9bcf36360e6b84-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/3a1dd98341fafc1dfe9bcf36360e6b84-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 866582,
        "gs_citation": 306,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=67701272483894908&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science, University of Toronto; Department of Computer Science, University of Toronto",
        "aff_domain": "cs.toronto.edu;cs.toronto.edu",
        "email": "cs.toronto.edu;cs.toronto.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Toronto",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utoronto.ca",
        "aff_unique_abbr": "U of T",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Toronto",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2d9600650f",
        "title": "Automatic Derivation of Statistical Algorithms: The EM Family and Beyond",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/0234c510bc6d908b28c70ff313743079-Abstract.html",
        "author": "Bernd Fischer; Johann Schumann; Wray Buntine; Alexander G. Gray",
        "abstract": "Machine learning has reached a point where many probabilistic meth- ods can be understood as variations, extensions and combinations of a much smaller set of abstract themes, e.g., as different instances of the EM algorithm. This enables the systematic derivation of algorithms cus- tomized for different models. Here, we describe the AUTO BAYES sys- tem which takes a high-level statistical model speci\ufb01cation, uses power- ful symbolic techniques based on schema-based program synthesis and computer algebra to derive an ef\ufb01cient specialized algorithm for learning that model, and generates executable code implementing that algorithm. This capability is far beyond that of code collections such as Matlab tool- boxes or even tools for model-independent optimization such as BUGS for Gibbs sampling: complex new algorithms can be generated with- out new programming, algorithms can be highly specialized and tightly crafted for the exact structure of the model and data, and ef\ufb01cient and commented code can be generated for different languages or systems. We present automatically-derived algorithms ranging from closed-form solutions of Bayesian textbook problems to recently-proposed EM algo- rithms for clustering, regression, and a multinomial form of PCA.",
        "bibtex": "@inproceedings{NIPS2002_0234c510,\n author = {Fischer, Bernd and Schumann, Johann and Buntine, Wray and Gray, Alexander},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Automatic Derivation of Statistical Algorithms: The EM Family and Beyond},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/0234c510bc6d908b28c70ff313743079-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/0234c510bc6d908b28c70ff313743079-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/0234c510bc6d908b28c70ff313743079-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 120405,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13803325940033319133&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Carnegie Mellon University; RIACS / NASA Ames; RIACS / NASA Ames; Helsinki Institute for IT",
        "aff_domain": "cs.cmu.edu;email.arc.nasa.gov;email.arc.nasa.gov;hiit.fi",
        "email": "cs.cmu.edu;email.arc.nasa.gov;email.arc.nasa.gov;hiit.fi",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;2",
        "aff_unique_norm": "Carnegie Mellon University;RIACS / NASA Ames;Helsinki Institute for IT",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.cmu.edu;;",
        "aff_unique_abbr": "CMU;;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "eea2ae4e7f",
        "title": "Bayesian Estimation of Time-Frequency Coefficients for Audio Signal Enhancement",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/81b3833e2504647f9d794f7d7b9bf341-Abstract.html",
        "author": "Patrick J. Wolfe; Simon J. Godsill",
        "abstract": "The Bayesian paradigm provides a natural and effective means of exploit- ing prior knowledge concerning the time-frequency structure of sound signals such as speech and music\u2014something which has often been over- looked in traditional audio signal processing approaches. Here, after con- structing a Bayesian model and prior distributions capable of taking into account the time-frequency characteristics of typical audio waveforms, we apply Markov chain Monte Carlo methods in order to sample from the resultant posterior distribution of interest. We present speech enhance- ment results which compare favourably in objective terms with standard time-varying \ufb01ltering techniques (and in several cases yield superior per- formance, both objectively and subjectively); moreover, in contrast to such methods, our results are obtained without an assumption of prior knowledge of the noise power.",
        "bibtex": "@inproceedings{NIPS2002_81b3833e,\n author = {Wolfe, Patrick and Godsill, Simon},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Bayesian Estimation of Time-Frequency Coefficients for Audio Signal Enhancement},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/81b3833e2504647f9d794f7d7b9bf341-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/81b3833e2504647f9d794f7d7b9bf341-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/81b3833e2504647f9d794f7d7b9bf341-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 181442,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5944858758494351651&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Engineering, University of Cambridge; Department of Engineering, University of Cambridge",
        "aff_domain": "eng.cam.ac.uk;eng.cam.ac.uk",
        "email": "eng.cam.ac.uk;eng.cam.ac.uk",
        "github": "",
        "project": "http://www-sigproc.eng.cam.ac.uk/~pjw47",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Cambridge",
        "aff_unique_dep": "Department of Engineering",
        "aff_unique_url": "https://www.cam.ac.uk",
        "aff_unique_abbr": "Cambridge",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "962031bd69",
        "title": "Bayesian Image Super-Resolution",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/88bfcf02e7f554f9e9ea350b699bc6a7-Abstract.html",
        "author": "Michael E. Tipping; Christopher M. Bishop",
        "abstract": "The  extraction  of a  single  high-quality  image  from  a  set  of  low(cid:173) resolution  images  is  an  important  problem  which  arises  in  fields  such as  remote  sensing,  surveillance,  medical  imaging and the  ex(cid:173) traction of still  images  from  video.  Typical  approaches are  based  on  the  use  of cross-correlation  to  register  the  images  followed  by  the  inversion  of the  transformation  from  the  unknown  high  reso(cid:173) lution image to the observed low  resolution images,  using regular(cid:173) ization to resolve  the  ill-posed  nature of the  inversion  process.  In  this paper we  develop a Bayesian treatment of the super-resolution  problem  in  which  the  likelihood  function  for  the  image  registra(cid:173) tion  parameters  is  based  on  a  marginalization  over  the  unknown  high-resolution  image.  This  approach  allows  us  to  estimate  the  unknown point spread function,  and is  rendered tractable through  the introduction of a  Gaussian  process  prior over images.  Results  indicate a  significant improvement over techniques  based on MAP  (maximum  a-posteriori)  point  optimization  of the  high  resolution  image and associated registration parameters.",
        "bibtex": "@inproceedings{NIPS2002_88bfcf02,\n author = {Tipping, Michael and Bishop, Christopher},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Bayesian Image Super-Resolution},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/88bfcf02e7f554f9e9ea350b699bc6a7-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/88bfcf02e7f554f9e9ea350b699bc6a7-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/88bfcf02e7f554f9e9ea350b699bc6a7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1440835,
        "gs_citation": 382,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8786829467746362640&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "http://research.microsoft.com/",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "4fcc857dfc",
        "title": "Bayesian Models of Inductive Generalization",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/80537a945c7aaa788ccfcdf1b99b5d8f-Abstract.html",
        "author": "Neville E. Sanjana; Joshua B. Tenenbaum",
        "abstract": "We argue that human inductive generalization is best explained in a Bayesian framework, rather than by traditional models based on simi- larity computations. We go beyond previous work on Bayesian concept learning by introducing an unsupervised method for constructing \ufb02ex- ible hypothesis spaces, and we propose a version of the Bayesian Oc- cam\u2019s razor that trades off priors and likelihoods to prevent under- or over-generalization in these \ufb02exible spaces. We analyze two published data sets on inductive reasoning as well as the results of a new behavioral study that we have carried out.",
        "bibtex": "@inproceedings{NIPS2002_80537a94,\n author = {Sanjana, Neville and Tenenbaum, Joshua},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Bayesian Models of Inductive Generalization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/80537a945c7aaa788ccfcdf1b99b5d8f-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/80537a945c7aaa788ccfcdf1b99b5d8f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/80537a945c7aaa788ccfcdf1b99b5d8f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 82344,
        "gs_citation": 57,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7838998185160225034&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology; Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology",
        "aff_domain": "mit.edu;mit.edu",
        "email": "mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Department of Brain and Cognitive Sciences",
        "aff_unique_url": "https://web.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7dedce2d79",
        "title": "Bayesian Monte Carlo",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/24917db15c4e37e421866448c9ab23d8-Abstract.html",
        "author": "Zoubin Ghahramani; Carl E. Rasmussen",
        "abstract": "We investigate Bayesian alternatives to classical Monte Carlo methods for evaluating integrals. Bayesian Monte Carlo (BMC) allows the in- corporation of prior knowledge, such as smoothness of the integrand, into the estimation. In a simple problem we show that this outperforms any classical importance sampling method. We also attempt more chal- lenging multidimensional integrals involved in computing marginal like- lihoods of statistical models (a.k.a. partition functions and model evi- dences). We \ufb01nd that Bayesian Monte Carlo outperformed Annealed Importance Sampling, although for very high dimensional problems or problems with massive multimodality BMC may be less adequate. One advantage of the Bayesian approach to Monte Carlo is that samples can be drawn from any distribution. This allows for the possibility of active design of sample points so as to maximise information gain.",
        "bibtex": "@inproceedings{NIPS2002_24917db1,\n author = {Ghahramani, Zoubin and Rasmussen, Carl},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Bayesian Monte Carlo},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/24917db15c4e37e421866448c9ab23d8-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/24917db15c4e37e421866448c9ab23d8-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/24917db15c4e37e421866448c9ab23d8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 146182,
        "gs_citation": 311,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=586640184817025424&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "http://www.gatsby.ucl.ac.uk",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "69ebdc2797",
        "title": "Bias-Optimal Incremental Problem Solving",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/40b5f25a228570053bc64a043c3f1833-Abstract.html",
        "author": "J\u00fcrgen Schmidhuber",
        "abstract": "Given is a problem sequence and a probability distribution (the bias) on programs computing solution candidates. We present an optimally fast way of incrementally solving each task in the sequence. Bias shifts are computed by program pre\ufb01xes that modify the distribution on their suf- \ufb01xes by reusing successful code for previous tasks (stored in non-modi\ufb01- able memory). No tested program gets more runtime than its probability times the total search time. In illustrative experiments, ours becomes the \ufb01rst general system to learn a universal solver for arbitrary  disk Tow- ers of Hanoi tasks (minimal solution size  ). It demonstrates the advantages of incremental learning by pro\ufb01ting from previously solved, simpler tasks involving samples of a simple context free language.",
        "bibtex": "@inproceedings{NIPS2002_40b5f25a,\n author = {Schmidhuber, J\\\"{u}rgen},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Bias-Optimal Incremental Problem Solving},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/40b5f25a228570053bc64a043c3f1833-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/40b5f25a228570053bc64a043c3f1833-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/40b5f25a228570053bc64a043c3f1833-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 99852,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4594052404163595471&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 6,
        "aff": "IDSIA, Galleria 2, 6928 Manno-Lugano, Switzerland",
        "aff_domain": "idsia.ch",
        "email": "idsia.ch",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "IDSIA, Galleria 2, 6928 Manno-Lugano, Switzerland",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "745ffbafc7",
        "title": "Binary Coding in Auditory Cortex",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/2321994d85d661d792223f647000c65f-Abstract.html",
        "author": "Michael R. Deweese; Anthony M. Zador",
        "abstract": "Cortical neurons have been reported to use both rate and temporal  codes. Here we describe a novel mode in which each neuron  generates exactly 0 or 1 action potentials, but not more, in response  to a stimulus. We used cell-attached recording, which ensured  single-unit isolation, to record responses in rat auditory cortex to  brief tone pips. Surprisingly, the majority of neurons exhibited  binary behavior with few multi-spike responses; several dramatic  examples consisted of exactly one spike on 100% of trials, with no  trial-to-trial variability in spike count. Many neurons were tuned to  stimulus frequency. Since individual trials yielded at most one  spike for most neurons, the information about stimulus frequency  was encoded in the population, and would not have been accessible  to later stages of processing that only had access to the activity of a  single unit. These binary units allow a more efficient population  code than is possible with conventional rate coding units, and are  consistent with a model of cortical processing  in which  synchronous packets of spikes propagate stably from one neuronal  population to the next.",
        "bibtex": "@inproceedings{NIPS2002_2321994d,\n author = {Deweese, Michael and Zador, Anthony},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Binary Coding in Auditory Cortex},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/2321994d85d661d792223f647000c65f-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/2321994d85d661d792223f647000c65f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/2321994d85d661d792223f647000c65f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 223059,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7438095015363583657&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Cold Spring Harbor Laboratory, Cold Spring Harbor, NY 11724; Cold Spring Harbor Laboratory, Cold Spring Harbor, NY 11724",
        "aff_domain": "cshl.edu;cshl.edu",
        "email": "cshl.edu;cshl.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Cold Spring Harbor Laboratory, Cold Spring Harbor, NY 11724",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "defc49377b",
        "title": "Binary Tuning is Optimal for Neural Rate Coding with High Temporal Resolution",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/e45823afe1e5120cec11fc4c379a0c67-Abstract.html",
        "author": "Matthias Bethge; David Rotermund; Klaus Pawelzik",
        "abstract": "Here we derive optimal gain functions for minimum mean square re(cid:173) construction from neural rate responses subjected to Poisson noise. The shape of these functions strongly depends on the length T of the time window within which spikes are counted in order to estimate the under(cid:173) lying firing rate. A phase transition towards pure binary encoding occurs if the maximum mean spike count becomes smaller than approximately three provided the minimum firing rate is zero. For a particular function class, we were able to prove the existence of a second-order phase tran(cid:173) sition analytically. The critical decoding time window length obtained from the analytical derivation is in precise agreement with the numerical results. We conclude that under most circumstances relevant to informa(cid:173) tion processing in the brain, rate coding can be better ascribed to a binary (low-entropy) code than to the other extreme of rich analog coding.",
        "bibtex": "@inproceedings{NIPS2002_e45823af,\n author = {Bethge, Matthias and Rotermund, David and Pawelzik, Klaus},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Binary Tuning is Optimal for Neural Rate Coding with High Temporal Resolution},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/e45823afe1e5120cec11fc4c379a0c67-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/e45823afe1e5120cec11fc4c379a0c67-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/e45823afe1e5120cec11fc4c379a0c67-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 679162,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16763862399827242339&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 11,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "http://www.neuro.uni-bremen.de/mbethge",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "32475d976f",
        "title": "Boosted Dyadic Kernel Discriminants",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/598920e11d1eb2a49501d59fce5ecbb7-Abstract.html",
        "author": "Baback Moghaddam; Gregory Shakhnarovich",
        "abstract": "We introduce a novel learning algorithm for binary classi(cid:12)cation with hyperplane discriminants based on pairs of training points from opposite classes (dyadic hypercuts). This algorithm is further extended to nonlinear discriminants using kernel functions satisfy- ing Mercer\u2019s conditions. An ensemble of simple dyadic hypercuts is learned incrementally by means of a con(cid:12)dence-rated version of Ad- aBoost, which provides a sound strategy for searching through the (cid:12)nite set of hypercut hypotheses. In experiments with real-world datasets from the UCI repository, the generalization performance of the hypercut classi(cid:12)ers was found to be comparable to that of SVMs and k-NN classi(cid:12)ers. Furthermore, the computational cost of classi(cid:12)cation (at run time) was found to be similar to, or bet- ter than, that of SVM. Similarly to SVMs, boosted dyadic kernel discriminants tend to maximize the margin (via AdaBoost). In contrast to SVMs, however, we o(cid:11)er an on-line and incremental learning machine for building kernel discriminants whose complex- ity (number of kernel evaluations) can be directly controlled (traded o(cid:11) for accuracy).",
        "bibtex": "@inproceedings{NIPS2002_598920e1,\n author = {Moghaddam, Baback and Shakhnarovich, Gregory},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Boosted Dyadic Kernel Discriminants},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/598920e11d1eb2a49501d59fce5ecbb7-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/598920e11d1eb2a49501d59fce5ecbb7-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/598920e11d1eb2a49501d59fce5ecbb7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 118136,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4618025531570529107&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Mitsubishi Electric Research Laboratory; MIT AI Laboratory",
        "aff_domain": "merl.com;ai.mit.edu",
        "email": "merl.com;ai.mit.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Mitsubishi Electric Research Laboratory;MIT AI Laboratory",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "c11c91c7f0",
        "title": "Boosting Density Estimation",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/3de568f8597b94bda53149c7d7f5958c-Abstract.html",
        "author": "Saharon Rosset; Eran Segal",
        "abstract": "Several authors have suggested viewing boosting as a gradient descent search for a good \ufb01t in function space. We apply gradient-based boosting methodology to the unsupervised learning problem of density estimation. We show convergence properties of the algorithm and prove that a strength of weak learnability prop- erty applies to this problem as well. We illustrate the potential of this approach through experiments with boosting Bayesian networks to learn density models.",
        "bibtex": "@inproceedings{NIPS2002_3de568f8,\n author = {Rosset, Saharon and Segal, Eran},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Boosting Density Estimation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/3de568f8597b94bda53149c7d7f5958c-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/3de568f8597b94bda53149c7d7f5958c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/3de568f8597b94bda53149c7d7f5958c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 116040,
        "gs_citation": 88,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11357803003019749782&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Statistics, Stanford University; Computer Science Department, Stanford University",
        "aff_domain": "stat.stanford.edu;cs.stanford.edu",
        "email": "stat.stanford.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "81f142c90e",
        "title": "Branching Law for Axons",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/66e8ba8216a1e152d72653d99a4f03ab-Abstract.html",
        "author": "Dmitri B. Chklovskii; Armen Stepanyants",
        "abstract": "What  determines  the  caliber  of  axonal  branches?  We  pursue  the  hypothesis  that  the  axonal  caliber  has  evolved  to  minimize  signal  propagation delays, while keeping arbor volume to  a  minimum.  We  show  that  for  a  general  cost  function  the  optimal  diameters  of  mother  (do)  and  daughter  (d], d2 )  branches at a bifurcation obey  h  a  ranc  mg  aw:  e  envatIOn  re les  on  t  e  fact  that the conduction  speed  scales  with  the  axon  diameter to  the  power  V  (v = 1  for  myelinated  axons  and  V  = 0.5  myelinated  axons).  We  test  the  branching  law  on  the  available  experimental data and find  a reasonable agreement.",
        "bibtex": "@inproceedings{NIPS2002_66e8ba82,\n author = {Chklovskii, Dmitri and Stepanyants, Armen},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Branching Law for Axons},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/66e8ba8216a1e152d72653d99a4f03ab-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/66e8ba8216a1e152d72653d99a4f03ab-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/66e8ba8216a1e152d72653d99a4f03ab-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1040383,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2383291700143660160&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "e923f58e07",
        "title": "Categorization Under Complexity: A Unified MDL Account of Human Learning of Regular and Irregular Categories",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/635440afdfc39fe37995fed127d7df4f-Abstract.html",
        "author": "David Fass; Jacob Feldman",
        "abstract": "We present an account of human concept learning-that is, learning of categories from examples-based on the principle of minimum descrip(cid:173) tion length (MDL). In support of this theory, we tested a wide range of two-dimensional concept types, including both regular (simple) and highly irregular (complex) structures, and found the MDL theory to give a good account of subjects' performance. This suggests that the intrin(cid:173) sic complexity of a concept (that is, its description -length) systematically influences its leamability.",
        "bibtex": "@inproceedings{NIPS2002_635440af,\n author = {Fass, David and Feldman, Jacob},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Categorization Under Complexity: A Unified MDL Account of Human Learning of Regular and Irregular Categories},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/635440afdfc39fe37995fed127d7df4f-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/635440afdfc39fe37995fed127d7df4f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/635440afdfc39fe37995fed127d7df4f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 800584,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17630490720331422740&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Psychology, Center for Cognitive Science, Rutgers University, Piscataway, NJ 08854; Department of Psychology, Center for Cognitive Science, Rutgers University, Piscataway, NJ 08854",
        "aff_domain": "ruccs.rutgers.edu;ruccs.rutgers.edu",
        "email": "ruccs.rutgers.edu;ruccs.rutgers.edu",
        "github": "",
        "project": "http://ruccs.rutgers.edu/~jacob/feldman.html",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Department of Psychology, Center for Cognitive Science, Rutgers University, Piscataway, NJ 08854",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "9f3c5cfac4",
        "title": "Charting a Manifold",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/8929c70f8d710e412d38da624b21c3c8-Abstract.html",
        "author": "Matthew Brand",
        "abstract": "We construct a nonlinear mapping from a high-dimensional sample space to a low-dimensional vector space, effectively recovering a Cartesian coordinate system for the manifold from which the data is sampled. The mapping preserves local geometric relations in the manifold and is pseudo-invertible. We show how to estimate the intrinsic dimensionality of the manifold from samples, decompose the sample data into locally linear low-dimensional patches, merge these patches into a single low- dimensional coordinate system, and compute forward and reverse map- pings between the sample and coordinate spaces. The objective functions are convex and their solutions are given in closed form.",
        "bibtex": "@inproceedings{NIPS2002_8929c70f,\n author = {Brand, Matthew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Charting a Manifold},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/8929c70f8d710e412d38da624b21c3c8-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/8929c70f8d710e412d38da624b21c3c8-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/8929c70f8d710e412d38da624b21c3c8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 631579,
        "gs_citation": 661,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7168273984437722490&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Mitsubishi Electric Research Labs",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "www.merl.com/people/brand/",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Mitsubishi Electric Research Labs",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.merl.com",
        "aff_unique_abbr": "MERL",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c8fb740e3b",
        "title": "Circuit Model of Short-Term Synaptic Dynamics",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/e0f7a4d0ef9b84b83b693bbf3feb8e6e-Abstract.html",
        "author": "Shih-Chii Liu; Malte Boegershausen; Pascal Suter",
        "abstract": "We describe a model of short-term synaptic depression that is derived from a silicon circuit implementation. The dynamics of this circuit model are similar to the dynamics of some present theoretical models of short- term depression except that the recovery dynamics of the variable de- scribing the depression is nonlinear and it also depends on the presynap- tic frequency. The equations describing the steady-state and transient re- sponses of this synaptic model \ufb01t the experimental results obtained from a fabricated silicon network consisting of leaky integrate-and-\ufb01re neu- rons and different types of synapses. We also show experimental data demonstrating the possible computational roles of depression. One pos- sible role of a depressing synapse is that the input can quickly bring the neuron up to threshold when the membrane potential is close to the rest- ing potential.",
        "bibtex": "@inproceedings{NIPS2002_e0f7a4d0,\n author = {Liu, Shih-Chii and Boegershausen, Malte and Suter, Pascal},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Circuit Model of Short-Term Synaptic Dynamics},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/e0f7a4d0ef9b84b83b693bbf3feb8e6e-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/e0f7a4d0ef9b84b83b693bbf3feb8e6e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/e0f7a4d0ef9b84b83b693bbf3feb8e6e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 202209,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11930033964111060783&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Institute of Neuroinformatics, University of Zurich and ETH Zurich; Institute of Neuroinformatics, University of Zurich and ETH Zurich; Institute of Neuroinformatics, University of Zurich and ETH Zurich",
        "aff_domain": "ini.phys.ethz.ch; ; ",
        "email": "ini.phys.ethz.ch; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Zurich",
        "aff_unique_dep": "Institute of Neuroinformatics",
        "aff_unique_url": "https://www.neuro.ethz.ch/",
        "aff_unique_abbr": "UZH",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "935aed7d39",
        "title": "Classifying Patterns of Visual Motion - a Neuromorphic Approach",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/046ddf96c233a273fd390c3d0b1a9aa4-Abstract.html",
        "author": "Jakob Heinzle; Alan Stocker",
        "abstract": "We report a system that classi\ufb01es and can learn to classify patterns of visual motion on-line. The complete system is described by the dynam- ics of its physical network architectures. The combination of the fol- lowing properties makes the system novel: Firstly, the front-end of the system consists of an aVLSI optical \ufb02ow chip that collectively computes 2-D global visual motion in real-time [1]. Secondly, the complexity of the classi\ufb01cation task is signi\ufb01cantly reduced by mapping the continu- ous motion trajectories to sequences of \u2019motion events\u2019. And thirdly, all the network structures are simple and with the exception of the optical \ufb02ow chip based on a Winner-Take-All (WTA) architecture. We demon- strate the application of the proposed generic system for a contactless man-machine interface that allows to write letters by visual motion. Re- garding the low complexity of the system, its robustness and the already existing front-end, a complete aVLSI system-on-chip implementation is realistic, allowing various applications in mobile electronic devices.",
        "bibtex": "@inproceedings{NIPS2002_046ddf96,\n author = {Heinzle, Jakob and Stocker, Alan A},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Classifying Patterns of Visual Motion - a Neuromorphic Approach},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/046ddf96c233a273fd390c3d0b1a9aa4-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/046ddf96c233a273fd390c3d0b1a9aa4-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/046ddf96c233a273fd390c3d0b1a9aa4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 417681,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10498622083606065915&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "www.ini.unizh.ch/~alan",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "4ce912b685",
        "title": "Cluster Kernels for Semi-Supervised Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/d6288499d0083cc34e60a077b7c4b3e1-Abstract.html",
        "author": "Olivier Chapelle; Jason Weston; Bernhard Sch\u00f6lkopf",
        "abstract": "We  propose  a  framework  to  incorporate  unlabeled  data in  kernel  classifier,  based on the idea that two points in the same cluster are  more likely  to have  the same label.  This is  achieved  by modifying  the eigenspectrum of the kernel matrix.  Experimental results assess  the validity of this  approach.",
        "bibtex": "@inproceedings{NIPS2002_d6288499,\n author = {Chapelle, Olivier and Weston, Jason and Sch\\\"{o}lkopf, Bernhard},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Cluster Kernels for Semi-Supervised Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/d6288499d0083cc34e60a077b7c4b3e1-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/d6288499d0083cc34e60a077b7c4b3e1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/d6288499d0083cc34e60a077b7c4b3e1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1479055,
        "gs_citation": 708,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11169729064595777936&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Max Planck Institute for Biological Cybernetics, 72076 Tiibingen, Germany; Max Planck Institute for Biological Cybernetics, 72076 Tiibingen, Germany; Max Planck Institute for Biological Cybernetics, 72076 Tiibingen, Germany",
        "aff_domain": "tuebingen.mpg.de;tuebingen.mpg.de;tuebingen.mpg.de",
        "email": "tuebingen.mpg.de;tuebingen.mpg.de;tuebingen.mpg.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Max Planck Institute for Biological Cybernetics, 72076 Tiibingen, Germany",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "6171733a2f",
        "title": "Clustering with the Fisher Score",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/47810f956e3d8fb8a32fb276448b464d-Abstract.html",
        "author": "Koji Tsuda; Motoaki Kawanabe; Klaus-Robert M\u00fcller",
        "abstract": "Recently the Fisher score (or the Fisher kernel) is increasingly used as a feature extractor for classi\ufb01cation problems. The Fisher score is a vector of parameter derivatives of loglikelihood of a probabilistic model. This paper gives a theoretical analysis about how class information is pre- served in the space of the Fisher score, which turns out that the Fisher score consists of a few important dimensions with class information and many nuisance dimensions. When we perform clustering with the Fisher score, K-Means type methods are obviously inappropriate because they make use of all dimensions. So we will develop a novel but simple clus- tering algorithm specialized for the Fisher score, which can exploit im- portant dimensions. This algorithm is successfully tested in experiments with arti\ufb01cial data and real data (amino acid sequences).",
        "bibtex": "@inproceedings{NIPS2002_47810f95,\n author = {Tsuda, Koji and Kawanabe, Motoaki and M\\\"{u}ller, Klaus-Robert},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Clustering with the Fisher Score},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/47810f956e3d8fb8a32fb276448b464d-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/47810f956e3d8fb8a32fb276448b464d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/47810f956e3d8fb8a32fb276448b464d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 144269,
        "gs_citation": 50,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10329760855832193951&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "AIST CBRC, 2-41-6, Aomi, Koto-ku, Tokyo, 135-0064, Japan; Fraunhofer FIRST, Kekul\u00b4estr. 7, 12489 Berlin, Germany + Dept. of CS, University of Potsdam, A.-Bebel-Str. 89, 14482 Potsdam, Germany; Fraunhofer FIRST, Kekul\u00b4estr. 7, 12489 Berlin, Germany + Dept. of CS, University of Potsdam, A.-Bebel-Str. 89, 14482 Potsdam, Germany",
        "aff_domain": "aist.go.jp;first.fhg.de;first.fhg.de",
        "email": "aist.go.jp;first.fhg.de;first.fhg.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+2;1+2",
        "aff_unique_norm": "AIST CBRC, 2-41-6, Aomi, Koto-ku, Tokyo, 135-0064, Japan;Fraunhofer FIRST, Kekul\u00b4estr. 7, 12489 Berlin, Germany;Dept. of CS, University of Potsdam, A.-Bebel-Str. 89, 14482 Potsdam, Germany",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": ";",
        "aff_country_unique": ""
    },
    {
        "id": "e5f2bfa764",
        "title": "Combining Dimensions and Features in Similarity-Based Representations",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/243facb29564e7b448834a7c9d901201-Abstract.html",
        "author": "Daniel J. Navarro; Michael D. Lee",
        "abstract": "This paper develops a new representational model of similarity data that combines continuous dimensions with discrete features. An al- gorithm capable of learning these representations is described, and a Bayesian model selection approach for choosing the appropriate number of dimensions and features is developed. The approach is demonstrated on a classic data set that considers the similarities between the numbers 0 through 9.",
        "bibtex": "@inproceedings{NIPS2002_243facb2,\n author = {Navarro, Daniel and Lee, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Combining Dimensions and Features in Similarity-Based Representations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/243facb29564e7b448834a7c9d901201-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/243facb29564e7b448834a7c9d901201-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/243facb29564e7b448834a7c9d901201-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 172262,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9821309233109244278&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Psychology, Ohio State University; Department of Psychology, University of Adelaide",
        "aff_domain": "osu.edu;psychology.adelaide.edu.au",
        "email": "osu.edu;psychology.adelaide.edu.au",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Ohio State University;Department of Psychology, University of Adelaide",
        "aff_unique_dep": "Department of Psychology;",
        "aff_unique_url": "https://www.osu.edu;",
        "aff_unique_abbr": "OSU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "8eeb5894c9",
        "title": "Combining Features for BCI",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/a70dc40477bc2adceef4d2c90f47eb82-Abstract.html",
        "author": "Guido Dornhege; Benjamin Blankertz; Gabriel Curio; Klaus-Robert M\u00fcller",
        "abstract": "Recently, interest is growing to develop an effective communication in- terface connecting the human brain to a computer, the \u2019Brain-Computer Interface\u2019 (BCI). One motivation of BCI research is to provide a new communication channel substituting normal motor output in patients with severe neuromuscular disabilities. In the last decade, various neuro- physiological cortical processes, such as slow potential shifts, movement related potentials (MRPs) or event-related desynchronization (ERD) of spontaneous EEG rhythms, were shown to be suitable for BCI, and, con- sequently, different independent approaches of extracting BCI-relevant EEG-features for single-trial analysis are under investigation. Here, we present and systematically compare several concepts for combining such EEG-features to improve the single-trial classi\ufb01cation. Feature combi- nations are evaluated on movement imagination experiments with 3 sub- jects where EEG-features are based on either MRPs or ERD, or both. Those combination methods that incorporate the assumption that the sin- gle EEG-features are physiologically mutually independent outperform the plain method of \u2019adding\u2019 evidence where the single-feature vectors are simply concatenated. These results strengthen the hypothesis that MRP and ERD re\ufb02ect at least partially independent aspects of cortical processes and open a new perspective to boost BCI effectiveness.",
        "bibtex": "@inproceedings{NIPS2002_a70dc404,\n author = {Dornhege, Guido and Blankertz, Benjamin and Curio, Gabriel and M\\\"{u}ller, Klaus-Robert},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Combining Features for BCI},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/a70dc40477bc2adceef4d2c90f47eb82-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/a70dc40477bc2adceef4d2c90f47eb82-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/a70dc40477bc2adceef4d2c90f47eb82-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 90447,
        "gs_citation": 86,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11568725417168449806&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Fraunhofer FIRST.IDA; Fraunhofer FIRST.IDA; Neurophysics Group, Dept. of Neurology, Klinikum Benjamin Franklin, Freie Universit\u00e4t Berlin; Fraunhofer FIRST.IDA + University of Potsdam",
        "aff_domain": "\u0003; ; ;",
        "email": "\u0003; ; ;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0+2",
        "aff_unique_norm": "Fraunhofer Institute for Software and Systems Engineering;Neurophysics Group, Dept. of Neurology, Klinikum Benjamin Franklin, Freie Universit\u00e4t Berlin;University of Potsdam",
        "aff_unique_dep": "FIRST.IDA;;",
        "aff_unique_url": "https://www.first.ida.fraunhofer.de/;;https://www.uni-potsdam.de",
        "aff_unique_abbr": "Fraunhofer FIRST.IDA;;UP",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+0",
        "aff_country_unique": "Germany;"
    },
    {
        "id": "6fc4886003",
        "title": "Concentration Inequalities for the Missing Mass and for Histogram Rule Error",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/be6c7b094f88532b6c6b35bbcd525ee8-Abstract.html",
        "author": "Luis E. Ortiz; David A. McAllester",
        "abstract": "This paper gives distribution-free concentration inequalities for the miss- ing mass and the error rate of histogram rules. Negative association meth- ods can be used to reduce these concentration problems to concentration questions about independent sums. Although the sums are independent, they are highly heterogeneous. Such highly heterogeneous independent sums cannot be analyzed using standard concentration inequalities such as Hoeffding\u2019s inequality, the Angluin-Valiant bound, Bernstein\u2019s in- equality, Bennett\u2019s inequality, or McDiarmid\u2019s theorem.",
        "bibtex": "@inproceedings{NIPS2002_be6c7b09,\n author = {Ortiz, Luis E and McAllester, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Concentration Inequalities for the Missing Mass and for Histogram Rule Error},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/be6c7b094f88532b6c6b35bbcd525ee8-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/be6c7b094f88532b6c6b35bbcd525ee8-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/be6c7b094f88532b6c6b35bbcd525ee8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 123501,
        "gs_citation": 92,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15082096191342106683&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 20,
        "aff": "Toyota Technological Institute at Chicago; University of Pennsylvania",
        "aff_domain": "tti-c.org;cis.upenn.edu",
        "email": "tti-c.org;cis.upenn.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Toyota Technological Institute at Chicago;University of Pennsylvania",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tti-chicago.org;https://www.upenn.edu",
        "aff_unique_abbr": "TTI Chicago;UPenn",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Chicago;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "05e91bc7f8",
        "title": "Concurrent Object Recognition and Segmentation by Graph Partitioning",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/b6cda17abb967ed28ec9610137aa45f7-Abstract.html",
        "author": "Stella X. Yu; Ralph Gross; Jianbo Shi",
        "abstract": "Segmentation and recognition have long been treated as two separate pro(cid:173) cesses.  We  propose a  mechanism  based  on  spectral  graph  partitioning  that readily combine the  two processes into one.  A part-based recogni(cid:173) tion system detects object patches, supplies their partial segmentations as  well as knowledge about the spatial configurations of the object.  The goal  of patch grouping is to find a set of patches that conform best to the object  configuration,  while the goal  of pixel grouping is  to  find  a set of pixels  that have  the  best low-level  feature  similarity.  Through pixel-patch  in(cid:173) teractions and between-patch competition encoded in the solution space,  these two processes are realized in  one joint optimization problem.  The  globally optimal partition is obtained by solving a constrained eigenvalue  problem.  We  demonstrate that the  resulting  object segmentation elimi(cid:173) nates false  positives for  the part detection, while  overcoming occlusion  and weak contours for the low-level edge detection.",
        "bibtex": "@inproceedings{NIPS2002_b6cda17a,\n author = {Yu, Stella X. and Gross, Ralph and Shi, Jianbo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Concurrent Object Recognition and Segmentation by Graph Partitioning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/b6cda17abb967ed28ec9610137aa45f7-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/b6cda17abb967ed28ec9610137aa45f7-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/b6cda17abb967ed28ec9610137aa45f7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1213671,
        "gs_citation": 95,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10969311528970886654&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Robotics Institute; Robotics Institute; Robotics Institute + Center for the Neural Basis of Cognition",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0+1",
        "aff_unique_norm": "Robotics Institute;Center for the Neural Basis of Cognition",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "019b9d51e9",
        "title": "Conditional Models on the Ranking Poset",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/936a40b7e8eea0dc537e5f2edee1387a-Abstract.html",
        "author": "Guy Lebanon; John D. Lafferty",
        "abstract": "A distance-based conditional model on the ranking poset is presented for use in classi\ufb01cation and ranking. The model is an extension of the Mallows  model, and generalizes the classi\ufb01er combination methods used by several ensemble learning algorithms, including error correcting output codes, discrete AdaBoost, logistic regression and cranking. The algebraic structure of the ranking poset leads to a simple Bayesian inter- pretation of the conditional model and its special cases. In addition to a unifying view, the framework suggests a probabilistic interpretation for error correcting output codes and an extension beyond the binary coding scheme.",
        "bibtex": "@inproceedings{NIPS2002_936a40b7,\n author = {Lebanon, Guy and Lafferty, John},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Conditional Models on the Ranking Poset},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/936a40b7e8eea0dc537e5f2edee1387a-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/936a40b7e8eea0dc537e5f2edee1387a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/936a40b7e8eea0dc537e5f2edee1387a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 107204,
        "gs_citation": 55,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5279193494265478437&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "School of Computer Science, Carnegie Mellon University; School of Computer Science, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7a9dc9ac2a",
        "title": "Constraint Classification for Multiclass Classification and Ranking",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/16026d60ff9b54410b3435b403afd226-Abstract.html",
        "author": "Sariel Har-Peled; Dan Roth; Dav Zimak",
        "abstract": "The constraint classi\ufb01cation framework captures many \ufb02avors of mul- ticlass classi\ufb01cation including winner-take-all multiclass classi\ufb01cation, multilabel classi\ufb01cation and ranking. We present a meta-algorithm for learning in this framework that learns via a single linear classi\ufb01er in high dimension. We discuss distribution independent as well as margin-based generalization bounds and present empirical and theoretical evidence showing that constraint classi\ufb01cation bene\ufb01ts over existing methods of multiclass classi\ufb01cation.",
        "bibtex": "@inproceedings{NIPS2002_16026d60,\n author = {Har-Peled, Sariel and Roth, Dan and Zimak, Dav},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Constraint Classification for Multiclass Classification and Ranking},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/16026d60ff9b54410b3435b403afd226-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/16026d60ff9b54410b3435b403afd226-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/16026d60ff9b54410b3435b403afd226-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 106054,
        "gs_citation": 256,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6334890279140350588&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Science, University of Illinois; Department of Computer Science, University of Illinois; Department of Computer Science, University of Illinois",
        "aff_domain": "uiuc.edu;uiuc.edu;uiuc.edu",
        "email": "uiuc.edu;uiuc.edu;uiuc.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Illinois",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://illinois.edu",
        "aff_unique_abbr": "UIUC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7f4f5f975e",
        "title": "Convergence Properties of Some Spike-Triggered Analysis Techniques",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/4aecfbe5d21e3f7912bf8eb29124423a-Abstract.html",
        "author": "Liam Paninski",
        "abstract": "vVe analyze the convergence properties of three spike-triggered data analysis techniques. All of our results are obtained in the set(cid:173) ting of a (possibly multidimensional) linear-nonlinear (LN) cascade model for stimulus-driven neural activity. We start by giving exact rate of convergence results for the common spike-triggered average (STA) technique. Next, we analyze a spike-triggered covariance method, variants of which have been recently exploited successfully by Bialek, Simoncelli, and colleagues. These first two methods suf(cid:173) fer from extraneous conditions on their convergence; therefore, we introduce an estimator for the LN model parameters which is de(cid:173) signed to be consistent under general conditions. We provide an algorithm for the computation of this estimator and derive its rate of convergence. We close with a brief discussion of the efficiency of these estimators and an application to data recorded from the primary motor cortex of awake, behaving primates.",
        "bibtex": "@inproceedings{NIPS2002_4aecfbe5,\n author = {Paninski, Liam},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Convergence Properties of Some Spike-Triggered Analysis Techniques},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/4aecfbe5d21e3f7912bf8eb29124423a-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/4aecfbe5d21e3f7912bf8eb29124423a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/4aecfbe5d21e3f7912bf8eb29124423a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 894863,
        "gs_citation": 243,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9645341791504894179&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 22,
        "aff": "Center for Neural Science, New York University",
        "aff_domain": "cns.nyu.edu",
        "email": "cns.nyu.edu",
        "github": "",
        "project": "http://www.cns.nyu.edu/rvliam",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Center for Neural Science, New York University",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "bd24c3c021",
        "title": "Convergent Combinations of Reinforcement Learning with Linear Function Approximation",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/cd3afef9b8b89558cd56638c3631868a-Abstract.html",
        "author": "Ralf Schoknecht; Artur Merke",
        "abstract": "Convergence  for  iterative  reinforcement  learning  algorithms  like  TD(O)  depends on the sampling strategy for  the transitions.  How(cid:173) ever,  in  practical  applications  it  is  convenient  to  take  transition  data  from  arbitrary  sources  without  losing  convergence.  In  this  paper we  investigate the problem of repeated synchronous updates  based on a  fixed  set of transitions.  Our main theorem yields  suffi(cid:173) cient  conditions  of convergence for  combinations  of reinforcement  learning algorithms and linear function approximation.  This allows  to analyse if a  certain reinforcement learning algorithm and a  cer(cid:173) tain function approximator are  compatible.  For the combination of  the residual gradient algorithm with grid-based linear interpolation  we  show  that there exists  a  universal  constant  learning rate such  that the  iteration  converges  independently  of the  concrete  transi(cid:173) tion data.",
        "bibtex": "@inproceedings{NIPS2002_cd3afef9,\n author = {Schoknecht, Ralf and Merke, Artur},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Convergent Combinations of Reinforcement Learning with Linear Function Approximation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/cd3afef9b8b89558cd56638c3631868a-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/cd3afef9b8b89558cd56638c3631868a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/cd3afef9b8b89558cd56638c3631868a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1761681,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=516021106401550589&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "ILKD University of Karlsruhe, Germany; Lehrstuhl Informatik 1 University of Dortmund, Germany",
        "aff_domain": "ilkd.uni-karlsruhe.de;udo.edu",
        "email": "ilkd.uni-karlsruhe.de;udo.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "ILKD University of Karlsruhe, Germany;Lehrstuhl Informatik 1 University of Dortmund, Germany",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "c0f65a5040",
        "title": "Coulomb Classifiers: Generalizing Support Vector Machines via an Analogy to Electrostatic Systems",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/e21e4e58ad9ab56e8a4634046da90113-Abstract.html",
        "author": "Sepp Hochreiter; Michael Mozer; Klaus Obermayer",
        "abstract": "We introduce a family of classi\ufb02ers based on a physical analogy to an electrostatic system of charged conductors. The family, called Coulomb classi\ufb02ers, includes the two best-known support-vector machines (SVMs), the \u201d{SVM and the C{SVM. In the electrostat- ics analogy, a training example corresponds to a charged conductor at a given location in space, the classi\ufb02cation function corresponds to the electrostatic potential function, and the training objective function corresponds to the Coulomb energy. The electrostatic framework provides not only a novel interpretation of existing algo- rithms and their interrelationships, but it suggests a variety of new methods for SVMs including kernels that bridge the gap between polynomial and radial-basis functions, objective functions that do not require positive-de\ufb02nite kernels, regularization techniques that allow for the construction of an optimal classi\ufb02er in Minkowski space. Based on the framework, we propose novel SVMs and per- form simulation studies to show that they are comparable or su- perior to standard SVMs. The experiments include classi\ufb02cation tasks on data which are represented in terms of their pairwise prox- imities, where a Coulomb Classi\ufb02er outperformed standard SVMs.",
        "bibtex": "@inproceedings{NIPS2002_e21e4e58,\n author = {Hochreiter, Sepp and Mozer, Michael C and Obermayer, Klaus},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Coulomb Classifiers: Generalizing Support Vector Machines via an Analogy to Electrostatic Systems},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/e21e4e58ad9ab56e8a4634046da90113-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/e21e4e58ad9ab56e8a4634046da90113-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/e21e4e58ad9ab56e8a4634046da90113-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 128029,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10525477179386600817&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Department of Electrical Engineering and Computer Science, Technische Universit\u00c4 at Berlin, 10587 Berlin, Germany; Department of Computer Science, University of Colorado, Boulder, CO 80309{0430, USA; Department of Electrical Engineering and Computer Science, Technische Universit\u00c4 at Berlin, 10587 Berlin, Germany",
        "aff_domain": "cs.tu-berlin.de;cs.colorado.edu;cs.tu-berlin.de",
        "email": "cs.tu-berlin.de;cs.colorado.edu;cs.tu-berlin.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Department of Electrical Engineering and Computer Science, Technische Universit\u00c4 at Berlin, 10587 Berlin, Germany;Department of Computer Science, University of Colorado, Boulder, CO 80309{0430, USA",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "5c91ed44c2",
        "title": "Critical Lines in Symmetry of Mixture Models and its Application to Component Splitting",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/2d3acd3e240c61820625fff66a19938f-Abstract.html",
        "author": "Kenji Fukumizu; Shotaro Akaho; Shun-ichi Amari",
        "abstract": "We show the existence of critical points as lines for the likelihood func- tion of mixture-type models. They are given by embedding of a critical point for models with less components. A suf\ufb01cient condition that the critical line gives local maxima or saddle points is also derived. Based on this fact, a component-split method is proposed for a mixture of Gaus- sian components, and its effectiveness is veri\ufb01ed through experiments.",
        "bibtex": "@inproceedings{NIPS2002_2d3acd3e,\n author = {Fukumizu, Kenji and Akaho, Shotaro and Amari, Shun-ichi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Critical Lines in Symmetry of Mixture Models and its Application to Component Splitting},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/2d3acd3e240c61820625fff66a19938f-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/2d3acd3e240c61820625fff66a19938f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/2d3acd3e240c61820625fff66a19938f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 284615,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9402787359787644067&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Institute of Statistical Mathematics; AIST; RIKEN",
        "aff_domain": "ism.ac.jp;aist.go.jp;brain.riken.go.jp",
        "email": "ism.ac.jp;aist.go.jp;brain.riken.go.jp",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Institute of Statistical Mathematics;Advanced Industrial Science and Technology;RIKEN",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.ism.ac.jp;https://www.aist.go.jp;https://www.riken.jp",
        "aff_unique_abbr": "ISM;AIST;RIKEN",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "c2a624e077",
        "title": "Data-Dependent Bounds for Bayesian Mixture Methods",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/53ed35c74a2ec275b837374f04396c03-Abstract.html",
        "author": "Ron Meir; Tong Zhang",
        "abstract": "We consider Bayesian mixture approaches, where a predictor is constructed by forming a weighted average of hypotheses from some space of functions. While such procedures are known to lead to optimal predictors in several cases, where su\u2013ciently accurate prior information is available, it has not been clear how they perform when some of the prior assumptions are violated. In this paper we establish data-dependent bounds for such procedures, extending previous randomized approaches such as the Gibbs algorithm to a fully Bayesian setting. The \ufb02nite-sample guarantees established in this work enable the utilization of Bayesian mixture approaches in agnostic settings, where the usual assumptions of the Bayesian paradigm fail to hold. Moreover, the bounds derived can be directly applied to non-Bayesian mixture approaches such as Bagging and Boosting.",
        "bibtex": "@inproceedings{NIPS2002_53ed35c7,\n author = {Meir, Ron and Zhang, Tong},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Data-Dependent Bounds for Bayesian Mixture Methods},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/53ed35c74a2ec275b837374f04396c03-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/53ed35c74a2ec275b837374f04396c03-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/53ed35c74a2ec275b837374f04396c03-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 133270,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11346281912122699013&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Technion, Haifa 32000, Israel; IBM T.J. Watson Research Center, Yorktown Heights, NY 10598, USA",
        "aff_domain": "ee.technion.ac.il;watson.ibm.com",
        "email": "ee.technion.ac.il;watson.ibm.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Technion, Haifa 32000, Israel;IBM T.J. Watson Research Center",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://www.ibm.com/research/watson",
        "aff_unique_abbr": ";IBM Watson",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Yorktown Heights",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "5c23e843a3",
        "title": "Derivative Observations in Gaussian Process Models of Dynamic Systems",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/5b8e4fd39d9786228649a8a8bec4e008-Abstract.html",
        "author": "E. Solak; R. Murray-smith; W. E. Leithead; D. J. Leith; Carl E. Rasmussen",
        "abstract": "Gaussian processes provide an approach to nonparametric modelling which allows a straightforward combination of function and derivative observations in an empirical model. This is of particular importance in identi\ufb01cation of nonlinear dynamic systems from experimental data. 1) It allows us to combine derivative information, and associated uncertainty with normal function observations into the learning and inference pro- cess. This derivative information can be in the form of priors speci\ufb01ed by an expert or identi\ufb01ed from perturbation data close to equilibrium. 2) It allows a seamless fusion of multiple local linear models in a consis- tent manner, inferring consistent models and ensuring that integrability constraints are met. 3) It improves dramatically the computational ef- \ufb01ciency of Gaussian process models for dynamic system identi\ufb01cation, by summarising large quantities of near-equilibrium data by a handful of linearisations, reducing the training set size \u2013 traditionally a problem for Gaussian process models.",
        "bibtex": "@inproceedings{NIPS2002_5b8e4fd3,\n author = {Solak, E. and Murray-smith, R. and Leithead, W. and Leith, D. and Rasmussen, Carl},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Derivative Observations in Gaussian Process Models of Dynamic Systems},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/5b8e4fd39d9786228649a8a8bec4e008-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/5b8e4fd39d9786228649a8a8bec4e008-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/5b8e4fd39d9786228649a8a8bec4e008-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 269682,
        "gs_citation": 448,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2427745334008109843&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 20,
        "aff": "Dept. Elec. & Electr. Eng., Strathclyde University, Glasgow G1 1QE, Scotland, UK; Dept. Computing Science, University of Glasgow, Glasgow G12 8QQ, Scotland, UK; Hamilton Institute, National Univ. of Ireland, Maynooth, Co. Kildare, Ireland; Hamilton Institute, National Univ. of Ireland, Maynooth, Co. Kildare, Ireland; Gatsby Computational Neuroscience Unit, University College London, UK",
        "aff_domain": "strath.ac.uk;dcs.gla.ac.uk;icu.strath.ac.uk;may.ie;gatsby.ucl.ac.uk",
        "email": "strath.ac.uk;dcs.gla.ac.uk;icu.strath.ac.uk;may.ie;gatsby.ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;2;3",
        "aff_unique_norm": "Dept. Elec. & Electr. Eng., Strathclyde University, Glasgow G1 1QE, Scotland, UK;Dept. Computing Science, University of Glasgow, Glasgow G12 8QQ, Scotland, UK;Hamilton Institute, National Univ. of Ireland, Maynooth, Co. Kildare, Ireland;University College London",
        "aff_unique_dep": ";;;Gatsby Computational Neuroscience Unit",
        "aff_unique_url": ";;;https://www.ucl.ac.uk",
        "aff_unique_abbr": ";;;UCL",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";London",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United Kingdom"
    },
    {
        "id": "36e83672f4",
        "title": "Developing Topography and Ocular Dominance Using Two aVLSI Vision Sensors and a Neurotrophic Model of Plasticity",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/f1e2b2c9255d552500a833ac828cd635-Abstract.html",
        "author": "Terry Elliott; J\u00f6rg Kramer",
        "abstract": "A neurotrophic model for the co-development of topography and ocular dominance columns in the primary visual cortex has recently been pro- posed. In the present work, we test this model by driving it with the output of a pair of neuronal vision sensors stimulated by disparate mov- ing patterns. We show that the temporal correlations in the spike trains generated by the two sensors elicit the development of re\ufb01ned topogra- phy and ocular dominance columns, even in the presence of signi\ufb01cant amounts of spontaneous activity and \ufb01xed-pattern noise in the sensors.",
        "bibtex": "@inproceedings{NIPS2002_f1e2b2c9,\n author = {Elliott, Terry and Kramer, J\\\"{o}rg},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Developing Topography and Ocular Dominance Using Two aVLSI Vision Sensors and a Neurotrophic Model of Plasticity},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/f1e2b2c9255d552500a833ac828cd635-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/f1e2b2c9255d552500a833ac828cd635-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/f1e2b2c9255d552500a833ac828cd635-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 99456,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16174442209448032719&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Dept. Electronics & Computer Science, University of Southampton, High\ufb01eld, Southampton, SO17 1BJ, United Kingdom; Institute of Neuroinformatics, University of Z\u00a8urich and ETH Z\u00a8urich, Winterthurerstrasse 190, 8057 Z\u00a8urich, Switzerland",
        "aff_domain": "ecs.soton.ac.uk;ini.phys.ethz.ch",
        "email": "ecs.soton.ac.uk;ini.phys.ethz.ch",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Dept. Electronics & Computer Science, University of Southampton, High\ufb01eld, Southampton, SO17 1BJ, United Kingdom;Institute of Neuroinformatics, University of Z\u00a8urich and ETH Z\u00a8urich, Winterthurerstrasse 190, 8057 Z\u00a8urich, Switzerland",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "ed121c8a6d",
        "title": "Discriminative Binaural Sound Localization",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/350db081a661525235354dd3e19b8c05-Abstract.html",
        "author": "Ehud Ben-reuven; Yoram Singer",
        "abstract": "Time difference of arrival (TDOA) is commonly used to estimate the az- imuth of a source in a microphone array. The most common methods to estimate TDOA are based on \ufb01nding extrema in generalized cross- correlation waveforms. In this paper we apply microphone array tech- niques to a manikin head. By considering the entire cross-correlation waveform we achieve azimuth prediction accuracy that exceeds extrema locating methods. We do so by quantizing the azimuthal angle and treating the prediction problem as a multiclass categorization task. We demonstrate the merits of our approach by evaluating the various ap- proaches on Sony\u2019s AIBO robot.",
        "bibtex": "@inproceedings{NIPS2002_350db081,\n author = {Ben-reuven, Ehud and Singer, Yoram},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Discriminative Binaural Sound Localization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/350db081a661525235354dd3e19b8c05-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/350db081a661525235354dd3e19b8c05-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/350db081a661525235354dd3e19b8c05-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 123121,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18045788403484187549&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "School of Computer Science & Engineering, The Hebrew University, Jerusalem 91904, Israel; School of Computer Science & Engineering, The Hebrew University, Jerusalem 91904, Israel",
        "aff_domain": "benreuven.com;cs.huji.ac.il",
        "email": "benreuven.com;cs.huji.ac.il",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "School of Computer Science & Engineering, The Hebrew University, Jerusalem 91904, Israel",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "4bfbf16547",
        "title": "Discriminative Densities from Maximum Contrast Estimation",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/a3eb043e7bf775de87763e9f8121c953-Abstract.html",
        "author": "Peter Meinicke; Thorsten Twellmann; Helge Ritter",
        "abstract": "We propose a framework for classi\ufb01er design based on discriminative densities for representation of the differences of the class-conditional dis- tributions in a way that is optimal for classi\ufb01cation. The densities are selected from a parametrized set by constrained maximization of some objective function which measures the average (bounded) difference, i.e. the contrast between discriminative densities. We show that maximiza- tion of the contrast is equivalent to minimization of an approximation of the Bayes risk. Therefore using suitable classes of probability den- sity functions, the resulting maximum contrast classi\ufb01ers (MCCs) can approximate the Bayes rule for the general multiclass case. In particular for a certain parametrization of the density functions we obtain MCCs which have the same functional form as the well-known Support Vec- tor Machines (SVMs). We show that MCC-training in general requires some nonlinear optimization but under certain conditions the problem is concave and can be tackled by a single linear program. We indicate the close relation between SVM- and MCC-training and in particular we show that Linear Programming Machines can be viewed as an approxi- mate realization of MCCs. In the experiments on benchmark data sets, the MCC shows a competitive classi\ufb01cation performance.",
        "bibtex": "@inproceedings{NIPS2002_a3eb043e,\n author = {Meinicke, Peter and Twellmann, Thorsten and Ritter, Helge},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Discriminative Densities from Maximum Contrast Estimation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/a3eb043e7bf775de87763e9f8121c953-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/a3eb043e7bf775de87763e9f8121c953-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/a3eb043e7bf775de87763e9f8121c953-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 180166,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11827776517868979317&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Neuroinformatics Group, University of Bielefeld, Bielefeld, Germany; Neuroinformatics Group, University of Bielefeld, Bielefeld, Germany; Neuroinformatics Group, University of Bielefeld, Bielefeld, Germany",
        "aff_domain": "techfak.uni-bielefeld.de;techfak.uni-bielefeld.de;techfak.uni-bielefeld.de",
        "email": "techfak.uni-bielefeld.de;techfak.uni-bielefeld.de;techfak.uni-bielefeld.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Neuroinformatics Group, University of Bielefeld, Bielefeld, Germany",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "1bbfc0e315",
        "title": "Discriminative Learning for Label Sequences via Boosting",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/f7e0b956540676a129760a3eae309294-Abstract.html",
        "author": "Yasemin Altun; Thomas Hofmann; Mark Johnson",
        "abstract": "This  paper  investigates  a  boosting  approach  to  discriminative  learning of label sequences based on a sequence rank loss function.  The proposed method combines many of the  advantages of boost(cid:173) ing schemes  with  the efficiency  of dynamic programming methods  and is  attractive both, conceptually and computationally.  In  addi(cid:173) tion, we also discuss alternative approaches based on the Hamming  loss for  label sequences.  The sequence boosting algorithm offers an  interesting alternative to  methods  based  on  HMMs  and  the  more  recently proposed  Conditional Random Fields.  Applications  areas  for the presented technique range from natural language processing  and  information extraction to  computational biology.  We  include  experiments  on  named  entity  recognition  and  part-of-speech  tag(cid:173) ging  which  demonstrate  the  validity  and  competitiveness  of  our  approach.",
        "bibtex": "@inproceedings{NIPS2002_f7e0b956,\n author = {Altun, Yasemin and Hofmann, Thomas and Johnson, Mark},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Discriminative Learning for Label Sequences via Boosting},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/f7e0b956540676a129760a3eae309294-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/f7e0b956540676a129760a3eae309294-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/f7e0b956540676a129760a3eae309294-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1527579,
        "gs_citation": 64,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4911207339274453151&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science; Department of Computer Science; Department of Cognitive and Linguistics Sciences + Department of Computer Science",
        "aff_domain": "cs.brown.edu;cs.brown.edu;brown.edu",
        "email": "cs.brown.edu;cs.brown.edu;brown.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1+0",
        "aff_unique_norm": "Unknown Institution;Department of Cognitive and Linguistics Sciences",
        "aff_unique_dep": "Department of Computer Science;",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "469160acc4",
        "title": "Distance Metric Learning with Application to Clustering with Side-Information",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/c3e4035af2a1cde9f21e1ae1951ac80b-Abstract.html",
        "author": "Eric P. Xing; Michael I. Jordan; Stuart Russell; Andrew Y. Ng",
        "abstract": "@cs.berkeley.edu",
        "bibtex": "@inproceedings{NIPS2002_c3e4035a,\n author = {Xing, Eric and Jordan, Michael and Russell, Stuart J and Ng, Andrew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Distance Metric Learning with Application to Clustering with Side-Information},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/c3e4035af2a1cde9f21e1ae1951ac80b-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/c3e4035af2a1cde9f21e1ae1951ac80b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/c3e4035af2a1cde9f21e1ae1951ac80b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 162270,
        "gs_citation": 4140,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6588442176337540489&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 28,
        "aff": "University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley",
        "aff_domain": "cs.berkeley.edu;cs.berkeley.edu;cs.berkeley.edu;cs.berkeley.edu",
        "email": "cs.berkeley.edu;cs.berkeley.edu;cs.berkeley.edu;cs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7062c2745c",
        "title": "Dopamine Induced Bistability Enhances Signal Processing in Spiny Neurons",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/2a34abd6ebbd7fcf5a4421229c946c0a-Abstract.html",
        "author": "Aaron J. Gruber; Sara A. Solla; James C. Houk",
        "abstract": "Single  unit  activity  in  the  striatum  of  awake  monkeys  shows  a  marked  dependence  on  the  expected  reward  that  a  behavior  will  elicit.  We  present  a  computational  model  of  spiny  neurons,  the  principal neurons of the striatum, to assess the hypothesis that di(cid:173) rect  neuromodulatory  effects  of dopamine  through  the  activation  of D 1  receptors  mediate  the  reward  dependency  of spiny  neuron  activity.  Dopamine  release  results  in  the  amplification  of key  ion  currents,  leading  to  the  emergence  of  bistability,  which  not  only  modulates the peak firing  rate but also introduces a  temporal and  state dependence  of the  model's  response,  thus improving the de(cid:173) tectability of temporally correlated inputs.",
        "bibtex": "@inproceedings{NIPS2002_2a34abd6,\n author = {Gruber, Aaron and Solla, Sara and Houk, James},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Dopamine Induced Bistability Enhances Signal Processing in Spiny Neurons},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/2a34abd6ebbd7fcf5a4421229c946c0a-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/2a34abd6ebbd7fcf5a4421229c946c0a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/2a34abd6ebbd7fcf5a4421229c946c0a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1646869,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6505682755811504306&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Departments of Biomedical Engineering; Departments of Biomedical Engineering + Physiology + Physics and Astronomy; Departments of Biomedical Engineering + Physiology",
        "aff_domain": "northwestern.edu;northwestern.edu;northwestern.edu",
        "email": "northwestern.edu;northwestern.edu;northwestern.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1+2;0+1",
        "aff_unique_norm": "Departments of Biomedical Engineering;Physiology;Physics and Astronomy",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": ";",
        "aff_country_unique": ""
    },
    {
        "id": "de387f5569",
        "title": "Dyadic Classification Trees via Structural Risk Minimization",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/6832a7b24bc06775d02b7406880b93fc-Abstract.html",
        "author": "Clayton Scott; Robert Nowak",
        "abstract": "Classi\ufb01cation trees are one of the most popular types of classi\ufb01ers, with ease of implementation and interpretation being among their attractive features. Despite the widespread use of classi\ufb01cation trees, theoretical analysis of their performance is scarce. In this paper, we show that a new family of classi\ufb01cation trees, called dyadic classi\ufb01cation trees (DCTs), are near optimal (in a minimax sense) for a very broad range of clas- si\ufb01cation problems. This demonstrates that other schemes (e.g., neural networks, support vector machines) cannot perform signi\ufb01cantly better than DCTs in many cases. We also show that this near optimal perfor- mance is attained with linear (in the number of training data) complexity growing and pruning algorithms. Moreover, the performance of DCTs on benchmark datasets compares favorably to that of standard CART, which is generally more computationally intensive and which does not possess similar near optimality properties. Our analysis stems from the- oretical results on structural risk minimization, on which the pruning rule for DCTs is based.",
        "bibtex": "@inproceedings{NIPS2002_6832a7b2,\n author = {Scott, Clayton and Nowak, Robert},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Dyadic Classification Trees via Structural Risk Minimization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/6832a7b24bc06775d02b7406880b93fc-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/6832a7b24bc06775d02b7406880b93fc-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/6832a7b24bc06775d02b7406880b93fc-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 98165,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16689560707728154412&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Electrical and Computer Engineering, Rice University; Department of Electrical and Computer Engineering, Rice University",
        "aff_domain": "rice.edu;rice.edu",
        "email": "rice.edu;rice.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Rice University",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.rice.edu",
        "aff_unique_abbr": "Rice",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7973fe1468",
        "title": "Dynamic Bayesian Networks with Deterministic Latent Tables",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/7cc532d783a7461f227a5da8ea80bfe1-Abstract.html",
        "author": "David Barber",
        "abstract": "The application of latent/hidden variable Dynamic Bayesian Net- works is constrained by the complexity of marginalising over latent variables. For this reason either small latent dimensions or Gaus- sian latent conditional tables linearly dependent on past states are typically considered in order that inference is tractable. We suggest an alternative approach in which the latent variables are modelled using deterministic conditional probability tables. This specialisa- tion has the advantage of tractable inference even for highly com- plex non-linear/non-Gaussian visible conditional probability tables. This approach enables the consideration of highly complex latent dynamics whilst retaining the bene(cid:12)ts of a tractable probabilistic model.",
        "bibtex": "@inproceedings{NIPS2002_7cc532d7,\n author = {Barber, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Dynamic Bayesian Networks with Deterministic Latent Tables},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/7cc532d783a7461f227a5da8ea80bfe1-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/7cc532d783a7461f227a5da8ea80bfe1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/7cc532d783a7461f227a5da8ea80bfe1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 112443,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3591248723905869369&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Institute for Adaptive and Neural Computation, Edinburgh University",
        "aff_domain": "anc.ed.ac.uk",
        "email": "anc.ed.ac.uk",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Institute for Adaptive and Neural Computation, Edinburgh University",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "54ac058941",
        "title": "Dynamic Structure Super-Resolution",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/a376033f78e144f494bfc743c0be3330-Abstract.html",
        "author": "Amos J. Storkey",
        "abstract": "The problem of super-resolution involves generating feasible higher resolution images, which are pleasing to the eye and realistic, from a given low resolution image. This might be attempted by us- ing simple (cid:12)lters for smoothing out the high resolution blocks or through applications where substantial prior information is used to imply the textures and shapes which will occur in the images. In this paper we describe an approach which lies between the two extremes. It is a generic unsupervised method which is usable in all domains, but goes beyond simple smoothing methods in what it achieves. We use a dynamic tree-like architecture to model the high resolution data. Approximate conditioning on the low resolution image is achieved through a mean (cid:12)eld approach.",
        "bibtex": "@inproceedings{NIPS2002_a376033f,\n author = {Storkey, Amos J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Dynamic Structure Super-Resolution},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/a376033f78e144f494bfc743c0be3330-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/a376033f78e144f494bfc743c0be3330-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/a376033f78e144f494bfc743c0be3330-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 105436,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11414930953741025056&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "e5193a25e6",
        "title": "Dynamical Causal Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/697e382cfd25b07a3e62275d3ee132b3-Abstract.html",
        "author": "David Danks; Thomas L. Griffiths; Joshua B. Tenenbaum",
        "abstract": "theories of human causal",
        "bibtex": "@inproceedings{NIPS2002_697e382c,\n author = {Danks, David and Griffiths, Thomas and Tenenbaum, Joshua},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Dynamical Causal Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/697e382cfd25b07a3e62275d3ee132b3-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/697e382cfd25b07a3e62275d3ee132b3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/697e382cfd25b07a3e62275d3ee132b3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 804997,
        "gs_citation": 86,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5190114622017402063&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 20,
        "aff": "Department of Psychology, Stanford University; Institute for Human & Machine Cognition, University of West Florida; Department of Brain & Cognitive Sciences, MIT",
        "aff_domain": "psych.stanford.edu;ai.uwf.edu;rnit.edu",
        "email": "psych.stanford.edu;ai.uwf.edu;rnit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Stanford University;Institute for Human & Machine Cognition, University of West Florida;Department of Brain & Cognitive Sciences, MIT",
        "aff_unique_dep": "Department of Psychology;;",
        "aff_unique_url": "https://www.stanford.edu;;",
        "aff_unique_abbr": "Stanford;;",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "3cd78782cb",
        "title": "Dynamical Constraints on Computing with Spike Timing in the Cortex",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/806fec5af7f5b48b8a31a003e171f3fb-Abstract.html",
        "author": "Arunava Banerjee; Alexandre Pouget",
        "abstract": "If the  cortex uses  spike timing to  compute,  the  timing of the  spikes  must  be  robust to  perturbations.  Based on  a  recent  framework  that  provides  a  simple  criterion  to  determine  whether  a  spike  sequence  produced by a generic network is  sensitive to  initial conditions, and  numerical  simulations  of  a  variety  of  network  architectures,  we  argue  within  the  limits  set  by  our  model  of the  neuron,  that  it  is  unlikely  that  precise  sequences  of  spike  timings  are  used  for  computation under conditions typically found  in the cortex.",
        "bibtex": "@inproceedings{NIPS2002_806fec5a,\n author = {Banerjee, Arunava and Pouget, Alexandre},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Dynamical Constraints on Computing with Spike Timing in the Cortex},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/806fec5af7f5b48b8a31a003e171f3fb-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/806fec5af7f5b48b8a31a003e171f3fb-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/806fec5af7f5b48b8a31a003e171f3fb-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1676313,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15382742245275886858&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Brain and Cognitive Sciences, University of Rochester, Rochester, New York 14627; Department of Brain and Cognitive Sciences, University of Rochester, Rochester, New York 14627",
        "aff_domain": "bcs.rochester.edu;bcs.rochester.edu",
        "email": "bcs.rochester.edu;bcs.rochester.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Department of Brain and Cognitive Sciences, University of Rochester, Rochester, New York 14627",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "d99fcbcc31",
        "title": "Effective Dimension and Generalization of Kernel Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/25db67c5657914454081c6a18e93d6dd-Abstract.html",
        "author": "Tong Zhang",
        "abstract": "We investigate the generalization performance of some learning prob- lems in Hilbert function Spaces. We introduce a concept of scale- sensitive effective data dimension, and show that it characterizes the con- vergence rate of the underlying learning problem. Using this concept, we can naturally extend results for parametric estimation problems in \ufb01nite dimensional spaces to non-parametric kernel learning methods. We de- rive upper bounds on the generalization performance and show that the resulting convergent rates are optimal under various circumstances.",
        "bibtex": "@inproceedings{NIPS2002_25db67c5,\n author = {Zhang, Tong},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Effective Dimension and Generalization of Kernel Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/25db67c5657914454081c6a18e93d6dd-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/25db67c5657914454081c6a18e93d6dd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/25db67c5657914454081c6a18e93d6dd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 111436,
        "gs_citation": 63,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1460701240357578803&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "IBM T.J. Watson Research Center",
        "aff_domain": "watson.ibm.com",
        "email": "watson.ibm.com",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "IBM",
        "aff_unique_dep": "Research Center",
        "aff_unique_url": "https://www.ibm.com/research/watson",
        "aff_unique_abbr": "IBM",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "T.J. Watson",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "218559f267",
        "title": "Efficient Learning Equilibrium",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/0d73a25092e5c1c9769a9f3255caa65a-Abstract.html",
        "author": "Ronen I. Brafman; Moshe Tennenholtz",
        "abstract": "We introduce efficient learning  equilibrium  (ELE), a normative ap(cid:173) proach to learning in non cooperative settings.  In ELE, the learn(cid:173) ing  algorithms  themselves  are  required  to  be  in  equilibrium.  In  addition,  the  learning  algorithms  arrive  at  a  desired  value  after  polynomial time,  and deviations from a prescribed ELE become ir(cid:173) rational after  polynomial time.  We  prove the existence of an ELE  in  the  perfect  monitoring  setting,  where  the  desired  value  is  the  expected payoff in a  Nash equilibrium.  We  also show that an ELE  does  not  always  exist  in  the  imperfect  monitoring  case.  Yet,  it  exists  in  the  special  case  of common-interest  games.  Finally,  we  extend our results to general stochastic games.",
        "bibtex": "@inproceedings{NIPS2002_0d73a250,\n author = {Brafman, Ronen and Tennenholtz, Moshe},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Efficient Learning Equilibrium},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/0d73a25092e5c1c9769a9f3255caa65a-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/0d73a25092e5c1c9769a9f3255caa65a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/0d73a25092e5c1c9769a9f3255caa65a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1545986,
        "gs_citation": 93,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15301140471877806786&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Computer Science Department, Ben-Gurion University, Beer-Sheva, Israel + Faculty of Industrial Engineering and Management, Technion, Haifa 32000, Israel; Computer Science Department, Stanford University, Stanford, CA 94305",
        "aff_domain": "cs.bgu.ac.il;robotics.stanford.edu",
        "email": "cs.bgu.ac.il;robotics.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2",
        "aff_unique_norm": "Computer Science Department, Ben-Gurion University, Beer-Sheva, Israel;Faculty of Industrial Engineering and Management, Technion, Haifa 32000, Israel;Computer Science Department, Stanford University, Stanford, CA 94305",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "fd351747a8",
        "title": "Evidence Optimization Techniques for Estimating Stimulus-Response Functions",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/229754d7799160502a143a72f6789927-Abstract.html",
        "author": "Maneesh Sahani; Jennifer F. Linden",
        "abstract": "An essential step in understanding the function of sensory nervous sys- tems is to characterize as accurately as possible the stimulus-response function (SRF) of the neurons that relay and process sensory informa- tion. One increasingly common experimental approach is to present a rapidly varying complex stimulus to the animal while recording the re- sponses of one or more neurons, and then to directly estimate a func- tional transformation of the input that accounts for the neuronal \ufb01ring. The estimation techniques usually employed, such as Wiener \ufb01ltering or other correlation-based estimation of the Wiener or Volterra kernels, are equivalent to maximum likelihood estimation in a Gaussian-output-noise regression model. We explore the use of Bayesian evidence-optimization techniques to condition these estimates. We show that by learning hyper- parameters that control the smoothness and sparsity of the transfer func- tion it is possible to improve dramatically the quality of SRF estimates, as measured by their success in predicting responses to novel input.",
        "bibtex": "@inproceedings{NIPS2002_229754d7,\n author = {Sahani, Maneesh and Linden, Jennifer},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Evidence Optimization Techniques for Estimating Stimulus-Response Functions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/229754d7799160502a143a72f6789927-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/229754d7799160502a143a72f6789927-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/229754d7799160502a143a72f6789927-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 117572,
        "gs_citation": 148,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13679712631330981607&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Gatsby Unit, UCL; Keck Center, UCSF",
        "aff_domain": "gatsby.ucl.ac.uk;phy.ucsf.edu",
        "email": "gatsby.ucl.ac.uk;phy.ucsf.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University College London;Keck Center, UCSF",
        "aff_unique_dep": "Gatsby Unit;",
        "aff_unique_url": "https://www.ucl.ac.uk;",
        "aff_unique_abbr": "UCL;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United Kingdom;"
    },
    {
        "id": "f088fb8921",
        "title": "Exact MAP Estimates by (Hyper)tree Agreement",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/fea9c11c4ad9a395a636ed944a28b51a-Abstract.html",
        "author": "Martin J. Wainwright; Tommi S. Jaakkola; Alan S. Willsky",
        "abstract": "We describe a method for computing provably exact maximum a poste- riori (MAP) estimates for a subclass of problems on graphs with cycles. The basic idea is to represent the original problem on the graph with cy- cles as a convex combination of tree-structured problems. A convexity argument then guarantees that the optimal value of the original problem (i.e., the log probability of the MAP assignment) is upper bounded by the combined optimal values of the tree problems. We prove that this upper bound is met with equality if and only if the tree problems share an opti- mal con\ufb01guration in common. An important implication is that any such shared con\ufb01guration must also be the MAP con\ufb01guration for the original problem. Next we develop a tree-reweighted max-product algorithm for attempting to \ufb01nd convex combinations of tree-structured problems that share a common optimum. We give necessary and suf\ufb01cient conditions for a \ufb01xed point to yield the exact MAP estimate. An attractive feature of our analysis is that it generalizes naturally to convex combinations of hypertree-structured distributions.",
        "bibtex": "@inproceedings{NIPS2002_fea9c11c,\n author = {Wainwright, Martin J and Jaakkola, Tommi and Willsky, Alan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Exact MAP Estimates by (Hyper)tree Agreement},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/fea9c11c4ad9a395a636ed944a28b51a-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/fea9c11c4ad9a395a636ed944a28b51a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/fea9c11c4ad9a395a636ed944a28b51a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 141652,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3607226157900586266&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of EECS, UC Berkeley; Department of EECS, Massachusetts Institute of Technology; Department of EECS, Massachusetts Institute of Technology",
        "aff_domain": "eecs.berkeley.edu;mit.edu;mit.edu",
        "email": "eecs.berkeley.edu;mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of California, Berkeley;Massachusetts Institute of Technology",
        "aff_unique_dep": "Department of Electrical Engineering and Computer Sciences;Department of Electrical Engineering and Computer Science",
        "aff_unique_url": "https://www.berkeley.edu;https://web.mit.edu",
        "aff_unique_abbr": "UC Berkeley;MIT",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Berkeley;Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e3909bd106",
        "title": "Expected and Unexpected Uncertainty: ACh and NE in the Neocortex",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/758a06618c69880a6cee5314ee42d52f-Abstract.html",
        "author": "Peter Dayan; Angela J. Yu",
        "abstract": "Inference and adaptation in noisy and changing, rich sensory environ- ments are rife with a variety of speci\ufb01c sorts of variability. Experimental and theoretical studies suggest that these different forms of variability play different behavioral, neural and computational roles, and may be reported by different (notably neuromodulatory) systems. Here, we re- \ufb01ne our previous theory of acetylcholine\u2019s role in cortical inference in the (oxymoronic) terms of expected uncertainty, and advocate a theory for norepinephrine in terms of unexpected uncertainty. We suggest that norepinephrine reports the radical divergence of bottom-up inputs from prevailing top-down interpretations, to in\ufb02uence inference and plasticity. We illustrate this proposal using an adaptive factor analysis model.",
        "bibtex": "@inproceedings{NIPS2002_758a0661,\n author = {Dayan, Peter and Yu, Angela J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Expected and Unexpected Uncertainty: ACh and NE in the Neocortex},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/758a06618c69880a6cee5314ee42d52f-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/758a06618c69880a6cee5314ee42d52f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/758a06618c69880a6cee5314ee42d52f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 168325,
        "gs_citation": 176,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12306678140911415245&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "5e4d355f11",
        "title": "Exponential Family PCA for Belief Compression in POMDPs",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/a11f9e533f28593768ebf87075ab34f2-Abstract.html",
        "author": "Nicholas Roy; Geoffrey J. Gordon",
        "abstract": "Geoffrey Gordon",
        "bibtex": "@inproceedings{NIPS2002_a11f9e53,\n author = {Roy, Nicholas and Gordon, Geoffrey J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Exponential Family PCA for Belief Compression in POMDPs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/a11f9e533f28593768ebf87075ab34f2-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/a11f9e533f28593768ebf87075ab34f2-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/a11f9e533f28593768ebf87075ab34f2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 94350,
        "gs_citation": 182,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16438449579979768740&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Robotics Institute, Carnegie Mellon University, Pittsburgh, PA 15213; Department of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213",
        "aff_domain": "ri.cmu.edu;cs.cmu.edu",
        "email": "ri.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Robotics Institute",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c02d30d59d",
        "title": "Extracting Relevant Structures with Side Information",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/04048aeca2c0f5d84639358008ed2ae7-Abstract.html",
        "author": "Gal Chechik; Naftali Tishby",
        "abstract": "The problem of extracting the relevant aspects of data, in face of multiple con\ufb02icting structures, is inherent to modeling of complex data. Extract- ing structure in one random variable that is relevant for another variable has been principally addressed recently via the information bottleneck method [15]. However, such auxiliary variables often contain more in- formation than is actually required due to structures that are irrelevant for the task. In many other cases it is in fact easier to specify what is irrelevant than what is, for the task at hand. Identifying the relevant structures, however, can thus be considerably improved by also mini- mizing the information about another, irrelevant, variable. In this paper we give a general formulation of this problem and derive its formal, as well as algorithmic, solution. Its operation is demonstrated in a synthetic example and in two real world problems in the context of text categoriza- tion and face images. While the original information bottleneck problem is related to rate distortion theory, with the distortion measure replaced by the relevant information, extracting relevant features while removing irrelevant ones is related to rate distortion with side information.",
        "bibtex": "@inproceedings{NIPS2002_04048aec,\n author = {Chechik, Gal and Tishby, Naftali},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Extracting Relevant Structures with Side Information},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/04048aeca2c0f5d84639358008ed2ae7-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/04048aeca2c0f5d84639358008ed2ae7-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/04048aeca2c0f5d84639358008ed2ae7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 131483,
        "gs_citation": 131,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1586595417697391484&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "School of Computer Science and Engineering and The Interdisciplinary Center for Neural Computation, The Hebrew University of Jerusalem, 91904, Israel; School of Computer Science and Engineering and The Interdisciplinary Center for Neural Computation, The Hebrew University of Jerusalem, 91904, Israel",
        "aff_domain": "cs.huji.ac.il;cs.huji.ac.il",
        "email": "cs.huji.ac.il;cs.huji.ac.il",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "School of Computer Science and Engineering and The Interdisciplinary Center for Neural Computation, The Hebrew University of Jerusalem, 91904, Israel",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "c07e22e93f",
        "title": "Fast Exact Inference with a Factored Model for Natural Language Parsing",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/6c97cd07663b099253bc569fe8d342bb-Abstract.html",
        "author": "Dan Klein; Christopher D. Manning",
        "abstract": "We present a novel generative model for natural language tree structures in which semantic (lexical dependency) and syntactic (PCFG) structures are scored with separate models. This factorization provides concep- tual simplicity, straightforward opportunities for separately improving the component models, and a level of performance comparable to simi- lar, non-factored models. Most importantly, unlike other modern parsing models, the factored model admits an extremely effective A* parsing al- gorithm, which enables ef\ufb01cient, exact inference.",
        "bibtex": "@inproceedings{NIPS2002_6c97cd07,\n author = {Klein, Dan and Manning, Christopher D},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Fast Exact Inference with a Factored Model for Natural Language Parsing},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/6c97cd07663b099253bc569fe8d342bb-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/6c97cd07663b099253bc569fe8d342bb-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/6c97cd07663b099253bc569fe8d342bb-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 69759,
        "gs_citation": 1130,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=836115945608013211&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 29,
        "aff": "Department of Computer Science, Stanford University; Department of Computer Science, Stanford University",
        "aff_domain": "cs.stanford.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a61ad18a26",
        "title": "Fast Kernels for String and Tree Matching",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/743394beff4b1282ba735e5e3723ed74-Abstract.html",
        "author": "Alex J. Smola; S.v.n. Vishwanathan",
        "abstract": "In this paper we present a new algorithm suitable for matching discrete  objects such as strings and trees in linear time, thus obviating dynarrtic  programming with quadratic time complexity. Furthermore, prediction  cost in many cases can be reduced to linear cost in the length of the se(cid:173) quence to be classified, regardless of the number of support vectors. This  improvement on the currently available algorithms makes string kernels  a viable alternative for the practitioner.",
        "bibtex": "@inproceedings{NIPS2002_743394be,\n author = {Smola, Alex and Vishwanathan, S.v.n.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Fast Kernels for String and Tree Matching},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/743394beff4b1282ba735e5e3723ed74-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/743394beff4b1282ba735e5e3723ed74-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/743394beff4b1282ba735e5e3723ed74-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1649583,
        "gs_citation": 479,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6389312066494765679&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Dept. of Compo Sci. & Automation, Indian Institute of Science, Bangalore, 560012, India; Machine Learning Group, RSISE, Australian National University, Canberra, ACT 0200, Australia",
        "aff_domain": "csa.iisc.ernet.in;anu.edu.au",
        "email": "csa.iisc.ernet.in;anu.edu.au",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Dept. of Compo Sci. & Automation, Indian Institute of Science, Bangalore, 560012, India;Machine Learning Group, RSISE, Australian National University, Canberra, ACT 0200, Australia",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "53750cf220",
        "title": "Fast Sparse Gaussian Process Methods: The Informative Vector Machine",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/d4dd111a4fd973394238aca5c05bebe3-Abstract.html",
        "author": "Neil D. Lawrence; Matthias Seeger; Ralf Herbrich",
        "abstract": "We present a framework for sparse Gaussian process (GP) methods which uses forward selection with criteria based on information- theoretic principles, previously suggested for active learning. Our goal is not only to learn d{sparse predictors (which can be evalu- ated in O(d) rather than O(n), d (cid:28) n, n the number of training points), but also to perform training under strong restrictions on time and memory requirements. The scaling of our method is at most O(n (cid:1) d2), and in large real-world classi(cid:12)cation experiments we show that it can match prediction performance of the popular support vector machine (SVM), yet can be signi(cid:12)cantly faster in training. In contrast to the SVM, our approximation produces esti- mates of predictive probabilities (\u2018error bars\u2019), allows for Bayesian model selection and is less complex in implementation.",
        "bibtex": "@inproceedings{NIPS2002_d4dd111a,\n author = {Lawrence, Neil and Seeger, Matthias and Herbrich, Ralf},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Fast Sparse Gaussian Process Methods: The Informative Vector Machine},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/d4dd111a4fd973394238aca5c05bebe3-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/d4dd111a4fd973394238aca5c05bebe3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/d4dd111a4fd973394238aca5c05bebe3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 122010,
        "gs_citation": 767,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11937243543322880392&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 21,
        "aff": "University of Sheffield; University of Edinburgh; Microsoft Research Ltd",
        "aff_domain": "dcs.shef.ac.uk;dai.ed.ac.uk;microsoft.com",
        "email": "dcs.shef.ac.uk;dai.ed.ac.uk;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University of Sheffield;University of Edinburgh;Microsoft Research",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.sheffield.ac.uk;https://www.ed.ac.uk;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "Sheffield;Edinburgh;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "3f2334dbe6",
        "title": "Fast Transformation-Invariant Factor Analysis",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/05a70454516ecd9194c293b0e415777f-Abstract.html",
        "author": "Anitha Kannan; Nebojsa Jojic; Brendan Frey",
        "abstract": "Dimensionality reduction techniques such as principal component analy- sis and factor analysis are used to discover a linear mapping between high dimensional data samples and points in a lower dimensional subspace. In [6], Jojic and Frey introduced mixture of transformation-invariant component analyzers (MTCA) that can account for global transforma- tions such as translations and rotations, perform clustering and learn lo- cal appearance deformations by dimensionality reduction. However, due to enormous computational requirements of the EM algorithm for learn- ing the model, O( is the dimensionality of a data sample, MTCA was not practical for most applications. In this paper, we demon- strate how fast Fourier transforms can reduce the computation to the or- . With this speedup, we show the effectiveness of MTCA der of in various applications - tracking, video textures, clustering video se- quences, object recognition, and object detection in images.",
        "bibtex": "@inproceedings{NIPS2002_05a70454,\n author = {Kannan, Anitha and Jojic, Nebojsa and Frey, Brendan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Fast Transformation-Invariant Factor Analysis},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/05a70454516ecd9194c293b0e415777f-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/05a70454516ecd9194c293b0e415777f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/05a70454516ecd9194c293b0e415777f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 458134,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9472541631925850791&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "University of Toronto, Toronto, Canada; Microsoft Research, Redmond, WA, USA; University of Toronto, Toronto, Canada",
        "aff_domain": "psi.utoronto.ca;microsoft.com;psi.utoronto.ca",
        "email": "psi.utoronto.ca;microsoft.com;psi.utoronto.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Toronto;Microsoft Research",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.utoronto.ca;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "U of T;MSR",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Toronto;Redmond",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Canada;United States"
    },
    {
        "id": "83352df3a5",
        "title": "Feature Selection and Classification on Matrix Data: From Large Margins to Small Covering Numbers",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/761c7920f470038d4c8a619c79eddd62-Abstract.html",
        "author": "Sepp Hochreiter; Klaus Obermayer",
        "abstract": "We investigate the problem of learning a classi\ufb02cation task for datasets which are described by matrices. Rows and columns of these matrices correspond to objects, where row and column ob- jects may belong to di\ufb01erent sets, and the entries in the matrix express the relationships between them. We interpret the matrix el- ements as being produced by an unknown kernel which operates on object pairs and we show that - under mild assumptions - these ker- nels correspond to dot products in some (unknown) feature space. Minimizing a bound for the generalization error of a linear classi- \ufb02er which has been obtained using covering numbers we derive an objective function for model selection according to the principle of structural risk minimization. The new objective function has the advantage that it allows the analysis of matrices which are not pos- itive de\ufb02nite, and not even symmetric or square. We then consider the case that row objects are interpreted as features. We suggest an additional constraint, which imposes sparseness on the row objects and show, that the method can then be used for feature selection. Finally, we apply this method to data obtained from DNA microar- rays, where \\column\" objects correspond to samples, \\row\" objects correspond to genes and matrix elements correspond to expression levels. Benchmarks are conducted using standard one-gene classi\ufb02- cation and support vector machines and K-nearest neighbors after standard feature selection. Our new method extracts a sparse set of genes and provides superior classi\ufb02cation results.",
        "bibtex": "@inproceedings{NIPS2002_761c7920,\n author = {Hochreiter, Sepp and Obermayer, Klaus},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Feature Selection and Classification on Matrix Data: From Large Margins to Small Covering Numbers},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/761c7920f470038d4c8a619c79eddd62-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/761c7920f470038d4c8a619c79eddd62-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/761c7920f470038d4c8a619c79eddd62-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 108289,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11351770213106791205&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Electrical Engineering and Computer Science, Technische Universit\u00c4 at Berlin; Department of Electrical Engineering and Computer Science, Technische Universit\u00c4 at Berlin",
        "aff_domain": "cs.tu-berlin.de;cs.tu-berlin.de",
        "email": "cs.tu-berlin.de;cs.tu-berlin.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Department of Electrical Engineering and Computer Science, Technische Universit\u00c4 at Berlin",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "814a4a5469",
        "title": "Feature Selection by Maximum Marginal Diversity",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/bd0cc810b580b35884bd9df37c0e8b0f-Abstract.html",
        "author": "Nuno Vasconcelos",
        "abstract": "We address the question of feature selection in the context of visual recognition. It is shown that, besides ef\ufb01cient from a computational standpoint, the infomax principle is nearly optimal in the minimum Bayes error sense. The concept of marginal diversity is introduced, lead- ing to a generic principle for feature selection (the principle of maximum marginal diversity) of extreme computational simplicity. The relation- ships between infomax and the maximization of marginal diversity are identi\ufb01ed, uncovering the existence of a family of classi\ufb01cation proce- dures for which near optimal (in the Bayes error sense) feature selection does not require combinatorial search. Examination of this family in light of recent studies on the statistics of natural images suggests that visual recognition problems are a subset of it.",
        "bibtex": "@inproceedings{NIPS2002_bd0cc810,\n author = {Vasconcelos, Nuno},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Feature Selection by Maximum Marginal Diversity},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/bd0cc810b580b35884bd9df37c0e8b0f-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/bd0cc810b580b35884bd9df37c0e8b0f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/bd0cc810b580b35884bd9df37c0e8b0f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1818565,
        "gs_citation": 124,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15581747876309036869&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 20,
        "aff": "Department of Electrical and Computer Engineering, University of California, San Diego",
        "aff_domain": "media.mit.edu",
        "email": "media.mit.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "124cde3297",
        "title": "Feature Selection in Mixture-Based Clustering",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/e58aea67b01fa747687f038dfde066f6-Abstract.html",
        "author": "Martin H. Law; Anil K. Jain; M\u00e1rio Figueiredo",
        "abstract": "There exist many approaches to clustering, but the important issue of feature selection, i.e., selecting the data attributes that are relevant for clustering, is rarely addressed. Feature selection for clustering is dif\ufb01cult due to the absence of class labels. We propose two approaches to feature selection in the context of Gaussian mixture-based clustering. In the \ufb01rst one, instead of making hard selections, we estimate feature saliencies. An expectation-maximization (EM) algorithm is derived for this task. The second approach extends Koller and Sahami\u2019s mutual-information- based feature relevance criterion to the unsupervised case. Feature selec- tion is then carried out by a backward search scheme. This scheme can be classi\ufb01ed as a \u201cwrapper\u201d, since it wraps mixture estimation in an outer layer that performs feature selection. Experimental results on synthetic and real data show that both methods have promising performance.",
        "bibtex": "@inproceedings{NIPS2002_e58aea67,\n author = {Law, Martin and Jain, Anil and Figueiredo, M\\'{a}rio},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Feature Selection in Mixture-Based Clustering},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/e58aea67b01fa747687f038dfde066f6-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/e58aea67b01fa747687f038dfde066f6-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/e58aea67b01fa747687f038dfde066f6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 113842,
        "gs_citation": 137,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13455740987521771758&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Dept. of Computer Science and Eng., Michigan State University, East Lansing, MI 48824, U.S.A.; Dept. of Computer Science and Eng., Michigan State University, East Lansing, MI 48824, U.S.A.; Instituto de Telecomunica\u00e7\u00f5es, Instituto Superior T\u00e9cnico, 1049-001 Lisboa, Portugal",
        "aff_domain": "cse.msu.edu;cse.msu.edu;lx.it.pt",
        "email": "cse.msu.edu;cse.msu.edu;lx.it.pt",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Dept. of Computer Science and Eng., Michigan State University, East Lansing, MI 48824, U.S.A.;Instituto de Telecomunica\u00e7\u00f5es, Instituto Superior T\u00e9cnico, 1049-001 Lisboa, Portugal",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "4f39807375",
        "title": "Field-Programmable Learning Arrays",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/3dd9424294b0292b6e89ea2bba2e1144-Abstract.html",
        "author": "Seth Bridges; Miguel Figueroa; Chris Diorio; David Hsu",
        "abstract": "This paper introduces the Field-Programmable Learning Array, a new paradigm for rapid prototyping of learning primitives and machine- learning algorithms in silicon. The FPLA is a mixed-signal counterpart to the all-digital Field-Programmable Gate Array in that it enables rapid prototyping of algorithms in hardware. Unlike the FPGA, the FPLA is targeted directly for machine learning by providing local, parallel, on- line analog learning using \ufb02oating-gate MOS synapse transistors. We present a prototype FPLA chip comprising an array of recon\ufb01gurable computational blocks and local interconnect. We demonstrate the via- bility of this architecture by mapping several learning circuits onto the prototype chip.",
        "bibtex": "@inproceedings{NIPS2002_3dd94242,\n author = {Bridges, Seth and Figueroa, Miguel and Diorio, Chris and Hsu, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Field-Programmable Learning Arrays},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/3dd9424294b0292b6e89ea2bba2e1144-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/3dd9424294b0292b6e89ea2bba2e1144-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/3dd9424294b0292b6e89ea2bba2e1144-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 108045,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4959964202972301036&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science and Engineering, University of Washington; Department of Computer Science and Engineering, University of Washington; Department of Computer Science and Engineering, University of Washington; Department of Computer Science and Engineering, University of Washington",
        "aff_domain": "cs.washington.edu;cs.washington.edu;cs.washington.edu;cs.washington.edu",
        "email": "cs.washington.edu;cs.washington.edu;cs.washington.edu;cs.washington.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Washington",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.washington.edu",
        "aff_unique_abbr": "UW",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Seattle",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "943951e31a",
        "title": "FloatBoost Learning for Classification",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/eb76c035d5d0a2bd2a0d0834b93c9c26-Abstract.html",
        "author": "Stan Z. Li; Zhenqiu Zhang; Heung-yeung Shum; Hongjiang Zhang",
        "abstract": "AdaBoost [3] minimizes an upper error bound which is an exponential function of the margin on the training set [14]. However, the ultimate goal in applications of pattern classi\ufb01cation is always minimum error rate. On the other hand, AdaBoost needs an effective procedure for learning weak classi\ufb01ers, which by itself is dif\ufb01cult especially for high dimensional data. In this paper, we present a novel procedure, called FloatBoost, for learning a better boosted classi\ufb01er. FloatBoost uses a backtrack mechanism after each iteration of AdaBoost to remove weak classi\ufb01ers which cause higher error rates. The resulting \ufb02oat-boosted classi\ufb01er consists of fewer weak classi\ufb01ers yet achieves lower error rates than AdaBoost in both training and test. We also propose a statistical model for learning weak classi\ufb01ers, based on a stagewise approximation of the posterior using an overcomplete set of scalar features. Experi- mental comparisons of FloatBoost and AdaBoost are provided through a dif\ufb01cult classi\ufb01cation problem, face detection, where the goal is to learn from training examples a highly nonlinear classi\ufb01er to differentiate be- tween face and nonface patterns in a high dimensional space. The results clearly demonstrate the promises made by FloatBoost over AdaBoost.",
        "bibtex": "@inproceedings{NIPS2002_eb76c035,\n author = {Li, Stan and Zhang, Zhenqiu and Shum, Heung-yeung and Zhang, Hongjiang},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {FloatBoost Learning for Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/eb76c035d5d0a2bd2a0d0834b93c9c26-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/eb76c035d5d0a2bd2a0d0834b93c9c26-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/eb76c035d5d0a2bd2a0d0834b93c9c26-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 129287,
        "gs_citation": 119,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5418145231255770578&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Microsoft Research Asia, Beijing, China; Institute of Automation, CAS, Beijing, China; Microsoft Research Asia, Beijing, China; Microsoft Research Asia, Beijing, China",
        "aff_domain": "microsoft.com; ; ; ",
        "email": "microsoft.com; ; ; ",
        "github": "",
        "project": "http://research.microsoft.com/",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Microsoft Research Asia;Institute of Automation, CAS, Beijing, China",
        "aff_unique_dep": "Research;",
        "aff_unique_url": "https://www.microsoft.com/en-us/research/group/asia;",
        "aff_unique_abbr": "MSRA;",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China;"
    },
    {
        "id": "a6b57ef413",
        "title": "Forward-Decoding Kernel-Based Phone Recognition",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/1a3f91fead97497b1a96d6104ad339f6-Abstract.html",
        "author": "Shantanu Chakrabartty; Gert Cauwenberghs",
        "abstract": "Forward decoding kernel machines (FDKM) combine large-margin clas(cid:173) sifiers  with  hidden  Markov  models  (HMM)  for  maximum  a  posteriori  (MAP)  adaptive sequence estimation.  State  transitions  in the  sequence  are conditioned on observed data using a kernel-based probability model  trained with a recursive scheme that deals effectively with noisy and par(cid:173) tially labeled data.  Training over very large data sets is accomplished us(cid:173) ing a sparse probabilistic support vector machine (SVM) model based on  quadratic entropy, and an  on-line stochastic steepest descent algorithm.  For speaker-independent continuous phone recognition,  FDKM trained  over 177 ,080 samples of the TlMIT database achieves 80.6% recognition  accuracy over the full  test set, without use  of a prior phonetic language  model.",
        "bibtex": "@inproceedings{NIPS2002_1a3f91fe,\n author = {Chakrabartty, Shantanu and Cauwenberghs, Gert},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Forward-Decoding Kernel-Based Phone Recognition},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/1a3f91fead97497b1a96d6104ad339f6-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/1a3f91fead97497b1a96d6104ad339f6-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/1a3f91fead97497b1a96d6104ad339f6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1428948,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9079498423860617770&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "602db62e83",
        "title": "Fractional Belief Propagation",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/35936504a37d53e03abdfbc7318d9ec7-Abstract.html",
        "author": "Wim Wiegerinck; Tom Heskes",
        "abstract": "We consider loopy belief propagation for approximate inference in prob- abilistic graphical models. A limitation of the standard algorithm is that clique marginals are computed as if there were no loops in the graph. To overcome this limitation, we introduce fractional belief propagation. Fractional belief propagation is formulated in terms of a family of ap- proximate free energies, which includes the Bethe free energy and the naive mean-\ufb01eld free as special cases. Using the linear response correc- tion of the clique marginals, the scale parameters can be tuned. Simula- tion results illustrate the potential merits of the approach.",
        "bibtex": "@inproceedings{NIPS2002_35936504,\n author = {Wiegerinck, Wim and Heskes, Tom},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Fractional Belief Propagation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/35936504a37d53e03abdfbc7318d9ec7-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/35936504a37d53e03abdfbc7318d9ec7-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/35936504a37d53e03abdfbc7318d9ec7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 100923,
        "gs_citation": 122,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13248978699829868376&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "b6be38b567",
        "title": "Gaussian Process Priors with Uncertain Inputs Application to Multiple-Step Ahead Time Series Forecasting",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/f3ac63c91272f19ce97c7397825cc15f-Abstract.html",
        "author": "Agathe Girard; Carl Edward Rasmussen; Joaquin Qui\u00f1onero Candela; Roderick Murray-Smith",
        "abstract": "We consider the problem of multi-step ahead prediction in time series analysis using the non-parametric Gaussian process model.  -step ahead forecasting of a discrete-time non-linear dynamic system can be per- formed by doing repeated one-step ahead predictions. For a state-space at time model of the form is based on the point estimates of the previous outputs. In this pa-   per, we show how, using an analytical Gaussian approximation, we can formally incorporate the uncertainty about intermediate regressor values, thus updating the uncertainty on the current prediction.",
        "bibtex": "@inproceedings{NIPS2002_f3ac63c9,\n author = {Girard, Agathe and Rasmussen, Carl and Candela, Joaquin Qui\\~{n}onero and Murray-Smith, Roderick},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Gaussian Process Priors with Uncertain Inputs Application to Multiple-Step Ahead Time Series Forecasting},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/f3ac63c91272f19ce97c7397825cc15f-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/f3ac63c91272f19ce97c7397825cc15f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/f3ac63c91272f19ce97c7397825cc15f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 152966,
        "gs_citation": 622,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6973287741393133962&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 30,
        "aff": "Department of Computing Science, University of Glasgow; Gatsby Unit, University College London; Informatics and Mathematical Modelling, Technical University of Denmark; Department of Computing Science, University of Glasgow + Hamilton Institute, National University of Ireland, Maynooth",
        "aff_domain": "dcs.gla.ac.uk;gatsby.ucl.ac.uk;imm.dtu.dk;dcs.gla.ac.uk",
        "email": "dcs.gla.ac.uk;gatsby.ucl.ac.uk;imm.dtu.dk;dcs.gla.ac.uk",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0+3",
        "aff_unique_norm": "University of Glasgow;University College London;Informatics and Mathematical Modelling, Technical University of Denmark;Hamilton Institute, National University of Ireland, Maynooth",
        "aff_unique_dep": "Department of Computing Science;Gatsby Unit;;",
        "aff_unique_url": "https://www.gla.ac.uk;https://www.ucl.ac.uk;;",
        "aff_unique_abbr": "UofG;UCL;;",
        "aff_campus_unique_index": "1;",
        "aff_campus_unique": ";London",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom;"
    },
    {
        "id": "f076a36337",
        "title": "Generalized\u00b2 Linear\u00b2 Models",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/7dc1c7653ac42a05642a667959c12239-Abstract.html",
        "author": "Geoffrey J. Gordon",
        "abstract": "We  introduce the Generalized2  Linear2  Model, a statistical estima(cid:173) tor which combines features of nonlinear regression and factor anal(cid:173) ysis.  A  (GL)2M  approximately  decomposes  a  rectangular  matrix  X  into  a  simpler  representation  j(g(A)h(B)).  Here  A  and  Bare  low-rank matrices,  while  j,  g,  and  h  are  link  functions.  (GL)2Ms  include  many  useful  models  as  special  cases,  including  principal  components analysis,  exponential-family peA, the infomax formu(cid:173) lation  of independent  components  analysis,  linear  regression,  and  generalized  linear  models.  They  also  include  new  and  interesting  special  cases,  one  of which  we  describe  below.  We  also  present  an  iterative  procedure  which  optimizes  the  parameters of a  (GL)2M.  This  procedure  reduces  to  well-known  algorithms  for  some  of the  special cases listed above;  for  other special cases,  it  is  new.",
        "bibtex": "@inproceedings{NIPS2002_7dc1c765,\n author = {Gordon, Geoffrey J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Generalized{^2} Linear{^2} Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/7dc1c7653ac42a05642a667959c12239-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/7dc1c7653ac42a05642a667959c12239-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/7dc1c7653ac42a05642a667959c12239-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1507875,
        "gs_citation": 87,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11423451403119038158&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "",
        "aff_domain": "es.emu.edu",
        "email": "es.emu.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "65bf72aa0d",
        "title": "Global Versus Local Methods in Nonlinear Dimensionality Reduction",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/5d6646aad9bcc0be55b2c82f69750387-Abstract.html",
        "author": "Vin D. Silva; Joshua B. Tenenbaum",
        "abstract": "Recently proposed algorithms for nonlinear dimensionality reduction fall broadly into two categories which have different advantages and disad- vantages: global (Isomap [1]), and local (Locally Linear Embedding [2], Laplacian Eigenmaps [3]). We present two variants of Isomap which combine the advantages of the global approach with what have previ- ously been exclusive advantages of local methods: computational spar- sity and the ability to invert conformal maps.",
        "bibtex": "@inproceedings{NIPS2002_5d6646aa,\n author = {Silva, Vin and Tenenbaum, Joshua},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Global Versus Local Methods in Nonlinear Dimensionality Reduction},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/5d6646aad9bcc0be55b2c82f69750387-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/5d6646aad9bcc0be55b2c82f69750387-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/5d6646aad9bcc0be55b2c82f69750387-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1035300,
        "gs_citation": 1306,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15390822387094332642&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Department of Mathematics, Stanford University, Stanford, CA 94305; Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology, Cambridge, MA 02139",
        "aff_domain": "math.stanford.edu;ai.mit.edu",
        "email": "math.stanford.edu;ai.mit.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Stanford University;Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology, Cambridge, MA 02139",
        "aff_unique_dep": "Department of Mathematics;",
        "aff_unique_url": "https://www.stanford.edu;",
        "aff_unique_abbr": "Stanford;",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "a8aebf61df",
        "title": "Going Metric: Denoising Pairwise Data",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/02e656adee09f8394b402d9958389b7d-Abstract.html",
        "author": "Volker Roth; Julian Laub; Klaus-Robert M\u00fcller; Joachim M. Buhmann",
        "abstract": "Pairwise  data in  empirical  sciences  typically  violate  metricity,  ei(cid:173) ther  due  to  noise  or  due  to  fallible  estimates,  and  therefore  are  hard  to  analyze  by  conventional  machine  learning  technology.  In  this  paper  we  therefore  study  ways  to  work  around  this  problem.  First,  we  present  an  alternative  embedding  to  multi-dimensional  scaling  (MDS)  that  allows  us  to  apply  a  variety  of classical  ma(cid:173) chine learning and signal processing algorithms.  The class of pair(cid:173) wise grouping algorithms which share the shift-invariance property  is  statistically  invariant  under  this  embedding  procedure,  leading  to  identical assignments  of objects to clusters.  Based on this  new  vectorial  representation,  denoising  methods  are  applied  in  a  sec(cid:173) ond  step.  Both steps  provide a  theoretically  well  controlled setup  to  translate  from  pairwise  data  to  the  respective  denoised  met(cid:173) ric  representation.  We  demonstrate the practical usefulness of our  theoretical  reasoning by  discovering structure in  protein sequence  data bases, visibly improving performance upon existing automatic  methods.",
        "bibtex": "@inproceedings{NIPS2002_02e656ad,\n author = {Roth, Volker and Laub, Julian and M\\\"{u}ller, Klaus-Robert and Buhmann, Joachim},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Going Metric: Denoising Pairwise Data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/02e656adee09f8394b402d9958389b7d-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/02e656adee09f8394b402d9958389b7d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/02e656adee09f8394b402d9958389b7d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1621671,
        "gs_citation": 93,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=21450468271941161&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Informatik III, University of Bonn + Fraunhofer FIRST.IDA; Informatik III, University of Bonn + Fraunhofer FIRST.IDA; Informatik III, University of Bonn; Fraunhofer FIRST.IDA + University of Potsdam",
        "aff_domain": "roth\u00a9cs.uni-bonn.de; jlaub\u00a9first.fhg.de; jb\u00a9cs.uni-bonn.de; klaus\u00a9first.fhg.de",
        "email": "roth\u00a9cs.uni-bonn.de; jlaub\u00a9first.fhg.de; jb\u00a9cs.uni-bonn.de; klaus\u00a9first.fhg.de",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;0;1+2",
        "aff_unique_norm": "Informatik III, University of Bonn;Fraunhofer Institute for Software and Systems Engineering;University of Potsdam",
        "aff_unique_dep": ";FIRST.IDA;",
        "aff_unique_url": ";https://www.first.ida.fraunhofer.de/;https://www.uni-potsdam.de",
        "aff_unique_abbr": ";Fraunhofer FIRST.IDA;UP",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1;1+1",
        "aff_country_unique": ";Germany"
    },
    {
        "id": "b21e9d5578",
        "title": "Graph-Driven Feature Extraction From Microarray Data Using Diffusion Kernels and Kernel CCA",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/074177d3eb6371e32c16c55a3b8f706b-Abstract.html",
        "author": "Jean-philippe Vert; Minoru Kanehisa",
        "abstract": "We present an algorithm to extract features from high-dimensional gene expression pro\ufb01les, based on the knowledge of a graph which links to- gether genes known to participate to successive reactions in metabolic pathways. Motivated by the intuition that biologically relevant features are likely to exhibit smoothness with respect to the graph topology, the algorithm involves encoding the graph and the set of expression pro- \ufb01les into kernel functions, and performing a generalized form of canoni- cal correlation analysis in the corresponding reproducible kernel Hilbert spaces. Function prediction experiments for the genes of the yeast S. Cerevisiae validate this approach by showing a consistent increase in performance when a state-of-the-art classi\ufb01er uses the vector of features instead of the original expression pro\ufb01le to predict the functional class of a gene.",
        "bibtex": "@inproceedings{NIPS2002_074177d3,\n author = {Vert, Jean-philippe and Kanehisa, Minoru},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Graph-Driven Feature Extraction From Microarray Data Using Diffusion Kernels and Kernel CCA},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/074177d3eb6371e32c16c55a3b8f706b-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/074177d3eb6371e32c16c55a3b8f706b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/074177d3eb6371e32c16c55a3b8f706b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 96554,
        "gs_citation": 141,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3568532018386830190&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Ecole des Mines de Paris; Bioinformatics Center, Kyoto University",
        "aff_domain": "mines.org;kuicr.kyoto-u.ac.jp",
        "email": "mines.org;kuicr.kyoto-u.ac.jp",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Ecole des Mines de Paris;Bioinformatics Center, Kyoto University",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "112fde66fd",
        "title": "Half-Lives of EigenFlows for Spectral Clustering",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/2056d8c1dec3d12cbce646b348d189d1-Abstract.html",
        "author": "Chakra Chennubhotla; Allan D. Jepson",
        "abstract": "Using a Markov chain perspective of spectral clustering we present an algorithm to automatically \ufb01nd the number of stable clusters in a dataset. The Markov chain\u2019s behaviour is characterized by the spectral properties of the matrix of transition probabilities, from which we derive eigen\ufb02ows along with their hal\ufb02ives. An eigen\ufb02ow describes the \ufb02ow of probabil- ity mass due to the Markov chain, and it is characterized by its eigen- value, or equivalently, by the hal\ufb02ife of its decay as the Markov chain is iterated. A ideal stable cluster is one with zero eigen\ufb02ow and in\ufb01- nite half-life. The key insight in this paper is that bottlenecks between weakly coupled clusters can be identi\ufb01ed by computing the sensitivity of the eigen\ufb02ow\u2019s hal\ufb02ife to variations in the edge weights. We propose a novel EIGENCUTS algorithm to perform clustering that removes these identi\ufb01ed bottlenecks in an iterative fashion.",
        "bibtex": "@inproceedings{NIPS2002_2056d8c1,\n author = {Chennubhotla, Chakra and Jepson, Allan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Half-Lives of EigenFlows for Spectral Clustering},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/2056d8c1dec3d12cbce646b348d189d1-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/2056d8c1dec3d12cbce646b348d189d1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/2056d8c1dec3d12cbce646b348d189d1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 232264,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15347863765788560122&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science, University of Toronto, Canada M5S 3H5; Department of Computer Science, University of Toronto, Canada M5S 3H5",
        "aff_domain": "cs.toronto.edu;cs.toronto.edu",
        "email": "cs.toronto.edu;cs.toronto.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Department of Computer Science, University of Toronto, Canada M5S 3H5",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "2eedfc2f08",
        "title": "Handling Missing Data with Variational Bayesian Learning of ICA",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/4cb811134b9d39fc3104bd06ce75abad-Abstract.html",
        "author": "Kwokleung Chan; Te-Won Lee; Terrence J. Sejnowski",
        "abstract": "Missing data is common in real-world datasets and is a problem for many estimation techniques. We have developed a variational Bayesian method to perform Independent Component Analysis (ICA) on high-dimensional data containing missing entries. Missing data are handled naturally in the Bayesian framework by integrating the generative density model. Mod- eling the distributions of the independent sources with mixture of Gaus- sians allows sources to be estimated with different kurtosis and skewness. The variational Bayesian method automatically determines the dimen- sionality of the data and yields an accurate density model for the ob- served data without over\ufb01tting problems. This allows direct probability estimation of missing values in the high dimensional space and avoids dimension reduction preprocessing which is not feasible with missing data.",
        "bibtex": "@inproceedings{NIPS2002_4cb81113,\n author = {Chan, Kwokleung and Lee, Te-Won and Sejnowski, Terrence J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Handling Missing Data with Variational Bayesian Learning of ICA},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/4cb811134b9d39fc3104bd06ce75abad-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/4cb811134b9d39fc3104bd06ce75abad-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/4cb811134b9d39fc3104bd06ce75abad-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 289045,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17665160141587625764&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "The Salk Institute, Computational Neurobiology Laboratory; The Salk Institute, Computational Neurobiology Laboratory; The Salk Institute, Computational Neurobiology Laboratory",
        "aff_domain": "salk.edu;salk.edu;salk.edu",
        "email": "salk.edu;salk.edu;salk.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "The Salk Institute, Computational Neurobiology Laboratory",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "e587709e2b",
        "title": "Hidden Markov Model of Cortical Synaptic Plasticity: Derivation of the Learning Rule",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/38651c4450f87348fcbe1f992746a954-Abstract.html",
        "author": "Michael Eisele; Kenneth D. Miller",
        "abstract": "Cortical synaptic plasticity depends on the relative timing of pre- and postsynaptic spikes and also on the temporal pattern of presynaptic spikes and of postsynaptic spikes. We study the hypothesis that cortical synap- tic plasticity does not associate individual spikes, but rather whole \ufb01r- ing episodes, and depends only on when these episodes start and how long they last, but as little as possible on the timing of individual spikes. Here we present the mathematical background for such a study. Stan- dard methods from hidden Markov models are used to de\ufb01ne what \u201c\ufb01r- ing episodes\u201d are. Estimating the probability of being in such an episode requires not only the knowledge of past spikes, but also of future spikes. We show how to construct a causal learning rule, which depends only on past spikes, but associates pre- and postsynaptic \ufb01ring episodes as if it also knew future spikes. We also show that this learning rule agrees with some features of synaptic plasticity in super\ufb01cial layers of rat visual cortex (Froemke and Dan, Nature 416:433, 2002).",
        "bibtex": "@inproceedings{NIPS2002_38651c44,\n author = {Eisele, Michael and Miller, Kenneth},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Hidden Markov Model of Cortical Synaptic Plasticity: Derivation of the Learning Rule},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/38651c4450f87348fcbe1f992746a954-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/38651c4450f87348fcbe1f992746a954-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/38651c4450f87348fcbe1f992746a954-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 184853,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1351566205290507057&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "W. M. Keck Center for Integrative Neuroscience, San Francisco, CA 94143-0444; W. M. Keck Center for Integrative Neuroscience, San Francisco, CA 94143-0444",
        "aff_domain": "phy.ucsf.edu;phy.ucsf.edu",
        "email": "phy.ucsf.edu;phy.ucsf.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "W. M. Keck Center for Integrative Neuroscience, San Francisco, CA 94143-0444",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "f5744259cd",
        "title": "How Linear are Auditory Cortical Responses?",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/7b4773c039d539af17c883eb9283dd14-Abstract.html",
        "author": "Maneesh Sahani; Jennifer F. Linden",
        "abstract": "By comparison to some other sensory cortices, the functional proper- ties of cells in the primary auditory cortex are not yet well understood. Recent attempts to obtain a generalized description of auditory cortical responses have often relied upon characterization of the spectrotempo- ral receptive \ufb01eld (STRF), which amounts to a model of the stimulus- response function (SRF) that is linear in the spectrogram of the stimulus. How well can such a model account for neural responses at the very \ufb01rst stages of auditory cortical processing? To answer this question, we de- velop a novel methodology for evaluating the fraction of stimulus-related response power in a population that can be captured by a given type of SRF model. We use this technique to show that, in the thalamo-recipient layers of primary auditory cortex, STRF models account for no more than 40% of the stimulus-related power in neural responses.",
        "bibtex": "@inproceedings{NIPS2002_7b4773c0,\n author = {Sahani, Maneesh and Linden, Jennifer},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {How Linear are Auditory Cortical Responses?},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/7b4773c039d539af17c883eb9283dd14-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/7b4773c039d539af17c883eb9283dd14-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/7b4773c039d539af17c883eb9283dd14-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 187962,
        "gs_citation": 165,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7993655379305817583&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Gatsby Unit, UCL; Keck Center, UCSF",
        "aff_domain": "gatsby.ucl.ac.uk;phy.ucsf.edu",
        "email": "gatsby.ucl.ac.uk;phy.ucsf.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University College London;Keck Center, UCSF",
        "aff_unique_dep": "Gatsby Unit;",
        "aff_unique_url": "https://www.ucl.ac.uk;",
        "aff_unique_abbr": "UCL;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United Kingdom;"
    },
    {
        "id": "ff576ac2f0",
        "title": "How the Poverty of the Stimulus Solves the Poverty of the Stimulus",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/04ad5632029cbfbed8e136e5f6f7ddfa-Abstract.html",
        "author": "Willem H. Zuidema",
        "abstract": "Language acquisition is  a  special kind of learning problem because  the outcome of learning of one generation is the input for  the next.  That makes it possible for  languages to  adapt to the particularities  of the  learner.  In  this  paper,  I  show  that  this  type  of  language  change has important consequences for models of the evolution and  acquisition of syntax.",
        "bibtex": "@inproceedings{NIPS2002_04ad5632,\n author = {Zuidema, Willem},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {How the Poverty of the Stimulus Solves the Poverty of the Stimulus},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/04ad5632029cbfbed8e136e5f6f7ddfa-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/04ad5632029cbfbed8e136e5f6f7ddfa-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/04ad5632029cbfbed8e136e5f6f7ddfa-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1757042,
        "gs_citation": 209,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15822984218071769653&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Language Evolution and Computation Research Unit and Institute for Cell, Animal and Population Biology, University of Edinburgh",
        "aff_domain": "ling.ed.ac.uk",
        "email": "ling.ed.ac.uk",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Language Evolution and Computation Research Unit and Institute for Cell, Animal and Population Biology, University of Edinburgh",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "dbaac3f1a9",
        "title": "How to Combine Color and Shape Information for 3D Object Recognition: Kernels do the Trick",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/b0df2270be9cb16c14537e5bc2f2d37b-Abstract.html",
        "author": "B. Caputo; Gy. Dork\u00f3",
        "abstract": "This paper presents a  kernel method that allows to combine  color  and shape information for  appearance-based object recognition.  It  doesn't require to define a new common representation, but use the  power of kernels to combine different representations together in an  effective manner.  These results are achieved using results of statis(cid:173) tical mechanics of spin glasses combined with Markov random fields  via kernel functions.  Experiments show  an increase in recognition  rate up  to 5.92%  with respect to conventional strategies.",
        "bibtex": "@inproceedings{NIPS2002_b0df2270,\n author = {Caputo, B. and Dork\\'{o}, Gy.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {How to Combine Color and Shape Information for 3D Object Recognition: Kernels do the Trick},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/b0df2270be9cb16c14537e5bc2f2d37b-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/b0df2270be9cb16c14537e5bc2f2d37b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/b0df2270be9cb16c14537e5bc2f2d37b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1406960,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16625229048146146495&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Smith-Kettlewell Eye Research Institute, 2318 Fillmore Street, 94115 San Francisco, California, USA; Department of Computer Science, Chair for Pattern Recognition, University of Erlangen-Nuremberg",
        "aff_domain": "ski.org;informatik.uni-erlangen.de",
        "email": "ski.org;informatik.uni-erlangen.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Smith-Kettlewell Eye Research Institute, 2318 Fillmore Street, 94115 San Francisco, California, USA;Department of Computer Science, Chair for Pattern Recognition, University of Erlangen-Nuremberg",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "03a85a8cae",
        "title": "Hyperkernels",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/ca0daec69b5adc880fb464895726dbdf-Abstract.html",
        "author": "Cheng S. Ong; Robert C. Williamson; Alex J. Smola",
        "abstract": "We consider the problem of choosing a kernel suitable for estimation using a Gaussian Process estimator or a Support Vector Machine. A novel solution is presented which involves de\ufb01ning a Reproducing Ker- nel Hilbert Space on the space of kernels itself. By utilizing an analog of the classical representer theorem, the problem of choosing a kernel from a parameterized family of kernels (e.g. of varying width) is reduced to a statistical estimation problem akin to the problem of minimizing a regularized risk functional. Various classical settings for model or kernel selection are special cases of our framework.",
        "bibtex": "@inproceedings{NIPS2002_ca0daec6,\n author = {Ong, Cheng and Williamson, Robert C and Smola, Alex},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Hyperkernels},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/ca0daec69b5adc880fb464895726dbdf-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/ca0daec69b5adc880fb464895726dbdf-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/ca0daec69b5adc880fb464895726dbdf-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 117014,
        "gs_citation": 92,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2429703127173360536&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Research School of Information Sciences and Engineering, The Australian National University; Research School of Information Sciences and Engineering, The Australian National University; Research School of Information Sciences and Engineering, The Australian National University",
        "aff_domain": "anu.edu.au;anu.edu.au;anu.edu.au",
        "email": "anu.edu.au;anu.edu.au;anu.edu.au",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Research School of Information Sciences and Engineering, The Australian National University",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "48c378c3cb",
        "title": "Identity Uncertainty and Citation Matching",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/d30960ce77e83d896503d43ba249caf7-Abstract.html",
        "author": "Hanna Pasula; Bhaskara Marthi; Brian Milch; Stuart Russell; Ilya Shpitser",
        "abstract": "Identity uncertainty is a pervasive problem in real-world data analysis. It arises whenever objects are not labeled with unique identi\ufb01ers or when those identi\ufb01ers may not be perceived perfectly. In such cases, two ob- servations may or may not correspond to the same object. In this paper, we consider the problem in the context of citation matching\u2014the prob- lem of deciding which citations correspond to the same publication. Our approach is based on the use of a relational probability model to de\ufb01ne a generative model for the domain, including models of author and title corruption and a probabilistic citation grammar. Identity uncertainty is handled by extending standard models to incorporate probabilities over the possible mappings between terms in the language and objects in the domain. Inference is based on Markov chain Monte Carlo, augmented with speci\ufb01c methods for generating ef\ufb01cient proposals when the domain contains many objects. Results on several citation data sets show that the method outperforms current algorithms for citation matching. The declarative, relational nature of the model also means that our algorithm can determine object characteristics such as author names by combining multiple citations of multiple papers.",
        "bibtex": "@inproceedings{NIPS2002_d30960ce,\n author = {Pasula, Hanna and Marthi, Bhaskara and Milch, Brian and Russell, Stuart J and Shpitser, Ilya},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Identity Uncertainty and Citation Matching},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/d30960ce77e83d896503d43ba249caf7-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/d30960ce77e83d896503d43ba249caf7-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/d30960ce77e83d896503d43ba249caf7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 83885,
        "gs_citation": 430,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=28146316497952788&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 27,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "citeseer.nj.nec.com",
        "author_num": 5,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "2fb6c727c9",
        "title": "Improving Transfer Rates in Brain Computer Interfacing: A Case Study",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/f6c79f4af478638c39b206ec30ab166b-Abstract.html",
        "author": "Peter Meinicke; Matthias Kaper; Florian Hoppe; Manfred Heumann; Helge Ritter",
        "abstract": "In this paper we present results of a study on brain computer interfacing. We adopted an approach of Farwell & Donchin [4], which we tried to improve in several aspects. The main objective was to improve the trans- fer rates based on of\ufb02ine analysis of EEG-data but within a more realistic setup closer to an online realization than in the original studies. The ob- jective was achieved along two different tracks: on the one hand we used state-of-the-art machine learning techniques for signal classi\ufb01cation and on the other hand we augmented the data space by using more electrodes for the interface. For the classi\ufb01cation task we utilized SVMs and, as mo- tivated by recent \ufb01ndings on the learning of discriminative densities, we accumulated the values of the classi\ufb01cation function in order to combine several classi\ufb01cations, which \ufb01nally lead to signi\ufb01cantly improved rates as compared with techniques applied in the original work. In combina- tion with the data space augmentation, we achieved competitive transfer rates at an average of 50.5 bits/min and with a maximum of 84.7 bits/min.",
        "bibtex": "@inproceedings{NIPS2002_f6c79f4a,\n author = {Meinicke, Peter and Kaper, Matthias and Hoppe, Florian and Heumann, Manfred and Ritter, Helge},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Improving Transfer Rates in Brain Computer Interfacing: A Case Study},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/f6c79f4af478638c39b206ec30ab166b-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/f6c79f4af478638c39b206ec30ab166b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/f6c79f4af478638c39b206ec30ab166b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 167577,
        "gs_citation": 156,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17514896754103942195&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "University of Bielefeld; University of Bielefeld; University of Bielefeld; University of Bielefeld; University of Bielefeld",
        "aff_domain": "techfak.uni-bielefeld.de;techfak.uni-bielefeld.de;techfak.uni-bielefeld.de; ;techfak.uni-bielefeld.de",
        "email": "techfak.uni-bielefeld.de;techfak.uni-bielefeld.de;techfak.uni-bielefeld.de; ;techfak.uni-bielefeld.de",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Bielefeld",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uni-bielefeld.de/",
        "aff_unique_abbr": "Uni Bielefeld",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "6f87c4b6c0",
        "title": "Improving a Page Classifier with Anchor Extraction and Link Analysis",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/98e6f17209029f4ae6dc9d88ec8eac2c-Abstract.html",
        "author": "William W. Cohen",
        "abstract": "Most text categorization systems use simple models of documents and document collections. In this paper we describe a technique that im- proves a simple web page classi\ufb01er\u2019s performance on pages from a new, unseen web site, by exploiting link structure within a site as well as page structure within hub pages. On real-world test cases, this technique signi\ufb01cantly and substantially improves the accuracy of a bag-of-words classi\ufb01er, reducing error rate by about half, on average. The system uses a variant of co-training to exploit unlabeled data from a new site. Pages are labeled using the base classi\ufb01er; the results are used by a restricted wrapper-learner to propose potential \u201cmain-category anchor wrappers\u201d; and \ufb01nally, these wrappers are used as features by a third learner to \ufb01nd a categorization of the site that implies a simple hub structure, but which also largely agrees with the original bag-of-words classi\ufb01er.",
        "bibtex": "@inproceedings{NIPS2002_98e6f172,\n author = {Cohen, William W},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Improving a Page Classifier with Anchor Extraction and Link Analysis},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/98e6f17209029f4ae6dc9d88ec8eac2c-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/98e6f17209029f4ae6dc9d88ec8eac2c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/98e6f17209029f4ae6dc9d88ec8eac2c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 54347,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17252928764111730333&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Center for Automated Learning and Discovery, Carnegie-Mellon University",
        "aff_domain": "wcohen.com",
        "email": "wcohen.com",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Center for Automated Learning and Discovery",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3ec45317df",
        "title": "Incremental Gaussian Processes",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/7bc1ec1d9c3426357e69acd5bf320061-Abstract.html",
        "author": "Joaquin Qui\u00f1onero-candela; Ole Winther",
        "abstract": "In this paper, we consider Tipping\u2019s relevance vector machine (RVM) [1] and formalize an incremental training strategy as a variant of the expectation-maximization (EM) algorithm that we call Subspace EM (SSEM). Working with a subset of active basis functions, the sparsity of the RVM solution will ensure that the number of basis functions and thereby the computational complexity is kept low. We also introduce a mean \ufb01eld approach to the intractable classi\ufb01cation model that is ex- pected to give a very good approximation to exact Bayesian inference and contains the Laplace approximation as a special case. We test the algorithms on two large data sets with O(103 (cid:0) 104) examples. The re- sults indicate that Bayesian learning of large data sets, e.g. the MNIST database is realistic.",
        "bibtex": "@inproceedings{NIPS2002_7bc1ec1d,\n author = {Candela, Joaquin Qui\\~{n}onero and Winther, Ole},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Incremental Gaussian Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/7bc1ec1d9c3426357e69acd5bf320061-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/7bc1ec1d9c3426357e69acd5bf320061-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/7bc1ec1d9c3426357e69acd5bf320061-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 102602,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7958099398231633957&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff": "Informatics and Mathematical Modelling, Technical University of Denmark; Informatics and Mathematical Modelling, Technical University of Denmark",
        "aff_domain": "imm.dtu.dk;imm.dtu.dk",
        "email": "imm.dtu.dk;imm.dtu.dk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Informatics and Mathematical Modelling, Technical University of Denmark",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "5f33e527a7",
        "title": "Independent Components Analysis through Product Density Estimation",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/a381c2c35c9157f6b67fd07d5a200ae1-Abstract.html",
        "author": "Trevor Hastie; Rob Tibshirani",
        "abstract": "We  present a  simple direct approach for  solving the ICA problem,  using density estimation and maximum likelihood.  Given a  candi(cid:173) date  orthogonal frame,  we  model  each  of the  coordinates  using  a  semi-parametric density estimate based on cubic splines.  Since our  estimates have two continuous derivatives, we  can easily run a sec(cid:173) ond order search for  the frame  parameters.  Our method performs  very favorably  when compared to state-of-the-art techniques.",
        "bibtex": "@inproceedings{NIPS2002_a381c2c3,\n author = {Hastie, Trevor and Tibshirani, Rob},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Independent Components Analysis through Product Density Estimation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/a381c2c35c9157f6b67fd07d5a200ae1-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/a381c2c35c9157f6b67fd07d5a200ae1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/a381c2c35c9157f6b67fd07d5a200ae1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1287007,
        "gs_citation": 108,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5628941953779277653&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "6502e5aba7",
        "title": "Inferring a Semantic Representation of Text via Cross-Language Correlation Analysis",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/d5e2fbef30a4eb668a203060ec8e5eef-Abstract.html",
        "author": "Alexei Vinokourov; Nello Cristianini; John Shawe-Taylor",
        "abstract": "The problem of learning a semantic representation of a text document from data is addressed, in the situation where a corpus of unlabeled paired documents is available, each pair being formed by a short En- glish document and its French translation. This representation can then be used for any retrieval, categorization or clustering task, both in a stan- dard and in a cross-lingual setting. By using kernel functions, in this case simple bag-of-words inner products, each part of the corpus is mapped to a high-dimensional space. The correlations between the two spaces are then learnt by using kernel Canonical Correlation Analysis. A set of directions is found in the \ufb01rst and in the second space that are max- imally correlated. Since we assume the two representations are com- pletely independent apart from the semantic content, any correlation be- tween them should re\ufb02ect some semantic similarity. Certain patterns of English words that relate to a speci\ufb01c meaning should correlate with cer- tain patterns of French words corresponding to the same meaning, across the corpus. Using the semantic representation obtained in this way we \ufb01rst demonstrate that the correlations detected between the two versions of the corpus are signi\ufb01cantly higher than random, and hence that a rep- resentation based on such features does capture statistical patterns that should re\ufb02ect semantic information. Then we use such representation both in cross-language and in single-language retrieval tasks, observing performance that is consistently and signi\ufb01cantly superior to LSI on the same data.",
        "bibtex": "@inproceedings{NIPS2002_d5e2fbef,\n author = {Vinokourov, Alexei and Cristianini, Nello and Shawe-Taylor, John},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Inferring a Semantic Representation of Text via Cross-Language Correlation Analysis},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/d5e2fbef30a4eb668a203060ec8e5eef-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/d5e2fbef30a4eb668a203060ec8e5eef-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/d5e2fbef30a4eb668a203060ec8e5eef-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 96068,
        "gs_citation": 350,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14300054563233700384&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff": "Dept. Computer Science, Royal Holloway, University of London; Dept. Computer Science, Royal Holloway, University of London; Dept. Statistics, UC Davis, Berkeley, US",
        "aff_domain": "cs.rhul.ac.uk;cs.rhul.ac.uk;support-vector.net",
        "email": "cs.rhul.ac.uk;cs.rhul.ac.uk;support-vector.net",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Dept. Computer Science, Royal Holloway, University of London;Dept. Statistics, UC Davis, Berkeley, US",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "11597ce1e1",
        "title": "Information Diffusion Kernels",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/5938b4d054136e5d59ada6ec9c295d7a-Abstract.html",
        "author": "Guy Lebanon; John D. Lafferty",
        "abstract": "A new family of kernels for statistical learning is introduced that ex- ploits the geometric structure of statistical models. Based on the heat equation on the Riemannian manifold de\ufb01ned by the Fisher informa- tion metric, information diffusion kernels generalize the Gaussian kernel of Euclidean space, and provide a natural way of combining generative statistical modeling with non-parametric discriminative learning. As a special case, the kernels give a new approach to applying kernel-based learning algorithms to discrete data. Bounds on covering numbers for the new kernels are proved using spectral theory in differential geometry, and experimental results are presented for text classi\ufb01cation.",
        "bibtex": "@inproceedings{NIPS2002_5938b4d0,\n author = {Lebanon, Guy and Lafferty, John},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Information Diffusion Kernels},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/5938b4d054136e5d59ada6ec9c295d7a-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/5938b4d054136e5d59ada6ec9c295d7a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/5938b4d054136e5d59ada6ec9c295d7a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 184800,
        "gs_citation": 92,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15766511881296190457&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": "School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213 USA; School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213 USA",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "4f354e9032",
        "title": "Information Regularization with Partially Labeled Data",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/1145a30ff80745b56fb0cecf65305017-Abstract.html",
        "author": "Martin Szummer; Tommi S. Jaakkola",
        "abstract": "Classi\ufb01cation with partially labeled data requires using a large number of unlabeled examples (or an estimated marginal P (x)), to further con- strain the conditional P (yjx) beyond a few available labeled examples. We formulate a regularization approach to linking the marginal and the conditional in a general way. The regularization penalty measures the information that is implied about the labels over covering regions. No parametric assumptions are required and the approach remains tractable even for continuous marginal densities P (x). We develop algorithms for solving the regularization problem for \ufb01nite covers, establish a limiting differential equation, and exemplify the behavior of the new regulariza- tion approach in simple cases.",
        "bibtex": "@inproceedings{NIPS2002_1145a30f,\n author = {Szummer, Martin and Jaakkola, Tommi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Information Regularization with Partially Labeled Data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/1145a30ff80745b56fb0cecf65305017-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/1145a30ff80745b56fb0cecf65305017-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/1145a30ff80745b56fb0cecf65305017-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 90170,
        "gs_citation": 137,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3911234489713668253&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "MIT AI Lab & CBCL; MIT AI Lab",
        "aff_domain": "ai.mit.edu;ai.mit.edu",
        "email": "ai.mit.edu;ai.mit.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "MIT AI Lab & CBCL;Massachusetts Institute of Technology",
        "aff_unique_dep": ";Artificial Intelligence Laboratory",
        "aff_unique_url": ";http://www.ai.mit.edu",
        "aff_unique_abbr": ";MIT AI Lab",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "76500ddedb",
        "title": "Informed Projections",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/cdf1e288ca02272e717c9d5e4cb180bd-Abstract.html",
        "author": "David Cohn",
        "abstract": "Low rank approximation techniques are widespread in pattern recogni- tion research \u2014 they include Latent Semantic Analysis (LSA), Proba- bilistic LSA, Principal Components Analysus (PCA), the Generative As- pect Model, and many forms of bibliometric analysis. All make use of a low-dimensional manifold onto which data are projected. Such techniques are generally \u201cunsupervised,\u201d which allows them to model data in the absence of labels or categories. With many practi- cal problems, however, some prior knowledge is available in the form of context. In this paper, I describe a principled approach to incorpo- rating such information, and demonstrate its application to PCA-based approximations of several data sets.",
        "bibtex": "@inproceedings{NIPS2002_cdf1e288,\n author = {Cohn, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Informed Projections},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/cdf1e288ca02272e717c9d5e4cb180bd-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/cdf1e288ca02272e717c9d5e4cb180bd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/cdf1e288ca02272e717c9d5e4cb180bd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 74957,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2458143495345995564&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu",
        "email": "cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "caab7afff3",
        "title": "Interpreting Neural Response Variability as Monte Carlo Sampling of the Posterior",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/a486cd07e4ac3d270571622f4f316ec5-Abstract.html",
        "author": "Patrik O. Hoyer; Aapo Hyv\u00e4rinen",
        "abstract": "The responses of cortical sensory neurons are notoriously variable, with the number of spikes evoked by identical stimuli varying signi\ufb01cantly from trial to trial. This variability is most often interpreted as \u2018noise\u2019, purely detrimental to the sensory system. In this paper, we propose an al- ternative view in which the variability is related to the uncertainty, about world parameters, which is inherent in the sensory stimulus. Speci\ufb01- cally, the responses of a population of neurons are interpreted as stochas- tic samples from the posterior distribution in a latent variable model. In addition to giving theoretical arguments supporting such a representa- tional scheme, we provide simulations suggesting how some aspects of response variability might be understood in this framework.",
        "bibtex": "@inproceedings{NIPS2002_a486cd07,\n author = {Hoyer, Patrik and Hyv\\\"{a}rinen, Aapo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Interpreting Neural Response Variability as Monte Carlo Sampling of the Posterior},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/a486cd07e4ac3d270571622f4f316ec5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 83862,
        "gs_citation": 246,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15817525870256428228&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Neural Networks Research Centre, Helsinki University of Technology; Neural Networks Research Centre, Helsinki University of Technology",
        "aff_domain": "hut.\ufb01; ",
        "email": "hut.\ufb01; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Neural Networks Research Centre, Helsinki University of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "696ac59634",
        "title": "Intrinsic Dimension Estimation Using Packing Numbers",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/1177967c7957072da3dc1db4ceb30e7a-Abstract.html",
        "author": "Bal\u00e1zs K\u00e9gl",
        "abstract": "We propose a new algorithm to estimate the intrinsic dimension of data sets. The method is based on geometric properties of the data and re- quires neither parametric assumptions on the data generating model nor input parameters to set. The method is compared to a similar, widely- used algorithm from the same family of geometric techniques. Experi- ments show that our method is more robust in terms of the data generating distribution and more reliable in the presence of noise.",
        "bibtex": "@inproceedings{NIPS2002_1177967c,\n author = {K\\'{e}gl, Bal\\'{a}zs},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Intrinsic Dimension Estimation Using Packing Numbers},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/1177967c7957072da3dc1db4ceb30e7a-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/1177967c7957072da3dc1db4ceb30e7a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/1177967c7957072da3dc1db4ceb30e7a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 235021,
        "gs_citation": 350,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5958107194992126003&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Science and Operations Research, University of Montreal",
        "aff_domain": "iro.umontreal.ca",
        "email": "iro.umontreal.ca",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Montreal",
        "aff_unique_dep": "Department of Computer Science and Operations Research",
        "aff_unique_url": "https://www.umontreal.ca",
        "aff_unique_abbr": "UM",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "1949fa8c99",
        "title": "Kernel Dependency Estimation",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/e069ea4c9c233d36ff9c7f329bc08ff1-Abstract.html",
        "author": "Jason Weston; Olivier Chapelle; Vladimir Vapnik; Andr\u00e9 Elisseeff; Bernhard Sch\u00f6lkopf",
        "abstract": "We consider the learning problem of finding a dependency between  a  general  class  of objects  and  another,  possibly  different,  general  class of objects.  The objects can be for  example:  vectors,  images,  strings, trees or graphs.  Such a task is made possible by employing  similarity  measures  in  both  input  and  output  spaces  using  ker(cid:173) nel  functions,  thus  embedding  the  objects  into  vector  spaces.  We  experimentally  validate  our  approach  on  several  tasks:  mapping  strings to strings, pattern recognition, and reconstruction from par(cid:173) tial images.",
        "bibtex": "@inproceedings{NIPS2002_e069ea4c,\n author = {Weston, Jason and Chapelle, Olivier and Vapnik, Vladimir and Elisseeff, Andr\\'{e} and Sch\\\"{o}lkopf, Bernhard},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Kernel Dependency Estimation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/e069ea4c9c233d36ff9c7f329bc08ff1-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/e069ea4c9c233d36ff9c7f329bc08ff1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/e069ea4c9c233d36ff9c7f329bc08ff1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1645196,
        "gs_citation": 236,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8017659239284973669&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Max Planck Institute for Biological Cybernetics, 72076 Tubingen, Germany; Max Planck Institute for Biological Cybernetics, 72076 Tubingen, Germany; Max Planck Institute for Biological Cybernetics, 72076 Tubingen, Germany; Max Planck Institute for Biological Cybernetics, 72076 Tubingen, Germany; NEC Research Institute, Princeton, NJ 08540 USA",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;1",
        "aff_unique_norm": "Max Planck Institute for Biological Cybernetics, 72076 Tubingen, Germany;NEC Research Institute, Princeton, NJ 08540 USA",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "a0877466c3",
        "title": "Kernel Design Using Boosting",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/dd28e50635038e9cf3a648c2dd17ad0a-Abstract.html",
        "author": "Koby Crammer; Joseph Keshet; Yoram Singer",
        "abstract": "The focus of the paper is the problem of learning kernel operators from empirical data. We cast the kernel design problem as the construction of an accurate kernel from simple (and less accurate) base kernels. We use the boosting paradigm to perform the kernel construction process. To do so, we modify the booster so as to accommodate kernel operators. We also devise an ef\ufb01cient weak-learner for simple kernels that is based on generalized eigen vector decomposition. We demonstrate the effective- ness of our approach on synthetic data and on the USPS dataset. On the USPS dataset, the performance of the Perceptron algorithm with learned kernels is systematically better than a \ufb01xed RBF kernel.",
        "bibtex": "@inproceedings{NIPS2002_dd28e506,\n author = {Crammer, Koby and Keshet, Joseph and Singer, Yoram},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Kernel Design Using Boosting},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/dd28e50635038e9cf3a648c2dd17ad0a-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/dd28e50635038e9cf3a648c2dd17ad0a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/dd28e50635038e9cf3a648c2dd17ad0a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 123361,
        "gs_citation": 188,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5764604682607317511&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "School of Computer Science & Engineering, The Hebrew University, Jerusalem 91904, Israel; School of Computer Science & Engineering, The Hebrew University, Jerusalem 91904, Israel; School of Computer Science & Engineering, The Hebrew University, Jerusalem 91904, Israel",
        "aff_domain": "cs.huji.ac.il;cs.huji.ac.il;cs.huji.ac.il",
        "email": "cs.huji.ac.il;cs.huji.ac.il;cs.huji.ac.il",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "School of Computer Science & Engineering, The Hebrew University, Jerusalem 91904, Israel",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "3fb6d92efd",
        "title": "Kernel-Based Extraction of Slow Features: Complex Cells Learn Disparity and Translation Invariance from Natural Images",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/db116b39f7a3ac5366079b1d9fe249a5-Abstract.html",
        "author": "Alistair Bray; Dominique Martinez",
        "abstract": "In Slow Feature Analysis  (SFA  [1]),  it has been demonstrated that  high-order invariant properties can be extracted by  projecting in(cid:173) puts  into  a  nonlinear  space  and  computing  the  slowest  changing  features  in  this  space;  this  has  been  proposed  as  a  simple  general  model for learning nonlinear invariances in the visual system.  How(cid:173) ever,  this  method is  highly constrained by the curse of dimension(cid:173) ality  which  limits  it  to  simple  theoretical simulations.  This  paper  demonstrates that by using a different but closely-related objective  function for extracting slowly varying features ([2,  3]),  and then ex(cid:173) ploiting the kernel trick, this  curse can be avoided.  Using this new  method we  show that  both the  complex cell  properties of transla(cid:173) tion invariance  and  disparity  coding  can  be  learnt  simultaneously  from  natural images when complex cells  are driven by simple cells  also learnt from  the image.",
        "bibtex": "@inproceedings{NIPS2002_db116b39,\n author = {Bray, Alistair and Martinez, Dominique},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Kernel-Based Extraction of Slow Features: Complex Cells Learn Disparity and Translation Invariance from Natural Images},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/db116b39f7a3ac5366079b1d9fe249a5-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/db116b39f7a3ac5366079b1d9fe249a5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/db116b39f7a3ac5366079b1d9fe249a5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1501111,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16972253628698541265&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "CORTEX Group, LORIA-INRIA, Nancy, France; CORTEX Group, LORIA-INRIA, Nancy, France",
        "aff_domain": "loria.fr;loria.jr",
        "email": "loria.fr;loria.jr",
        "github": "",
        "project": "http://www.loria.fr/equipes/cortex/",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "CORTEX Group, LORIA-INRIA, Nancy, France",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "f4e5eeef0b",
        "title": "Knowledge-Based Support Vector Machine Classifiers",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/934b535800b1cba8f96a5d72f72f1611-Abstract.html",
        "author": "Glenn M. Fung; Olvi L. Mangasarian; Jude W. Shavlik",
        "abstract": "Prior knowledge  in the  form  of multiple  polyhedral sets,  each  be(cid:173) longing to one of two categories, is introduced into a  reformulation  of a  linear support vector machine classifier.  The resulting formu(cid:173) lation leads to a linear program that can be solved efficiently.  Real  world examples, from DNA sequencing and breast cancer prognosis,  demonstrate the effectiveness  of the proposed method.  Numerical  results  show  improvement  in  test  set  accuracy  after  the  incorpo(cid:173) ration of prior knowledge into ordinary, data-based linear support  vector  machine  classifiers.  One  experiment  also  shows  that  a  lin(cid:173) ear classifier,  based solely on prior knowledge,  far  outperforms the  direct  application of prior knowledge  rules  to classify data.  Keywords:  use  and  refinement  of prior  knowledge,  sup(cid:173) port  vector machines,  linear programming",
        "bibtex": "@inproceedings{NIPS2002_934b5358,\n author = {Fung, Glenn and Mangasarian, Olvi and Shavlik, Jude},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Knowledge-Based Support Vector Machine Classifiers},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/934b535800b1cba8f96a5d72f72f1611-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/934b535800b1cba8f96a5d72f72f1611-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/934b535800b1cba8f96a5d72f72f1611-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2061243,
        "gs_citation": 266,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11689688499127318392&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "74624d0cec",
        "title": "Learning About Multiple Objects in Images: Factorial Learning without Factorial Search",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/c3614206a443012045cfd75d2600af2d-Abstract.html",
        "author": "Christopher K. I. Williams; Michalis K. Titsias",
        "abstract": "We consider data which are images containing views of multiple objects. Our task is to learn about each of the objects present in the images. This task can be approached as a factorial learning problem, where each image must be explained by instantiating a model for each of the objects present with the correct instantiation parameters. A major problem with learning a factorial model is that as the number of objects increases, there is a combinatorial explosion of the number of con\ufb01gurations that need to be considered. We develop a method to extract object models sequentially from the data by making use of a robust statistical method, thus avoid- ing the combinatorial explosion, and present results showing successful extraction of objects from real images.",
        "bibtex": "@inproceedings{NIPS2002_c3614206,\n author = {Williams, Christopher K. I. and Titsias, Michalis},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Learning About Multiple Objects in Images: Factorial Learning without Factorial Search},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/c3614206a443012045cfd75d2600af2d-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/c3614206a443012045cfd75d2600af2d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/c3614206a443012045cfd75d2600af2d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 164751,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15422557384217411128&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "School of Informatics, University of Edinburgh, Edinburgh EH1 2QL, UK; School of Informatics, University of Edinburgh, Edinburgh EH1 2QL, UK",
        "aff_domain": "ed.ac.uk;sms.ed.ac.uk",
        "email": "ed.ac.uk;sms.ed.ac.uk",
        "github": "",
        "project": "http://anc.ed.ac.uk",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Edinburgh",
        "aff_unique_dep": "School of Informatics",
        "aff_unique_url": "https://www.ed.ac.uk",
        "aff_unique_abbr": "Edinburgh",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Edinburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "d1aac9e3a9",
        "title": "Learning Attractor Landscapes for Learning Motor Primitives",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/23c97e9cb93576e45d2feaf00d0e8502-Abstract.html",
        "author": "Auke J. Ijspeert; Jun Nakanishi; Stefan Schaal",
        "abstract": "Many control problems take place in continuous state-action spaces, e.g., as in manipulator robotics, where the control objective is of- ten de\ufb02ned as \ufb02nding a desired trajectory that reaches a particular goal state. While reinforcement learning o\ufb01ers a theoretical frame- work to learn such control policies from scratch, its applicability to higher dimensional continuous state-action spaces remains rather limited to date. Instead of learning from scratch, in this paper we suggest to learn a desired complex control policy by transforming an existing simple canonical control policy. For this purpose, we represent canonical policies in terms of di\ufb01erential equations with well-de\ufb02ned attractor properties. By nonlinearly transforming the canonical attractor dynamics using techniques from nonparametric regression, almost arbitrary new nonlinear policies can be gener- ated without losing the stability properties of the canonical sys- tem. We demonstrate our techniques in the context of learning a set of movement skills for a humanoid robot from demonstrations of a human teacher. Policies are acquired rapidly, and, due to the properties of well formulated di\ufb01erential equations, can be re-used and modi\ufb02ed on-line under dynamic changes of the environment. The linear parameterization of nonparametric regression moreover lends itself to recognize and classify previously learned movement skills. Evaluations in simulations and on an actual 30 degree-of- freedom humanoid robot exemplify the feasibility and robustness of our approach.",
        "bibtex": "@inproceedings{NIPS2002_23c97e9c,\n author = {Ijspeert, Auke and Nakanishi, Jun and Schaal, Stefan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Learning Attractor Landscapes for Learning Motor Primitives},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/23c97e9cb93576e45d2feaf00d0e8502-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/23c97e9cb93576e45d2feaf00d0e8502-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/23c97e9cb93576e45d2feaf00d0e8502-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 681255,
        "gs_citation": 880,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=415785132359889643&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 14,
        "aff": "University of Southern California, Los Angeles, CA 90089-2520, USA + EPFL, Swiss Federal Institute of Technology, Lausanne, Switzerland; ATR Human Information Science Laboratories, Kyoto 619-0288, Japan; University of Southern California, Los Angeles, CA 90089-2520, USA + ATR Human Information Science Laboratories, Kyoto 619-0288, Japan",
        "aff_domain": "usc.edu;his.atr.co.jp;usc.edu",
        "email": "usc.edu;his.atr.co.jp;usc.edu",
        "github": "",
        "project": "http://lslwww.epfl.ch/~ijspeert/",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2;0+2",
        "aff_unique_norm": "University of Southern California, Los Angeles, CA 90089-2520, USA;EPFL, Swiss Federal Institute of Technology, Lausanne, Switzerland;ATR Human Information Science Laboratories, Kyoto 619-0288, Japan",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": ";",
        "aff_country_unique": ""
    },
    {
        "id": "9fdd71cb1d",
        "title": "Learning Graphical Models with Mercer Kernels",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/5f6371c9126149517d9ba475def53139-Abstract.html",
        "author": "Francis R. Bach; Michael I. Jordan",
        "abstract": "We present a class of algorithms for learning the structure of graphical models from data. The algorithms are based on a measure known as the kernel generalized variance (KGV), which essentially allows us to treat all variables on an equal footing as Gaussians in a feature space obtained from Mercer kernels. Thus we are able to learn hybrid graphs involving discrete and continuous variables of arbitrary type. We explore the computational properties of our approach, showing how to use the kernel trick to compute the relevant statistics in linear time. We illustrate our framework with experiments involving discrete and continuous data.",
        "bibtex": "@inproceedings{NIPS2002_5f6371c9,\n author = {Bach, Francis and Jordan, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Learning Graphical Models with Mercer Kernels},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/5f6371c9126149517d9ba475def53139-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/5f6371c9126149517d9ba475def53139-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/5f6371c9126149517d9ba475def53139-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 101279,
        "gs_citation": 97,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6869262442294615717&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Division of Computer Science, University of California, Berkeley, CA 94720; Computer Science and Statistics, University of California, Berkeley, CA 94720",
        "aff_domain": "cs.berkeley.edu;cs.berkeley.edu",
        "email": "cs.berkeley.edu;cs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Division of Computer Science, University of California, Berkeley, CA 94720;Computer Science and Statistics, University of California, Berkeley, CA 94720",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "5866cdddb7",
        "title": "Learning Semantic Similarity",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/778609db5dc7e1a8315717a9cdd8fd6f-Abstract.html",
        "author": "Jaz Kandola; Nello Cristianini; John S. Shawe-taylor",
        "abstract": "The standard representation of text documents as bags of words  suffers from well known limitations, mostly due to its inability to  exploit semantic similarity between terms. Attempts to incorpo(cid:173) rate some notion of term similarity include latent semantic index(cid:173) ing [8], the use of semantic networks [9], and probabilistic methods  [5]. In this paper we propose two methods for inferring such sim(cid:173) ilarity from a corpus. The first one defines word-similarity based  on document-similarity and viceversa, giving rise to a system of  equations whose equilibrium point we use to obtain a semantic  similarity measure. The second method models semantic relations  by means of a diffusion process on a graph defined by lexicon and  co-occurrence information. Both approaches produce valid kernel  functions parametrised by a real number. The paper shows how  the alignment measure can be used to successfully perform model  selection over this parameter. Combined with the use of support  vector machines we obtain positive results.",
        "bibtex": "@inproceedings{NIPS2002_778609db,\n author = {Kandola, Jaz and Cristianini, Nello and Shawe-taylor, John},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Learning Semantic Similarity},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/778609db5dc7e1a8315717a9cdd8fd6f-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/778609db5dc7e1a8315717a9cdd8fd6f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/778609db5dc7e1a8315717a9cdd8fd6f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1570412,
        "gs_citation": 254,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12528607468498129361&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Royal Holloway, University of London; Royal Holloway, University of London; University of California, Berkeley",
        "aff_domain": "cs.rhul.ac.uk;cs.rhul.ac.uk;support-vector.net",
        "email": "cs.rhul.ac.uk;cs.rhul.ac.uk;support-vector.net",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of London;University of California, Berkeley",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.royalholloway.ac.uk;https://www.berkeley.edu",
        "aff_unique_abbr": "RHUL;UC Berkeley",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Royal Holloway;Berkeley",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "85706969f4",
        "title": "Learning Sparse Multiscale Image Representations",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/c182f930a06317057d31c73bb2fedd4f-Abstract.html",
        "author": "Phil Sallee; Bruno A. Olshausen",
        "abstract": "We describe a method for learning sparse multiscale image repre- sentations using a sparse prior distribution over the basis function coe(cid:14)cients. The prior consists of a mixture of a Gaussian and a Dirac delta function, and thus encourages coe(cid:14)cients to have exact zero values. Coe(cid:14)cients for an image are computed by sampling from the resulting posterior distribution with a Gibbs sampler. The learned basis is similar to the Steerable Pyramid basis, and yields slightly higher SNR for the same number of active coe(cid:14)cients. De- noising using the learned image model is demonstrated for some standard test images, with results that compare favorably with other denoising methods.",
        "bibtex": "@inproceedings{NIPS2002_c182f930,\n author = {Sallee, Phil and Olshausen, Bruno},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Learning Sparse Multiscale Image Representations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/c182f930a06317057d31c73bb2fedd4f-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/c182f930a06317057d31c73bb2fedd4f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/c182f930a06317057d31c73bb2fedd4f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 167842,
        "gs_citation": 108,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5318395138766592170&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science and Center for Neuroscience, UC Davis; Department of Psychology and Center for Neuroscience, UC Davis",
        "aff_domain": "cs.ucdavis.edu;ucdavis.edu",
        "email": "cs.ucdavis.edu;ucdavis.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Department of Computer Science and Center for Neuroscience, UC Davis;Department of Psychology and Center for Neuroscience, UC Davis",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "aeecdc4a67",
        "title": "Learning Sparse Topographic Representations with Products of Student-t Distributions",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/bb1662b7c5f22a0f905fd59e718ca05e-Abstract.html",
        "author": "Max Welling; Simon Osindero; Geoffrey E. Hinton",
        "abstract": "We propose a model for natural images in which the probability of an im- age is proportional to the product of the probabilities of some \ufb01lter out- puts. We encourage the system to \ufb01nd sparse features by using a Student- t distribution to model each \ufb01lter output. If the t-distribution is used to model the combined outputs of sets of neurally adjacent \ufb01lters, the sys- tem learns a topographic map in which the orientation, spatial frequency and location of the \ufb01lters change smoothly across the map. Even though maximum likelihood learning is intractable in our model, the product form allows a relatively ef\ufb01cient learning procedure that works well even for highly overcomplete sets of \ufb01lters. Once the model has been learned it can be used as a prior to derive the \u201citerated Wiener \ufb01lter\u201d for the pur- pose of denoising images.",
        "bibtex": "@inproceedings{NIPS2002_bb1662b7,\n author = {Welling, Max and Osindero, Simon and Hinton, Geoffrey E},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Learning Sparse Topographic Representations with Products of Student-t Distributions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/bb1662b7c5f22a0f905fd59e718ca05e-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/bb1662b7c5f22a0f905fd59e718ca05e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/bb1662b7c5f22a0f905fd59e718ca05e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 237301,
        "gs_citation": 204,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2931226442611737150&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Computer Science, University of Toronto; Department of Computer Science, University of Toronto; Gatsby Unit, University College London",
        "aff_domain": "cs.toronto.edu;cs.toronto.edu;gatsby.ucl.ac.uk",
        "email": "cs.toronto.edu;cs.toronto.edu;gatsby.ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of Toronto;University College London",
        "aff_unique_dep": "Department of Computer Science;Gatsby Unit",
        "aff_unique_url": "https://www.utoronto.ca;https://www.ucl.ac.uk",
        "aff_unique_abbr": "U of T;UCL",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Toronto;London",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "Canada;United Kingdom"
    },
    {
        "id": "709417cecd",
        "title": "Learning a Forward Model of a Reflex",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/10907813b97e249163587e6246612e21-Abstract.html",
        "author": "Bernd Porr; Florentin W\u00f6rg\u00f6tter",
        "abstract": "We develop a systems theoretical treatment of a behavioural system that interacts with its environment in a closed loop situation such that its mo- tor actions in\ufb02uence its sensor inputs. The simplest form of a feedback is a re\ufb02ex. Re\ufb02exes occur always \u201ctoo late\u201d; i.e., only after a (unpleas- ant, painful, dangerous) re\ufb02ex-eliciting sensor event has occurred. This de\ufb01nes an objective problem which can be solved if another sensor input exists which can predict the primary re\ufb02ex and can generate an earlier reaction. In contrast to previous approaches, our linear learning algo- rithm allows for an analytical proof that this system learns to apply feed- forward control with the result that slow feedback loops are replaced by their equivalent feed-forward controller creating a forward model. In other words, learning turns the reactive system into a pro-active system. By means of a robot implementation we demonstrate the applicability of the theoretical results which can be used in a variety of different areas in physics and engineering.",
        "bibtex": "@inproceedings{NIPS2002_10907813,\n author = {Porr, Bernd and W\\\"{o}rg\\\"{o}tter, Florentin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Learning a Forward Model of a Reflex},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/10907813b97e249163587e6246612e21-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/10907813b97e249163587e6246612e21-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/10907813b97e249163587e6246612e21-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 306404,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12922800984039789228&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "University of Stirling; University of Stirling",
        "aff_domain": "cn.stir.ac.uk;cn.stir.ac.uk",
        "email": "cn.stir.ac.uk;cn.stir.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Stirling",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stir.ac.uk",
        "aff_unique_abbr": "Stirling",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "deff95a1ca",
        "title": "Learning in Spiking Neural Assemblies",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/619205da514e83f869515c782a328d3c-Abstract.html",
        "author": "David Barber",
        "abstract": "We consider a statistical framework for learning in a class of net- works of spiking neurons. Our aim is to show how optimal local learning rules can be readily derived once the neural dynamics and desired functionality of the neural assembly have been speci\ufb02ed, in contrast to other models which assume (sub-optimal) learning rules. Within this framework we derive local rules for learning tem- poral sequences in a model of spiking neurons and demonstrate its superior performance to correlation (Hebbian) based approaches. We further show how to include mechanisms such as synaptic de- pression and outline how the framework is readily extensible to learning in networks of highly complex spiking neurons. A stochas- tic quantal vesicle release mechanism is considered and implications on the complexity of learning discussed.",
        "bibtex": "@inproceedings{NIPS2002_619205da,\n author = {Barber, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Learning in Spiking Neural Assemblies},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/619205da514e83f869515c782a328d3c-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/619205da514e83f869515c782a328d3c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/619205da514e83f869515c782a328d3c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 123142,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13276942638107156687&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "InstituteforAdaptiveandNeuralComputation, EdinburghUniversity",
        "aff_domain": "anc.ed.ac.uk",
        "email": "anc.ed.ac.uk",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "InstituteforAdaptiveandNeuralComputation, EdinburghUniversity",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "d1ceed8d68",
        "title": "Learning in Zero-Sum Team Markov Games Using Factored Value Functions",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/4ae67a7dd7e491f8fb6f9ea0cf25dfdb-Abstract.html",
        "author": "Michail G. Lagoudakis; Ronald Parr",
        "abstract": "We present a new method for learning good strategies in zero-sum Markov games in which each side is composed of multiple agents col- laborating against an opposing team of agents. Our method requires full observability and communication during learning, but the learned poli- cies can be executed in a distributed manner. The value function is rep- resented as a factored linear architecture and its structure determines the necessary computational resources and communication bandwidth. This approach permits a tradeoff between simple representations with little or no communication between agents and complex, computationally inten- sive representations with extensive coordination between agents. Thus, we provide a principled means of using approximation to combat the exponential blowup in the joint action space of the participants. The ap- proach is demonstrated with an example that shows the ef\ufb01ciency gains over naive enumeration.",
        "bibtex": "@inproceedings{NIPS2002_4ae67a7d,\n author = {Lagoudakis, Michail G. and Parr, Ronald},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Learning in Zero-Sum Team Markov Games Using Factored Value Functions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/4ae67a7dd7e491f8fb6f9ea0cf25dfdb-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/4ae67a7dd7e491f8fb6f9ea0cf25dfdb-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/4ae67a7dd7e491f8fb6f9ea0cf25dfdb-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 94463,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3703712782103703851&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science, Duke University; Department of Computer Science, Duke University",
        "aff_domain": "cs.duke.edu;cs.duke.edu",
        "email": "cs.duke.edu;cs.duke.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Duke University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.duke.edu",
        "aff_unique_abbr": "Duke",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "cfdeb4cfb2",
        "title": "Learning to Classify Galaxy Shapes Using the EM Algorithm",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/ea119a40c1592979f51819b0bd38d39d-Abstract.html",
        "author": "Sergey Kirshner; Igor V. Cadez; Padhraic Smyth; Chandrika Kamath",
        "abstract": "We describe the application of probabilistic model-based learning to the problem of automatically identifying classes of galaxies, based on both morphological and pixel intensity characteristics. The EM algorithm can be used to learn how to spatially orient a set of galaxies so that they are geometrically aligned. We augment this \u201cordering-model\u201d with a mixture model on objects, and demonstrate how classes of galaxies can be learned in an unsupervised manner using a two-level EM algorithm. The resulting models provide highly accurate classi\u00a3cation of galaxies in cross-validation experiments.",
        "bibtex": "@inproceedings{NIPS2002_ea119a40,\n author = {Kirshner, Sergey and Cadez, Igor and Smyth, Padhraic and Kamath, Chandrika},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Learning to Classify Galaxy Shapes Using the EM Algorithm},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/ea119a40c1592979f51819b0bd38d39d-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/ea119a40c1592979f51819b0bd38d39d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/ea119a40c1592979f51819b0bd38d39d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 78078,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17697369853210222828&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Information and Computer Science, University of California, Irvine, CA 92697-3425; Sparta Inc., 23382 Mill Creek Drive #100, Laguna Hills, CA 92653; Information and Computer Science, University of California, Irvine, CA 92697-3425; Center for Applied Scientific Computing, Lawrence Livermore National Laboratory, Livermore, CA 94551",
        "aff_domain": "ics.uci.edu;sparta.com;ics.uci.edu;llnl.gov",
        "email": "ics.uci.edu;sparta.com;ics.uci.edu;llnl.gov",
        "github": "",
        "project": "http://sundog.stsci.edu/",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;2",
        "aff_unique_norm": "Information and Computer Science, University of California, Irvine, CA 92697-3425;Sparta Inc., 23382 Mill Creek Drive #100, Laguna Hills, CA 92653;Center for Applied Scientific Computing, Lawrence Livermore National Laboratory, Livermore, CA 94551",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "1d8c6e0216",
        "title": "Learning to Detect Natural Image Boundaries Using Brightness and Texture",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/b5c01503041b70d41d80e3dbe31bbd8c-Abstract.html",
        "author": "David R. Martin; Charless C. Fowlkes; Jitendra Malik",
        "abstract": "The goal of this work is to accurately detect and localize boundaries in natural scenes using local image measurements. We formulate features that respond to characteristic changes in brightness and texture associated with natural boundaries. In order to combine the information from these features in an optimal way, a classi\ufb01er is trained using human labeled images as ground truth. We present precision-recall curves showing that the resulting detector outperforms existing approaches.",
        "bibtex": "@inproceedings{NIPS2002_b5c01503,\n author = {Martin, David and Fowlkes, Charless and Malik, Jitendra},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Learning to Detect Natural Image Boundaries Using Brightness and Texture},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/b5c01503041b70d41d80e3dbe31bbd8c-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/b5c01503041b70d41d80e3dbe31bbd8c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/b5c01503041b70d41d80e3dbe31bbd8c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 118391,
        "gs_citation": 139,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15918220796209896361&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "d5ae5aa6af",
        "title": "Learning to Perceive Transparency from the Statistics of Natural Scenes",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/d542599794c1cf067d90638b5d3911f3-Abstract.html",
        "author": "Anat Levin; Assaf Zomet; Yair Weiss",
        "abstract": "Certain simple images are known to trigger a percept of trans- parency: the input image I is perceived as the sum of two images I(x; y) = I1(x; y) + I2(x; y). This percept is puzzling. First, why do we choose the \\more complicated\" description with two images rather than the \\simpler\" explanation I(x; y) = I1(x; y) + 0 ? Sec- ond, given the in\ufb02nite number of ways to express I as a sum of two images, how do we compute the \\best\" decomposition ? Here we suggest that transparency is the rational percept of a sys- tem that is adapted to the statistics of natural scenes. We present a probabilistic model of images based on the qualitative statistics of derivative \ufb02lters and \\corner detectors\" in natural scenes and use this model to \ufb02nd the most probable decomposition of a novel image. The optimization is performed using loopy belief propa- gation. We show that our model computes perceptually \\correct\" decompositions on synthetic images and discuss its application to real images.",
        "bibtex": "@inproceedings{NIPS2002_d5425997,\n author = {Levin, Anat and Zomet, Assaf and Weiss, Yair},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Learning to Perceive Transparency from the Statistics of Natural Scenes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/d542599794c1cf067d90638b5d3911f3-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/d542599794c1cf067d90638b5d3911f3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/d542599794c1cf067d90638b5d3911f3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 130361,
        "gs_citation": 140,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11492776665338282002&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "705784e3f0",
        "title": "Learning to Take Concurrent Actions",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/4b4edc2630fe75800ddc29a7b4070add-Abstract.html",
        "author": "Khashayar Rohanimanesh; Sridhar Mahadevan",
        "abstract": "We investigate a general semi-Markov Decision Process (SMDP) framework for modeling concurrent decision making, where agents learn optimal plans over concurrent temporally extended actions. We introduce three types of parallel termination schemes { all, any and continue { and theoretically and experimentally compare them.",
        "bibtex": "@inproceedings{NIPS2002_4b4edc26,\n author = {Rohanimanesh, Khashayar and Mahadevan, Sridhar},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Learning to Take Concurrent Actions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/4b4edc2630fe75800ddc29a7b4070add-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/4b4edc2630fe75800ddc29a7b4070add-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/4b4edc2630fe75800ddc29a7b4070add-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 150661,
        "gs_citation": 77,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18429676078454918609&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science, University of Massachusetts; Department of Computer Science, University of Massachusetts",
        "aff_domain": "cs.umass.edu;cs.umass.edu",
        "email": "cs.umass.edu;cs.umass.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Massachusetts",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.umass.edu",
        "aff_unique_abbr": "UMass",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d2925f885f",
        "title": "Learning with Multiple Labels",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/653ac11ca60b3e021a8c609c7198acfc-Abstract.html",
        "author": "Rong Jin; Zoubin Ghahramani",
        "abstract": "In this paper,  we  study a  special kind of learning problem in which  each  training  instance  is  given  a  set  of  (or  distribution  over)  candidate  class  labels  and  only  one  of the  candidate  labels  is  the  correct  one.  Such  a  problem  can  occur,  e.g.,  in  an  information  retrieval  setting  where  a  set  of words  is  associated  with  an  image,  or  if  classes  labels  are  organized  hierarchically.  We  propose  a  novel  discriminative  approach  for  handling  the  ambiguity  of class  labels  in the  training  examples.  The  experiments with the  proposed  approach over five  different UCI datasets  show that our approach is  able  to  find  the  correct label  among the  set of candidate  labels  and  actually  achieve  performance  close  to  the  case  when  each  training  instance  is  given  a  single  correct  label.  In  contrast,  naIve  methods  degrade rapidly as  more ambiguity is introduced into the labels.",
        "bibtex": "@inproceedings{NIPS2002_653ac11c,\n author = {Jin, Rong and Ghahramani, Zoubin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Learning with Multiple Labels},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/653ac11ca60b3e021a8c609c7198acfc-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/653ac11ca60b3e021a8c609c7198acfc-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/653ac11ca60b3e021a8c609c7198acfc-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1566153,
        "gs_citation": 508,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1968014348917934155&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "School of Computer Science, Carnegie Mellon University; Gatsby Computational Neuroscience Unit, University College London",
        "aff_domain": "es.emu.edu;gatsby.ucl.ae.uk",
        "email": "es.emu.edu;gatsby.ucl.ae.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Carnegie Mellon University;University College London",
        "aff_unique_dep": "School of Computer Science;Gatsby Computational Neuroscience Unit",
        "aff_unique_url": "https://www.cmu.edu;https://www.ucl.ac.uk",
        "aff_unique_abbr": "CMU;UCL",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Pittsburgh;London",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "id": "6af603aa79",
        "title": "Linear Combinations of Optic Flow Vectors for Estimating Self-Motion - a Real-World Test of a Neural Model",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/df1f1d20ee86704251795841e6a9405a-Abstract.html",
        "author": "Matthias O. Franz; Javaan S. Chahl",
        "abstract": "The tangential neurons in the \ufb02y brain are sensitive to the typical optic \ufb02ow patterns generated during self-motion. In this study, we examine whether a simpli\ufb01ed linear model of these neurons can be used to esti- mate self-motion from the optic \ufb02ow. We present a theory for the con- struction of an estimator consisting of a linear combination of optic \ufb02ow vectors that incorporates prior knowledge both about the distance distri- bution of the environment, and about the noise and self-motion statistics of the sensor. The estimator is tested on a gantry carrying an omnidirec- tional vision sensor. The experiments show that the proposed approach leads to accurate and robust estimates of rotation rates, whereas transla- tion estimates turn out to be less reliable.",
        "bibtex": "@inproceedings{NIPS2002_df1f1d20,\n author = {Franz, Matthias and Chahl, Javaan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Linear Combinations of Optic Flow Vectors for Estimating Self-Motion - a Real-World Test of a Neural Model},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/df1f1d20ee86704251795841e6a9405a-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/df1f1d20ee86704251795841e6a9405a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/df1f1d20ee86704251795841e6a9405a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 100570,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2267269733007910099&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "MPI f\u00a8ur biologische Kybernetik; Center of Visual Sciences, RSBS",
        "aff_domain": "tuebingen.mpg.de;zappa.anu.edu.au",
        "email": "tuebingen.mpg.de;zappa.anu.edu.au",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "MPI f\u00a8ur biologische Kybernetik;Center of Visual Sciences, RSBS",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "599401fe7c",
        "title": "Location Estimation with a Differential Update Network",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/c8dfece5cc68249206e4690fc4737a8d-Abstract.html",
        "author": "Ali Rahimi; Trevor Darrell",
        "abstract": "Given a set of hidden variables with an a-priori Markov structure, we derive an online algorithm which approximately updates the posterior as pairwise measurements between the hidden variables become available. The update is performed using Assumed Density Filtering: to incorporate each pairwise measurement, we compute the optimal Markov structure which represents the true posterior and use it as a prior for incorporating the next measurement. We demonstrate the resulting algorithm by cal- culating globally consistent trajectories of a robot as it navigates along a 2D trajectory. To update a trajectory of length t, the update takes O(t). When all conditional distributions are linear-Gaussian, the algorithm can be thought of as a Kalman Filter which simpli\ufb01es the state covariance matrix after incorporating each measurement.",
        "bibtex": "@inproceedings{NIPS2002_c8dfece5,\n author = {Rahimi, Ali and Darrell, Trevor},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Location Estimation with a Differential Update Network},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/c8dfece5cc68249206e4690fc4737a8d-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/c8dfece5cc68249206e4690fc4737a8d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/c8dfece5cc68249206e4690fc4737a8d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 133377,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4533145825743600303&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "11e897398b",
        "title": "Manifold Parzen Windows",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/2d969e2cee8cfa07ce7ca0bb13c7a36d-Abstract.html",
        "author": "Pascal Vincent; Yoshua Bengio",
        "abstract": "The similarity between objects is a fundamental element of many learn- ing algorithms. Most non-parametric methods take this similarity to be \ufb01xed, but much recent work has shown the advantages of learning it, in particular to exploit the local invariances in the data or to capture the possibly non-linear manifold on which most of the data lies. We propose a new non-parametric kernel density estimation method which captures the local structure of an underlying manifold through the leading eigen- vectors of regularized local covariance matrices. Experiments in density estimation show signi\ufb01cant improvements with respect to Parzen density estimators. The density estimators can also be used within Bayes classi- \ufb01ers, yielding classi\ufb01cation rates similar to SVMs and much superior to the Parzen classi\ufb01er.",
        "bibtex": "@inproceedings{NIPS2002_2d969e2c,\n author = {Vincent, Pascal and Bengio, Yoshua},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Manifold Parzen Windows},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/2d969e2cee8cfa07ce7ca0bb13c7a36d-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/2d969e2cee8cfa07ce7ca0bb13c7a36d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/2d969e2cee8cfa07ce7ca0bb13c7a36d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 121720,
        "gs_citation": 176,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7007780751823226085&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "http://www.iro.umontreal.ca/",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "24d5339bd7",
        "title": "Margin Analysis of the LVQ Algorithm",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/bbaa9d6a1445eac881750bea6053f564-Abstract.html",
        "author": "Koby Crammer; Ran Gilad-bachrach; Amir Navot; Naftali Tishby",
        "abstract": "Prototypes based algorithms are commonly used to reduce the computa- tional complexity of Nearest-Neighbour (NN) classi\ufb01ers. In this paper we discuss theoretical and algorithmical aspects of such algorithms. On the theory side, we present margin based generalization bounds that sug- gest that these kinds of classi\ufb01ers can be more accurate then the 1-NN rule. Furthermore, we derived a training algorithm that selects a good set of prototypes using large margin principles. We also show that the 20 years old Learning Vector Quantization (LVQ) algorithm emerges natu- rally from our framework.",
        "bibtex": "@inproceedings{NIPS2002_bbaa9d6a,\n author = {Crammer, Koby and Gilad-bachrach, Ran and Navot, Amir and Tishby, Naftali},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Margin Analysis of the LVQ Algorithm},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/bbaa9d6a1445eac881750bea6053f564-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/bbaa9d6a1445eac881750bea6053f564-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/bbaa9d6a1445eac881750bea6053f564-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 76161,
        "gs_citation": 331,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3257170521568874267&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "School of Computer Science and Engineering and Interdisciplinary Center for Neural Computation, The Hebrew University, Jerusalem, Israel; School of Computer Science and Engineering and Interdisciplinary Center for Neural Computation, The Hebrew University, Jerusalem, Israel; School of Computer Science and Engineering and Interdisciplinary Center for Neural Computation, The Hebrew University, Jerusalem, Israel; School of Computer Science and Engineering and Interdisciplinary Center for Neural Computation, The Hebrew University, Jerusalem, Israel",
        "aff_domain": "cs.huji.ac.il;cs.huji.ac.il;cs.huji.ac.il;cs.huji.ac.il",
        "email": "cs.huji.ac.il;cs.huji.ac.il;cs.huji.ac.il;cs.huji.ac.il",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "School of Computer Science and Engineering and Interdisciplinary Center for Neural Computation, The Hebrew University, Jerusalem, Israel",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "af473d0ec9",
        "title": "Margin-Based Algorithms for Information Filtering",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/1e8c391abfde9abea82d75a2d60278d4-Abstract.html",
        "author": "Nicol\u00f2 Cesa-bianchi; Alex Conconi; Claudio Gentile",
        "abstract": "In this work, we study an information \ufb01ltering model where the relevance labels associated to a sequence of feature vectors are realizations of an unknown probabilistic linear function. Building on the analysis of a re- stricted version of our model, we derive a general \ufb01ltering rule based on the margin of a ridge regression estimator. While our rule may observe the label of a vector only by classfying the vector as relevant, experiments on a real-world document \ufb01ltering problem show that the performance of our rule is close to that of the on-line classi\ufb01er which is allowed to observe all labels. These empirical results are complemented by a theo- retical analysis where we consider a randomized variant of our rule and prove that its expected number of mistakes is never much larger than that of the optimal \ufb01ltering rule which knows the hidden linear model.",
        "bibtex": "@inproceedings{NIPS2002_1e8c391a,\n author = {Cesa-bianchi, Nicol\\`{o} and Conconi, Alex and Gentile, Claudio},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Margin-Based Algorithms for Information Filtering},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/1e8c391abfde9abea82d75a2d60278d4-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/1e8c391abfde9abea82d75a2d60278d4-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/1e8c391abfde9abea82d75a2d60278d4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 120600,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2603210133653071362&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "DTI, University of Milan; DTI, University of Milan; CRII, Universit\u00e0 dell\u2019Insubria",
        "aff_domain": "dti.unimi.it;dti.unimi.it;dsi.unimi.it",
        "email": "dti.unimi.it;dti.unimi.it;dsi.unimi.it",
        "github": "",
        "project": "http://www.kermitproject.org/",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "DTI, University of Milan;CRII, Universit\u00e0 dell\u2019Insubria",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "15234b7b07",
        "title": "Maximally Informative Dimensions: Analyzing Neural Responses to Natural Signals",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/69dafe8b58066478aea48f3d0f384820-Abstract.html",
        "author": "Tatyana Sharpee; Nicole C. Rust; William Bialek",
        "abstract": "Abstract Unavailable",
        "bibtex": "@inproceedings{NIPS2002_69dafe8b,\n author = {Sharpee, Tatyana and Rust, Nicole and Bialek, William},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Maximally Informative Dimensions: Analyzing Neural Responses to Natural Signals},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/69dafe8b58066478aea48f3d0f384820-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/69dafe8b58066478aea48f3d0f384820-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/69dafe8b58066478aea48f3d0f384820-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 122730,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=73723003517226142&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 20,
        "aff": "Sloan\u2013Swartz Center for Theoretical Neurobiology, Department of Physiology, University of California at San Francisco, San Francisco, California 94143\u20130444; Center for Neural Science, New York University, New York, NY 10003; Department of Physics, Princeton University, Princeton, New Jersey 08544",
        "aff_domain": "phy.ucsf.edu;cns.nyu.edu;princeton.edu",
        "email": "phy.ucsf.edu;cns.nyu.edu;princeton.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Sloan\u2013Swartz Center for Theoretical Neurobiology, Department of Physiology, University of California at San Francisco, San Francisco, California 94143\u20130444;Center for Neural Science, New York University, New York, NY 10003;Department of Physics, Princeton University, Princeton, New Jersey 08544",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "772b15f836",
        "title": "Maximum Likelihood and the Information Bottleneck",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/f3d9de86462c28781cbe5c47ef22c3e5-Abstract.html",
        "author": "Noam Slonim; Yair Weiss",
        "abstract": "",
        "bibtex": "@inproceedings{NIPS2002_f3d9de86,\n author = {Slonim, Noam and Weiss, Yair},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Maximum Likelihood and the Information Bottleneck},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/f3d9de86462c28781cbe5c47ef22c3e5-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/f3d9de86462c28781cbe5c47ef22c3e5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/f3d9de86462c28781cbe5c47ef22c3e5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 159615,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9319934441101754282&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "School of Computer Science & Engineering, Hebrew University, Jerusalem 91904, Israel; School of Computer Science & Engineering, Hebrew University, Jerusalem 91904, Israel",
        "aff_domain": "cs.huji.ac.il;cs.huji.ac.il",
        "email": "cs.huji.ac.il;cs.huji.ac.il",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "School of Computer Science & Engineering, Hebrew University, Jerusalem 91904, Israel",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "1c049023cc",
        "title": "Mean Field Approach to a Probabilistic Model in Information Retrieval",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/a07c2f3b3b907aaf8436a26c6d77f0a2-Abstract.html",
        "author": "Bin Wu; K. Wong; David Bodoff",
        "abstract": "We study an explicit parametric model of documents, queries, and rel- evancy assessment for Information Retrieval (IR). Mean-\ufb01eld methods are applied to analyze the model and derive ef\ufb01cient practical algorithms to estimate the parameters in the problem. The hyperparameters are es- timated by a fast approximate leave-one-out cross-validation procedure based on the cavity method. The algorithm is further evaluated on several benchmark databases by comparing with standard algorithms in IR.",
        "bibtex": "@inproceedings{NIPS2002_a07c2f3b,\n author = {Wu, Bin and Wong, K. and Bodoff, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Mean Field Approach to a Probabilistic Model in Information Retrieval},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/a07c2f3b3b907aaf8436a26c6d77f0a2-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/a07c2f3b3b907aaf8436a26c6d77f0a2-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/a07c2f3b3b907aaf8436a26c6d77f0a2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 136274,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16728322441332947802&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Physics, Hong Kong University of Science and Technology; Department of Physics, Hong Kong University of Science and Technology; Department of ISMT, Hong Kong University of Science and Technology",
        "aff_domain": "ust.hk;ust.hk;ust.hk",
        "email": "ust.hk;ust.hk;ust.hk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Department of Physics, Hong Kong University of Science and Technology;Department of ISMT, Hong Kong University of Science and Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "3fe1a200d0",
        "title": "Minimax Differential Dynamic Programming: An Application to Robust Biped Walking",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/e6c2dc3dee4a51dcec3a876aa2339a78-Abstract.html",
        "author": "Jun Morimoto; Christopher G. Atkeson",
        "abstract": "We developed a robust control policy design method in high-dimensional state space by using differential dynamic programming with a minimax criterion. As an example, we applied our method to a simulated \ufb01ve link biped robot. The results show lower joint torques from the optimal con- trol policy compared to a hand-tuned PD servo controller. Results also show that the simulated biped robot can successfully walk with unknown disturbances that cause controllers generated by standard differential dy- namic programming and the hand-tuned PD servo to fail. Learning to compensate for modeling error and previously unknown disturbances in conjunction with robust control design is also demonstrated.",
        "bibtex": "@inproceedings{NIPS2002_e6c2dc3d,\n author = {Morimoto, Jun and Atkeson, Christopher},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Minimax Differential Dynamic Programming: An Application to Robust Biped Walking},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/e6c2dc3dee4a51dcec3a876aa2339a78-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/e6c2dc3dee4a51dcec3a876aa2339a78-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/e6c2dc3dee4a51dcec3a876aa2339a78-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 203456,
        "gs_citation": 129,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=640448372040444899&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": "Human Information Science Labs, Department 3, ATR International, Keihanna Science City, Kyoto, JAPAN, 619-0288; The Robotics Institute and HCII, Carnegie Mellon University, 5000 Forbes Ave., Pittsburgh, USA, 15213 + Human Information Science Laboratories, Department 3, ATRInternational",
        "aff_domain": "atr.co.jp;cs.cmu.edu",
        "email": "atr.co.jp;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+2",
        "aff_unique_norm": "Human Information Science Labs, Department 3, ATR International, Keihanna Science City, Kyoto, JAPAN, 619-0288;The Robotics Institute and HCII, Carnegie Mellon University, 5000 Forbes Ave., Pittsburgh, USA, 15213;Human Information Science Laboratories, Department 3, ATRInternational",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "2ada9f450f",
        "title": "Mismatch String Kernels for SVM Protein Classification",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/12b1e42dc0746f22cf361267de07073f-Abstract.html",
        "author": "Eleazar Eskin; Jason Weston; William S. Noble; Christina S. Leslie",
        "abstract": "We introduce a class of string kernels, called mismatch kernels, for use with support vector machines (SVMs) in a discriminative approach to the protein classi\ufb01cation problem. These kernels measure sequence sim- ilarity based on shared occurrences of  -length subsequences, counted with up to mismatches, and do not rely on any generative model for the positive training sequences. We compute the kernels ef\ufb01ciently using a mismatch tree data structure and report experiments on a benchmark SCOP dataset, where we show that the mismatch kernel used with an SVM classi\ufb01er performs as well as the Fisher kernel, the most success- ful method for remote homology detection, while achieving considerable computational savings.",
        "bibtex": "@inproceedings{NIPS2002_12b1e42d,\n author = {Eskin, Eleazar and Weston, Jason and Noble, William and Leslie, Christina},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Mismatch String Kernels for SVM Protein Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/12b1e42dc0746f22cf361267de07073f-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/12b1e42dc0746f22cf361267de07073f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/12b1e42dc0746f22cf361267de07073f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 103240,
        "gs_citation": 984,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10484798326463698033&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 32,
        "aff": "Department of Computer Science, Columbia University; Department of Computer Science, Columbia University; Max-Planck Institute, Tuebingen, Germany; Department of Genome Sciences, University of Washington",
        "aff_domain": "cs.columbia.edu;cs.columbia.edu;tuebingen.mpg.de;gs.washington.edu",
        "email": "cs.columbia.edu;cs.columbia.edu;tuebingen.mpg.de;gs.washington.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2",
        "aff_unique_norm": "Columbia University;Max-Planck Institute, Tuebingen, Germany;Department of Genome Sciences, University of Washington",
        "aff_unique_dep": "Department of Computer Science;;",
        "aff_unique_url": "https://www.columbia.edu;;",
        "aff_unique_abbr": "Columbia;;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "50b975abbd",
        "title": "Modeling Midazolam's Effect on the Hippocampus and Recognition Memory",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/716e1b8c6cd17b771da77391355749f3-Abstract.html",
        "author": "Kenneth J. Malmberg; Ren\u00e9 Zeelenberg; Richard M. Shiffrin",
        "abstract": "'~1idazolam",
        "bibtex": "@inproceedings{NIPS2002_716e1b8c,\n author = {Malmberg, Kenneth and Zeelenberg, Ren\\'{e} and Shiffrin, Richard},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Modeling Midazolam\\textquotesingle s Effect on the Hippocampus and Recognition Memory},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/716e1b8c6cd17b771da77391355749f3-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/716e1b8c6cd17b771da77391355749f3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/716e1b8c6cd17b771da77391355749f3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1306278,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16847554549320090423&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Psychology, Indiana University, Bloomington, IN 47405; Department of Psychology, Indiana University, Bloomington, IN 47405; Departments of Cognitive Science and Psychology, Indiana University, Bloomington, TN 47405",
        "aff_domain": "indiana.edu;indiana.edu;indiana.edu",
        "email": "indiana.edu;indiana.edu;indiana.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Department of Psychology, Indiana University, Bloomington, IN 47405;Departments of Cognitive Science and Psychology, Indiana University, Bloomington, TN 47405",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "5660b7db92",
        "title": "Monaural Speech Separation",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/db9eeb7e678863649bce209842e0d164-Abstract.html",
        "author": "Guoning Hu; Deliang Wang",
        "abstract": "that deals with",
        "bibtex": "@inproceedings{NIPS2002_db9eeb7e,\n author = {Hu, Guoning and Wang, Deliang},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Monaural Speech Separation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/db9eeb7e678863649bce209842e0d164-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/db9eeb7e678863649bce209842e0d164-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/db9eeb7e678863649bce209842e0d164-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 272675,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9694508192502633223&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Biophysics Program Department of Computer and Information Science & Center of Cognitive Science The Ohio State University; The Ohio State University, Columbus, OH 43210",
        "aff_domain": "osu.edu;cis.ohio-state.edu",
        "email": "osu.edu;cis.ohio-state.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Biophysics Program Department of Computer and Information Science & Center of Cognitive Science The Ohio State University;The Ohio State University, Columbus, OH 43210",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "321399c726",
        "title": "Morton-Style Factorial Coding of Color in Primary Visual Cortex",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/6cd9313ed34ef58bad3fdd504355e72c-Abstract.html",
        "author": "Javier R. Movellan; Thomas Wachtler; Thomas D. Albright; Terrence Sejnowski",
        "abstract": "We introduce the notion of Morton-style factorial coding and illustrate how it may help understand information integration and perceptual cod- ing in the brain. We show that by focusing on average responses one may miss the existence of factorial coding mechanisms that become only apparent when analyzing spike count histograms. We show evidence suggesting that the classical/non-classical receptive \ufb01eld organization in the cortex effectively enforces the development of Morton-style factorial codes. This may provide some cues to help understand perceptual cod- ing in the brain and to develop new unsupervised learning algorithms. While methods like ICA (Bell & Sejnowski, 1997) develop independent codes, in Morton-style coding the goal is to make two or more external aspects of the world become independent when conditioning on internal representations.",
        "bibtex": "@inproceedings{NIPS2002_6cd9313e,\n author = {Movellan, Javier and Wachtler, Thomas and Albright, Thomas D. and Sejnowski, Terrence},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Morton-Style Factorial Coding of Color in Primary Visual Cortex},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/6cd9313ed34ef58bad3fdd504355e72c-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/6cd9313ed34ef58bad3fdd504355e72c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/6cd9313ed34ef58bad3fdd504355e72c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 240320,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10727316273962774075&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Institute for Neural Computation, University of California San Diego; Sloan Center for Theoretical Neurobiology, The Salk Institute; Howard Hughes Medical Institute, The Salk Institute; Computational Neurobiology Laboratory, The Salk Institute",
        "aff_domain": "inc.ucsd.edu;salk.edu;salk.edu;salk.edu",
        "email": "inc.ucsd.edu;salk.edu;salk.edu;salk.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "Institute for Neural Computation, University of California San Diego;Sloan Center for Theoretical Neurobiology, The Salk Institute;Howard Hughes Medical Institute, The Salk Institute;Computational Neurobiology Laboratory, The Salk Institute",
        "aff_unique_dep": ";;;",
        "aff_unique_url": ";;;",
        "aff_unique_abbr": ";;;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "6370341c9e",
        "title": "Multiclass Learning by Probabilistic Embeddings",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/96671501524948bc3937b4b30d0e57b9-Abstract.html",
        "author": "Ofer Dekel; Yoram Singer",
        "abstract": "We describe a new algorithmic framework for learning multiclass catego- rization problems. In this framework a multiclass predictor is composed of a pair of embeddings that map both instances and labels into a common space. In this space each instance is assigned the label it is nearest to. We outline and analyze an algorithm, termed Bunching, for learning the pair of embeddings from labeled data. A key construction in the analysis of the algorithm is the notion of probabilistic output codes, a generaliza- tion of error correcting output codes (ECOC). Furthermore, the method of multiclass categorization using ECOC is shown to be an instance of Bunching. We demonstrate the advantage of Bunching over ECOC by comparing their performance on numerous categorization problems.",
        "bibtex": "@inproceedings{NIPS2002_96671501,\n author = {Dekel, Ofer and Singer, Yoram},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Multiclass Learning by Probabilistic Embeddings},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/96671501524948bc3937b4b30d0e57b9-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/96671501524948bc3937b4b30d0e57b9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/96671501524948bc3937b4b30d0e57b9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 104445,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14196055827161772725&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "School of Computer Science & Engineering, The Hebrew University, Jerusalem 91904, Israel; School of Computer Science & Engineering, The Hebrew University, Jerusalem 91904, Israel",
        "aff_domain": "cs.huji.ac.il;cs.huji.ac.il",
        "email": "cs.huji.ac.il;cs.huji.ac.il",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "School of Computer Science & Engineering, The Hebrew University, Jerusalem 91904, Israel",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "1c41db5efd",
        "title": "Multiple Cause Vector Quantization",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/1368ba1ab6ed38bb1f26f36673739d54-Abstract.html",
        "author": "David A. Ross; Richard S. Zemel",
        "abstract": "We propose a model that can learn parts-based representations of high- dimensional data. Our key assumption is that the dimensions of the data can be separated into several disjoint subsets, or factors, which take on values independently of each other. We assume each factor has a small number of discrete states, and model it using a vector quantizer. The selected states of each factor represent the multiple causes of the input. Given a set of training examples, our model learns the association of data dimensions with factors, as well as the states of each VQ. Inference and learning are carried out ef\ufb01ciently via variational algorithms. We present applications of this model to problems in image decomposition, collaborative \ufb01ltering, and text classi\ufb01cation.",
        "bibtex": "@inproceedings{NIPS2002_1368ba1a,\n author = {Ross, David and Zemel, Richard},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Multiple Cause Vector Quantization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/1368ba1ab6ed38bb1f26f36673739d54-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/1368ba1ab6ed38bb1f26f36673739d54-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/1368ba1ab6ed38bb1f26f36673739d54-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 129871,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12499764388827448050&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff": "Department of Computer Science, University of Toronto; Department of Computer Science, University of Toronto",
        "aff_domain": "cs.toronto.edu;cs.toronto.edu",
        "email": "cs.toronto.edu;cs.toronto.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Toronto",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utoronto.ca",
        "aff_unique_abbr": "U of T",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Toronto",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "abfc6dfb3b",
        "title": "Multiplicative Updates for Nonnegative Quadratic Programming in Support Vector Machines",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/dc16622ddc767e6bc1200fe5df2fbdfb-Abstract.html",
        "author": "Fei Sha; Lawrence K. Saul; Daniel D. Lee",
        "abstract": "We derive multiplicative updates for solving the nonnegative quadratic programming problem in support vector machines (SVMs). The updates have a simple closed form, and we prove that they converge monotoni- cally to the solution of the maximum margin hyperplane. The updates optimize the traditionally proposed objective function for SVMs. They do not involve any heuristics such as choosing a learning rate or deciding which variables to update at each iteration. They can be used to adjust all the quadratic programming variables in parallel with a guarantee of im- provement at each iteration. We analyze the asymptotic convergence of the updates and show that the coef\ufb01cients of non-support vectors decay geometrically to zero at a rate that depends on their margins. In practice, the updates converge very rapidly to good classi\ufb01ers.",
        "bibtex": "@inproceedings{NIPS2002_dc16622d,\n author = {Sha, Fei and Saul, Lawrence and Lee, Daniel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Multiplicative Updates for Nonnegative Quadratic Programming in Support Vector Machines},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/dc16622ddc767e6bc1200fe5df2fbdfb-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/dc16622ddc767e6bc1200fe5df2fbdfb-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/dc16622ddc767e6bc1200fe5df2fbdfb-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 82284,
        "gs_citation": 358,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12972663402457793872&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 23,
        "aff": "Department of Computer and Information Science; Department of Computer and Information Science; Department of Electrical and System Engineering",
        "aff_domain": "cis.upenn.edu;cis.upenn.edu;ee.upenn.edu",
        "email": "cis.upenn.edu;cis.upenn.edu;ee.upenn.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Department of Computer and Information Science;Department of Electrical and System Engineering",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "f020fceb29",
        "title": "Nash Propagation for Loopy Graphical Games",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/26505e0494662534f633586941b77d0c-Abstract.html",
        "author": "Luis E. Ortiz; Michael Kearns",
        "abstract": "We introduce NashProp, an iterative and local message-passing algo- rithm for computing Nash equilibria in multi-player games represented by arbitrary undirected graphs. We provide a formal analysis and exper- imental evidence demonstrating that NashProp performs well on large graphical games with many loops, often converging in just a dozen itera- tions on graphs with hundreds of nodes. NashProp generalizes the tree algorithm of (Kearns et al. 2001), and can be viewed as similar in spirit to belief propagation in probabilis- tic inference, and thus complements the recent work of (Vickrey and Koller 2002), who explored a junction tree approach. Thus, as for prob- abilistic inference, we have at least two promising general-purpose ap- proaches to equilibria computation in graphs.",
        "bibtex": "@inproceedings{NIPS2002_26505e04,\n author = {Ortiz, Luis E and Kearns, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Nash Propagation for Loopy Graphical Games},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/26505e0494662534f633586941b77d0c-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/26505e0494662534f633586941b77d0c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/26505e0494662534f633586941b77d0c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 132934,
        "gs_citation": 100,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5841564667108814470&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Computer and Information Science, University of Pennsylvania; Department of Computer and Information Science, University of Pennsylvania",
        "aff_domain": "cis.upenn.edu;cis.upenn.edu",
        "email": "cis.upenn.edu;cis.upenn.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Pennsylvania",
        "aff_unique_dep": "Department of Computer and Information Science",
        "aff_unique_url": "https://www.upenn.edu",
        "aff_unique_abbr": "UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "44a93b53b5",
        "title": "Neural Decoding of Cursor Motion Using a Kalman Filter",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/169779d3852b32ce8b1a1724dbf5217d-Abstract.html",
        "author": "W Wu; M. J. Black; Y. Gao; M. Serruya; A. Shaikhouni; J. P. Donoghue; Elie Bienenstock",
        "abstract": "The direct neural control of external devices such as computer displays or prosthetic limbs requires the accurate decoding of neural activity rep- resenting continuous movement. We develop a real-time control system using the spiking activity of approximately 40 neurons recorded with an electrode array implanted in the arm area of primary motor cortex. In contrast to previous work, we develop a control-theoretic approach that explicitly models the motion of the hand and the probabilistic re- lationship between this motion and the mean \ufb01ring rates of the cells in 70 bins. We focus on a realistic cursor control task in which the sub- ject must move a cursor to \u201chit\u201d randomly placed targets on a computer monitor. Encoding and decoding of the neural data is achieved with a Kalman \ufb01lter which has a number of advantages over previous linear \ufb01ltering techniques. In particular, the Kalman \ufb01lter reconstructions of hand trajectories in off-line experiments are more accurate than previ- ously reported results and the model provides insights into the nature of the neural coding of movement.",
        "bibtex": "@inproceedings{NIPS2002_169779d3,\n author = {Wu, W and Black, M. and Gao, Y. and Serruya, M. and Shaikhouni, A. and Donoghue, J. and Bienenstock, Elie},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Neural Decoding of Cursor Motion Using a Kalman Filter},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/169779d3852b32ce8b1a1724dbf5217d-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/169779d3852b32ce8b1a1724dbf5217d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/169779d3852b32ce8b1a1724dbf5217d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 191177,
        "gs_citation": 225,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8833121176478800908&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff": "Division of Applied Mathematics; Dept. of Computer Science; Dept. of Neuroscience; Division of Biology and Medicine; Division of Biology and Medicine; Division of Biology and Medicine; Division of Biology and Medicine",
        "aff_domain": "cfm.brown.edu;cs.brown.edu;cfm.brown.edu;dam.brown.edu;brown.edu;brown.edu;brown.edu",
        "email": "cfm.brown.edu;cs.brown.edu;cfm.brown.edu;dam.brown.edu;brown.edu;brown.edu;brown.edu",
        "github": "",
        "project": "",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3;3;3;3",
        "aff_unique_norm": "Division of Applied Mathematics;University Affiliation Not Specified;Dept. of Neuroscience;Division of Biology and Medicine",
        "aff_unique_dep": ";Department of Computer Science;;",
        "aff_unique_url": ";;;",
        "aff_unique_abbr": ";;;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "b1c1fce5cf",
        "title": "Neuromorphic Bisable VLSI Synapses with Spike-Timing-Dependent Plasticity",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/273448411df1962cba1db6c05b3213c9-Abstract.html",
        "author": "Giacomo Indiveri",
        "abstract": "We present analog neuromorphic circuits for implementing bistable syn- apses with spike-timing-dependent plasticity (STDP) properties. In these types of synapses, the short-term dynamics of the synaptic ef\ufb01cacies are governed by the relative timing of the pre- and post-synaptic spikes, while on long time scales the ef\ufb01cacies tend asymptotically to either a potentiated state or to a depressed one. We fabricated a prototype VLSI chip containing a network of integrate and \ufb01re neurons interconnected via bistable STDP synapses. Test results from this chip demonstrate the synapse\u2019s STDP learning properties, and its long-term bistable charac- teristics.",
        "bibtex": "@inproceedings{NIPS2002_27344841,\n author = {Indiveri, Giacomo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Neuromorphic Bisable VLSI Synapses with Spike-Timing-Dependent Plasticity},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/273448411df1962cba1db6c05b3213c9-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/273448411df1962cba1db6c05b3213c9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/273448411df1962cba1db6c05b3213c9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 169183,
        "gs_citation": 76,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18339181268856766098&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Institute of Neuroinformatics, University/ETH Zurich, CH-8057 Zurich, Switzerland",
        "aff_domain": "ini.phys.ethz.ch",
        "email": "ini.phys.ethz.ch",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Institute of Neuroinformatics, University/ETH Zurich, CH-8057 Zurich, Switzerland",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "b74dab7292",
        "title": "Nonparametric Representation of Policies and Value Functions: A Trajectory-Based Approach",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/01894d6f048493d2cacde3c579c315a3-Abstract.html",
        "author": "Christopher G. Atkeson; Jun Morimoto",
        "abstract": "A longstanding goal of reinforcement learning is to develop non- parametric representations of policies and value functions that support rapid learning without suffering from interference or the curse of di- mensionality. We have developed a trajectory-based approach, in which policies and value functions are represented nonparametrically along tra- jectories. These trajectories, policies, and value functions are updated as the value function becomes more accurate or as a model of the task is up- dated. We have applied this approach to periodic tasks such as hopping and walking, which required handling discount factors and discontinu- ities in the task dynamics, and using function approximation to represent value functions at discontinuities. We also describe extensions of the ap- proach to make the policies more robust to modeling error and sensor noise.",
        "bibtex": "@inproceedings{NIPS2002_01894d6f,\n author = {Atkeson, Christopher and Morimoto, Jun},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Nonparametric Representation of Policies and Value Functions: A Trajectory-Based Approach},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/01894d6f048493d2cacde3c579c315a3-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/01894d6f048493d2cacde3c579c315a3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/01894d6f048493d2cacde3c579c315a3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 242618,
        "gs_citation": 98,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4782898947938763761&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": "Robotics Institute and HCII, Carnegie Mellon University; ATR Human Information Science Laboratories, Dept. 3 + Robotics Institute and HCII, Carnegie Mellon University",
        "aff_domain": "cmu.edu;atr.co.jp",
        "email": "cmu.edu;atr.co.jp",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+0",
        "aff_unique_norm": "Robotics Institute and HCII, Carnegie Mellon University;ATR Human Information Science Laboratories, Dept. 3",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "d7dd07551e",
        "title": "On the Complexity of Learning the Kernel Matrix",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/46a558d97954d0692411c861cf78ef79-Abstract.html",
        "author": "Olivier Bousquet; Daniel Herrmann",
        "abstract": "We investigate data based procedures for selecting the kernel when learn- ing with Support Vector Machines. We provide generalization error bounds by estimating the Rademacher complexities of the corresponding function classes. In particular we obtain a complexity bound for function classes induced by kernels with given eigenvectors, i.e., we allow to vary the spectrum and keep the eigenvectors \ufb01x. This bound is only a loga- rithmic factor bigger than the complexity of the function class induced by a single kernel. However, optimizing the margin over such classes leads to over\ufb01tting. We thus propose a suitable way of constraining the class. We use an ef\ufb01cient algorithm to solve the resulting optimization problem, present preliminary experimental results, and compare them to an alignment-based approach.",
        "bibtex": "@inproceedings{NIPS2002_46a558d9,\n author = {Bousquet, Olivier and Herrmann, Daniel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {On the Complexity of Learning the Kernel Matrix},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/46a558d97954d0692411c861cf78ef79-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/46a558d97954d0692411c861cf78ef79-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/46a558d97954d0692411c861cf78ef79-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 107353,
        "gs_citation": 176,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5782562317623181989&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "be6dc44af3",
        "title": "On the Dirichlet Prior and Bayesian Regularization",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/1819932ff5cf474f4f19e7c7024640c2-Abstract.html",
        "author": "Harald Steck; Tommi S. Jaakkola",
        "abstract": "A  common  objective  in  learning  a  model  from  data is  to  recover  its network structure,  while the model parameters are of minor in(cid:173) terest.  For example,  we  may  wish  to  recover  regulatory  networks  from high-throughput data sources.  In  this paper we  examine how  Bayesian  regularization  using  a  product  of independent  Dirichlet  priors over  the model  parameters affects  the  learned model  struc(cid:173) ture  in  a  domain  with  discrete  variables.  We  show  that  a  small  scale  parameter - often interpreted as  \"equivalent sample size\"  or  \"prior  strength\"  - leads  to  a  strong  regularization  of  the  model  structure (sparse graph)  given a  sufficiently large data set.  In  par(cid:173) ticular, the empty graph is obtained in the limit of a vanishing scale  parameter.  This is  diametrically opposite to what one may expect  in  this  limit,  namely  the  complete graph from  an  (unregularized)  maximum likelihood estimate.  Since  the prior affects  the parame(cid:173) ters as  expected, the scale parameter balances a  trade-off  between  regularizing  the  parameters  vs.  the  structure  of  the  model.  We  demonstrate  the  benefits  of optimizing  this  trade-off in  the  sense  of predictive accuracy.",
        "bibtex": "@inproceedings{NIPS2002_1819932f,\n author = {Steck, Harald and Jaakkola, Tommi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {On the Dirichlet Prior and Bayesian Regularization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/1819932ff5cf474f4f19e7c7024640c2-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/1819932ff5cf474f4f19e7c7024640c2-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/1819932ff5cf474f4f19e7c7024640c2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1725270,
        "gs_citation": 141,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12720719658608278778&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Artificial Intelligence Laboratory, Massachusetts Institute of Technology; Artificial Intelligence Laboratory, Massachusetts Institute of Technology",
        "aff_domain": "ai.mit.edu;ai.mit.edu",
        "email": "ai.mit.edu;ai.mit.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Artificial Intelligence Laboratory, Massachusetts Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "2e2cc564e3",
        "title": "One-Class LP Classifiers for Dissimilarity Representations",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/6e5025ccc7d638ae4e724da8938450a6-Abstract.html",
        "author": "Elzbieta Pekalska; David M.J. Tax; Robert Duin",
        "abstract": "Problems in which abnormal or novel situations should be detected can be approached by describing the domain of the class of typical exam- ples. These applications come from the areas of machine diagnostics, fault detection, illness identi\ufb01cation or, in principle, refer to any prob- lem where little knowledge is available outside the typical class. In this paper we explain why proximities are natural representations for domain descriptors and we propose a simple one-class classi\ufb01er for dissimilarity representations. By the use of linear programming an ef\ufb01cient one-class description can be found, based on a small number of prototype objects. This classi\ufb01er can be made (1) more robust by transforming the dissimi- larities and (2) cheaper to compute by using a reduced representation set. Finally, a comparison to a comparable one-class classi\ufb01er by Campbell and Bennett is given.",
        "bibtex": "@inproceedings{NIPS2002_6e5025cc,\n author = {Pekalska, Elzbieta and Tax, David M.J. and Duin, Robert},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {One-Class LP Classifiers for Dissimilarity Representations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/6e5025ccc7d638ae4e724da8938450a6-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/6e5025ccc7d638ae4e724da8938450a6-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/6e5025ccc7d638ae4e724da8938450a6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 156276,
        "gs_citation": 67,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8829477685481438499&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "abed40888a",
        "title": "Optimality of Reinforcement Learning Algorithms with Linear Function Approximation",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/228bbc2f87caeb21bb7f6949fddcb91d-Abstract.html",
        "author": "Ralf Schoknecht",
        "abstract": "There are several reinforcement learning algorithms that yield ap(cid:173) proximate solutions for the problem of policy evaluation when the  value function is represented with a linear function approximator.  In this paper we show that each of the solutions is optimal with  respect to a specific objective function. Moreover, we characterise  the different solutions as images of the optimal exact value func(cid:173) tion under different projection operations. The results presented  here will be useful for comparing the algorithms in terms of the  error they achieve relative to the error of the optimal approximate  solution.",
        "bibtex": "@inproceedings{NIPS2002_228bbc2f,\n author = {Schoknecht, Ralf},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Optimality of Reinforcement Learning Algorithms with Linear Function Approximation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/228bbc2f87caeb21bb7f6949fddcb91d-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/228bbc2f87caeb21bb7f6949fddcb91d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/228bbc2f87caeb21bb7f6949fddcb91d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1596614,
        "gs_citation": 70,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8636619903875963999&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "ILKD University of Karlsruhe, Germany",
        "aff_domain": "ilkd.uni-karlsruhe.de",
        "email": "ilkd.uni-karlsruhe.de",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "ILKD University of Karlsruhe, Germany",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "879698e583",
        "title": "Optoelectronic Implementation of a FitzHugh-Nagumo Neural Model",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/1c6a0198177bfcc9bd93f6aab94aad3c-Abstract.html",
        "author": "Alexandre R. Romariz; Kelvin Wagner",
        "abstract": "An optoelectronic implementation of a spiking neuron model based on the FitzHugh-Nagumo equations is presented. A tunable semiconduc- tor laser source and a spectral \ufb01lter provide a nonlinear mapping from driver voltage to detected signal. Linear electronic feedback completes the implementation, which allows either electronic or optical input sig- nals. Experimental results for a single system and numeric results of model interaction con\ufb01rm that important features of spiking neural mod- els can be implemented through this approach.",
        "bibtex": "@inproceedings{NIPS2002_1c6a0198,\n author = {Romariz, Alexandre and Wagner, Kelvin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Optoelectronic Implementation of a FitzHugh-Nagumo Neural Model},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/1c6a0198177bfcc9bd93f6aab94aad3c-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/1c6a0198177bfcc9bd93f6aab94aad3c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/1c6a0198177bfcc9bd93f6aab94aad3c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 272885,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18053147362569921372&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Optoelectronic Computing Systems Center, University of Colorado, Boulder, CO, USA 80309-0425 + Electrical Engineering Department, University of Bras\u00edlia, Brazil; Optoelectronic Computing Systems Center, University of Colorado, Boulder, CO, USA 80309-0425",
        "aff_domain": "colorado.edu; ",
        "email": "colorado.edu; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0",
        "aff_unique_norm": "Optoelectronic Computing Systems Center, University of Colorado, Boulder, CO, USA 80309-0425;Electrical Engineering Department, University of Bras\u00edlia, Brazil",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "4d1268f9ec",
        "title": "PAC-Bayes & Margins",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/68d309812548887400e375eaa036d2f1-Abstract.html",
        "author": "John Langford; John Shawe-Taylor",
        "abstract": "Abstract Unavailable",
        "bibtex": "@inproceedings{NIPS2002_68d30981,\n author = {Langford, John and Shawe-Taylor, John},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {PAC-Bayes \\&amp; Margins},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/68d309812548887400e375eaa036d2f1-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/68d309812548887400e375eaa036d2f1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/68d309812548887400e375eaa036d2f1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 2375630,
        "gs_citation": 355,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=388726107797209086&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "a0fc0c3abd",
        "title": "Parametric Mixture Models for Multi-Labeled Text",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/3147da8ab4a0437c15ef51a5cc7f2dc4-Abstract.html",
        "author": "Naonori Ueda; Kazumi Saito",
        "abstract": "We propose probabilistic generative models, called parametric mix- ture models (PMMs), for multiclass, multi-labeled text categoriza- tion problem. Conventionally, the binary classi(cid:12)cation approach has been employed, in which whether or not text belongs to a cat- egory is judged by the binary classi(cid:12)er for every category. In con- trast, our approach can simultaneously detect multiple categories of text using PMMs. We derive e(cid:14)cient learning and prediction algo- rithms for PMMs. We also empirically show that our method could signi(cid:12)cantly outperform the conventional binary methods when ap- plied to multi-labeled text categorization using real World Wide Web pages.",
        "bibtex": "@inproceedings{NIPS2002_3147da8a,\n author = {Ueda, Naonori and Saito, Kazumi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Parametric Mixture Models for Multi-Labeled Text},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/3147da8ab4a0437c15ef51a5cc7f2dc4-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/3147da8ab4a0437c15ef51a5cc7f2dc4-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/3147da8ab4a0437c15ef51a5cc7f2dc4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 108060,
        "gs_citation": 494,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16825844654756307922&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "f337e90893",
        "title": "Prediction and Semantic Association",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/cb8da6767461f2812ae4290eac7cbc42-Abstract.html",
        "author": "Thomas L. Griffiths; Mark Steyvers",
        "abstract": "We  explore  the  consequences  of  viewing  semantic  association  as  the result of attempting to predict the concepts likely to arise in a  particular context.  We  argue that the success of existing accounts  of semantic representation comes as a result of indirectly addressing  this problem, and show that a closer correspondence to human data  can be obtained by taking a  probabilistic approach that explicitly  models the generative structure of language.",
        "bibtex": "@inproceedings{NIPS2002_cb8da676,\n author = {Griffiths, Thomas and Steyvers, Mark},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Prediction and Semantic Association},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/cb8da6767461f2812ae4290eac7cbc42-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/cb8da6767461f2812ae4290eac7cbc42-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/cb8da6767461f2812ae4290eac7cbc42-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1647500,
        "gs_citation": 158,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16127359712652592638&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Psychology, Stanford University, Stanford, CA 94305-2130; Department of Psychology, Stanford University, Stanford, CA 94305-2130",
        "aff_domain": "psych.stanford.edu;psych.stanford.edu",
        "email": "psych.stanford.edu;psych.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Department of Psychology, Stanford University, Stanford, CA 94305-2130",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "e396f69a46",
        "title": "Prediction of Protein Topologies Using Generalized IOHMMs and RNNs",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/ffedf5be3a86e2ee281d54cdc97bc1cf-Abstract.html",
        "author": "Gianluca Pollastri; Pierre Baldi; Alessandro Vullo; Paolo Frasconi",
        "abstract": "We develop and test new machine learning methods for the predic- tion of topological representations of protein structures in the form of coarse- or (cid:12)ne-grained contact or distance maps that are transla- tion and rotation invariant. The methods are based on generalized input-output hidden Markov models (GIOHMMs) and generalized recursive neural networks (GRNNs). The methods are used to pre- dict topology directly in the (cid:12)ne-grained case and, in the coarse- grained case, indirectly by (cid:12)rst learning how to score candidate graphs and then using the scoring function to search the space of possible con(cid:12)gurations. Computer simulations show that the pre- dictors achieve state-of-the-art performance.",
        "bibtex": "@inproceedings{NIPS2002_ffedf5be,\n author = {Pollastri, Gianluca and Baldi, Pierre and Vullo, Alessandro and Frasconi, Paolo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Prediction of Protein Topologies Using Generalized IOHMMs and RNNs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/ffedf5be3a86e2ee281d54cdc97bc1cf-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/ffedf5be3a86e2ee281d54cdc97bc1cf-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/ffedf5be3a86e2ee281d54cdc97bc1cf-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 95386,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8849186171641749758&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "b77e085261",
        "title": "Ranking with Large Margin Principle: Two Approaches",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/51de85ddd068f0bc787691d356176df9-Abstract.html",
        "author": "Amnon Shashua; Anat Levin",
        "abstract": "We discuss the problem of ranking k instances with the use of a \"large  margin\" principle. We introduce two main approaches: the first is the  \"fixed margin\" policy in which the margin of the closest neighboring  classes is being maximized - which turns out to be a direct generaliza(cid:173) tion of SVM to ranking learning. The second approach allows for k - 1  different margins where the sum of margins is maximized. This approach  is shown to reduce to lI-SVM when the number of classes k = 2. Both  approaches are optimal in size of 21 where I is the total number of training  examples. Experiments performed on visual classification and \"collab(cid:173) orative filtering\" show that both approaches outperform existing ordinal  regression algorithms applied for ranking and multi-class SVM applied  to general multi-class classification.",
        "bibtex": "@inproceedings{NIPS2002_51de85dd,\n author = {Shashua, Amnon and Levin, Anat},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Ranking with Large Margin Principle: Two Approaches},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/51de85ddd068f0bc787691d356176df9-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/51de85ddd068f0bc787691d356176df9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/51de85ddd068f0bc787691d356176df9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1656781,
        "gs_citation": 525,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16811849647089600743&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff": "School of CS&E, Hebrew University of Jerusalem; School of CS&E, Hebrew University of Jerusalem",
        "aff_domain": "cs.huji.ac.il;cs.huji.ac.il",
        "email": "cs.huji.ac.il;cs.huji.ac.il",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "School of CS&E, Hebrew University of Jerusalem",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "8a7c4f3faa",
        "title": "Rate Distortion Function in the Spin Glass State: A Toy Model",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/41e7637e7b6a9f27a98b84d3a185c7c0-Abstract.html",
        "author": "Tatsuto Murayama; Masato Okada",
        "abstract": "We applied statistical mechanics to an inverse problem of linear mapping to investigate the physics of optimal lossy compressions. We used the replica symmetry breaking technique with a toy model to demonstrate Shannon\u2019s result. The rate distortion function, which is widely known as the theoretical limit of the compression with a \ufb01delity criterion, is derived. Numerical study shows that sparse constructions of the model provide suboptimal compressions.",
        "bibtex": "@inproceedings{NIPS2002_41e7637e,\n author = {Murayama, Tatsuto and Okada, Masato},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Rate Distortion Function in the Spin Glass State: A Toy Model},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/41e7637e7b6a9f27a98b84d3a185c7c0-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/41e7637e7b6a9f27a98b84d3a185c7c0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/41e7637e7b6a9f27a98b84d3a185c7c0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 84477,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1460496400637370642&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Laboratory for Mathematical Neuroscience, RIKEN Brain Science Institute, Saitama, 351-0198, JAPAN; Laboratory for Mathematical Neuroscience, RIKEN Brain Science Institute, Saitama, 351-0198, JAPAN",
        "aff_domain": "brain.riken.go.jp;brain.riken.go.jp",
        "email": "brain.riken.go.jp;brain.riken.go.jp",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Laboratory for Mathematical Neuroscience, RIKEN Brain Science Institute, Saitama, 351-0198, JAPAN",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "f20608d219",
        "title": "Rational Kernels",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/ddd9dda6bfaf0bb1525a8a27c3ee6131-Abstract.html",
        "author": "Corinna Cortes; Patrick Haffner; Mehryar Mohri",
        "abstract": "We introduce a general family of kernels based on weighted transduc- ers or rational relations, rational kernels, that can be used for analysis of variable-length sequences or more generally weighted automata, in appli- cations such as computational biology or speech recognition. We show that rational kernels can be computed ef\ufb01ciently using a general algo- rithm of composition of weighted transducers and a general single-source shortest-distance algorithm. We also describe several general families of positive de\ufb01nite symmetric rational kernels. These general kernels can be combined with Support Vector Machines to form ef\ufb01cient and power- ful techniques for spoken-dialog classi\ufb01cation: highly complex kernels become easy to design and implement and lead to substantial improve- ments in the classi\ufb01cation accuracy. We also show that the string kernels considered in applications to computational biology are all speci\ufb01c in- stances of rational kernels.",
        "bibtex": "@inproceedings{NIPS2002_ddd9dda6,\n author = {Cortes, Corinna and Haffner, Patrick and Mohri, Mehryar},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Rational Kernels},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/ddd9dda6bfaf0bb1525a8a27c3ee6131-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/ddd9dda6bfaf0bb1525a8a27c3ee6131-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/ddd9dda6bfaf0bb1525a8a27c3ee6131-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 109460,
        "gs_citation": 115,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16180157972453148408&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "66ae1c9688",
        "title": "Real Time Voice Processing with Audiovisual Feedback: Toward Autonomous Agents with Perfect Pitch",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/43351f7bf9a215be70c2c2caa7555002-Abstract.html",
        "author": "Lawrence K. Saul; Daniel D. Lee; Charles L. Isbell; Yann L. Cun",
        "abstract": "We have implemented a real time front end for detecting voiced speech and estimating its fundamental frequency. The front end performs the signal processing for voice-driven agents that attend to the pitch contours of human speech and provide continuous audiovisual feedback. The al- gorithm we use for pitch tracking has several distinguishing features: it makes no use of FFTs or autocorrelation at the pitch period; it updates the pitch incrementally on a sample-by-sample basis; it avoids peak picking and does not require interpolation in time or frequency to obtain high res- olution estimates; and it works reliably over a four octave range, in real time, without the need for postprocessing to produce smooth contours. The algorithm is based on two simple ideas in neural computation: the introduction of a purposeful nonlinearity, and the error signal of a least squares \ufb01t. The pitch tracker is used in two real time multimedia applica- tions: a voice-to-MIDI player that synthesizes electronic music from vo- calized melodies, and an audiovisual Karaoke machine with multimodal feedback. Both applications run on a laptop and display the user\u2019s pitch scrolling across the screen as he or she sings into the computer.",
        "bibtex": "@inproceedings{NIPS2002_43351f7b,\n author = {Saul, Lawrence and Lee, Daniel and Isbell, Charles and Cun, Yann},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Real Time Voice Processing with Audiovisual Feedback: Toward Autonomous Agents with Perfect Pitch},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/43351f7bf9a215be70c2c2caa7555002-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/43351f7bf9a215be70c2c2caa7555002-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/43351f7bf9a215be70c2c2caa7555002-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 512787,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3861702581765866751&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "Department of Computer and Information Science; Department of Electrical and System Engineering; Georgia Tech College of Computing; NEC Research Institute",
        "aff_domain": "cis.upenn.edu;ee.upenn.edu;cc.gatech.edu;research.nj.nec.com",
        "email": "cis.upenn.edu;ee.upenn.edu;cc.gatech.edu;research.nj.nec.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "Department of Computer and Information Science;Department of Electrical and System Engineering;Georgia Tech College of Computing;NEC Research Institute",
        "aff_unique_dep": ";;;",
        "aff_unique_url": ";;;",
        "aff_unique_abbr": ";;;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "ae40d4281f",
        "title": "Real-Time Monitoring of Complex Industrial Processes with Particle Filters",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/86e78499eeb33fb9cac16b7555b50767-Abstract.html",
        "author": "Rub\u00e9n Morales-Men\u00e9ndez; Nando de Freitas; David Poole",
        "abstract": "This paper discusses the application of particle \ufb01ltering algorithms to fault diagnosis in complex industrial processes. We consider two ubiq- uitous processes: an industrial dryer and a level tank. For these appli- cations, we compared three particle \ufb01ltering variants: standard parti- cle \ufb01ltering, Rao-Blackwellised particle \ufb01ltering and a version of Rao- Blackwellised particle \ufb01ltering that does one-step look-ahead to select good sampling regions. We show that the overhead of the extra process- ing per particle of the more sophisticated methods is more than compen- sated by the decrease in error and variance.",
        "bibtex": "@inproceedings{NIPS2002_86e78499,\n author = {Morales-Men\\'{e}ndez, Rub\\'{e}n and de Freitas, Nando and Poole, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Real-Time Monitoring of Complex Industrial Processes with Particle Filters},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/86e78499eeb33fb9cac16b7555b50767-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/86e78499eeb33fb9cac16b7555b50767-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/86e78499eeb33fb9cac16b7555b50767-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 186301,
        "gs_citation": 79,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18130500293218650895&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Dept. of Mechatronics and Automation, ITESM campus Monterrey, Monterrey, NL M\u00e9xico; Dept. of Computer Science, University of British Columbia, Vancouver, BC, V6T 1Z4, Canada; Dept. of Computer Science, University of British Columbia, Vancouver, BC, V6T 1Z4, Canada",
        "aff_domain": "itesm.mx;cs.ubc.ca;cs.ubc.ca",
        "email": "itesm.mx;cs.ubc.ca;cs.ubc.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Dept. of Mechatronics and Automation, ITESM campus Monterrey, Monterrey, NL M\u00e9xico;Dept. of Computer Science, University of British Columbia, Vancouver, BC, V6T 1Z4, Canada",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "67274d7f54",
        "title": "Real-Time Particle Filters",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/2d2ca7eedf739ef4c3800713ec482e1a-Abstract.html",
        "author": "Cody Kwok; Dieter Fox; Marina Meila",
        "abstract": "Particle \ufb01lters estimate the state of dynamical systems from sensor infor- mation. In many real time applications of particle \ufb01lters, however, sensor information arrives at a signi\ufb01cantly higher rate than the update rate of the \ufb01lter. The prevalent approach to dealing with such situations is to update the particle \ufb01lter as often as possible and to discard sensor information that cannot be processed in time. In this paper we present real-time particle \ufb01l- ters, which make use of all sensor information even when the \ufb01lter update rate is below the update rate of the sensors. This is achieved by represent- ing posteriors as mixtures of sample sets, where each mixture component integrates one observation arriving during a \ufb01lter update. The weights of the mixture components are set so as to minimize the approximation error introduced by the mixture representation. Thereby, our approach focuses computational resources (samples) on valuable sensor information. Exper- iments using data collected with a mobile robot show that our approach yields strong improvements over other approaches.",
        "bibtex": "@inproceedings{NIPS2002_2d2ca7ee,\n author = {Kwok, Cody and Fox, Dieter and Meila, Marina},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Real-Time Particle Filters},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/2d2ca7eedf739ef4c3800713ec482e1a-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/2d2ca7eedf739ef4c3800713ec482e1a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/2d2ca7eedf739ef4c3800713ec482e1a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 280629,
        "gs_citation": 399,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13225333233689620473&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "Dept. of Computer Science & Engineering; Dept. of Computer Science & Engineering; Dept. of Statistics",
        "aff_domain": "cs.washington.edu;cs.washington.edu;stat.washington.edu",
        "email": "cs.washington.edu;cs.washington.edu;stat.washington.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Dept. of Computer Science & Engineering;Dept. of Statistics",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "3fdec0815d",
        "title": "Reconstructing Stimulus-Driven Neural Networks from Spike Times",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/b166b57d195370cd41f80dd29ed523d9-Abstract.html",
        "author": "Duane Q. Nykamp",
        "abstract": "We present a method to distinguish direct connections between two neu- rons from common input originating from other, unmeasured neurons. The distinction is computed from the spike times of the two neurons in response to a white noise stimulus. Although the method is based on a highly idealized linear-nonlinear approximation of neural response, we demonstrate via simulation that the approach can work with a more re- alistic, integrate-and-\ufb01re neuron model. We propose that the approach exempli\ufb01ed by this analysis may yield viable tools for reconstructing stimulus-driven neural networks from data gathered in neurophysiology experiments.",
        "bibtex": "@inproceedings{NIPS2002_b166b57d,\n author = {Nykamp, Duane},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Reconstructing Stimulus-Driven Neural Networks from Spike Times},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/b166b57d195370cd41f80dd29ed523d9-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/b166b57d195370cd41f80dd29ed523d9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/b166b57d195370cd41f80dd29ed523d9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 91831,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16635770515306756934&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "UCLA Mathematics Department",
        "aff_domain": "math.ucla.edu",
        "email": "math.ucla.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "UCLA Mathematics Department",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "f0d68ded9c",
        "title": "Recovering Articulated Model Topology from Observed Rigid Motion",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/d51b416788b6ee70eb0c381c06efc9f1-Abstract.html",
        "author": "Leonid Taycher; John Iii; Trevor Darrell",
        "abstract": "Accurate representation of articulated motion is a challenging problem for machine perception. Several successful tracking algorithms have been developed that model human body as an articulated tree. We pro- pose a learning-based method for creating such articulated models from observations of multiple rigid motions. This paper is concerned with recovering topology of the articulated model, when the rigid motion of constituent segments is known. Our approach is based on \ufb01nding the Maximum Likelihood tree shaped factorization of the joint probability density function (PDF) of rigid segment motions. The topology of graph- ical model formed from this factorization corresponds to topology of the underlying articulated body. We demonstrate the performance of our al- gorithm on both synthetic and real motion capture data.",
        "bibtex": "@inproceedings{NIPS2002_d51b4167,\n author = {Taycher, Leonid and Iii, John and Darrell, Trevor},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Recovering Articulated Model Topology from Observed Rigid Motion},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/d51b416788b6ee70eb0c381c06efc9f1-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/d51b416788b6ee70eb0c381c06efc9f1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/d51b416788b6ee70eb0c381c06efc9f1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 99676,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17418981529544439491&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 10,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "953e71d899",
        "title": "Recovering Intrinsic Images from a Single Image",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/fa2431bf9d65058fe34e9713e32d60e6-Abstract.html",
        "author": "Marshall F. Tappen; William T. Freeman; Edward H. Adelson",
        "abstract": "We present an algorithm that uses multiple cues to recover shading and re\ufb02ectance intrinsic images from a single image. Using both color in- formation and a classi\ufb01er trained to recognize gray-scale patterns, each image derivative is classi\ufb01ed as being caused by shading or a change in the surface\u2019s re\ufb02ectance. Generalized Belief Propagation is then used to propagate information from areas where the correct classi\ufb01cation is clear to areas where it is ambiguous. We also show results on real images.",
        "bibtex": "@inproceedings{NIPS2002_fa2431bf,\n author = {Tappen, Marshall and Freeman, William and Adelson, Edward},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Recovering Intrinsic Images from a Single Image},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/fa2431bf9d65058fe34e9713e32d60e6-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/fa2431bf9d65058fe34e9713e32d60e6-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/fa2431bf9d65058fe34e9713e32d60e6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 162792,
        "gs_citation": 643,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16354014511515522235&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 34,
        "aff": "MIT Artificial Intelligence Laboratory; MIT Artificial Intelligence Laboratory; MIT Artificial Intelligence Laboratory",
        "aff_domain": "ai.mit.edu;ai.mit.edu;ai.mit.edu",
        "email": "ai.mit.edu;ai.mit.edu;ai.mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "MIT Artificial Intelligence Laboratory",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "1f00861d93",
        "title": "Regularized Greedy Importance Sampling",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/71560ce98c8250ce57a6a970c9991a5f-Abstract.html",
        "author": "Finnegan Southey; Dale Schuurmans; Ali Ghodsi",
        "abstract": "Greedy importance sampling is an unbiased estimation technique that re- duces the variance of standard importance sampling by explicitly search- ing for modes in the estimation objective. Previous work has demon- strated the feasibility of implementing this method and proved that the technique is unbiased in both discrete and continuous domains. In this paper we present a reformulation of greedy importance sampling that eliminates the free parameters from the original estimator, and introduces a new regularization strategy that further reduces variance without com- promising unbiasedness. The resulting estimator is shown to be effective for dif\ufb01cult estimation problems arising in Markov random \ufb01eld infer- ence. In particular, improvements are achieved over standard MCMC estimators when the distribution has multiple peaked modes.",
        "bibtex": "@inproceedings{NIPS2002_71560ce9,\n author = {Southey, Finnegan and Schuurmans, Dale and Ghodsi, Ali},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Regularized Greedy Importance Sampling},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/71560ce98c8250ce57a6a970c9991a5f-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/71560ce98c8250ce57a6a970c9991a5f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/71560ce98c8250ce57a6a970c9991a5f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 115647,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=484056282041853186&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "School of Computer Science, University of Waterloo; School of Computer Science, University of Waterloo; School of Computer Science, University of Waterloo",
        "aff_domain": "cs.uwaterloo.ca;cs.uwaterloo.ca;cs.uwaterloo.ca",
        "email": "cs.uwaterloo.ca;cs.uwaterloo.ca;cs.uwaterloo.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Waterloo",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://uwaterloo.ca",
        "aff_unique_abbr": "UWaterloo",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Waterloo",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "fac23fee6b",
        "title": "Reinforcement Learning to Play an Optimal Nash Equilibrium in Team Markov Games",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/f8e59f4b2fe7c5705bf878bbd494ccdf-Abstract.html",
        "author": "Xiaofeng Wang; Tuomas Sandholm",
        "abstract": "Multiagent learning is a key problem in AI. In the presence of multi- ple Nash equilibria, even agents with non-con\ufb02icting interests may not be able to learn an optimal coordination policy. The problem is exac- cerbated if the agents do not know the game and independently receive noisy payoffs. So, multiagent reinforfcement learning involves two inter- related problems: identifying the game and learning to play. In this paper, we present optimal adaptive learning, the \ufb01rst algorithm that converges to an optimal Nash equilibrium with probability 1 in any team Markov game. We provide a convergence proof, and show that the algorithm\u2019s parameters are easy to set to meet the convergence conditions.",
        "bibtex": "@inproceedings{NIPS2002_f8e59f4b,\n author = {Wang, Xiaofeng and Sandholm, Tuomas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Reinforcement Learning to Play an Optimal Nash Equilibrium in Team Markov Games},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/f8e59f4b2fe7c5705bf878bbd494ccdf-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/f8e59f4b2fe7c5705bf878bbd494ccdf-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/f8e59f4b2fe7c5705bf878bbd494ccdf-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 120390,
        "gs_citation": 397,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8606101263493169559&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "ECE Department, Carnegie Mellon University; CS Department, Carnegie Mellon University",
        "aff_domain": "andrew.cmu.edu;cs.cmu.edu",
        "email": "andrew.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "ECE Department, Carnegie Mellon University;CS Department, Carnegie Mellon University",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "8a7c953d33",
        "title": "Replay, Repair and Consolidation",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html",
        "author": "Szabolcs K\u00e1li; Peter Dayan",
        "abstract": "A standard view of memory consolidation is that episodes are stored tem- porarily in the hippocampus, and are transferred to the neocortex through replay. Various recent experimental challenges to the idea of transfer, particularly for human memory, are forcing its re-evaluation. However, although there is independent neurophysiological evidence for replay, short of transfer, there are few theoretical ideas for what it might be doing. We suggest and demonstrate two important computational roles associated with neocortical indices.",
        "bibtex": "@inproceedings{NIPS2002_d5e2c0ad,\n author = {K\\'{a}li, Szabolcs and Dayan, Peter},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Replay, Repair and Consolidation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/d5e2c0adad503c91f91df240d0cd4e49-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 110813,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8717614466555886698&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Institute of Experimental Medicine, Hungarian Academy of Sciences; Gatsby Computational Neuroscience Unit, University College London",
        "aff_domain": "koki.hu;gatsby.ucl.ac.uk",
        "email": "koki.hu;gatsby.ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Institute of Experimental Medicine, Hungarian Academy of Sciences;University College London",
        "aff_unique_dep": ";Gatsby Computational Neuroscience Unit",
        "aff_unique_url": ";https://www.ucl.ac.uk",
        "aff_unique_abbr": ";UCL",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";London",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United Kingdom"
    },
    {
        "id": "5bb869814d",
        "title": "Retinal Processing Emulation in a Programmable 2-Layer Analog Array Processor CMOS Chip",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/88a839f2f6f1427879fc33ee4acf4f66-Abstract.html",
        "author": "R. Carmona; F. Jim\u00e9nez-garrido; R. Dominguez-castro; S. Espejo; A. Rodriguez-v\u00e1zquez",
        "abstract": "A bio-inspired model for an analog programmable array processor (APAP), based on studies on the vertebrate retina, has permitted the realization of complex programmable spatio-temporal dynam- ics in VLSI. This model mimics the way in which images are pro- cessed in the visual pathway, rendering a feasible alternative for the implementation of early vision applications in standard tech- nologies. A prototype chip has been designed and fabricated in a 0.5(cid:22)m standard CMOS process. Computing power per area and power consumption is amongst the highest reported for a single chip. Design challenges, trade-o(cid:11)s and some experimental results are presented in this paper.",
        "bibtex": "@inproceedings{NIPS2002_88a839f2,\n author = {Carmona, R. and Jim\\'{e}nez-garrido, F. and Dominguez-castro, R. and Espejo, S. and Rodriguez-v\\'{a}zquez, A.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Retinal Processing Emulation in a Programmable 2-Layer Analog Array Processor CMOS Chip},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/88a839f2f6f1427879fc33ee4acf4f66-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/88a839f2f6f1427879fc33ee4acf4f66-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/88a839f2f6f1427879fc33ee4acf4f66-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 438497,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:gmkS7FXNPWwJ:scholar.google.com/&scioq=Retinal+Processing+Emulation+in+a+Programmable+2-Layer+Analog+Array+Processor+CMOS+Chip&hl=en&as_sdt=0,5",
        "gs_version_total": 9,
        "aff": "Instituto de Microelectr\u0013 onica de Sevilla-CNM-CSIC; Instituto de Microelectr\u0013 onica de Sevilla-CNM-CSIC; Instituto de Microelectr\u0013 onica de Sevilla-CNM-CSIC; Instituto de Microelectr\u0013 onica de Sevilla-CNM-CSIC; Instituto de Microelectr\u0013 onica de Sevilla-CNM-CSIC",
        "aff_domain": "imse.cnm.es; ; ; ;",
        "email": "imse.cnm.es; ; ; ;",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Instituto de Microelectr\u0013 onica de Sevilla-CNM-CSIC",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "d56eea0b97",
        "title": "Robust Novelty Detection with Single-Class MPM",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/e2ad76f2326fbc6b56a45a56c59fafdb-Abstract.html",
        "author": "Laurent E. Ghaoui; Michael I. Jordan; Gert R. Lanckriet",
        "abstract": "the  \"single-class  minimax  probabil(cid:173)",
        "bibtex": "@inproceedings{NIPS2002_e2ad76f2,\n author = {Ghaoui, Laurent and Jordan, Michael and Lanckriet, Gert},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Robust Novelty Detection with Single-Class MPM},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/e2ad76f2326fbc6b56a45a56c59fafdb-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/e2ad76f2326fbc6b56a45a56c59fafdb-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/e2ad76f2326fbc6b56a45a56c59fafdb-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1589252,
        "gs_citation": 143,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8242554135230468655&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "EECS, V.C. Berkeley; EECS, V.C. Berkeley; Computer Science and Statistics, V.C. Berkeley",
        "aff_domain": "eecs.berkeley.edu;eecs.berkeley.edu;cs.berkeley.edu",
        "email": "eecs.berkeley.edu;eecs.berkeley.edu;cs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "EECS, V.C. Berkeley;Computer Science and Statistics, V.C. Berkeley",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "157c8685a1",
        "title": "Scaling of Probability-Based Optimization Algorithms",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/1943102704f8f8f3302c2b730728e023-Abstract.html",
        "author": "J. L. Shapiro",
        "abstract": "Population-based Incremental Learning is  shown require very sen(cid:173) sitive scaling of its learning rate.  The learning rate must scale with  the system size in a  problem-dependent way.  This is  shown in two  problems:  the needle-in-a haystack, in which the learning rate must  vanish exponentially in the  system size,  and in  a  smooth function  in  which  the  learning rate must  vanish  like  the  square root of the  system size.  Two methods are proposed for  removing this sensitiv(cid:173) ity.  A learning dynamics which obeys detailed balance is shown to  give consistent performance over the entire range of learning rates.  An  analog  of mutation  is  shown  to  require  a  learning  rate  which  scales as  the inverse system size, but is  problem independent.",
        "bibtex": "@inproceedings{NIPS2002_19431027,\n author = {Shapiro, J.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Scaling of Probability-Based Optimization Algorithms},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/1943102704f8f8f3302c2b730728e023-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/1943102704f8f8f3302c2b730728e023-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/1943102704f8f8f3302c2b730728e023-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1443753,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13626563580822453863&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science University of Manchester",
        "aff_domain": "cs.man.ac.uk",
        "email": "cs.man.ac.uk",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Department of Computer Science University of Manchester",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "86bcd73d94",
        "title": "Selectivity and Metaplasticity in a Unified Calcium-Dependent Model",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/b440509a0106086a67bc2ea9df0a1dab-Abstract.html",
        "author": "Luk Chong Yeung; Brian S. Blais; Leon N. Cooper; Harel Z. Shouval",
        "abstract": "A uni(cid:12)ed, biophysically motivated Calcium-Dependent Learning model has been shown to account for various rate-based and spike time-dependent paradigms for inducing synaptic plasticity. Here, we investigate the properties of this model for a multi-synapse neuron that receives inputs with di(cid:11)erent spike-train statistics. In addition, we present a physiological form of metaplasticity, an activity-driven regulation mechanism, that is essential for the ro- bustness of the model. A neuron thus implemented develops stable and selective receptive (cid:12)elds, given various input statistics",
        "bibtex": "@inproceedings{NIPS2002_b440509a,\n author = {Yeung, Luk Chong and Blais, Brian and Cooper, Leon and Shouval, Harel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Selectivity and Metaplasticity in a Unified Calcium-Dependent Model},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/b440509a0106086a67bc2ea9df0a1dab-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/b440509a0106086a67bc2ea9df0a1dab-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/b440509a0106086a67bc2ea9df0a1dab-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 169060,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9406329094910292981&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Physics Department and Institute for Brain & Neural Systems, Brown University; Department of Science & Technology, Bryant College + Institute for Brain & Neural Systems, Brown University; Institute for Brain & Neural Systems, Physics Department and Department of Neuroscience, Brown University; Institute for Brain & Neural Systems and Physics Department, Brown University",
        "aff_domain": "physics.brown.edu;bryant.edu;brown.edu;brown.edu",
        "email": "physics.brown.edu;bryant.edu;brown.edu;brown.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+2;3;4",
        "aff_unique_norm": "Physics Department and Institute for Brain & Neural Systems, Brown University;Department of Science & Technology, Bryant College;Institute for Brain & Neural Systems, Brown University;Institute for Brain & Neural Systems, Physics Department and Department of Neuroscience, Brown University;Institute for Brain & Neural Systems and Physics Department, Brown University",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": ";;;;",
        "aff_unique_abbr": ";;;;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "9be032dc83",
        "title": "Self Supervised Boosting",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/cd0cbcc668fe4bc58e0af3cc7e0a653d-Abstract.html",
        "author": "Max Welling; Richard S. Zemel; Geoffrey E. Hinton",
        "abstract": "Boosting algorithms and successful applications thereof abound for clas- si\ufb01cation and regression learning problems, but not for unsupervised learning. We propose a sequential approach to adding features to a ran- dom \ufb01eld model by training them to improve classi\ufb01cation performance between the data and an equal-sized sample of \u201cnegative examples\u201d gen- erated from the model\u2019s current estimate of the data density. Training in each boosting round proceeds in three stages: \ufb01rst we sample negative examples from the model\u2019s current Boltzmann distribution. Next, a fea- ture is trained to improve classi\ufb01cation performance between data and negative examples. Finally, a coef\ufb01cient is learned which determines the importance of this feature relative to ones already in the pool. Negative examples only need to be generated once to learn each new feature. The validity of the approach is demonstrated on binary digits and continuous synthetic data.",
        "bibtex": "@inproceedings{NIPS2002_cd0cbcc6,\n author = {Welling, Max and Zemel, Richard and Hinton, Geoffrey E},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Self Supervised Boosting},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/cd0cbcc668fe4bc58e0af3cc7e0a653d-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/cd0cbcc668fe4bc58e0af3cc7e0a653d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/cd0cbcc668fe4bc58e0af3cc7e0a653d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 625034,
        "gs_citation": 65,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10510550065663996157&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 20,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "7bee8907ff",
        "title": "Shape Recipes: Scene Representations that Refer to the Image",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/2b8eba3cb0d0f1d761cb74d94a5ace36-Abstract.html",
        "author": "William T. Freeman; Antonio Torralba",
        "abstract": "The goal of low-level vision is to estimate an underlying scene, given an observed image. Real-world scenes (eg, albedos or shapes) can be very complex, conventionally requiring high dimensional representations which are hard to estimate and store. We propose a low-dimensional rep- resentation, called a scene recipe, that relies on the image itself to de- scribe the complex scene con\ufb01gurations. Shape recipes are an example: these are the regression coef\ufb01cients that predict the bandpassed shape from image data. We describe the bene\ufb01ts of this representation, and show two uses illustrating their properties: (1) we improve stereo shape estimates by learning shape recipes at low resolution and applying them at full resolution; (2) Shape recipes implicitly contain information about lighting and materials and we use them for material segmentation.",
        "bibtex": "@inproceedings{NIPS2002_2b8eba3c,\n author = {Freeman, William and Torralba, Antonio},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Shape Recipes: Scene Representations that Refer to the Image},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/2b8eba3cb0d0f1d761cb74d94a5ace36-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/2b8eba3cb0d0f1d761cb74d94a5ace36-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/2b8eba3cb0d0f1d761cb74d94a5ace36-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 209367,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7709284774573160314&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff": "Artificial Intelligence Laboratory, Massachusetts Institute of Technology; Artificial Intelligence Laboratory, Massachusetts Institute of Technology",
        "aff_domain": "ai.mit.edu;ai.mit.edu",
        "email": "ai.mit.edu;ai.mit.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Artificial Intelligence Laboratory, Massachusetts Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "0731dc08a2",
        "title": "Source Separation with a Sensor Array using Graphical Models and Subband Filtering",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/2952351097998ac1240cb2ab7333a3d2-Abstract.html",
        "author": "Hagai Attias",
        "abstract": "Source separation is an important problem at the intersection of several \ufb01elds, including machine learning, signal processing, and speech tech- nology. Here we describe new separation algorithms which are based on probabilistic graphical models with latent variables. In contrast with existing methods, these algorithms exploit detailed models to describe source properties. They also use subband \ufb01ltering ideas to model the reverberant environment, and employ an explicit model for background and sensor noise. We leverage variational techniques to keep the compu- tational complexity per EM iteration linear in the number of frames.",
        "bibtex": "@inproceedings{NIPS2002_29523510,\n author = {Attias, Hagai},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Source Separation with a Sensor Array using Graphical Models and Subband Filtering},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/2952351097998ac1240cb2ab7333a3d2-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/2952351097998ac1240cb2ab7333a3d2-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/2952351097998ac1240cb2ab7333a3d2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 246827,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10663184027053308395&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Microsoft Research",
        "aff_domain": "microsoft.com",
        "email": "microsoft.com",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Microsoft Corporation",
        "aff_unique_dep": "Microsoft Research",
        "aff_unique_url": "https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "MSR",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "50c6b62180",
        "title": "Spectro-Temporal Receptive Fields of Subthreshold Responses in Auditory Cortex",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/7e3b7a5bafcb0fa8e8dfe3ea6aca9186-Abstract.html",
        "author": "Christian K. Machens; Michael Wehr; Anthony M. Zador",
        "abstract": "How do cortical neurons represent the acoustic environment? This ques- tion is often addressed by probing with simple stimuli such as clicks or tone pips. Such stimuli have the advantage of yielding easily interpreted answers, but have the disadvantage that they may fail to uncover complex or higher-order neuronal response properties. Here we adopt an alternative approach, probing neuronal responses with complex acoustic stimuli, including animal vocalizations and music. We have used in vivo whole cell methods in the rat auditory cortex to record subthreshold membrane potential \ufb02uctuations elicited by these stimuli. Whole cell recording reveals the total synaptic input to a neuron from all the other neurons in the circuit, instead of just its output\u2014a sparse bi- nary spike train\u2014as in conventional single unit physiological recordings. Whole cell recording thus provides a much richer source of information about the neuron\u2019s response. Many neurons responded robustly and reliably to the complex stimuli in our ensemble. Here we analyze the linear component\u2014the spectro- temporal receptive \ufb01eld (STRF)\u2014of the transformation from the sound (as represented by its time-varying spectrogram) to the neuron\u2019s mem- brane potential. We \ufb01nd that the STRF has a rich dynamical structure, including excitatory regions positioned in general accord with the predic- tion of the simple tuning curve. We also \ufb01nd that in many cases, much of the neuron\u2019s response, although deterministically related to the stimulus, cannot be predicted by the linear component, indicating the presence of as-yet-uncharacterized nonlinear response properties.",
        "bibtex": "@inproceedings{NIPS2002_7e3b7a5b,\n author = {Machens, Christian K and Wehr, Michael and Zador, Anthony},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Spectro-Temporal Receptive Fields of Subthreshold Responses in Auditory Cortex},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/7e3b7a5bafcb0fa8e8dfe3ea6aca9186-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/7e3b7a5bafcb0fa8e8dfe3ea6aca9186-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/7e3b7a5bafcb0fa8e8dfe3ea6aca9186-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 149379,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8925971405895811966&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Cold Spring Harbor Laboratory; Cold Spring Harbor Laboratory; Cold Spring Harbor Laboratory",
        "aff_domain": "cshl.edu;cshl.edu;cshl.edu",
        "email": "cshl.edu;cshl.edu;cshl.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Cold Spring Harbor Laboratory",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cshl.edu",
        "aff_unique_abbr": "CSHL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "16bba4a263",
        "title": "Speeding up the Parti-Game Algorithm",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/d827f12e35eae370ba9c65b7f6026695-Abstract.html",
        "author": "Maxim Likhachev; Sven Koenig",
        "abstract": "In this paper, we introduce an ef\ufb01cient replanning algorithm for nonde- terministic domains, namely what we believe to be the \ufb01rst incremental heuristic minimax search algorithm. We apply it to the dynamic dis- cretization of continuous domains, resulting in an ef\ufb01cient implemen- tation of the parti-game reinforcement-learning algorithm for control in high-dimensional domains.",
        "bibtex": "@inproceedings{NIPS2002_d827f12e,\n author = {Likhachev, Maxim and Koenig, Sven},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Speeding up the Parti-Game Algorithm},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/d827f12e35eae370ba9c65b7f6026695-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/d827f12e35eae370ba9c65b7f6026695-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/d827f12e35eae370ba9c65b7f6026695-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 133190,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13500086097758348006&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "School of Computer Science, Carnegie Mellon University; College of Computing, Georgia Institute of Technology",
        "aff_domain": "cs.cmu.edu;cc.gatech.edu",
        "email": "cs.cmu.edu;cc.gatech.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Carnegie Mellon University;Georgia Institute of Technology",
        "aff_unique_dep": "School of Computer Science;College of Computing",
        "aff_unique_url": "https://www.cmu.edu;https://www.gatech.edu",
        "aff_unique_abbr": "CMU;Georgia Tech",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Pittsburgh;Atlanta",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d0a5461639",
        "title": "Spike Timing-Dependent Plasticity in the Address Domain",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/0a87257e5308197df43230edf4ad1dae-Abstract.html",
        "author": "R. J. Vogelstein; Francesco Tenore; Ralf Philipp; Miriam S. Adlerstein; David H. Goldberg; Gert Cauwenberghs",
        "abstract": "Address-event representation (AER), originally proposed as a means to communicate sparse neural events between neuromorphic chips, has proven ef\ufb01cient in implementing large-scale networks with arbitrary, con\ufb01gurable synaptic connectivity. In this work, we further extend the functionality of AER to implement arbitrary, con\ufb01gurable synaptic plas- ticity in the address domain. As proof of concept, we implement a bi- ologically inspired form of spike timing-dependent plasticity (STDP) based on relative timing of events in an AER framework. Experimen- tal results from an analog VLSI integrate-and-\ufb01re network demonstrate address domain learning in a task that requires neurons to group corre- lated inputs.",
        "bibtex": "@inproceedings{NIPS2002_0a87257e,\n author = {Vogelstein, R. and Tenore, Francesco and Philipp, Ralf and Adlerstein, Miriam and Goldberg, David and Cauwenberghs, Gert},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Spike Timing-Dependent Plasticity in the Address Domain},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/0a87257e5308197df43230edf4ad1dae-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/0a87257e5308197df43230edf4ad1dae-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/0a87257e5308197df43230edf4ad1dae-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 87028,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1658930173728702818&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Department of Biomedical Engineering; Department of Electrical and Computer Engineering; Department of Electrical and Computer Engineering; Department of Electrical and Computer Engineering; Department of Electrical and Computer Engineering; Department of Electrical and Computer Engineering",
        "aff_domain": "jhu.edu;jhu.edu;jhu.edu;jhu.edu;jhu.edu;jhu.edu",
        "email": "jhu.edu;jhu.edu;jhu.edu;jhu.edu;jhu.edu;jhu.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1;1;1",
        "aff_unique_norm": "Department of Biomedical Engineering;Unknown Institution",
        "aff_unique_dep": "Biomedical Engineering;Department of Electrical and Computer Engineering",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "b5bdea4f2b",
        "title": "Spikernels: Embedding Spiking Neurons in Inner-Product Spaces",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/06d5ae105ea1bea4d800bc96491876e9-Abstract.html",
        "author": "Lavi Shpigelman; Yoram Singer; Rony Paz; Eilon Vaadia",
        "abstract": "Inner-product operators, often referred to as kernels in statistical learning, de- \ufb01ne a mapping from some input space into a feature space. The focus of this paper is the construction of biologically-motivated kernels for cortical ac- tivities. The kernels we derive, termed Spikernels, map spike count sequences into an abstract vector space in which we can perform various prediction tasks. We discuss in detail the derivation of Spikernels and describe an ef\ufb01cient al- gorithm for computing their value on any two sequences of neural population spike counts. We demonstrate the merits of our modeling approach using the Spikernel and various standard kernels for the task of predicting hand move- ment velocities from cortical recordings. In all of our experiments all the ker- nels we tested outperform the standard scalar product used in regression with the Spikernel consistently achieving the best performance.",
        "bibtex": "@inproceedings{NIPS2002_06d5ae10,\n author = {Shpigelman, Lavi and Singer, Yoram and Paz, Rony and Vaadia, Eilon},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Spikernels: Embedding Spiking Neurons in Inner-Product Spaces},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/06d5ae105ea1bea4d800bc96491876e9-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/06d5ae105ea1bea4d800bc96491876e9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/06d5ae105ea1bea4d800bc96491876e9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 124028,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13304350716339891710&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "School of Computer Science and Engineering; School of Computer Science and Engineering; Interdisciplinary Center for Neural Computation + Dept. of Physiology, Hadassah Medical School; Interdisciplinary Center for Neural Computation + Dept. of Physiology, Hadassah Medical School",
        "aff_domain": "cs.huji.ac.il;cs.huji.ac.il;hbf.huji.ac.il;hbf.huji.ac.il",
        "email": "cs.huji.ac.il;cs.huji.ac.il;hbf.huji.ac.il;hbf.huji.ac.il",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1+2;1+2",
        "aff_unique_norm": "University Affiliation Not Specified;Interdisciplinary Center for Neural Computation;Dept. of Physiology, Hadassah Medical School",
        "aff_unique_dep": "School of Computer Science and Engineering;Neural Computation;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": ";",
        "aff_country_unique": ""
    },
    {
        "id": "21374cb541",
        "title": "Stability-Based Model Selection",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/37d097caf1299d9aa79c2c2b843d2d78-Abstract.html",
        "author": "Tilman Lange; Mikio L. Braun; Volker Roth; Joachim M. Buhmann",
        "abstract": "Model selection is linked to model assessment, which is the problem of comparing different models, or model parameters, for a speci\ufb01c learning task. For supervised learning, the standard practical technique is cross- validation, which is not applicable for semi-supervised and unsupervised settings. In this paper, a new model assessment scheme is introduced which is based on a notion of stability. The stability measure yields an upper bound to cross-validation in the supervised case, but extends to semi-supervised and unsupervised problems. In the experimental part, the performance of the stability measure is studied for model order se- lection in comparison to standard techniques in this area.",
        "bibtex": "@inproceedings{NIPS2002_37d097ca,\n author = {Lange, Tilman and Braun, Mikio and Roth, Volker and Buhmann, Joachim},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Stability-Based Model Selection},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/37d097caf1299d9aa79c2c2b843d2d78-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/37d097caf1299d9aa79c2c2b843d2d78-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/37d097caf1299d9aa79c2c2b843d2d78-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 126425,
        "gs_citation": 160,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2328648181052394889&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Institute of Computer Science, Dept. III, University of Bonn; Institute of Computer Science, Dept. III, University of Bonn; Institute of Computer Science, Dept. III, University of Bonn; Institute of Computer Science, Dept. III, University of Bonn",
        "aff_domain": "cs.uni-bonn.de;cs.uni-bonn.de;cs.uni-bonn.de;cs.uni-bonn.de",
        "email": "cs.uni-bonn.de;cs.uni-bonn.de;cs.uni-bonn.de;cs.uni-bonn.de",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Institute of Computer Science, Dept. III, University of Bonn",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "5d27b843b2",
        "title": "Stable Fixed Points of Loopy Belief Propagation Are Local Minima of the Bethe Free Energy",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/d2a27e83d429f0dcae6b937cf440aeb1-Abstract.html",
        "author": "Tom Heskes",
        "abstract": "We extend recent work on the connection between loopy belief propagation and the Bethe free energy. Constrained minimization of the Bethe free energy can be turned into an unconstrained saddle-point problem. Both converging double-loop algorithms and standard loopy belief propagation can be inter- preted as attempts to solve this saddle-point problem. Stability analysis then leads us to conclude that stable (cid:12)xed points of loopy belief propagation must be (local) minima of the Bethe free energy. Perhaps surprisingly, the converse need not be the case: minima can be unstable (cid:12)xed points. We illustrate this with an example and discuss implications.",
        "bibtex": "@inproceedings{NIPS2002_d2a27e83,\n author = {Heskes, Tom},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Stable Fixed Points of Loopy Belief Propagation Are Local Minima of the Bethe Free Energy},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/d2a27e83d429f0dcae6b937cf440aeb1-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/d2a27e83d429f0dcae6b937cf440aeb1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/d2a27e83d429f0dcae6b937cf440aeb1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 117945,
        "gs_citation": 237,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4914692706718189768&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "SNN, University of Nijmegen",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "SNN, University of Nijmegen",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": ""
    },
    {
        "id": "b04192fec8",
        "title": "Stochastic Neighbor Embedding",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/6150ccc6069bea6b5716254057a194ef-Abstract.html",
        "author": "Geoffrey E. Hinton; Sam T. Roweis",
        "abstract": "We describe a probabilistic approach to the task of placing objects, de- scribed by high-dimensional vectors or by pairwise dissimilarities, in a low-dimensional space in a way that preserves neighbor identities. A Gaussian is centered on each object in the high-dimensional space and the densities under this Gaussian (or the given dissimilarities) are used to de\ufb01ne a probability distribution over all the potential neighbors of the object. The aim of the embedding is to approximate this distribu- tion as well as possible when the same operation is performed on the low-dimensional \u201cimages\u201d of the objects. A natural cost function is a sum of Kullback-Leibler divergences, one per object, which leads to a simple gradient for adjusting the positions of the low-dimensional im- ages. Unlike other dimensionality reduction methods, this probabilistic framework makes it easy to represent each object by a mixture of widely separated low-dimensional images. This allows ambiguous objects, like the document count vector for the word \u201cbank\u201d, to have versions close to the images of both \u201criver\u201d and \u201c\ufb01nance\u201d without forcing the images of outdoor concepts to be located close to those of corporate concepts.",
        "bibtex": "@inproceedings{NIPS2002_6150ccc6,\n author = {Hinton, Geoffrey E and Roweis, Sam},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Stochastic Neighbor Embedding},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/6150ccc6069bea6b5716254057a194ef-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/6150ccc6069bea6b5716254057a194ef-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/6150ccc6069bea6b5716254057a194ef-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 258595,
        "gs_citation": 2468,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12963151088939470648&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 21,
        "aff": "Department of Computer Science, University of Toronto; Department of Computer Science, University of Toronto",
        "aff_domain": "cs.toronto.edu;cs.toronto.edu",
        "email": "cs.toronto.edu;cs.toronto.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Toronto",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utoronto.ca",
        "aff_unique_abbr": "U of T",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Toronto",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "ff483e7826",
        "title": "String Kernels, Fisher Kernels and Finite State Automata",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/03255088ed63354a54e0e5ed957e9008-Abstract.html",
        "author": "Craig Saunders; Alexei Vinokourov; John S. Shawe-taylor",
        "abstract": "In  this  paper  we  show  how  the  generation  of documents  can  be  thought of as a  k-stage Markov process,  which leads to a  Fisher ker(cid:173) nel from which the n-gram and string kernels can be re-constructed.  The Fisher  kernel  view  gives  a  more flexible  insight into the string  kernel  and  suggests  how  it  can  be  parametrised  in  a  way  that  re(cid:173) flects  the  statistics  of the training corpus.  Furthermore,  the  prob(cid:173) abilistic  modelling  approach  suggests  extending  the  Markov  pro(cid:173) cess  to  consider  sub-sequences  of varying  length,  rather  than  the  standard fixed-length  approach  used  in  the  string kernel.  We give  a  procedure  for  determining  which  sub-sequences  are  informative  features  and  hence  generate  a  Finite State  Machine  model,  which  can  again  be  used  to  obtain  a  Fisher  kernel.  By  adjusting  the  parametrisation we  can also influence the weighting received by the  features .  In this way we  are able to obtain a  logarithmic weighting  in  a  Fisher  kernel.  Finally,  experiments  are  reported  comparing  the  different  kernels  using  the  standard  Bag  of Words  kernel  as  a  baseline.",
        "bibtex": "@inproceedings{NIPS2002_03255088,\n author = {Saunders, Craig and Vinokourov, Alexei and Shawe-taylor, John},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {String Kernels, Fisher Kernels and Finite State Automata},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/03255088ed63354a54e0e5ed957e9008-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/03255088ed63354a54e0e5ed957e9008-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/03255088ed63354a54e0e5ed957e9008-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1418167,
        "gs_citation": 59,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2984160902218624222&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science, Royal Holloway, University of London; Department of Computer Science, Royal Holloway, University of London; Department of Computer Science, Royal Holloway, University of London",
        "aff_domain": "lcs.rhul.ac.uk;lcs.rhul.ac.uk;lcs.rhul.ac.uk",
        "email": "lcs.rhul.ac.uk;lcs.rhul.ac.uk;lcs.rhul.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Department of Computer Science, Royal Holloway, University of London",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "cb634082d5",
        "title": "Support Vector Machines for Multiple-Instance Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/3e6260b81898beacda3d16db379ed329-Abstract.html",
        "author": "Stuart Andrews; Ioannis Tsochantaridis; Thomas Hofmann",
        "abstract": "This  paper  presents  two  new  formulations  of  multiple-instance  learning as a  maximum margin problem.  The proposed extensions  of the  Support  Vector  Machine  (SVM)  learning  approach lead  to  mixed integer quadratic programs that can be solved  heuristically.  Our generalization of SVMs  makes  a  state-of-the-art classification  technique,  including  non-linear  classification via kernels,  available  to  an  area that  up  to now  has  been  largely  dominated by  special  purpose methods.  We  present  experimental  results  on  a  pharma(cid:173) ceutical data set and on applications in automated image indexing  and document categorization.",
        "bibtex": "@inproceedings{NIPS2002_3e6260b8,\n author = {Andrews, Stuart and Tsochantaridis, Ioannis and Hofmann, Thomas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Support Vector Machines for Multiple-Instance Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/3e6260b81898beacda3d16db379ed329-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/3e6260b81898beacda3d16db379ed329-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/3e6260b81898beacda3d16db379ed329-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1635523,
        "gs_citation": 2143,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8780222589722983991&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Computer Science, Brown University, Providence, RI 02912; Department of Computer Science, Brown University, Providence, RI 02912; Department of Computer Science, Brown University, Providence, RI 02912",
        "aff_domain": "cs.brown.edu;cs.brown.edu;cs.brown.edu",
        "email": "cs.brown.edu;cs.brown.edu;cs.brown.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Department of Computer Science, Brown University, Providence, RI 02912",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "e403019f82",
        "title": "Temporal Coherence, Natural Image Sequences, and the Visual Cortex",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/00a03ec6533ca7f5c644d198d815329c-Abstract.html",
        "author": "Jarmo Hurri; Aapo Hyv\u00e4rinen",
        "abstract": "We show that two important properties of the primary visual cortex emerge when the principle of temporal coherence is applied to natural image sequences. The properties are simple-cell-like receptive \ufb01elds and complex-cell-like pooling of simple cell outputs, which emerge when we apply two different approaches to temporal coherence. In the \ufb01rst approach we extract receptive \ufb01elds whose outputs are as temporally co- herent as possible. This approach yields simple-cell-like receptive \ufb01elds (oriented, localized, multiscale). Thus, temporal coherence is an alterna- tive to sparse coding in modeling the emergence of simple cell receptive \ufb01elds. The second approach is based on a two-layer statistical generative model of natural image sequences. In addition to modeling the temporal coherence of individual simple cells, this model includes inter-cell tem- poral dependencies. Estimation of this model from natural data yields both simple-cell-like receptive \ufb01elds, and complex-cell-like pooling of simple cell outputs. In this completely unsupervised learning, both lay- ers of the generative model are estimated simultaneously from scratch. This is a signi\ufb01cant improvement on earlier statistical models of early vision, where only one layer has been learned, and others have been \ufb01xed a priori.",
        "bibtex": "@inproceedings{NIPS2002_00a03ec6,\n author = {Hurri, Jarmo and Hyv\\\"{a}rinen, Aapo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Temporal Coherence, Natural Image Sequences, and the Visual Cortex},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/00a03ec6533ca7f5c644d198d815329c-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/00a03ec6533ca7f5c644d198d815329c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/00a03ec6533ca7f5c644d198d815329c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 156400,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15652497493959769142&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Neural Networks Research Centre, Helsinki University of Technology; Neural Networks Research Centre, Helsinki University of Technology",
        "aff_domain": "hut.fi;hut.fi",
        "email": "hut.fi;hut.fi",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Neural Networks Research Centre, Helsinki University of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "8df7dea7ed",
        "title": "The Decision List Machine",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/b0b79da57b95837f14be95aaa4d54cf8-Abstract.html",
        "author": "Marina Sokolova; Mario Marchand; Nathalie Japkowicz; John S. Shawe-taylor",
        "abstract": "We introduce a new learning algorithm for decision lists to allow features that are constructed from the data and to allow a trade- o\ufb01 between accuracy and complexity. We bound its generalization error in terms of the number of errors and the size of the classi\ufb02er it \ufb02nds on the training data. We also compare its performance on some natural data sets with the set covering machine and the support vector machine.",
        "bibtex": "@inproceedings{NIPS2002_b0b79da5,\n author = {Sokolova, Marina and Marchand, Mario and Japkowicz, Nathalie and Shawe-taylor, John},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {The Decision List Machine},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/b0b79da57b95837f14be95aaa4d54cf8-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/b0b79da57b95837f14be95aaa4d54cf8-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/b0b79da57b95837f14be95aaa4d54cf8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 98454,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "SITE,UniversityofOttawa; SITE,UniversityofOttawa; SITE,UniversityofOttawa; RoyalHolloway,UniversityofLondon",
        "aff_domain": "site.uottawa.ca;site.uottawa.ca;site.uottawa.ca;cs.rhul.ac.uk",
        "email": "site.uottawa.ca;site.uottawa.ca;site.uottawa.ca;cs.rhul.ac.uk",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "SITE,UniversityofOttawa;RoyalHolloway,UniversityofLondon",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "9c81558054",
        "title": "The Effect of Singularities in a Learning Machine when the True Parameters Do Not Lie on such Singularities",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/c2ba1bc54b239208cb37b901c0d3b363-Abstract.html",
        "author": "Sumio Watanabe; Shun-ichi Amari",
        "abstract": "A lot of learning machines with hidden variables used in infor- mation science have singularities in their parameter spaces. At singularities, the Fisher information matrix becomes degenerate, resulting that the learning theory of regular statistical models does not hold. Recently, it was proven that, if the true parameter is contained in singularities, then the coe(cid:14)cient of the Bayes gen- eralization error is equal to the pole of the zeta function of the Kullback information. In this paper, under the condition that the true parameter is almost but not contained in singularities, we show two results. (1) If the dimension of the parameter from in- puts to hidden units is not larger than three, then there exits a region of true parameters where the generalization error is larger than those of regular models, however, if otherwise, then for any true parameter, the generalization error is smaller than those of regular models. (2) The symmetry of the generalization error and the training error does not hold in singular models in general.",
        "bibtex": "@inproceedings{NIPS2002_c2ba1bc5,\n author = {Watanabe, Sumio and Amari, Shun-ichi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {The Effect of Singularities in a Learning Machine when the True Parameters Do Not Lie on such Singularities},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/c2ba1bc54b239208cb37b901c0d3b363-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/c2ba1bc54b239208cb37b901c0d3b363-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/c2ba1bc54b239208cb37b901c0d3b363-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 84471,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5784875206431698990&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Precision and Intelligence Laboratory, Tokyo Institute of Technology; Laboratory for Mathematical Neuroscience, RIKEN Brain Science Institute",
        "aff_domain": "pi.titech.ac.jp;brain.riken.go.jp",
        "email": "pi.titech.ac.jp;brain.riken.go.jp",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Precision and Intelligence Laboratory, Tokyo Institute of Technology;Laboratory for Mathematical Neuroscience, RIKEN Brain Science Institute",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "56ff1dcb7d",
        "title": "The RA Scanner: Prediction of Rheumatoid Joint Inflammation Based on Laser Imaging",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/211c1e0b83b9c69fa9c4bdede203c1e3-Abstract.html",
        "author": "Anton Schwaighofer; Volker Tresp; Peter Mayer; Alexander K. Scheel; Gerhard A. M\u00fcller",
        "abstract": "We describe the RA scanner, a novel system for the examination of pa- tients suffering from rheumatoid arthritis. The RA scanner is based on a novel laser-based imaging technique which is sensitive to the optical characteristics of \ufb01nger joint tissue. Based on the laser images, \ufb01nger joints are classi\ufb01ed according to whether the in\ufb02ammatory status has improved or worsened. To perform the classi\ufb01cation task, various lin- ear and kernel-based systems were implemented and their performances were compared. Special emphasis was put on measures to reliably per- form parameter tuning and evaluation, since only a very small data set was available. Based on the results presented in this paper, it was con- cluded that the RA scanner permits a reliable classi\ufb01cation of patholog- ical \ufb01nger joints, thus paving the way for a further development from prototype to product stage.",
        "bibtex": "@inproceedings{NIPS2002_211c1e0b,\n author = {Schwaighofer, Anton and Tresp, Volker and Mayer, Peter and Scheel, Alexander and M\\\"{u}ller, Gerhard},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {The RA Scanner: Prediction of Rheumatoid Joint Inflammation Based on Laser Imaging},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/211c1e0b83b9c69fa9c4bdede203c1e3-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/211c1e0b83b9c69fa9c4bdede203c1e3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/211c1e0b83b9c69fa9c4bdede203c1e3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 73371,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=233557860544759279&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "TU Graz, Institute for Theoretical Computer Science; Siemens Corporate Technology, Department of Neural Computation; Siemens Corporate Technology, Department of Neural Computation; University G\u00a8ottingen, Department of Medicine, Nephrology and Rheumatology; University G\u00a8ottingen, Department of Medicine, Nephrology and Rheumatology",
        "aff_domain": "igi.tugraz.at;tresp.org;mchp.siemens.de;gwdg.de;med.uni-goettingen.de",
        "email": "igi.tugraz.at;tresp.org;mchp.siemens.de;gwdg.de;med.uni-goettingen.de",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;2;2",
        "aff_unique_norm": "TU Graz, Institute for Theoretical Computer Science;Siemens Corporate Technology, Department of Neural Computation;University G\u00a8ottingen, Department of Medicine, Nephrology and Rheumatology",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "260285d37d",
        "title": "The Stability of Kernel Principal Components Analysis and its Relation to the Process Eigenspectrum",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/f516dfb84b9051ed85b89cdc3a8ab7f5-Abstract.html",
        "author": "Christopher Williams; John S. Shawe-taylor",
        "abstract": "In  this paper we  analyze the relationships between the eigenvalues  of the m  x m  Gram matrix K  for  a kernel k(\u00b7, .)  corresponding to a  sample  Xl, ... ,Xm  drawn from  a  density p(x)  and the eigenvalues  of the corresponding continuous eigenproblem.  We  bound the dif(cid:173) ferences between the two spectra and provide a performance bound  on kernel peA.",
        "bibtex": "@inproceedings{NIPS2002_f516dfb8,\n author = {Williams, Christopher and Shawe-taylor, John},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {The Stability of Kernel Principal Components Analysis and its Relation to the Process Eigenspectrum},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/f516dfb84b9051ed85b89cdc3a8ab7f5-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/f516dfb84b9051ed85b89cdc3a8ab7f5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/f516dfb84b9051ed85b89cdc3a8ab7f5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1438541,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10693241091484582193&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Royal Holloway University of London; School of Informatics University of Edinburgh",
        "aff_domain": "john\u00a9cs.rhul.ac.uk; c.k.i.williams\u00a9ed.ac.uk",
        "email": "john\u00a9cs.rhul.ac.uk; c.k.i.williams\u00a9ed.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Royal Holloway, University of London;School of Informatics University of Edinburgh",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.royalholloway.ac.uk;",
        "aff_unique_abbr": "RHUL;",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Egham;",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United Kingdom;"
    },
    {
        "id": "6fd38aed7c",
        "title": "Theory-Based Causal Inference",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/e77dbaf6759253c7c6d0efc5690369c7-Abstract.html",
        "author": "Joshua B. Tenenbaum; Thomas L. Griffiths",
        "abstract": "People routinely make sophisticated causal inferences unconsciously, ef- fortlessly, and from very little data \u2013 often from just one or a few ob- servations. We argue that these inferences can be explained as Bayesian computations over a hypothesis space of causal graphical models, shaped by strong top-down prior knowledge in the form of intuitive theories. We present two case studies of our approach, including quantitative mod- els of human causal judgments and brief comparisons with traditional bottom-up models of inference.",
        "bibtex": "@inproceedings{NIPS2002_e77dbaf6,\n author = {Tenenbaum, Joshua and Griffiths, Thomas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Theory-Based Causal Inference},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/e77dbaf6759253c7c6d0efc5690369c7-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/e77dbaf6759253c7c6d0efc5690369c7-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/e77dbaf6759253c7c6d0efc5690369c7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 90394,
        "gs_citation": 167,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15058879918849980257&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Department of Brain and Cognitive Sciences, MIT, Cambridge, MA 02139; Department of Brain and Cognitive Sciences, MIT, Cambridge, MA 02139",
        "aff_domain": "mit.edu;mit.edu",
        "email": "mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Department of Brain and Cognitive Sciences, MIT, Cambridge, MA 02139",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "f3964d3440",
        "title": "Timing and Partial Observability in the Dopamine System",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/13111c20aee51aeb480ecbd988cd8cc9-Abstract.html",
        "author": "Nathaniel D. Daw; Aaron C. Courville; David S. Touretzky",
        "abstract": "According to a series of in\ufb02uential models, dopamine (DA) neurons sig- nal reward prediction error using a temporal-difference (TD) algorithm. We address a problem not convincingly solved in these accounts: how to maintain a representation of cues that predict delayed consequences. Our new model uses a TD rule grounded in partially observable semi-Markov processes, a formalism that captures two largely neglected features of DA experiments: hidden state and temporal variability. Previous models pre- dicted rewards using a tapped delay line representation of sensory inputs; we replace this with a more active process of inference about the under- lying state of the world. The DA system can then learn to map these inferred states to reward predictions using TD. The new model can ex- plain previously vexing data on the responses of DA neurons in the face of temporal variability. By combining statistical model-based learning with a physiologically grounded TD theory, it also brings into contact with physiology some insights about behavior that had previously been con\ufb01ned to more abstract psychological models.",
        "bibtex": "@inproceedings{NIPS2002_13111c20,\n author = {Daw, Nathaniel and Courville, Aaron C and Touretzky, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Timing and Partial Observability in the Dopamine System},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/13111c20aee51aeb480ecbd988cd8cc9-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/13111c20aee51aeb480ecbd988cd8cc9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/13111c20aee51aeb480ecbd988cd8cc9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 75717,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11369180292640384204&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "Computer Science Department + Center for the Neural Basis of Cognition; Robotics Institute + Center for the Neural Basis of Cognition; Computer Science Department + Center for the Neural Basis of Cognition",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2+1;0+1",
        "aff_unique_norm": "Computer Science Department;Center for the Neural Basis of Cognition;Robotics Institute",
        "aff_unique_dep": "Computer Science;;",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": ";;",
        "aff_country_unique": ""
    },
    {
        "id": "0b64c36e23",
        "title": "Topographic Map Formation by Silicon Growth Cones",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/3323fe11e9595c09af38fe67567a9394-Abstract.html",
        "author": "Brian Taba; Kwabena A. Boahen",
        "abstract": "We  describe  a  self-configuring  neuromorphic  chip  that  uses  a  model of activity-dependent axon remodeling to  automatically wire  topographic  maps  based  solely  on  input  correlations.  Axons  are  guided by growth cones, which are modeled in analog VLSI for the  first  time.  Growth  cones  migrate  up  neurotropin  gradients,  which  are  represented  by  charge  diffusing  in  transistor  channels.  Virtual  axons  move  by  rerouting  address-events.  We  refined  an  initially  gross topographic projection by simulating retinal wave input.",
        "bibtex": "@inproceedings{NIPS2002_3323fe11,\n author = {Taba, Brian and Boahen, Kwabena A},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Topographic Map Formation by Silicon Growth Cones},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/3323fe11e9595c09af38fe67567a9394-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/3323fe11e9595c09af38fe67567a9394-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/3323fe11e9595c09af38fe67567a9394-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1545115,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16726902806245928065&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Bioengineering, University of Pennsylvania; Department of Bioengineering, University of Pennsylvania",
        "aff_domain": "neuroengineering.upenn.edu;neuroengineering.upenn.edu",
        "email": "neuroengineering.upenn.edu;neuroengineering.upenn.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Department of Bioengineering, University of Pennsylvania",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "b299cb9fe7",
        "title": "Transductive and Inductive Methods for Approximate Gaussian Process Regression",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/329e6581efbc90bd92a1f22c4ba2103d-Abstract.html",
        "author": "Anton Schwaighofer; Volker Tresp",
        "abstract": "Gaussian process regression allows a simple analytical treatment of ex- act Bayesian inference and has been found to provide good performance, yet scales badly with the number of training data. In this paper we com- pare several approaches towards scaling Gaussian processes regression to large data sets: the subset of representers method, the reduced rank approximation, online Gaussian processes, and the Bayesian commit- tee machine. Furthermore we provide theoretical insight into some of our experimental results. We found that subset of representers methods can give good and particularly fast predictions for data sets with high and medium noise levels. On complex low noise data sets, the Bayesian committee machine achieves signi\ufb01cantly better accuracy, yet at a higher computational cost.",
        "bibtex": "@inproceedings{NIPS2002_329e6581,\n author = {Schwaighofer, Anton and Tresp, Volker},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Transductive and Inductive Methods for Approximate Gaussian Process Regression},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/329e6581efbc90bd92a1f22c4ba2103d-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/329e6581efbc90bd92a1f22c4ba2103d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/329e6581efbc90bd92a1f22c4ba2103d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 70301,
        "gs_citation": 124,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14728016295756976246&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "TU Graz, Institute for Theoretical Computer Science; Siemens Corporate Technology CT IC4",
        "aff_domain": "; ",
        "email": "; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "TU Graz, Institute for Theoretical Computer Science;Siemens Corporate Technology CT IC4",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "2c6b9cb27a",
        "title": "Unsupervised Color Constancy",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/149815eb972b3c370dee3b89d645ae14-Abstract.html",
        "author": "Kinh Tieu; Erik G. Miller",
        "abstract": "In [1] we introduced a linear statistical model of joint color changes in images due to variation in lighting and certain non-geometric camera pa- rameters. We did this by measuring the mappings of colors in one image of a scene to colors in another image of the same scene under different lighting conditions. Here we increase the \ufb02exibility of this color \ufb02ow model by allowing \ufb02ow coef\ufb01cients to vary according to a low order polynomial over the image. This allows us to better \ufb01t smoothly vary- ing lighting conditions as well as curved surfaces without endowing our model with too much capacity. We show results on image matching and shadow removal and detection.",
        "bibtex": "@inproceedings{NIPS2002_149815eb,\n author = {Tieu, Kinh and Miller, Erik},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Unsupervised Color Constancy},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/149815eb972b3c370dee3b89d645ae14-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/149815eb972b3c370dee3b89d645ae14-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/149815eb972b3c370dee3b89d645ae14-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 190367,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2486375087739896435&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Artificial Intelligence Laboratory, Massachusetts Institute of Technology; Computer Science Division, UC Berkeley",
        "aff_domain": "ai.mit.edu;cs.berkeley.edu",
        "email": "ai.mit.edu;cs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Artificial Intelligence Laboratory, Massachusetts Institute of Technology;University of California, Berkeley",
        "aff_unique_dep": ";Computer Science Division",
        "aff_unique_url": ";https://www.berkeley.edu",
        "aff_unique_abbr": ";UC Berkeley",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "74089685bf",
        "title": "Using Manifold Stucture for Partially Labeled Classification",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/f976b57bb9dd27aa2e7e7df2825893a6-Abstract.html",
        "author": "Mikhail Belkin; Partha Niyogi",
        "abstract": "We consider the general problem of utilizing both labeled and un(cid:173) labeled data to improve classification accuracy. Under t he assump(cid:173) tion that the data lie on a submanifold in a high dimensional space,  we develop an algorithmic framework to classify a partially labeled  data set in a principled manner . The central idea of our approach is  that classification functions are naturally defined only on t he sub(cid:173) manifold in question rather than the total ambient space. Using the  Laplace Beltrami operator one produces a basis for a Hilbert space  of square integrable functions on the submanifold. To recover such  a basis , only unlab eled examples are required. Once a basis is ob(cid:173) tained , training can be performed using the labeled data set. Our  algorithm models the manifold using the adjacency graph for the  data and approximates the Laplace Beltrami operator by the graph  Laplacian. Practical applications to image and text classification  are considered.",
        "bibtex": "@inproceedings{NIPS2002_f976b57b,\n author = {Belkin, Mikhail and Niyogi, Partha},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Using Manifold Stucture for Partially Labeled Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/f976b57bb9dd27aa2e7e7df2825893a6-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/f976b57bb9dd27aa2e7e7df2825893a6-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/f976b57bb9dd27aa2e7e7df2825893a6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1336770,
        "gs_citation": 382,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4434629518924585641&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "University of Chicago Department of Mathematics; University of Chicago Depts of Computer Science and Statistics",
        "aff_domain": "math.uchicago.edu;cs.uchicago.edu",
        "email": "math.uchicago.edu;cs.uchicago.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Chicago Department of Mathematics;University of Chicago Depts of Computer Science and Statistics",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "3ab7673f98",
        "title": "Using Tarjan's Red Rule for Fast Dependency Tree Construction",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/0f46c64b74a6c964c674853a89796c8e-Abstract.html",
        "author": "Dan Pelleg; Andrew W. Moore",
        "abstract": "We focus on the problem of ef\ufb01cient learning of dependency trees. It is well-known that given the pairwise mutual information coef\ufb01cients, a minimum-weight spanning tree algorithm solves this problem exactly and in polynomial time. However, for large data-sets it is the construc- tion of the correlation matrix that dominates the running time. We have developed a new spanning-tree algorithm which is capable of exploiting partial knowledge about edge weights. The partial knowledge we main- tain is a probabilistic con\ufb01dence interval on the coef\ufb01cients, which we derive by examining just a small sample of the data. The algorithm is able to \ufb02ag the need to shrink an interval, which translates to inspec- tion of more data for the particular attribute pair. Experimental results show running time that is near-constant in the number of records, with- out signi\ufb01cant loss in accuracy of the generated trees. Interestingly, our spanning-tree algorithm is based solely on Tarjan\u2019s red-edge rule, which is generally considered a guaranteed recipe for bad performance.",
        "bibtex": "@inproceedings{NIPS2002_0f46c64b,\n author = {Pelleg, Dan and Moore, Andrew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Using Tarjan\\textquotesingle s Red Rule for Fast Dependency Tree Construction},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/0f46c64b74a6c964c674853a89796c8e-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/0f46c64b74a6c964c674853a89796c8e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/0f46c64b74a6c964c674853a89796c8e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 74579,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17949443697221068160&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "School of Computer Science, Carnegie-Mellon University; School of Computer Science, Carnegie-Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "School of Computer Science, Carnegie-Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "1a7c09451d",
        "title": "VIBES: A Variational Inference Engine for Bayesian Networks",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/9978b7063e297d84bb2ac8e46c1c845f-Abstract.html",
        "author": "Christopher M. Bishop; David Spiegelhalter; John Winn",
        "abstract": "In recent years variational methods have become a popular tool for approximate inference and learning in a wide variety of proba- bilistic models. For each new application, however, it is currently necessary (cid:12)rst to derive the variational update equations, and then to implement them in application-speci(cid:12)c code. Each of these steps is both time consuming and error prone. In this paper we describe a general purpose inference engine called VIBES (\u2018Variational Infer- ence for Bayesian Networks\u2019) which allows a wide variety of proba- bilistic models to be implemented and solved variationally without recourse to coding. New models are speci(cid:12)ed either through a simple script or via a graphical interface analogous to a drawing package. VIBES then automatically generates and solves the vari- ational equations. We illustrate the power and (cid:13)exibility of VIBES using examples from Bayesian mixture modelling.",
        "bibtex": "@inproceedings{NIPS2002_9978b706,\n author = {Bishop, Christopher and Spiegelhalter, David and Winn, John},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {VIBES: A Variational Inference Engine for Bayesian Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/9978b7063e297d84bb2ac8e46c1c845f-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/9978b7063e297d84bb2ac8e46c1c845f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/9978b7063e297d84bb2ac8e46c1c845f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 188670,
        "gs_citation": 128,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16039001860577238908&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Microsoft Research, Cambridge, CB3 0FB, U.K.; MRC Biostatistics Unit, Cambridge, U.K.; Department of Physics, University of Cambridge, U.K.",
        "aff_domain": "research.microsoft.com;mrc-bsu.cam.ac.uk;www.inference.phy.cam.ac.uk",
        "email": "research.microsoft.com;mrc-bsu.cam.ac.uk;www.inference.phy.cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Microsoft Research, Cambridge, CB3 0FB, U.K.;MRC Biostatistics Unit, Cambridge, U.K.;University of Cambridge",
        "aff_unique_dep": ";;Department of Physics",
        "aff_unique_url": ";;https://www.cam.ac.uk",
        "aff_unique_abbr": ";;Cambridge",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United Kingdom"
    },
    {
        "id": "dc2fa20b5b",
        "title": "Value-Directed Compression of POMDPs",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/14ea0d5b0cf49525d1866cb1e95ada5d-Abstract.html",
        "author": "Pascal Poupart; Craig Boutilier",
        "abstract": "We examine the problem of generating state-space compressions of POMDPs in a way that minimally impacts decision quality. We analyze the impact of compres- sions on decision quality, observing that compressions that allow accurate policy evaluation (prediction of expected future reward) will not affect decision qual- ity. We derive a set of suf\ufb01cient conditions that ensure accurate prediction in this respect, illustrate interesting mathematical properties these confer on lossless lin- ear compressions, and use these to derive an iterative procedure for \ufb01nding good linear lossy compressions. We also elaborate on how structured representations of a POMDP can be used to \ufb01nd such compressions.",
        "bibtex": "@inproceedings{NIPS2002_14ea0d5b,\n author = {Poupart, Pascal and Boutilier, Craig},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Value-Directed Compression of POMDPs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/14ea0d5b0cf49525d1866cb1e95ada5d-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/14ea0d5b0cf49525d1866cb1e95ada5d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/14ea0d5b0cf49525d1866cb1e95ada5d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 91962,
        "gs_citation": 174,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7286956870596218977&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": "Department of Computer Science, University of Toronto; Department of Computer Science, University of Toronto",
        "aff_domain": "cs.toronto.edu;cs.toronto.edu",
        "email": "cs.toronto.edu;cs.toronto.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Toronto",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utoronto.ca",
        "aff_unique_abbr": "U of T",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Toronto",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "7a673e6df7",
        "title": "Visual Development Aids the Acquisition of Motion Velocity Sensitivities",
        "site": "https://papers.nips.cc/paper_files/paper/2002/hash/443dec3062d0286986e21dc0631734c9-Abstract.html",
        "author": "Robert A. Jacobs; Melissa Dominguez",
        "abstract": "We consider the hypothesis that systems learning aspects of visual per- ception may bene\ufb01t from the use of suitably designed developmental pro- gressions during training. Four models were trained to estimate motion velocities in sequences of visual images. Three of the models were \u201cde- velopmental models\u201d in the sense that the nature of their input changed during the course of training. They received a relatively impoverished visual input early in training, and the quality of this input improved as training progressed. One model used a coarse-to-multiscale develop- mental progression (i.e. it received coarse-scale motion features early in training and \ufb01ner-scale features were added to its input as training progressed), another model used a \ufb01ne-to-multiscale progression, and the third model used a random progression. The \ufb01nal model was non- developmental in the sense that the nature of its input remained the same throughout the training period. The simulation results show that the coarse-to-multiscale model performed best. Hypotheses are offered to account for this model\u2019s superior performance. We conclude that suit- ably designed developmental sequences can be useful to systems learn- ing to estimate motion velocities. The idea that visual development can aid visual learning is a viable hypothesis in need of further study.",
        "bibtex": "@inproceedings{NIPS2002_443dec30,\n author = {Jacobs, Robert and Dominguez, Melissa},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Becker and S. Thrun and K. Obermayer},\n pages = {},\n publisher = {MIT Press},\n title = {Visual Development Aids the Acquisition of Motion Velocity Sensitivities},\n url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/443dec3062d0286986e21dc0631734c9-Paper.pdf},\n volume = {15},\n year = {2002}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2002/file/443dec3062d0286986e21dc0631734c9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2002/file/443dec3062d0286986e21dc0631734c9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 55291,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9724649308753195861&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Brain and Cognitive Sciences, University of Rochester; Department of Computer Science, University of Rochester",
        "aff_domain": "bcs.rochester.edu;cs.rochester.edu",
        "email": "bcs.rochester.edu;cs.rochester.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Department of Brain and Cognitive Sciences, University of Rochester;University of Rochester",
        "aff_unique_dep": ";Department of Computer Science",
        "aff_unique_url": ";https://www.rochester.edu",
        "aff_unique_abbr": ";U of R",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United States"
    }
]